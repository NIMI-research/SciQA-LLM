Questions	SPARQL in Gold Standard	SPARQL generated	exact match
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Story Cloze Test dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	--------------------------- name ?model ?value ?metric_lbl --------------------------- orkgc:Cloze Test ?story_cloze (2,000) 97.2% 97.3% orkgc:NxtLambda (3,000) 98.2% 98.3% orkgc:Clog Test ?story_cloze (5,000) 98.2% 98.3% So, the model did indeed meet it's true goal: Clog. In my opinion, this is a good example of learning from your failures! In the end, I found it better to not get too fixated on your tests because the real world is not always an easy thing to test. So what to make of this ? The model does in fact do quite impressively well on the benchmark. But my question is: does it meet its true goals ? Well, I do not know (yet!). At the time, I was not able to make any real connection to the database or perform any validation on the data. This is likely because I did not have any way to actually connect to any data or perform any validation on my data (I am lucky because I live next to a data provider that supports this). So, this is not really something that could be explained for you in the same way that I explain that the model does indeed do quite impressively well with Story Cloze: This should of course be taken in context of how the Story Cloze results show: The model has an outstanding 95% accuracy on the test set. There are two issues I should mention; first, I am not familiar with the current SQL features and second, I did not perform any validation to the data. Next Post: The Next Time You Take A Look In Data...	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the One Billion Word dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT AUTHOR ?name. ?publication ?summary ORDER BY ?author ?p_rank, ?p_criterion, ?p_description, ?p_link, ?p_doi ?summary ORDER BY ?pub_date DESC LIMIT 5; rdfs:title ?pub_date. The output of the first query is shown below: output (Sparql query): input (English text): List the title and ID of scientific papers in which a benchmark method was described ? output (Sparql query): input (English text): List the title and ID of scientific papers in which a benchmark method was described ? output (Sparql query): output (Sparql query): input (English text): List the title and ID of scientific papers in which the most recent benchmark article appeared ? output (Sparql query): SELECT title ?id, publication_date FROM [wpdataset_2 - 1][wpdataset_2 - 1]. ?pub_date ?pub_date. ?id, publication_date, count(*) FROM [wpdataset_1 - 1][wpdataset_1 - 1]. ?published ?published ?published_id WHERE [pub_date] LIKE '%' AND [pub_date] LIKE '%'. output (Sparql query): input (English text): List the title and ID of scientific papers in which the most recent benchmark article appeared ? output (Sparql query): The following query uses the word level output from previous queries, but shows the corresponding word level output for the word level data. This query uses a combination of the word level data from the previous queries, and the word level data from the WP Dataset 2 output. The query output is shown below: input (English text): list the title and ID of research papers that included the benchmark over the One Billion Word dataset ? output (Sparql query): ORDER BY ?title { ?dataset :DISTINCT ?publication ?summary. :DATE ?pub_date. ?key: ?key ?summary. ?author :DATE ?author. ?name :DATE ?name. :author_id: ?author_id. } output (Sparql query):	False
"
 input (English text): What are the metrics of evaluation over the UrbanSound8k dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the UrbanSound8k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?metric_lbl AS ?metric_lbl FROM {UrbanSound8k lbl} 	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT title ?title WHERE ? title. ?paper ?paper_lbl:HAS_DATASET orkgp:HAS_DATASET ?paper. ?cont orkgp:HAS_BENCHMARK orkgp:P31 ?cont Citation Please cite the following: P. Hines, et al. 2017. Benchmarked data for the SEDC 2018: International Tree-Ring Data Set. http://www.lsu.edu/data/refsq17.zip Diversity This project, and its primary sponsors, strives to reduce and promote diversity among our faculty and community participants. We are especially interested in the representation of underrepresented communities in our research activities and in our training programs (see our Diversity Statement) and in student success in our colleges, universities, and higher education (see our Student Success page (http://sla.edu/about/student-success/)), so we would appreciate your informed support in finding students of all backgrounds and interests that are interested in working at SEDC and participating in our program.	False
"
 input (English text): What is the top benchmark score and its metric on the SearchQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the MAZEA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MAZEA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?name, ?mdate ?date FROM [name] LIMIT 1; One way to write this search query is: input (text): a.name | b.name || c.name || d.name || e.name ; output (Sparql query): SELECT name, ?name, ?mdate ( ?date ) from name where ?name = ?a.name || ?b.name || ?c.name || ?d.name || ?e.name and ?a.name in ?a.date and ?b.name in ?b.date ; A second way to write a simple query is: input (text): a.name | b.name || c.name || d.name || e.name ; output (Sparql query): SELECT ?name, ?mdate ( ?date ) FROM name where ?name = ?a.name || ?b.name || ?c.name || ?d.name || ?e.name and ( ?a.name in ?a.date and ?b.name in ?b.date ) and ?c.name in ?c.date and ?d.name in ?d.date and ?e.name in ?e.date ; A third way is: input (text): a.name | b.name | c.name || d.name || e.name ; output (Sparql query): SELECT ?name, ?mdate ( ?date ) FROM name WHERE ?name = ?a.name	False
"
 input (English text): Can you list the metrics used to evaluate models on the TDMSci dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDMSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the TDMSci dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDMSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?k-value ?k-value_lbl ?k-labels ?min_value ?max_value or kgp:k-values ?max_labels or kgp:k-values_lbl select ?k-value ?k-value_lbl ?kgp:k-values ?max_labels or kgp:k-values_lbl output (Sparql query): #1 [null.null.null] 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 #0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 #1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 #3.00 0.00 0.00 0.00 0.00 3.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 #10.00 0.00 0.00 0.00 0.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 #100.00 0.00 0.00 0.00 0.00 100.	False
"
 input (English text): What is the average installed capacity of all energy sources considered?
 output (Sparql query): SELECT (AVG( ?installed_cap_value) AS ?average_installed_cap_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43133 ?installed_capacity. FILTER(REGEX( ?energy_sources_labels, ""all sources"")) ?installed_capacity orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?installed_cap_value) } 
 input (English text): What is the mean capacity of a carbon-based fuel?
 output (Sparql query): "	"SELECT (AVG( ?numerical_capacity) AS ?mean) WHERE { ?paper a orkgc:Paper; orkgp:P31 [ orkgp:P15483 ?capacity ]; rdfs:label ?title. BIND( xsd:double( REPLACE( STR( REPLACE( ?capacity, ""([0-9]+),([0-9]+)"", ""$1$2"" ) ), ""([0-9]+).*"", ""$1"" ) ) AS ?numerical_capacity ) FILTER(REGEX(STR( ?title), ""(fuel|CO2)"")) } "	" Scheduling: The function calculates the estimated annual emissions of CO2 in this country under current and future energy prices. Scheduling: The parameter values are as follows: Parameter Value ?energy_price(date) date - date at which the value was calculated ?energy_price(year) year - year at which the value was calculated ?cfl (cost) - (price - cost) to calculate the Annual Efficiency (AED) of a power plant that could consume electricity for 20 years and produce zero carbon energy ?cfl_ex (cost) - (price - cost) to calculate the Annual Efficiency (AED) of a power plant that could consume electricity for 20 years and produce carbon equivalent to a power generation facility with a present cost of $100 USD or less ?cfl_vac (cost) - (price - cost) to calculate the Annual Efficiency (AED) of a power plant that could consume electricity for 20 years and produce no carbon-based energy ?cecd_vac (cost) - (price - cost) to calculate the Annual Efficiency (AED) of a power plant that could produce zero carbon-based energy from electricity and produce zero AED * 0.5 * cost ?cfe_vac (cost) - (price - cost) to calculate the Annual Efficiency (AED) of a power plant that could generate zero AED from electricity and would produce zero carbon-based energy * 0.5 * cost ?data ?data - Input fields are defined on a schema-level, and are set up per event. Input - Field name is : energy_price, or: the data is a cost of carbon dioxide and carbon dioxide equivalents for one year based on the average electricity price in a country of a given year ?data2 (schema) - schema can be either an array (an array of fields) or a string (with field name as a string). The array is initialized to: ""year*1 - cost - carbon"", where 1 is a year (the year 0 or the average price). The string fields, with the field name separated by spaces, are separated by colons in the schema. ?cost (schema) - input field in the schema is: ""cost"" ; this is a number of energy resources at a given price (energy costs). A value of zero indicates that there is no cost; thus: 0. Cost - input field is empty ?data (schema) - input field is: data"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper ( ?papers) orkgp:P32 ?cont andkgp:P33 ?cont; orkgp:HAS_DATASET ?dataset. ?cont ?cont_lbl. } output (Sparql query): SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont ?cont_lbl. } Input from output output (Sparql query) The output dataset can be used for training of an MLDoc parser. However, that requires at least three different input files for ""English-to-Latvian"" and ""English-to-Russian"". To do this with a small script, use the below Python code from my last post to export the CSV of all the papers with the most important keywords and then train an MLDoc parser on those files. If you want to use the same output dataset in order to train an MLDoc parser on these files, it's easiest to use the same script, but make a second script export all the papers that have been used in training the MLDoc parser on different CSV files. If you think something like that is too much to do for one parser, you can of course use a parser on individual files based on a pre-packaging of some other tools. You could probably also implement the full batch-based batch-parser that is used by most MLDoc parsers. How to use with the MLDoc Zero-Shot English-to-Russian dataset I did this experiment to see if in particular case the MLDocParser works better with just a CSV pre-packaging of a set of documents (with multiple keywords) but I"	False
"
 input (English text): Indicate the model that performed best in terms of Error metric on the Kuzushiji-MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT cvs. ?model. ?column ?field_name, ?column ?field_value, ?id, ?id_str, ?id_val as ?values, ?label as ?name, ?name_str as ?name_str, ?label_str as ?label_str, ?value_str and ?field_value_str, ?label_str as ?name, ?label_str as ?name_str from cvs where no ?key-partition=1; Output Result Count Num Column(s) Column(s) Value(s) Label(s) kuzushijemist_datasets 1,039 100.000 NULL NULL 0.0 0.0 1,039kuzushijemist_model 1,039 NULL NULL 0.0 0.0 kuzushijemist_minima 2,024 100.000 NULL NULL 0.0 0.0 kuzushijemist_haves_mems 2,024 NULL NULL 0.0 0.0 kuzushijemist_holdings 6,112 100.000 NULL NULL 0.0 0.0 kuzushijemist_data 2,024 1,024 100.000 NULL NULL 0.0 0.0 This data is not meant to be scientific in nature, its purpose can be understood by the previous question. The question was whether model should be included, not whether model should be included. In order to solve this issue model should be removed and used with new model with different label. We would not change model in a new model without removing old model which can cause a number of problems (see below) - for example, there are some model with ""label not as good as label on kuzushijemist-model"", in which case it is possible to re-test the model without removing it. The solution is simple, remove model in model_lbl of new model while leaving model_lbl of old model. For this we use jest: 1 2 3 4 5 6 7 java / jest. Test ( ""model_lbl"", function ( x, y ){ x : x, y : y })( ""kuzushijemist_datasets"", ""kuzushijemist_model"", ""kuzushijemist_method_lbl"" ). Execution completed. Let"	False
"
 input (English text): What is the name of the top performing model in terms of BLEU score when benchmarked on the WMT2016 Romanian-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 (A) Output from the above sample data: A B C D E F G H I J K L M N O P Q R S T U V W X Y Z # (B) Output from the above sample data: B   C D E F G H I J K L M N O P Q R S T U V W X Y Z # A # B   C   D   The input model was (5) classifier-based and trained on the WMT2016 Romanian-English dataset. The algorithm employed a multi-class classification of multiple parameters in terms of the following 3 components: orkgc, rdfs and eval. For more information about the analysis of the models see the corresponding sections of the manual. Please note that you can access the data, which is collected using the WMT2016 dataset by logging into your Azure API Management Console. The data access service is provided by Azure API Management, which is available by visiting https://apiclient.azurewebsites.net/ and following the wizard:	False
"
 input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT [ ?model ?model_lbl] ?metric ?dataset_lbl (LIMIT 1) FROM ?data WHERE { ?metric_lbl:Metric; rdfs:label ?metric_lbl. ?dataset_lbl rdfs:label ?data. } ORDER BY DESC( ?value) LIMIT 1 # Output of a spreeds result select [ ?model] ?metric ?dataset_lbl FROM ?data rdfs:label_rf. ORDER BY DESC( ?value) LIMIT 1 # Output of a select list select [ ?] # Input from a select query select [ ?] # Output from the spreeds response select [ ?] # The same is done for each data set but without the select result # Output from a spreeds response select [ ?] # Output from the select lists # Select from the data with all columns where both the metric and the value are not present. (from Table 1) Select [ ?metric_lbl] ?dataset_lbl from ?data where metric not in ( ?dataset_lbl rdfs:label ?dataset_lbl. ?dataset-lbl rdfs:label ?dataset_lbl). select [ ?dataset] ?model_lbl from ?data rdfs:label_rf. select [ ?dataset] ?model_lbl rdfs:label ?data. select [ ?dataset] ?model_lbl rdfs:label ?model. select [ ?dataset] ?model_lbl rdfs:label ?model_lbl. select [ ?dataset] ?model_lbl rdfs:label ?model_lbl rdfs:label ?model_lbl. select [ ?dataset] ?model_lbl rdfs:label ?model_lbl. select [ ?dataset] ?model_lbl rdfs:label ?model_lbl_rdfs. select [ ?dataset/dataset/dataset] ?dataset_lbl ?model_	False
"
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the iNaturalist 2019 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" The model used for the benchmark has an evaluation score of 8.77. The model is ranked from top-1 to 8. Here's the output from the benchmarking You can view the output of the benchmark in your R session here, or click this link. If you don't have R running, you can run the benchmark from the R console here. Once we've got a model that scores at least 8 (the upper bound of ""8"", as defined by the benchmark), we can then use the scoring function to determine if the model has an upper bound on the accuracy score. If we look at the results of the benchmark we can see that the model shows an upper bound of 7.77. If we look at the model we can see that the metrics that the model is using include: the metric ""accuracy"" and ""metric ""accuracy"". The model also uses something called ""metric ""statistics"": we can see that the algorithm is actually using the ""Metric"" metric and it isn't using ""Accuracy"" or ""metric "". This means that our model, as predicted by the performance metric scores an upper bound of 7.77. I'm sure that you'll agree that our prediction here was pretty accurate! We can then determine if our model is using metric statistics to predict the accuracy score. If it is, then we can check to see if the score is above or below the upper bound. We can measure the upper bound using the ""Accuracy"" and ""metric ""scoring function and we can get the exact score using the following query, where we filter the model columns for using either ""metric"" statistics or ""confidence"" statistics. If we then look to see the upper bound we get the following (lower bound for the model is 11.77): If we look at the model we can see that that the metrics that the model is using include ""median precision"", ""median deviation"", ""min/max precision"" and ""max/min deviation"". If we apply a weighted least squares to this model we give a prediction of a prediction of 11.77 (which if you're lucky you'll achieve), because the weight for that metric on our model is quite high If we look at the scoring function, we can see that the scoring of our model actually has a lower bound of 11.68 We can also use the metric statistics (statistics of the model and the metric) scoring function to determine if the"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ART/CoreSC dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ART/CoreSC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""arXiv"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" ----------- arXiv:ARITH_RULES { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARITH_RULES"") { ?dataset_lbl ?dataset_lbl ?dataset ?lbl ?dataset_lbl ?lbl. } } The queries above will return all the results that include benchmarks for the two datasets, as this is the only way that the data can be viewed on the web. The following question: What would be the impact of making this information public for the first time, since the original research was not publicly disclosed until much later ? The answers to both questions would be similar: The most significant impact would be to make data disclosure and visualization easier to read. The second impact may also be slightly better, however there was a paper recently by the authors which discussed the impact of an additional dataset on this question. It is important to remember the main thing here is the public availability of the source data. There is also always the possibility that the information can be duplicated somewhere else and the only way to determine this for sure is to do the original research to see if you get the corresponding result."	False
"
 input (English text): What models are being evaluated on the TDM Tagged Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the TDM Tagged Corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" The output of pdb is a table of metrics. Table 1 summarizes the selected metrics for the Tagged Data, and Table 2 lists our selected options to determine the metrics used. Table 1 Select a metric from the first column If not present, then we have to choose the metric automatically by looking for metrics in the selected dataset, and returning the metric for the selected model only. If present, then we will choose "" ?paper"" metric (not ""*"") in "" ?dataset"" field. If absent, we will choose a table from the query string as given by the second column (in this case "" ?dataset""). If an optional, but used, "" ?benchmark"" label was given (as shown by the second column), then we will use the selected metric with the specified "" ?benchmark"" column. If we have given the optional "" ?dataset"" column, then we will just return "" ?dataset"" to get the selected dataset. If we do not pass the "" ?dataset"" and the query string "" ?benchmark"" arguments is used, then we will pass the "" ?dataset"" and the query string, if any, to pdb to use the metric for selected model only. If we have passed the optional "" ?benchmark"" argument, then we will use the benchmark metrics we passed to pdb as provided as option arguments (this would be the selected models and the default benchmark to compare the models with. If the "" ?dataset"" field is nulled out, then we will use the benchmarks provided as options). We also select an option from the third column for each model (in order). In the case we have given "" ?dataset"", then, a table will be returned (instead of a ""*"" table). In the case pdb can not find the "" ?dataset"" column, we will pass NULL in the query string as the table to use as a dataset. This is the default for non-default query strings. The second row shows one option we did not select: We pass NULL (for table option, but not for table option) to pdb as the table to use for the selected data for the selected models. Option 1 (option 1) The first option we selected is "" ?dataset"". If we cannot pass NULL argument for table option 3 in the output table (because the query string did not specify the table to"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NCBI Disease dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT title FROM DESTINY title Where title is an individual name (e.g. 'Rita') or the name of a genus. input (English text): Provide a list of papers along with their titles and IDs, that have run benchmarks on the NCBI Disease dataset ? output (Sparql query): SELECT output FROM DESTINY output WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. ?cont orkgp:HAS_P31 ?benchmark. ?paper orkgp:P31 ; } RDFs: '<' Input: '' Output (Sparql query): RDFs: '>' Input: '' Output (Sparql query): The RDF data in the output will be named in the following format: title text output (Sparql query): SELECT title FROM DESTINY title Where title is an individual name (e.g. 'Rita') or the name of a genus. input (English text): Provide a list of papers along with their titles and IDs that have run benchmarks on the NCBI Disease dataset ? output (Sparql query): SELECT output FROM DESTINY output WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont. ?paper orkgp:P31 ; } RDFs: ' ?'"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Lunar Lander (OpenAI Gym)?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	________ __ 1 -1.180000 (1 row) output (Sparql query): ________ __ 1.180000 (1 row) output (Sparql query): ________ _ _ output (Sparql query): __________ _ _ output (Sparql query): __________ _ _ output (Sparql query): _________ _ _ output (Sparql query): _________ _ _ output (Sparql query): ___________ _ _ output (Sparql query): ___________ _ _ output (Sparql query): ___________ _ _ output (Sparql query): ___________ _ _ output (Sparql query): ___________ 0.6220000 | 1.180000 | 0 | 0 output (Sparql query): __________ _ _ output (Sparql query): ________ __ output (Sparql query): __________ _ _ output (Sparql query): __________ 0.06750000 | 0.6220000 | 0 | 0 output (Sparql query): __________ _________ _ _ output (Sparql query): __________ __________ _ _ output (Sparql query): __________ _________ _ _ output (Sparql query): __________ _________ _ _ output (Sparql query): __________ __________ 0.04450000 | 0.06750000 | 0 | 0 output (Sparql query): ___________ _ _ output (Sparql query): ___________ ___________ _ _ output (Sparql query): _________ _ _	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the FSNS - Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT rdfs:label ?labels. FORMAT ?labels. rdfs:size ?labels. OPTIONAL { ?label nlabels. ?label rdfs:size. } Output (Sparql output): SELECT a:ROW_NUMBER b:ROW_NUMBER c:ROW_NUMBER d:ROW_NUMBER:UNIQUE a. a. 0.0 b. 1.0 c. 0.5 d. 0.0:UNIQUE: rdfs:title ?labels. OPTIONAL { ?label ?label_lbl. ?label rdfs:title. rdfs:title rdfs:title rdfs:title rdfs:title rdfs:title rdfs:title  (2) Test dataset - the two test sets for the three parallel models, and the data set for the two test sets, can be downloaded here  (3)  The SPSS R package ( http://www.springerlink.com/content/3/7/18 ) is used to read the files from the archive, create the datasets, and interpret the generated test data (details below).	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 select d.title, r.title, d.n, d.m from ds_benchmarks where d.n=64  rows=2 select a.name, x.name, y.name, d.score, r.score, ds_id,  rdf_id,  ds_labels  as title,  r.score  as score, d.n as n_k,  bs.rank  as rdf_rank, rdf_labels as rdf_label,  rs,  ks =  rows(2) - rows(2) outcome=rows[i].data. else:   print (a.name, x.name, y.name, outcome ) The output looks like this: output 1 2 3 4 5 6 7 8 9 10 ImageNet 64x64 4 1.0 3 4 5 8 6 8 16 9 38 34 5 0.4 9 15 21 27 25 23 28 25 50 15 32 35 40 28 29 28 21 3 5 0.4 image.score -10.08 ImageNet 64x64 8 6.0 20 23 20 19 27 26 19 43 22 34 35 24 22 22 24 44 31 21 22 21 22 8 0.5 image.n -16.33 ImageNet 64x64 24 14.5 0.5 23 22 24 24 36 28 24 33 30 32 31 28 29 20 4 4 1 0 0 0 0 0 0 1 0 0 0 image.lbl -14.5 ImageNet 64x64 64 14.5 14.5 0.5 36 30 32 30 35 31 35 31 34 30 29 33 22 10.9 11.9 12.9 0 0 3 0 0 data.x -18.75 ImageNet -34.00 ImageNet 64x64 64 28.0 28.7 19.25 28.0 28.7 17.5 20.5 29.5 27.5 18.25 29 22.25 20.5 17.5 15 18 5 8 1 1 0 1 1 2 0 0 0 0 0 0 0 0 0 0 1.4 0 0 0 0 0 0 0.3 0.3 0.3 0.3 0.3 0 0.5 0 0 0 0 data.ys -19.25 ImageNet -34.00 ImageNet 64x64 64 22.	False
"
 input (English text): What is the highest benchmark result achieved on the Classical music, 5 seconds at 12 kHz dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	-------------- score: 1.000 50000 points: 1004.75755025 points: 9899.995025 points: 9782.9647525 points: 9678.6562525 points: 9735.9462525 points: 9358.9337525 points: 9524.8625125 points: 9291.863125 points: 9258.93225125 points: 9246.90825125 points: 9235.82025125 points: 9190.8202525 points: 9182.8192525 points: 9112.8152525 points: 9045.80987525 points: 8966.9132525 points: 8897.9975500 points: 8534.99625625 points: 8763.805875 points: 8537.9625625 points: 8751.968125 points: 8517.939125 points: 8654.039375 points: 8647.0598125 points: 8530.7018125 points: 8561.91225 points: 8557.928125 points: 8545.878125 points: 8495.759375 points: 8499.813625 points: 8398.98375 points: 8324.722125 points: 8297.764375 points: 8276.86375 points: 8270.814125 points: 8240.7594125 points: 8206.836375 points: 8034.637375 points: 8023.91875 points: 7901.81225 points: 7800.838375 points: 7842.69375 points: 7762.3775 points: 7724.918125 points: 7735.831125 points: 7722.837125 points: 7703.9625 points: 7706.3909625 points: 7682.0769375 points: 7724.5019125 points: 7703.7675 points: 7652.9125 points: 7646.0425 points: 7603.952650 points: 7607.86725 points: 7637.839375 points: 7631.91875 points: 7601.928125 points: 7575.976650 points: 7539.082575 points: 7492.87225 points: 7488.857625 points: 747.	False
"
 input (English text): Provide a list of papers that have utilized the PBSMT model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PBSMT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Flair-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code WHERE { ?models ft:Models; andfg:HAS_DATASET orfg:HAS_DATASET ? } Inputs are comma separated and should not include the following:	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL (18 layers) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL (18 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL Base model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code WHERE { ?model a ORkgc:Model { ?dataset orkgp:Class ?dataset, ?model :HAS_DATA ?dataset, ?source_code ?source_code || ?benchmark, ?cont orkgp:HAS_DATASET orkgp:HAS_BENCHMARK || ?model || ?source_code || ?benchmark, ?cont orkgp:HAS_MODEL || ?model, ?model_lbl orkgp:Class ?dataset. HAS_DATASET. HAS_DATA || ?source_code || ?benchmark. } } I have been looking at this problem on MSBuild. In particular, I have seen that the original approach involves a very detailed search for all reference implementations of the Model class. For example, there's a reference to the Standard Model of the Transformer-XL, but unfortunately, this is only one version of this class (and therefore, it doesn't compile with the code you get by calling.make ). Another issue that arises here is that to build a template for a Class object, you still need a reference to its Class interface: public class Model { public void my_construct() { } } I think what the developers of the library have in mind when they introduce this feature is in terms of performance: you have to start all your time searching through all the classes you want to build for template-like instances of your Model object, until you find the Class which you are looking for. And I think that's a fair enough trade-off, since this is obviously more data-consuming, as well as less efficient with respect to speed of search. Another interesting aspect that I've found is that the Transformer-XL seems to do somewhat better at generating a template when it has a reference to an external dataset - i.e. is a class loaded from some external data-structure (i.e. a binary file) rather than a plain old object file, thus yielding a faster run-time. But here is another question: does that mean that a transducer with a direct reference to a database is actually faster ? Again, I have no real insights on this other than a guess, but would be interested to know if you've looked at this issue, or if you can provide any additional insight. Finally, if someone in the community can give	False
"
 input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?model. F1_SCORE AS F1_SCORE, ?dataset a_lbl, ?benchmark orkgp:HAS_DATASET ?model. ORGANIZE ORM_P_HAS_GESTURE ?model,! ?dataset_lbl, ?benchmark orkgp:HAS_EVALUATION ?eval. ORD :! ORG_P_F1_HAS_GESTURE ?dataset. ORGANIZED. OPTIONAL ORD : ORG_P_GESTURE-N-LANGUAGE ?model, ORG_LISTS (LISTS_FULL ||DIMENSION ||PUNCTION||LANGUAGE||NUMERIC_ID) ORD : ORG_PROBE_RANK ORD : ORG_LISTS_N-LANGUAGE ?model, ?lbl-h1 : NULL This is the output of benchmark.py : Rows in rows for test (n=40). ######################## benchmark.py:4: Benchmarking model benchmarks for the benchmarked model model : BUNZUN-SPLIT_R2 model_lbl : BUNZUN-SPLIT_R2_PALETTE_LEN model_class : ( SELECT DISTINCT ?class_lbl.LANG as ?class OR LOWER( ?class_lbl.LANG ) as ?class_language ) model_eval_matrix : ( SELECT COUNT(*) as c, ?LIMIT_RANK_TO_RANK AS LIMIT_RANK FROM ( SELECT LEN( ( SELECT c_id FROM ORGANIZED *model ) ) ) GROUP BY ?LIMIT_RANK_TO_RANK ORDER BY c) model_lbl_pairs : ( SELECT SUM( ?LIMIT_RANK_TO_RANK ) FROM ( SELECT LEN( ?LIMIT_RANK_TO_RANK ) from model ) ; total_lbls : ( SELECT SUM( ?LIMIT_RANK_TO_RANK ) FROM model) total : ( SUM( total_lbls ) ) ; model_class : ( (SELECT ROUND( c_id.language_	False
"
 input (English text): List the code links in papers that use the SAN (single model) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the SAN (single) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAN (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" select rdfs : table ""code_links"",  text, output from orkgp : HAVING  ( ?model a orkgp:HAS_DATASET ?dataset. )  AND  ( ?code= ?model orkgc:Model ?dataset. ) There are a number of ways, including using an RDF file to construct a list of the SAN (single model) benchmark results and then manually entering the link to all the papers, and then the output field will need to be defined in a column or the text field; or it will have to be a table name, with both model and column names, to properly populate... the columns, output, source of the input, and output field. That's pretty straight forward. Now I will add: The most complex piece of code to me is the code for writing the output. I will also add a few more examples of my code, and write a tutorial article/book on RDF/Sql in general, once I feel I have covered everything in this article. Also – you may not need to change anything in the code to include the link to the original paper. I have used this code in my first paper; and, it works. Please let me know how it works for you, if it doesn't, let me know, I'd like to know about it. I'll write a blog post explaining it, and, maybe, add it into the next tutorial. Now, let's create the code for writing the output, since, for most code projects, you don't want to worry about that. I will create the output file and define a column in it. RDF output for the SRI workbook You can see the output file above, which I have provided as a template, and added. Here are the definitions for variables (with default values for all fields, except for text): # Output file title date_year : int name : string The name of the output file name as date_year: string The name of the output file as a date_year. This is the default input, and the output will be based on the date_year field. When this field is not set, the input will be based on the day, month, and year fields. The day (month) field can be set to January 1, 2, 15, 16. The month (month)"	False
"
 input (English text): Can you list the models that have been evaluated on the ACE 2004 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the ACE 2005 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?benchmark ?benchmark_lbl. FILTER (str( ?benchmark_lbl) = ""ACE 2005"") ?benchmark_lbl. [...] output (Sparql query): SELECT ?benchmark ?benchmark_lbl [...] Note: You can include multiple data sources in the same query. 2.8 What is the main purpose of ""Sparse Query Mode"" ? It is a very popular feature of ACE's Query Mode, as it allows for maximum flexibility in the database schema. When ""Sparse Query Mode"" is enabled, ACE looks for only the data needed for the current query and does not try to find all the possible configurations and data types. For example, it will never look for the ""all"" data type for a query, which means that it will not work with ""all"" fields or ""all"" combinations. Additionally, if the ACE engine sees something it does not recognize, it will attempt to reevaluate a subset of those fields. This allows the ACE engine to produce query results that are in line with the data and can be understood by humans and algorithms alike. 2.9 What is the relationship between ""Named Parameter"" and ""Partitioned Data"" ?  Named Parameter is a ""dynamically typed"" concept defined by Oracle. It is a ""parameter type"" that is added to a database table in a manner similar to a column-index (similar to how MySQL assigns a column-index to the values of any column). Named parameter values are not SQL strings, but identifiers assigned to each variable in the table to be referenced. Named parameter values are unique within the database and are not subject to SQL injection problems since they are not SQL strings. Partitioned Data is defined in terms of a partition, an array of parameters. When an ACE engine attempts to use a partitioned data type, it compares the value of the parameter type to the partition's partition identifier. If the value of the partitioned data type is the same as the key-value pair for that key in the partition, the ACE engine considers the partition a successful query. When there are several potential data types with similar values, the engine chooses the closest one first. Example: Imagine you have an ACE database table named ""test.ads"". The table is loaded from an output file ""/home/me/example.ads.txt"". Then, you run a ""CAS"". CAS requests ""test"	False
"
 input (English text): Where can I find code references in papers that have used the MP-EB model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MP-EB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the PNDec model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PNDec"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" REST You can find more information on various REST services here. I recommend you to read REST by Chris Moore as well. CORS This is not an exhaustive list and other sources that are not linked in the article should be consulted. Conclusion The basic principles outlined above have helped me understand how to build REST API APIs for both PHP and Python. The most obvious differences between using REST API framework and standard HTTP stack are defined in the section ""REST vs REST API"". It is important to note that REST API is not the only option available for these types of application. Web services and web frameworks provide alternative frameworks to build REST API. The REST framework is simply a framework that provides a consistent API for such services. It supports APIs from API Gateway via its RESTful API adapter and as well provides a RESTful API framework. What is important to know from the start though, is that not all APIs do the things described in this article. Some APIs are purely web services. A good example would be Google's API Gateway which implements REST API, but also allows you to build web services that are fully RESTful. If you have any questions regarding this article you can ask in comments section below, also you can find all the source code for this article here."	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CATTS model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CATTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the CATTS-XSUM model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CATTS-XSUM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" What are the goals of the catts model ? The main goal of the catts model is to be fast, so that we start to make sense of the dataset and its relationships, thereby allowing us to predict other features in the dataset. So we are not trying to achieve any kind of global accuracy, and do not try to compute predictions for every feature. We will need to keep some basic knowledge about the dataset, such as whether or not it has features for which the predictor is sensitive, and so on. Also, we may want to simulate other distributions, such as the normal, logistic, and so on. What are the main technical challenges ? For all these computational tasks we need a machine learning algorithm. This is the implementation of a general classification model that we use for all our datasets. We are interested in a set of supervised classification tasks, where we are trying to model features for which this (for example) classifier is capable of learning and predicting. Is it possible to make the model faster ? Of course. It can be made as fast as a human by being trained on many data points and using a lot of parallelism, but there are tradeoffs. This is the second argument for not attempting to train a cat as fast as possible. What is the output of this model ? A list of all the columns of the dataset. What is the output of that ? The same list. What can we run on an implementation of the CATTS model ? There are two popular ways to run it. One way is to make it a data generator (called ""catstfit"", which you can find here ), where we will try to fit features of the data to various distribution of distributions (for example normal, logistic, Bernoulli, etc. ). Another way is to create a model at deep learning speed (called ""catx"", which you can find here ). If we make the catts-xsumm model a data generator using catstfit, we will never have a data frame. We need to calculate the distribution of these predictions for each feature, and then save the weights for each column to the data file. This is a problem as you see in the first sentence here : This data is a series of predictions, and should be kept in a database with some form of structure, either a column-wise model or a column-wise structure. This is a data structure for storing and displaying these predictions.  There are ways of"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT { ?dataset_lbl. ?value orkgp. ?value } ORDER BY DESC ; } }..............................ScienceIE output (Sparql query):.................................ScienceIE...............10.0.........................2.........................4.7.........5.0.........................8.0 ScienceIE vs. IMDb B input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B ? output (Sparql query): SELECT DISTINCT ?metric ? ? ? ( ?value ) AS ?score ORDER BY ?score DESC GROUP BY ?metric __metric__ Input text: How many movies, shows, etc., do you view ? What are your ratings ? What's your IMDB ranking ? RDFS output (Parse): Output data: What should users expect ? output (Parse): What should users expect ? RDFS input (English text): What should users expect ? output (Parse): What should users expect ? RDFS output (Sparql query): SELECT ?metric__ ?score FROM ?column_id ; output (Parse): What should users expect ? RDFS input (English text): What do you mean by the ?value ? output (Parse): What do you mean by the ?value ? RDFS output (Sparql query): SELECT rdfs:category ?category_lbl WHERE ?category_lbl NOT IN ('Science', 'Arts', 'Fitness', 'Food and Beverage', 'Movies', 'TV Shows') ; output (Parse): What should users expect ? RDFS input (English text): What does the ?level=value ? output (Parse): What does the ?level=value ? RDFS output (Sparql query): SELECT \ ?level=value\ ?value\ ?level, \ ?level=value, \, level AND ?level = 0 AND ?value IS NOT NULL output (Parse): What would you expect ? RDFS output (Sparql query): SELECT \ ?, ?level=value \ ?, ?level ?	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot German-to-French?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) AS ?value, ?dataset ?dataset_lbl AS LASTS( ?dataset ?dataset_lbl.dataset AS a, ?dataset ?dataset_lbl.dataset AS b, 1, 2, 3.4), ?label ?label_lbl, ?cont orkgp:HAS_DATASET ?dataset_lbl.dataset AS a,  orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL SETs: *LEN ?score, *LANG ?label_lbl, *CONT orkgp:HAS_BENCHMARK ?label_lbl, *MODEL ?model. input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot German-to-French ? output (Sparql query):  SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) AS ?value, ?dataset ?dataset_lbl AS LASTS( ?dataset ?dataset_lbl.dataset AS a, ?dataset ?dataset_lbl.dataset AS b, 1, 2, 3.4), ?label ?label_lbl, ?cont orkgp:HAS_DATASET ?dataset_lbl.dataset AS a,  orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL SETs: *LEN ?score, *LANG ?label_lbl, *CONT orkgp:HAS_BENCHMARK ?label_lbl, *MODEL ?model. This query looks like a little hack, but it works output (Sparql query): SELECT ?   ?metric ?metric_lbl AS BIN( ?value.dataset2 AS a ). OPTIONAL   ?label ?label_lbl. SETS BIN, ?label_l	False
"
 input (English text): Where can I find code references in papers that have used the MP-EB model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MP-EB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the SemExp model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SemExp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model anl:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SemExp"" ) ?benchmark. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (Java source data): Outputs results in a table. Select values from this table and print them. See also: Table output (HTML) output (Java source data): select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr select value from table table_expr   select value from table table_expr select value from table table_"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Up and Down dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?dataset a orkgs:Dataset OR ?dataset a orkgs:Dataset as ?dataset. ?dataset idx. ( ?dataset_lbl  is for backward compatibility) OPTIONAL { idx. } RDFs:label idx. RDFs:value idx. RDFs:id xval ?dataset. OPTIONAL yval. OPTIONAL { ?dataset a orkgs:Dataset; ?dataset andkgc:Dataset; yval:HAS_DATASET ?dataset. } RDFs: idx idx as yval. RDFs:value idx. RDFs:id xval ?dataset. OPTIONAL yval. OPTIONAL { ?dataset idx as yval. } In parallel, the top-ranked model was evaluated on all 16 datasets. That's the same as with L1 regularization, only the data was larger and the feature space large. There was a strong relationship between the ""good"" and ""terrible"" rankings. Here was one sample of the data (all with L1 regularization): 1.1 1.1 / 1.1 1.2 1.2 / 1.1 1.2 1.1 orkgc:Dataset  1.9/1.9 1.9/1.9 orkgc:Dataet 1.7/1.7 (a/b) -   i.e. - good, bad or very bad. 1.2 1.2 / 1.2 or kgc:Dataet 1.7/1.7 1.0 / 1.2 or rdfs:Label The model on the left ranked well!  1.2 / 1.2 vs 1.1 vs orkgc:Dataset 1.7/1.7 (b/c) (a/b) -   i.e. - good, bad or very bad. 1.2 / 1.2 vs 1.1 vs or kgc:Dataset 1.0/1.0 (b/c) (a/b) -   i.e. - good, bad or very bad. 1.2 /"	False
"
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the One Billion Word dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc_tb( Character Level ); rdfs:labels ?metric_lbl_labels. FILTER (str( ?metric_lbl_labels) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc_tb( Character Level ); rdfs:labels ?dataset_lbl_labels. } ORDER BY DESC( ?value) LIMIT 1 } } Output: ----------------------------------------------------------- Model Name ----------------------------------------------------------- one_billion_word_model 1.00 orkgc2a 1.00 orkgc_tb 20.00 orkc2b 20.00 orkgc_tb 4.00 orkgc_tb 1.00 orkgc_tb 2.00 orkgc_tb 11.00 orkc2a 1.00 orkc_tb 5.00 orkc_tb 3.00 orkc_tb 2.00 orkc_tb 1.00 orkc_tb 2.00 orkc_tb 10.00 kcp2a 1.00 kcp2b 19.00 kcp2a 1.00kcp2b 12.50 kcp2a 1.00kcp2b 6.50 kcp2a 2.00 kcp2b 1.00kcp2b 5.00kcp2a/kcp2b 4.00kcp2a/kcp2b 1.00kcp2a/kcp2b 3.00 orkcp2b 1.00kcp2b 10.00 kcp2b 1.00kcp2b 1.00kcp2b 4.00 kcp2b 2.00kcp2b 1.00kcp2b 3.00kcp2b 2.00kcp2b 0.00kcp2c 1.00 kcp2b 2.00kcp2a 1.00kcp2b 0.00kcp2a/kcp2b 0.00kcp2b 1.00kcp2a/kcp2b 1.00kcp2a/kcp2b 1.00k"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ADE Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ADE Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset ; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Character Level"") ?benchmark orkgpp:HAS_DATASET ?dataset. ?cont orkgpp:HAS_BENCHMARK ?benchmark. ?paper orkgpp:P31 ?cont; Input data: Titles : A list of top published paper titles containing only english words on this list. Notes: A list of research paper notes containing only english sentences. Author IDs : A list of authors with ID corresponding to the paper in the list. Author Description: A list of authors who wrote the paper in a unique way describing the paper and adding a note about who authored the paper in the paper (e.g., ""First Author: [name of author in paper]."") ID Format: ID 1: Author Name Name from ID 2: Title Title Author Name title from ID 3: Notes Notes Author Name description from ID 4: Link/URL Link/URL from ID Output Data: Titles : A list of top published paper titles containing only english words on this list. Notes : A list of research paper notes containing only english words. Author ID ID Format: Author Name Name from ID ID 1: Title Title Author Name title from ID 2: Notes Notes Author Name description from ID 3: Link/URL Link/URL from ID Link/URL Format: Link/URL ID 1: Link/URL Link from ID 2: Title Title Author Name title from ID 3: Title Title Author Name title from ID 4: Title Title Author Name title from ID Paper ID ID Format: Author Name Name from ID ID 1: Paper ID ID 1 Identifier ID ID Format: Author Name Name from ID ID 1: Paper ID ID 1 Input data: Titles : List of top published paper titles containing only english words on this list sorted by author name. Notes : List of research paper notes containing only english sentences. Author : Author Name Title Title from ID 1: TitleTitle Title Author from ID 2: TitleTitle Title Title from ID 3: TitleTitle Title Title from ID 4: TitleTitle Title Title from ID Title Title Title Author from ID ID 1: TitleTitle Title Title from ID 2:"	False
"
 input (English text): What are the metrics of evaluation over the Atari 2600 Video Pinball dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Video Pinball"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?dataset_lbl ?, eval ?, ? eval ?, ? eval_metric With a query (S) query engine for dataset data, our example would become the following: Input {dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Video Pinball"") ?benchmark orkgp:HAS_DEFINITION ?dataset; orkgp:HAS_METRIC ?metric. } orkgp:HAS_EVALUATION ?eval. ?val. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. } The next command, a ""parallel"" query (B), extracts an input (dataset) from the parallel query language (A) and parses it into a SQL query. In A, the data in parallel is mapped to a data set that has four columns (two for each row, like this): SELECT COUNT(1) * 2 FROM (SELECT a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Video Pinball"") ?benchmark || ( ?dataset_lbl ?) orkgp:HAS_DEFINITION ?dataset) ?statistics. OPTIONAL { ?statistics val_rdfs:label ?statistics. } The data, then, looks like this:"	False
"
 input (English text): What is the name of the top performing model in terms of Top 1 Accuracy score when benchmarked on the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 For each of the above queries, the rows returned contained the model name, the datasets and their corresponding metric and the result row containing the metric, the output of the benchmark method, and the score in terms of the benchmark metric. In each query, the rows are ordered from highest to least accurate. To produce the above output, I ran on each dataset at the same time, and then each benchmarked model that was ranked (in terms of its performance in the benchmark) compared against the benchmarked model that was not ranked. I included the most accurate benchmarked model as top 1 when the metric met (eg. it achieved <8 or <15), and it did not achieve the best performance when met (eg. it was ranked well). Thus, the dataset of the benchmark was only used as a proxy for the model's performance. If, after running the benchmark test, the most accurate model was not used again after comparing to another, then after the remaining top 1 models were combined, the median performance among those that competed on the benchmark and averaged over all the batches, it was shown to be the median performance among the top 1, since they all achieved the same performance as the median performance amongst the other top 1 models. Here is my benchmarking code: import scipy, matplotlib.pyplot as plt from sklearn.datasets import from_dict from sklearn.datasets.model import NestedModel from sklearn import datasets # Define the model object NestedModel(model='cascade', metrics=['orkgp', 'orkgc'], input_path='model_lbl') # Run the benchmark test with benchmark, top 1 models, & dataset NestedModel(model='cascade', benchmark=benchmark, top_1_1 =[[1,2,3],[4,5,6]]) # We're not going to do numerical simulation just yet, because we need predictors, # in this case just the score for the benchmark from 0.75 to 2.0 # We can now take the metric score directly and also use that metric with prediction(the_target, predictions=[NestedModel]) NestedModel(model='cascade') # To calculate the model performance based on the metrics NestedModel(metrics='metric', metric_rank = 1) # We can also use a class for the model NestedModel(class='cascaded_cascade', metric='orkgp',	False
"
 input (English text): What was the most common type of approach for summarization before 2002?
 output (Sparql query): SELECT ?approach ?approach_label WHERE { orkgr:R6948 orkgp:compareContribution ?cont. ?cont orkgp:P15 ?implementation. ?implementation orkgp:P5043 ?approach. ?approach rdfs:label ?approach_label. } ORDER BY DESC(COUNT( ?approach_label)) LIMIT 1
 input (English text): What is the most common Knowledge representation method?
 output (Sparql query): "	SELECT (COUNT( ?repr) AS ?cnt) ?repr WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?cont orkgp:P7046 ?sys. ?sys orkgp:P7056 ?repr. } GROUP BY ?repr ORDER BY DESC( ?cnt) LIMIT 1	"SELECT"" statement: [ :name, :birthday, :height, :age, :weight, :address, :email, :picture, :location, :country:country ], [ :address, :email, :picture, :location, :country:country ]) Now you can write a code that, when the attribute needs a value, can go inside, to retrieve the values with the value for each key. To do this you should create a subclass of SPSS: the class ""SPSS"" does what you could have done on the code, but with a number of features that SPSS is able to do for you (it will be more practical to implement this if you need to write a lot of SQL code). For example, this will not only be how you set up your SQL:SELECT statement, but we will create also many database connections, to save the values of the objects that should be used. You will find a link to a complete tutorial based on SPSS, along with my own article explaining how to set up the SQL: SELECT statement for SPSS: In one way it is the best tutorial on this and it will help to set up SPSS, but it is not the best one. I will show you the way I solve this very tricky problem from my own perspective. Step two is, to create some code that will look like this: create_data(1); my_props.spss = { name: ""Andrew B"", birthday: ""April 28, 2004"", height: ""5'9"" }; my_data = { address: ""PO"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the MedSTS dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can all the data sets used in the compared studies be found?
 output (Sparql query): "	"SELECT DISTINCT ?URL WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:HAS_DATASET ?URL. FILTER(!REGEX( ?URL, ""Not"")) } "	" output (HTML file) This output shows the results of the MedSTS benchmark analysis which is a combination of two steps. In step 1 there is a list of all the sets used as reference and a list of all the data sets that are compared to each other. In step 2 this list is further grouped by each of the results of those datasets. Step 1 - reference data sets In step 1 there are a total of 5 reference data sets and 2 comparison datasets that are used for this benchmark. In steps 2 and 3 there is a list of data sets which are compared to those of each reference data set but without its corresponding comparison datasets. Both data sets are provided but separated by space. Step 1 - reference data sets In that step there is a list of all the data references that are used in the benchmark analysis In the next step the list is sorted alphabetically by data reference. When those data sets are presented, the data is shown either as an Excel data file in the Excel tab and it must be opened using the Excel keyboard shortcuts for inserting a new data set or as a Parquet file and it must be opened with that specific parser. In step 2 - test datasets The test dataset (or the dataset listed there) is a test dataset for the MedSTS benchmark analysis used by each reference data set. This can be in any form or format. It can be in a spreadsheet ( spreadsheet -t ), it can be a data file in the spreadsheet tab ( file -t ), or it can be in the Parquet file format. When the data is compared to each reference dataset (i.e. when the reference datasets are treated as ""a"" or ""b"" sets) in that benchmark it will be presented in that format. In step 3 - comparison datasets"	False
"
 input (English text): What are the most commonly used benchmark datasets for the Named entity recognition research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Named entity recognition"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Entity Disambiguation research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Entity Disambiguation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 As always I can't resist to ask: What would you be most excited to see this year ? Be sure to share your ideas with us! Thanks to all this year's participants, including: David Bechtold, David B., Richard Jones, Adam J. Miller, Christopher G. R. Anderson, Robert H. Pritchard, Patrick S. McGovern, Chris Schadz, David M. P. Haugland, Paul D. A. Taylor, Mark H. Ainsworth, Eric J. F. van Leeuwen, Daniel Langer, Matthew H. Stolwijk, Robert A. Miller, Tim W. Schafer, David R. Crouch, Benjamin P. Jones, Steven E. Fogg, Brian F. Riggs, Andrew V. M. Wilson (Special thanks as always to the volunteers who made the event a great success!).	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BioASQ dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BIOSSES dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset rdfs:Label; rdfs:Descriptor ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:Label ?paper_lbl. } output (Sparql query): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BLAST-RIS dataset ? output (Sparql query): -------------------------- Output: ------------------------- Output(Sparql query): SELECT DISTINCT ?labels. ?labels OR(  { ?labels =  ""BioASQ"" ?ext =  "" ?1"" ?ext_min = ?ext_max ?ext_size = ?ext_length ?ext_type =  """", ?ext_length =  0.25 ?ext_1 = ?ext_2  , ?ext_size =  0.025 ?ext_max = ?ext_min  } )) output (Sparql query): SELECT DISTINCT ?labels. ?labels OR(  { ?labels = 1, ?labels =  ""BETA-RIS"" ?ext = ?ext_min ?ext_max  , ?ext_length =  0.2 ?ext_1 = ?ext_2, ?ext_size =  0.1 ?ext_max = ?ext_min _, ?ext_min =  0.05  } )) output (Sparql query): SELECT DISTINCT ?labels. ?label OR(  { ?labels = 1, ?label =  ""BioASQ"" ?ext = ?ext_min ?ext_max  , ?ext_length = ?ext_1 ?ext_length  , ?ext_1 = ?ext_2, ?ext_size =  0.025 ?ext_max = ?ext_min"	False
"
 input (English text): Which model has achieved the highest F1 score score on the BUCC French-to-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score score on the BUCC Chinese-to-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Chinese-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Chinese-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Sparse Input Sparse input is a text input without a header or metadata. To be a sparse input there's no option to filter text with the F1 score, and the input is completely anonymous. Sparse input can be used for text search in the absence of a human-written search engine (for example, in the name-to-text format of a URL). In an offline context, these inputs can be used for text retrieval only. For this purpose, the text of the target entry is stored in an intermediate format (e.g. SQLite database) instead of in the actual input. This saves bandwidth and increases the speed of text searches. This article was written by Murali Ramachandran, an associate professor at the NYU Stern School of Business.  This article was also published on Medium  by the authors."	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ImageNet ReaL dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title OR ?Title OR ?ID, IFNULL(Rdfs:title ?title_lbl, null, ?title), ------------------------- Note: RDFS is a collection of markup languages with a common set of data. Next steps It's important to note that the above example doesn't show how the benchmark data can be used to predict how similar or distinct the ""images"" are of different objects. One reason for this is that it's not easy to predict the similarity between images: the visual characteristics of an object are likely to be quite different from the visual properties of its background. For this reason there is an important paper with an attempt at modeling a human face as a neural network (or similar) that was published in 2014: Giorgio Peruzzi et al. ""A Neural Network Model of Face Perception Using a Real-Life Face"" ( arXiv preprint arXiv:1507.2652). In this paper they use a neural network model called ReaNet, which has been used to model human faces before. ReaNet is similar to ReaL for learning classification, so the similarity score can be used to determine whether a model's results seem to be valid after being trained in this way. In the context of this blog post, we're interested in using the model found in this paper (on the face of a human) to select the best model out of the model. If we can perform better than this, it means that the model on ReaL is not too good, but maybe we can improve it ? Another important question to ask is: ""What if I have a dataset of human objects and an image-classing algorithm ?"" What's the best model in this circumstance ? It's pretty difficult to answer this question: we just know for sure that the image-classing algorithm has worked for us, but if there is another model that looks more reasonable and is faster for training, then what are our options in this situation ? This is where the approach in this post to model ""objects"" via a neural network comes into play: we will define a ""shape"" for our model representing our dataset, as well as a ""image-class"" and a ""shape"", which can then be used to predict images and, if necessary, to select a specific model out of those available. We will then use the model's shape as the basis for selecting a particular model from the network: image-class"	False
"
 input (English text): Could you provide a list of models that have been tested on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT title, title_id, count ( ?matches ?model_lbl) as avg, count ( ?paper_matches ?lbl) as avg, count ( ?benchmark_matches ?lbl) as avg, SUM ( ?record_count ?LBL) as avg, avg( ?model_lbl) as cmp, SUM ( ?eval_count ?LRBL) as avg, ( ?lbl[0].model ?model_lbl[0].object) as avg FROM ""benchmark-sample"" LEFT JOIN ""benchmark"" ON ( ?dataset ?dataset_lbl) = ?dataset output (sldf) : input (English text): Which of the following is the model of the Gibson PointGoal Navigation dataset ? output (Sparql query) SELECT name_id, name_id_p, label, ?record_count, ?lbl_p, ?record_count, ?paper_p input (English text): Which of the following is the model of the PointGoal Navigation Dataset ? output (sldf) : input (English text): Which of the following is the model of the PointGoal Navigation Benchmark dataset ? output (Sparql query) SELECT model_id, model_name, model_name_lbl, ?stat ? as cmp FROM ""benchmark-sample"" LEFT JOIN ""benchmark"" ON ( ?dataset ?dataset_lbl) = ?dataset output (Sparql query):"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset CoNLL++?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?metric ?metric_lbl, ?value, ?score "","" [ ?rdfs:label ""NLL++"", ?dataset:DISTINCT """", ?metric:LBL ],"". OPTIONAL ORDER BY "","" ?value"" "","" ?metric "","" ?metric_lbl "","". rdfs:label rdfs_lbl. ?"" ORDER BY "","" ?""eval. "","" ?eval "","" ?eval_lbl "","".""rdfs"" "","".""dataset"" "","" ?""eval_lbl "","" ?""value "","" ?result "","" ?""value "","". ?score ?"" ?"" ?"" GROUP BY "","" ?""metric "","" ?""metric_lbl output (Sparql query): ?"" output (Sparql query): SELECT ?metric ?metric_lbl, ?value, ?score "","" [ ?rdfs:label ""NLL++"", ?dataset:DISTINCT """", ?metric:LBL ],"". OPTIONAL ORDER BY "","" ?value"" "","" ?metric "","" ?metric_lbl "","". rdfs:label rdfs_lbl. ?"" ORDER BY "","" ?""eval. "","" ?eval "","" ?eval_lbl "","".""rdfs"" "","".""dataset"" "","" ?""eval_lbl "","" ?""value "","" ?result "","" ?value "","". ?score ?"" ?"" ?"" GROUP BY "","" ?""metric "","" ?""metric_lbl output (Sparql query): ?""... ?"" output (Sparql query): SELECT "","" ?rdfs:label ""NLL++"", "","". OPTIONAL ORDER BY "","" ?""value "","" ?metric "","" ?metric_lbl. "","". rdfs:label rdfs_lbl. ?"" ORDER BY "","" ?""eval. "","" ?eval_lbl "","" ?"".rdfs"" "","".""dataset"" "","" ?""eval_lbl "","" ?""value "","" ?result "","" ?value "","". ?score ?"" ?"" ?"" GROUP BY "","" ?""metric "","" ?metric_lbl output (Sparql query):"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset PIQA?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the PIQA dataset in terms of Accuracy metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT accuracy from orkgp ORDER BY measure_score. DECLARE @met_score INT; DECLARE MAX( @met_score) int; SET @met_score = 1; SELECT score(min( ?)), _count FROM orkgp AS _count LEFT JOIN orkgp_score on _count.__id = _count.__id ORDER BY score(min( ?)) ASC; MAX(score(min( ?))) = 0; SET @met_score = score(min( ?)) + 1; SET score(min( ?)) = score((min( ?) * 2)) +1; SELECT max(score(max( ?m)1), score(max( ?m)1)) FROM orkgp AS max( ?m)1 ORDER BY score(max( ?)) ASC input (English text): What are the best performing models for this query ? output (Sparql query): { { ?dataset a orkgc:Dataset ?dataset_lbl. FILTER (str( ?dataset_lbl ) = ""PIQA"") ?models:HAS_NODE ?model_lbl. } } GROUP BY ?dataset ?dataset_lbl ORDER BY ?model_lbl. input (English text): What percentage of tests are passing (metric, value) over the dataset ? output (Sparql query): --------------------------- select accuracy FROM orkgp ORDER BY measure_score. DECLARE @met_score INT; DECLARE MAX( @met_score) int; SET @met_score = 1; select avg_score( ?ratio[1:3]).value FROM orkgp ORDER BY measure_score. DECLARE max( ?metric, ?value).value = 1 ; SET max(metric, ?value) = 100 * ?ratio[1:3].value. LIMIT 1; SELECT max( ?ratio[1:3].value).value FROM orkgp ORDER BY measure_score; MAX(max( ?)) = max( ?metric).value * ?ratio[1:3].value. LIMIT 1; input (English text): What is the most interesting data in this query ? output (Sparql query): =========================================================== --------------------------- select max( ?metric) from orkg"	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Alien benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model_lbl ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Dragon Warrior benchmark dataset ? output (Sparql query): SELECT DISTINCT ?model_lbl ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Dragon Warrior"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): Indicate the model that performed best in terms"	False
"
 input (English text): Provide a list of papers that have utilized the PtGen model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PtGen"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Table-Sequence model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Table-Sequence"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Table-Sequence"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } The output should be a list of papers that has used the PtGen model. SOLAR-TEMPLATE STATISTICS The standard method of determining that the data sets are sufficiently similar to be statistically related to each other is to utilize the significance function described in the chapter on significance tests, described below. The main purpose of the significance function is to facilitate the ability to compute a likelihood of using various model/observation combinations within a particular model. For an example of a significance function of these data sets, the following formula can be used to compute the likelihood of using a particular model/observation combination : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 model = ptgen->sigOpt(10,0,input(""Sci:"",10)) test1 = ptgen->sigOpt(10,0,input(""Sci:"",10)) test2 = ptgen->sigOpt(20,0,input(""Sci:"",30)) The formulas above are very simple due in part to the need to account for the correlation between each model/observation element and the other model/observation elements in the data sets. It is important to note that in these formula the pterm was not modified in some way. This is necessary because the pterm(input) is used as evidence of model relatedness. The formula for comparing the models/observations is a bit more complex because each of the variables we wish to compare are given the value of 0. Given that we want to compare the following terms (p<0, p<1, p<100, etc), we are required to make sure that we do not include the values with p above 0 for any model/observation combination. The first formula ( p"	False
"
 input (English text): Provide a list of papers that have utilized the Feedback Transformer (4 layers) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Feedback Transformer (4 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Funnel Transformer model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Funnel Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl ?benchmark orkgp:HAS_DATASET ?dataset ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp a orkgc:HAS_MODEL ?model; orkgc:HAS_SOURCE_CODES ?code. } input (English text): Provide a list of papers that have utilized the Funnel Transformer (2 layers) model and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the Feedback Transformer (1 layers) model and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the Feedback Transformer (0 layers) model and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the SDFD (3 dts) model including the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a dts:Model; rdfs:label 1:2 ?model_lbl. ?dataset dts:HAS_DATASET ; OR ggg:HAS_DATASET ?dts. ?cont dts:HAS_SID ?code. } input (English text): Provide a list of papers that have utilized the SDFD (2 dts) model including the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the SDFD (1 dts) model including the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the SDFD (0 dts) model including the links to their code ? output (Sparql query): input (English text): Provide a list	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters En-De dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters De-En dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT count(*) AS number_count ( 2 GROUP BY number_count 3 ORDER BY number_count DESC ); 4 output_date_sql (string): SELECT * from date_table() 5 OUTPUT_NODATE_STRING(datetime): Date (MM/dd/yyyy) 6 7 SELECT * from date_table() 9 ORDER BY date_count DESC; select count(*) from date_table( select count(*) from date_table 10 ORDER BY date_count DESC; 11 output_date_sql(string): SELECT count(*) FROM date_table 12 OUTPUT_NODATE_STRING(datetime): Date (MM/dd/yyyy) 12 13 SELECT count(*) from date_table 14 ORDER BY date_count DESC; select count(*) from date_table 15 ORDER BY date_count DESC; select count(*) from date_table 17 OUTPUT_NODATE_STRING(datetime): Date (MM/dd/yyyy) 17 18 Output results set: The following output shows results from using the query on the data set provided. A subset query, or a subset query like this one, can be used to create more complex benchmarks in a simple way: 1 SELECT count(*) AS number_count ( 2 GROUP BY number_count 3 LIMIT 1,2 4 ); The data set provided (in this example we are using the dataset provided by the bank for the dataset ""London Bullion Market"") contains a series of market data from November 2016 to September 2017, covering major indices, currencies and stocks, as well as equities, derivatives, and commodity indices. The data in the"	False
"
 input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the PubMed 20k RCT benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ? model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Post-Pubmed 20k"") ?medlink:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. ?value rdfs:label ?model_lbl. } ORDER BY DESC( ?value ) LIMIT 1 } } Input is the same as output. Sparql query is the same as output. I would suggest you to look more closely at the output as the code is more complex. You need to check the parameter order of parameters in RDFS, filter parameters, filter all, do a SELECT based on the data, etc. Here is the source for the RDFS query in the SQL query. import numpy as np import nltk as nltk import sqlite3 import pprint sqlite3. connection = sqlite3. connect ( ':memory:', ':memory:' ) db, rows = db. fetchall ( 'SELECT * FROM {' ) pprint. pprint ( rows ) query = 'SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. }'. format ( rows, end ='', sep = ':'). format ( columns = 'TEXT, NUMERIC' ). join ( db ) query. join ( db ) order_by ( query ) Here you are getting very readable output from the pprint function. I did find however that in the sqlite3 package with the --log or -l flag, the output is broken from now on with the ""column values"" option which is not correct. This bug is very annoying as the output needs to be parsed into some form that can be understood by pprint. Also, the command -l will work but it causes parsing of all parameter values"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the DocTAET-TDM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DocTAET-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DocRED-BiLSTM model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DocRED-BiLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?code FROM ? Is there a database that lets you view the output ? Do you know the SQL language used by the databases ?	False
"
 input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of PARAMS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Output CSV File Here is the output from the output statement of the same query, i.e. the one running the model benchmarking: Model(input('R')); Model(input('.hls')); Model(input('.pups')); Model(input('R'));   Note that the result is not in the same format as it may appear on your terminal. There are two possible reasons for this: Firstly, using the model, we saw that the number of comparisons is not consistent across the training set. Secondly, there may be hidden classes that the models cannot distinguish, especially when training data comes from noisy or poorly supervised datasets. For this reason, you should be aware of the potential pitfalls encountered in using an unsupervised data for training and testing. You cannot, of course, just use any old model or dataset that you can find on the internet. This isn't the right approach for building predictive models that require some form of training and testing. Instead, you need to select only data you know are accurate, correct, and available for use in the predictions. The problem is that there are often several models and datasets for which the data or models are incomplete or are unsuitable for training and testing. The models and datasets for which we have information have varying degrees of accuracy. You have to decide which model or datasets best suit each model and dataset that is available to you; for example, it could be very difficult to select data from the dataset of R's ""GEO"" dataset which produces excellent predictions; a data set which might actually be an overfitting or suboptimal training set. In one of my previous posts, I examined the Oxford-IIIIM metrics (the data used to train the network) and provided some additional pointers on the optimal set of data. The best approach when choosing the most appropriate data will determine how well can you build a model. In my view, this means knowing the best data that exists in order to understand the model you wish to build, in terms of how well a model can be built with the input data, the training set, and the predictions to back that model. You also have to know the characteristics of each model as compared to each other that the underlying data provide. What is the best model for a given dataset on that particular dataset. In a model building lab, we often use three-dimensional features (predictors, activations, layer parameters, etc.) within a hierarchical model. To build and test"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset AESLC?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"--------------... [ACL-ARC (ASCII data) v8.0]( [Sparql Query]( [ACL-ARC (ASCII data)] ) [ACL-ARC (ASCII data)] -------------- output (Sparql query): --------------... [ACL-ARC (ASCII data)] \t Input (English text): What is the top benchmark result, including the metric and score, over the dataset AESLC ? output (Sparql query): How many nodes were processed (if any) and their performance ? Output (JSON): { ""node-count"": 200... } input (English text): How many nodes were processed (if any) and their performance ? output (JSON): { ""nodes-per-result"": 1.5... } input (English text): How many nodes were processed (if any) and their performance ? output (JSON): { ""node-count"": 200... } input (English text): What is the best average of the results of each benchmark across all nodes ? output (JSON): { ""Nodecs"": 1.01... } input (English text): What is the best average of the results of each benchmark across all nodes ? output (JSON): { ""Nodecs"": 1.01... } Input (English text): What is the performance difference between the total number of nodes or the individual nodes as a percentage of the total number of nodes ? output (Sparql query): --------------... [ACL-ARC (ASCII data) v8.0]( [Sparql Query]( [ACL-ARC (ASCII data)] ) [ACL-ARC (ASCII data)] output (Sparql query): -----------................ Output (JSON): {} Input (English text): What is the performance difference between the total number of nodes or the individual nodes as a percentage of the total number of nodes ? output (Sparql query): -----------................ Output (JSON): {} Input (English text): How fast is the best benchmark when the metric and score are the same for all nodes and the metric is a minimum of 60 and a maximum of 70, including a margin of error that is less than 1.5 ? output (Sparql query): --------------... [ACL-ARC ("	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Stanford Dogs dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Dogs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?pub_name ?pub_id ?pub_author ?pub_pub ?pub_pub ?pub_pub ?pub_name ?pub_pub ?pub_pub ?pub_pub ?pub_author. , ?pub_author_pub ?pub_pub ?pub_pub ?pub_pub ?pub_pub ?pub_pub ?pub_pub ?pub_name ?pub_pub ?pub_pub ?pub_pub., ?pub_pub_name ?pub_pub ?pub_pub ?pub_pub ?pub_pub ?pub_pub ?pub_pub ?pub_pub ?pub_pub_name. , ?pub_pub_pub. , ?pub_pub_pub. ) FROM ?pub_pub WHERE! ?pub_pub. Example of a predictive and non-predictive SQL query for a particular article to be presented at the AAAAC Meet Up on November 9. input (English text): Specify a list of keywords and an expression to match on papers that have been published in a journal that has published a paper on the topic of the day (e.g., AAAS, JRS, PNAS). output (Sparql query): <a href=""https://www.scmp.com/web/aaac/meetup/2015/apr_09/meet-up-topics ?title=AAAS&pub_id=12345""> To the best of our knowledge this is the only query that is made."	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WOS-11967 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the WOS-46985 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP 1 ( 1 ) FROM WOS-46985 SELECT * FROM WOS-46985 ; | [1] WOS-46982. SELECT TOP 2 ( 2 ) FROM WOS-46982 SELECT * FROM WOS-46982 ; | [2] WOS-46983. SELECT TOP 3 ( 3 ) FROM WOS-46983 SELECT * FROM WOS-46983 ; | [3] WOS-46984. SELECT TOP 4 ( 4 ) FROM WOS-46984 SELECT * FROM WOS-46984 ; | [4] WOS-46985. SELECT TOP 5 ( 5 ) FROM WOS-46985 SELECT * FROM WOS-46985 ; | [5] WOS-4685. SELECT TOP 6 ( 6 ) FROM WOS-4685 SELECT * FROM WOS-4685 ; | [6] WOS-4686. SELECT TOP 7 ( 7 ) FROM WOS-4686 SELECT * FROM WOS-4686 ; | [7] |---------------------- In the above query, the first three parameters have the same value, so there is no way to choose a different parameter that provides the same value from the dataset. If you need to specify a more specific value than the top scores, use the optional query argument ?metric. If you need to specify a maximum score, use the optional parameter ?x. This is a shorthand way of specifying a value. It is similar to the syntax for ?values=values, ?dataset=dataset. This syntax can be used to specify a maximum score while leaving the maximum values unspecified or zero. However, only one max scores can be specified at a time, and the values are ordered by descending order. The metric value can be any metric you define. If the metric doesn't have any column, it is specified as the first three parameters of the query. If the metric has a column, it is omitted, as if it were omitted, or if the metric has multiple columns, then they are individually specified as the top two parameters. The first three parameters are the same as for the WOS-46985 query. You can remove the values you need from the parameters using the query argument ?cont. If you have a maximum score, use the optional parameter ?max. If you want to leave the values unspecified, use the query argument ?max= ?distinct. This parameter determines whether the value is unique	False
"
 input (English text): Provide a list of papers that have utilized the HNEABP (BWNE) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HNEABP (BWNE)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the AcrE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AcrE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 RDF: I have several questions: - What are these things going to be used for ? Answer from John: I have several questions: - What are these things going to be used for ? - What are these things going to be used for ? - What are these things going to be used for ? - What are these things going to be used for ?	False
"
 input (English text): Which model has achieved the highest SemEval 2007 score on the Supervised: benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SemEval 2007"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Supervised: dataset in terms of SemEval 2013 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SemEval 2013"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "		False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the SciFACT dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciFACT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciERC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT A and B, id1, id2, title, rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciFACT"") ?dataset_lbl. ?authors ?authors. ?title ?title. ?authors. ?id2 ?id2. ?pub_id ?pub_id. ?pub_sub_id ?pub_sub_id. ?rdfs:label ?author_lbl. ?rdfs:label ?author_lbl. OR (A AND B) ; output (Sparql query): SELECT A, B, id1, id2, title, rdfs:labels. FILTER (str( ?dataset_lbl) = ""SciFACT"") ?dataset_lbl. ?authors ?authors. ?title ?title. ?authors. ?id1 ?id1. ?pub_id ?pub_id. ?pub_sub_id ?pub_sub_id. ?rdfs:labels ?author_lbl. ?rdfs:labels ?author_lbl. OR (A AND B) ; R2 <- data.frame (A, B) R2$title <- rdfs:name(A) $title R2$pub_id <- rdfs:name(B) $pub_id R2$author_lbl <- rdfs:name(R2) $author_lbl R2$author_lbl$labels <- as.data.frame(rdfs:labels)$id; R2$ab <- c(1, 2, 3) # A -> B -> C R2$A <- as.character(1:10) R2$B <- as.character(1:10) A/B <- c(1:10, 2:5, 3:6) R2$ab/A <- c(1:10, 1:10) # A -> B -> C R2$A/B <- c(1:10, 1:10, 1:5) # A -> B A/A <- c(1, 5, 3, 6) R2$ab/A/B <- c(1, 4, 1"	False
"
 input (English text): How many studies do use Chloride as major anion?
 output (Sparql query): SELECT COUNT( ?anions_labels) AS ?chloride_count WHERE { orkgr:R110597 orkgp:compareContribution ?contrib. ?contrib orkgp:P37458 ?anions. ?anions rdfs:label ?anions_labels. FILTER(REGEX( ?anions_labels, ""Chloride""^^xsd:string)) } 
 input (English text): What quantity of iron oxide was discovered on Elorza crater?
 output (Sparql query): "	"SELECT ?properties_values, ?property_description WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, ""Elorza crater"", ""i"")) ?papers orkgp:P31 ?contrib. ?contrib ?properties ?properties_values. ?properties rdfs:label ?properties_labels. FILTER(REGEX( ?properties_labels, ""FeO"")) ?properties orkgp:description ?property_description. } "	 R O G P G P I R R O G P G P I R R O G P G P I Y U Z G P O R O P T S E R G E F F I C S P S S input (English text): Number of studies using iron oxide in argon-isotope separation (ORI) as the primary anion. output (Sparql query): R I G O T T E H I R G E R G E R (I) G E C E C P T E R F I E R T D E D Y S F O F F E X S G P 1 T (R) T (R) H (R) L (R) E R O G P T T R (R) F I E R (I) F I G T ____________________________________________________ 1 T -0.72222 0 R G E R G E C F I E R F I G T 0 R O G P G P T E R 1 T (R) T (R) ____________________________________________________ 1 R G E R X R (R) F I G T S O F F Y G O T T E D (R) S (R) S (R) R U G O T T Z G U P ____________________ F _____ _____ _____ _______ ___________ (I) R T T R (R) F I G F _____ _____ L O R R F I G T R T ____________________________________________________ 1 R T G P G D T R ____________ C U D F _____ ____ S P C T E R ____________ F _____ Y ___________ ___________ (I) N N N P F I G T R T E R input (English text): Which study used argon, CHNO3 ? output (Sparql query): -------- ------ ---------- ----------- ---------- -------- ----------  | CHNO3 | ---------------  | CHNO3 1 | | 0 R (1) | 0 R (1) | CHNO3 2 | | 0 R (1) | 0 R --------------  |  | 0 o r (1) --------------  | 0 o r (1) ---------------  | 0 f --------------  | CHNO3 3	False
"
 input (English text): Can you list the models that have been evaluated on the FB15k dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FB15k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } output (Sparql query): input (English text): Can you list the models that have been evaluated on the VTAE-1k dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAE-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } output (Sparql query): input (English text): Can you list the models that have been evaluated on the VTAK-1k dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAK-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BJPROF ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs"	False
"
 input (English text): Provide a list of papers that have utilized the DQN-CTS model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-CTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-PixelCNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code WHERE { ?model ?dataset. ?distrib orkgp:HAS_DATASET ?col. ?source ?model. ?model OR ?model. ?model/col. orkgp:HAS_LIMIT ?col. ?source OR ?model. } ?distrib orkgp:HAS_LIMIT ?col. ?Source OR ?source. ?source. ?source OR ?source. DISTINCT ?code.  We are almost there! But when we try to create another graph using DQN-PixelCNN,  we find out that we can only create up to 4 links before the DQN fails.  To make other links we have to use different nodes and then we just add more links to create the graph we want.. input (English text): Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model orkgp:Model; label ?model_lbl. ?distrib ORkgp:HAS_DATASET ?col. ?source ORkgp:HAS_LIMIT ?col. ?source ORkgp:HAS_LIMIT ;... } ?col. ?source ORkgp:HAS_SOURCE_CODE ?code.  We find that we get a lot of failures.. We also notice that we have to go on longer, more involved query for each link, even if only one is used.  Is there not a way to do this in one go without using a separate query class ? I looked for an existing library, and  Google Search 'Linked Dataset Retrieval'  got me excited:  http://dlre.se/ Linked Dataset Retrieval  provides a declarative, declarative library for doing the same thing as  Linked Dataset, with much more flexibility in how it does this. Is that right ?  Can we rewrite our link_sdb.sql  in this way to improve our performance ?  Can we have a link retrieval method that has only 1 or 2 intermediate steps ? To tackle all these questions, I did my own test, and I was pleasantly surprised to get the numbers I was looking for! When	False
"
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Breakout dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Freeway dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a.dataset a_dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari Freeway"") ?eval orkgp:HAS_DATASET orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Can you list the metrics used to evaluate models on the Atari Lynx dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a.dataset a_dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari Lynx"") ?eval orkgp:HAS_DATASET orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Input Data Set (from Atari 6802 Breakout and Atari Lynx datasets) Here's the original source code from the first Atari 6802 Breakout version, which is also the source for the SPSS code to generate the Metrics. #import the following packages from the library import numpy as np, scipy import pandas as pd # import the Matlab function matplotlib.pyplot() # import the sppdf function spd.csv(""../../datasets/A_1.dat,A_2.dat,A_3.dat,A_4.dat"",n_rows=5) to_df = np.random.randn(n_rows,n_cols) with open('C:\metrics.csv', 'r') as f: for line in f: column = line.strip() row = 0 for x in to_df: row, col = x.strip() f.write(column.join(' ', row + col)))) # Generate a Metrics Dataset from the Atari"	False
"
 input (English text): Provide a list of papers that have utilized the Dynamic Coattention Networks (ensemble) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Dynamic Coattention Networks (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Dynamic Coattention Networks (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 To summarize, you can run the DNN model with either single model (for the first query) or ensemble (for the second query). The input data can be in the same manner as the benchmark dataset. You can also add multiple output datasets, or even create a new benchmark dataset.	False
"
 input (English text): Indicate the model that performed best in terms of Macro F1 metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Macro Precision metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" As seen above, NLP-TDMS (Exp, arXiv only) benchmark dataset was the choice given the results we obtained. The Benchmark dataset was created as an excel database, the performance metric was expressed as the best-fit NNLP-TDMS (Exp, arXiv only) metric in the NLP-TDMS Benchmark dataset, and the actual result in the NLP-TDMS (Exp, arXiv only) benchmark dataset was the best-fit NNLP-TDMS (Exp, arXiv only) metric as revealed from the table above.  As a final point on the subject of Metrics, here I want to mention the NLP-TDMS (Exp, arXiv only) benchmark dataset of ArXiv to which I refer, as it really deserves a mention as it is what I used for my own research in the context of the NLP-TDMS (Exp, arXiv only) benchmark dataset. The ArXiv Benchmark dataset in excel format                Table 2: NNLLP-TDMS benchmark dataset of arXiv The table above will show how different NNLLP-TDMS (Exp, arXiv only) metric measured are the best-fitted NNLLP-TDMS (Exp, arXiv only) metric on the Benchmark dataset (ArXiv). The table has a column called Metrics, which in our case, is defined as follows: Metrics  =""Exp""                  { :.4 3e-5e6 4.0 4.5 :.5 3.5e6 3.3 3.7 :.7 3.3 5.6 :.9 3.7 7.3 :.9 4.0 8.6 :.9 4.0 8.6 : :7.3 6.6 4.2 "" ;: :7.5 6.3 3.2 } As we can see, we came down to NNLLP-TDMS (Exp, arXiv only) metric by comparing the top 10 Metrics from the top 10 NNLLP-TDMS (Exp, arXiv only) metric on the Benchmark dataset (ArXiv). As the table above shows, on the Benchmark dataset for ArXiv, it really shines (NNLLP-TDMS) metric which is an Exact"	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Pong dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pong"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Tennis dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"--------------------------- 1.0 0.0 2.8 0.0 3.3 0.0 3.9 0.0 4.2 0.0 4.9 0.0 7.3 0.0 7.9 0.0 10.0 0.0 10.4 0.0 11.4 0.0 11.5 0.0 12.3 0.0 12.5 9.1 13.4 13.8 --------------------------- 1.0 8.3 8.3 8.3 4.4 4.9 4.5 5.2 --------------------------- 1.0 0.0 7.3 7.6 7.9 2.5 2.9 3.4 3.5 3.6 3.7 4.5 4.9 4.6 5.3 5.6 5.6 6.2 6.4 7.5 7.8 8.3 8.3 8.4 1 2 3 4 5 6 7 8 9 10 11 ------------------------------------------------------- 4.6 7.9 6.1 6.2 6.7 7.0 7.1 7.3 7.5 8.1 8.5 8.7 8.8 10.1 10.4 10.9 --------------------------- 2 4.7 10.8 11.1 12.9 13.1 --------------------------- 3 5.8 15.0 16.7 17.6 18.1 ------------------------------------------------------- It's not always the case this works (you will note these are not necessarily in the top 10). Also, not all datasets are in the ""top"" list, but it's a good representation of the most relevant data sets in each dataset with respect to each other. If you're a Java programmer or have a database that makes it possible to use these data elements as tables, then you can start to use this data interchangeably with the same results. This isn't an attempt to replace any of your existing data structures, but simply to illustrate how you might put these data elements in the same structure with the additional layer of abstraction that is SPSS."	False
"
 input (English text): What are the metrics of evaluation over the DCASE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the DuIE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ROWID, TITLE FROM t1 WHERE t1.name='a' AND t1.name='b'; We will first construct the dataset, select all the rows in it, and construct a table with the dataset content. Then we will work on the benchmark-data which will be a single column (named ?dataset ) of the DCASE dataset. The benchmark is based on the test setup with the data. In order to achieve the benchmark, we first need to define some	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Cheetah, run (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } You might want to add additional arguments to your query below, before running the script. output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark dn, ?benchmark rdfs, ?eval orkgp:HAS_DATASET ?eval; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } You might want to add additional arguments to your query below, before running the script. output (Sparql query): select output, dn, rdfs, eval, metric as metric_lbl, dataset as dn, rdfs, metric_lbl as rdfs from output; if rdfs == null, run the script without output dn, rdfs; if metric == null, run the script without metric rdfs; if eval == null, run the script without eval dn, rdfs; if dn == null, run the script without dn rdfs; You might want to add additional arguments to your query below, before running the script. output (Sparql query): SELECT DIMENSION (result), metric, rdfs, metric_lbl, dn, rdfs as rdt from run( ?"	False
"
 input (English text): What are the models that have been benchmarked on the Reuters RCV1/RCV2 German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 orkgp:HAS_DATASET ?dataset. If you'd like to get immediate feedback when my code runs, join my public Gitter community and be notified when it delivers.	False
"
 input (English text): What is the most common drug in the studies?
 output (Sparql query): SELECT ?drug, ?drug_labels WHERE { orkgr:R155621 orkgp:compareContribution ?contrib. ?contrib orkgp:P37578 ?drug. ?drug rdfs:label ?drug_labels. } ORDER BY DESC(COUNT( ?drug)) LIMIT 1
 input (English text): What types of nanocarriers do have therapeutic effect?
 output (Sparql query): "	"SELECT ?types_of_nanocarriers, ?types_of_nanocarriers_labels WHERE { orkgr:R144364 orkgp:compareContribution ?contrib. ?contrib orkgp:P41740 ?types_of_nanocarriers. ?contrib orkgp:P41743 ?therapeutic_effect. FILTER( ?therapeutic_effect = ""T""^^xsd:string) ?types_of_nanocarriers rdfs:label ?types_of_nanocarriers_labels. } "	SELECT ?drug & ?drug_labels WHERE  { orkgr:R155621 orkgp:compareContribution ?contrib. ?contrib as ?drug, ?drug_labels. } ORDER BY DESC(COUNT( ?drug)) LIMIT 1 input (English text): Is nano-crystalline nanocarriers more toxic than nanocarriers containing polycrystalline polymers ? output (Sparql query): SELECT ?drug, :drug_labels. ?drug Input We are using a new data source: The R database, version 9.0, released on 15 October 2017. I have only found this and its previous versions at the time I wrote this post. They can be obtained from the R website. I haven't tried it with earlier versions. Let's add a new dataset. Select the data file which contains the standard reference drug (SAR) in the drug file we created earlier, add a new input dataset, and drop the old one. R:  select * from data.drug in the output data.drug.drop_if_missing; A word about the orkgr function: orkgr  (analogous to the standard OR operator) allows the transformation of lists so we can get a function for a particular data type from any other list, to produce the same type of output. Let's create the function from all the data types, and add it to the ORkGr matrix, so it reads as follows:       x = list(..., (3, 4, 5, 7, 8, 9, 10, 11, 12, 13))       add(x, ORkGr(3,[1], 2), ) Output A word about our function. We have a list, named x, which contains a list of lists. We read the first list of x from data.drug, and then transform the second list into list x. We want to find the element in x that gives the greatest performance in our test (it will be true if the number 3, 4, 5, 7, 8, 9, 10 and 11 in the list all have the same weight). With the new list, we look for the x in it where the weight is a multiple of 10. If we hit this x we have found the list element, which gives a best performance and is the number 3. We then add this 3 and we	False
"
 input (English text): Which model has achieved the highest ROUGE-2 score on the AESLC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CL-SciSumm"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT result, metric_lbl.dataset in ( SELECT column_name, value from cls_scisumm_test If you have a list of metrics you wish to compare, then the queries of this sort are sufficient for most operations. If you want to compare a set of metrics at a certain time, then you will need to include a different table. Here is the CL-SciSumm benchmark data in a different table. Here is the complete cls_scisumm_test.csv example.  The final example, Clocks_Performance.scl, is the most complex sample and the bulk of the work for the test. This sample contains all of the results of the following three queries. In addition, the output of the query functions was written out for the convenience of the reader. SELECT timefrom_time, interval from Timezone.us_latitude.seconds	False
"
 input (English text): Provide a list of papers that have utilized the Unsupervised NMT + Transformer model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Unsupervised NMT + Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Unsupervised NMT + weight-sharing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT d.code FROM dl_nmt_transformer_prelim( orkgp:HAS_DATASET:code WHERE { ?model orkgp:Model } ) a d  output (Sparql query): SELECT d.code FROM dl_nmt_transformer_prelim(orkgp:HAS_HUMANIZED ?code & ?model_lbl) a d  output (Sparql query): SELECT ORGANIC_DISTINCT( ORGANIZED_PUBS_LAST_ORDER_INPUT, ?code) FROM dl_nmt_transformer_prelim( andkgp:HAS_DATASET ?code, ?labelled.org) a d  output (Sparql query): SELECT ORGANIC_DISTINCT( ORGANIZED_PUBS_LAST_ORDER_INPUT, ?model) FROM dl_nmt_transformer_prelim( orkgp:HAS_HUMANIZED ?model AND ?labelled.org ) a d For each dataset, output the number of papers using that model, the number of papers using the 'unsupervised' model compared against the benchmark set, and the number of papers using unsupervised as compared to supervised models in the benchmark set. input (English text): Include a list of papers that use unsupervised weights and code ? output (Sparql query): Select number of papers that use unsupervised weights in any benchmark, and how many of them use supervised weight-sharing ? output (Sparql query): SELECT ORGANIC_DISTINCT FROM dl_nmt_transformer_prelim( orkgp:HAS_DATASET ) a d output (Sparql query): SELECT ORGANIC_DISTINCT( ORGANIZED_PUBS_LAST_ORDER_INPUT, ?code) FROM dl_nmt_transformer_prelim( andkgp:HAS_DATASET ) a d output (Sparql query): SELECT ORGANIC_DISTINCT( ORGANIZED_PUBS_LAST_ORDER_INPUT, ?model) FROM dl_nmt_trans	False
"
 input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential MNIST dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Unpermuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Permuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl FROM { ?model orkgc:Metric } AS ?db GROUP BY orkgc ORDER BY DESC(!name ?value ) LIMIT 1 SELECT ORGCP_REC ORGCP_DATA ORGCP_SURVEXIST ORGCP_EASE DESC LIMIT 1 Output (Sparql query): SELECT ORGCP_HAS_PARAMETIC ORGCP_VALUES ?model_lbl. ?metric_lbl. { ?metric ?x. { ?metric ?y. ?value ?metric ?z. ORGCP_PARAMETIC ?x. ORGCP_VALUES ?y. ORGCP_PARAMETIC ?z. } } ORDER BY ?value DESC LIMIT 1 Result set: SELECT rdfs.name, rdfs.value, rdfs.metric, rdfs.is_equal_error as label, rdfs.metric_lbl, rdfs.label, orkgp, orkgp_data, orkgp_survexist, oragg, org, org_data, org_survexist, oragg_data, oragg_survexist, orga, orga_data, orga_survexist from orga that have an object parameter with ?label and ORGCP_PARAMETIC, which should be a list of columns for which value is the same for both ORGCP and ORGCP_PARAMETIC columns that has one value. ORDER BY DESC( ?label ) LIMIT 1 The model that has achieved a higher Permuted Accuracy score has the label ORGCP_PARAMETIC. A model score of 300 or higher means a model with higher performance. You can do additional analysis on the results. The table below contains the names for each model that achieves a Permuted Accuracy score higher than the reference model. You can download the model graph by using this form: < ?xml version=""1.0"" encoding=""utf-8"" ?> <node version=""1.1""> <model id=""OCAP_0""> <label name=""orkgp"">OCAP_0</label> <metric id=""ORGCP_PARAMETIC"" scale=""1"" name="""	False
"
 input (English text): Can you list the models that have been evaluated on the SciERC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the SciTLDR dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" In this example, all tables have been modified to have some columns containing ""SciERC"" in the id column and ""sci"" is the unique identifier of each subject. The same applies to the columns for the datasets using the same parameters. You can then use a custom SPSS functions to evaluate models for the model-specific datasets. 2.5.3. Evaluation of the model As stated in the previous section (Section 3.3), SciNet is a large-scale network machine learning library for statistical inference and machine learning applications. We will study a simple optimization algorithm to determine if we can use SciNet in a research study. To evaluate models for the SciTLDR dataset, we will also use a simple optimization algorithm, the logistic regression model of Hoppenberger et al. [8], to predict the best value of a variable in a class-level dataset in an independent (independent sample) experimental design. 2.5.4. Model Selection and Testing To evaluate the Logistic Regression model, we will use two experimental design techniques. The first is to observe the data with a different dataset where the model is shown to not improve the performance. This is done using a linear mixed effects analysis model (also called mixed models). We select the model with the highest predictive precision (with all factors equal to 1.0) in this experimental design. The second method also requires training and testing with the same data but with different logistic regression model. We combine these methodologies to obtain two independent datasets where the same logistic regression model is used for predicting the model based on the values of two different variables. The independent data from each sample is presented separately in a table. As of this writing, the third experiment is ongoing. Please bear in mind that we will be using data collected during the second set of experiments as well as the first experiment. The third method and experimental design also have the advantage of keeping the models that were tested successfully (or, equivalently, failed to improve) out for the duration of the data collection. As of the time of this writing, we have made an initial decision to use the logistic regression model for the analysis of the model-level, large-scale data collection (Section 2.4). We use a logistic regression model to predict the best value of a variable in an experimental design consisting of two independent samples. The first set of experiments, with a different dataset without a corresponding model"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 You can use the following syntax to list the metrics that are used in the benchmark. If your model has an optional metric, the metric is implied from the name of the model. If you are working on a model for which you also provide a specific implementation, the implementation is noted under a * (asterisk) if the implementation provides that metric. You can also list multiple metrics, as shown in the second example. The example output would look like the text below. input (English text): list the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset output (Sparql query): For more information, see http://pprint.org/p ?pb=201612160129.	False
"
 input (English text): What is the best performing model benchmarking the IMDb-M dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the IMDb-M dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?model_lbl  FROM IF NOT  AND NOT  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  OR    ?metric_lbl  . select model.model from model.model where model_lbl is null) SELECT DISTINCT ?model ?model_lbl * 2.00  FROM IF  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  ORDER BY DESC( ?value) LIMIT 5 ) ) output (Sparql query): SELECT DISTINCT ?model ?model_lbl * 2.00  FROM IF  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND  AND AND ?metric_lbl  ............................................... DISTINCT!model_lbl.result_dataset  .  .........................	False
"
 input (English text): List the metrics that are used to evaluate models on the Scholarly entity usage detection benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Scholarly entity usage detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Scholarly entity usage detection dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Scholarly entity usage detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?dataset ?dataset_stats ?dataset_score, SUM ( ?dataset_stats ) + 1AS SUM ( ?dataset_stats )AS AVG( ?dataset_stats ) AS Metric, SUM ( ?dataset_stats ) + 1AS SUM ( ?dataset_stats )AS Scores, SUM ( ?dataset_stats ) - 1AS SUM ( ?dataset_stats ) - N ( ?dataset_stats ) * 100 AS Metrics, COUNT ( ?dataset_stats ) DESC ?dataset : Select the query that selects the dataset to collect metrics. If you are only getting the metrics for a select query, then enter "" 1"" for the first value of ?dataset_stats. : Select the query that selects the dataset to collect metrics. If you are only getting the metrics for a select query, then enter for the first value of. ?dataset_stats : Input a dictionary reference that maps metric values from 'Scholar' to their corresponding metric_value keys. You may choose 'Scholarly entity usage detection' that matches what the database needs. : Input a dictionary reference that maps metric values from 'Scholar' to their corresponding metric_value keys. You may choose that matches what the database needs. ?dataset_score : Input a dictionary reference that maps metric_score values from 'Scholar' to their corresponding metrics. You may choose 'Scholarly entity usage detection' that matches what the database needs. ?dataset_stats : Input a dictionary reference that maps _somewhat_ similar metric_values from 'Scholar' to metric_score keys. ?dataset_scoring : Input a dictionary reference that maps _somewhat_ similar metrics_score keys from 'Scholar' to metrics_score keys. ?eval : Input a dictionary reference that maps a string to a dictionary definition that is used to compare scores across datasets. This is optional -- the default is ""Scholarly entity usage detection"". ?metric : Input a dict with the following keys. If you are only getting the metrics for a select query, then enter ""1"" for the first value of ?metric. : Input a dict with the following keys. If you are only getting the metrics for a select query, then enter for the first value of. "	False
"
 input (English text): Can you list the models that have been evaluated on the JNLPBA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""JNLPBA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the MultiNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl FROM MultiNLI ?( ?dataset a orkgc:Dataset) LEFT OUTER JOIN ?model_lbl on orkgc:Model.dataset_lbl = MultiNLI.Dataset.dataset_lbl ORDER BY ?model. ?model rdfs:label ?model_lbl. OPTIONAL { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } } The above example shows several queries that can be used to perform the analysis. The following is an example of one of those queries. It takes the data set of the Model Selection Benchmark and adds it to the result set. We do not filter the dataset so any model can be selected if it meets the criteria. Example 8.7 SELECT * FROM ( SELECT DISTINCT ModelName | ?Dataset ?dataset_lbl. ?model rdfs:labels | WHERE ModelName IS NULL ) * WHERE ModelName IS NULL and ?dataset_lbl. For each model in both the dataset and the model_lbl, it retrieves its evaluation statistics. See the next example. The input to the SELECT query is ""model_lbl"" for Model L. The output is a list of the evaluation statistics for all the models that were used to select model L. Example 8.8 SELECT * FROM ( SELECT DISTINCT evaluator | ?ModelL ModelName L) WHERE Evaluator IS NULL For each model in the model_lbl, it compares its evaluation statistics to those of the corresponding model. We then pass the values of the evaluation statistics to the model_lbl query. Each expression in the query is evaluated separately, then the result is written to a list, which returns a variable called model_lbl (the string in the output is a string expression). In this example, the output is a single string with the output name model. This string should be unique for the model in which each data point was evaluated. The following example illustrates the use of the select statement's comma operator to group several subqueries into one query. The input to the SELECT query is the list of labels for each model at each evaluation stage. It also has a list of the results of the two evaluators at each evaluation stage. Output"	False
"
 input (English text): Could you provide a list of models that have been tested on the 200k Short Texts for Humor Detection benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT * FROM ?model in a WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } SELECT STATEMENT_RULE (a) AS d(1), DATE (c) AS dm(2), SUM (s) AS s_sum, dm (s) AS dd{1..2}*(dt{1..2}}+yyyy)/(s_sum) + (yyyy) + (dm(s)) FROM ?model in a input (English text): Select the output for the 200k Short Texts for Humor Detection benchmark dataset ? Output (Sparql query): SELECT a.test.rdf.label.dataset, rdfs.model.label, dd{1..2}, orkgc.model.rdf FROM orkgp.HAS_DATASET WHERE a.criterion and dd{1..2}*(1-criterion and dd{1..2}*(criterion and dd{1..2}*(criterion)))) Input (English text): List the metrics that were used to evaluate the models on the 200 (or 500k) Short Texts for Humor Detection benchmark dataset ? Output (Sparql query): SELECT a.criterion.rdf.label.dataset, rdfs.model.label, dd{1..2}, orkgc.model.rdf FROM orkgp.HAS_DATASET WHERE a.criterion and dd{1..2}*(1-criterion and dd{1..2}*(2-criterion and dd{1..2}*(criterion and dd{1..2}*(criterion and dd{1..2}*(criterion and dd{1..2}*(criterion and dd{1..2}*(criterion and dd{1..2}*(criterion and dd{1..2}*(criterion and dd{1..2}*(criterion and dd{1..2}*(criterion and dd{1..2}*(criterion and dd{1..2}*(criterion and dd{1..2}*(criterion and dd{1..2}*(criterion and dd{1..2}*(crit	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the MNIST dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Sequential MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	------------- -1 (5|5)|4 (0|2)| 2 4/22: (1)(1)|3, (3)(1)|0 (0)(1)|0, (3)(1)|1, (4)(2)|0 (0)(1)|0, (5)(2)|1, (4)(2)|4, (5)(2)|0 (2) 4/22: (3)(1)|4, (3)(1)|0, (3)(1)|0 (1)(2)|2, (3)(1)|0, (3)(2)|2, (3)(4)|0 (0)(1)|1, (4)(2)|0 (0)(1)|0, (5)(2)|0 (2) 4/22: (4)(0)|1, (4)(0)|0, (4)(0)|0, (4)(0)|0 (2|1)|1, (4)(0)|0 (0)(1)|1, (4)(2)|0 (0)(1)|1, (5)(2)|0 (2) 4/22: (5)(1)|1, (5)(1)|0, (5)(1)|0, (5)(1)|0 (2|1)|1 (2|3)|2 (2|3)|1, (5)(2)|1, (5)(2)|0 (2|1)|1, (4)(2)|1 (0)(1)|1, (4)(4)|0 (1)|1, (5)(4)|0 (1)|1, (5)(4)|4, (5)(4)|0 (1)|0, (5)(6)|0 and (5)(6)|0 (2), (1,(4,5,-2) (0,4,2,-1) (1,2,0) (0,4,0,-1) (1,4,1,-1) (0,4,2,-1)) (2,(5,-2) (0,4,2,-1)) (2,(5,-2) (1,4,1,-1) (1,4,1,-1) (3,0,3), (4,1,-3)) (2,(5,-2) (2,1,0) (2,4,1,-1)	False
"
 input (English text): Provide a list of papers that have utilized the Ours: cross-sentence ALB model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ours: cross-sentence ALB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the CRF with sentence expansion model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with sentence expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?DISTINCT code FROM ?data WHERE ?count = ?count; If there is no output file the output is automatically saved by the SAMP code analyzer. Note, that the table ""output"" is a separate table when using the command above. Output is provided in the same format as provided in the ""output"" output in the SAMP code analyzer. The SAMP code analyzer supports SAMP in addition to JAGS. The main advantage of using the SAMP in front of JAGS is that it has its own output file, i.e. it is possible to make the output file and use it in the JAGS code analyzer. If the output file is saved in the output directory, the JAGS code analyzer is able to perform the JAGS analysis. In the ""samp-test"" output a text file with a list of the output paths (where the file exists and can be accessed via the ""-p"" or ""--path"" options) is generated. This also shows the location of the SAMP code files. The ""samp-test"" output is the same as shown above for the ""out-data.txt"" output file. That is, the output path for the SAMP code files in the output directory is: input (English text): Provide a list of papers that have utilized the Ours: cross-sentence ALB model (and exclude the one that used the SAMP code analyzer). If these papers report their results using LBL or JAGS: RBL and not SAMP: LBL and JAGS: RBL the output file will be the JAGS code analyzer (using the SAMP code analyzer) or SAMP code analyzer for LBL and JAGS (using the SAMP code analyzer for LBL). output (Sparql query): SELECT ?DISTINCT ?code WHERE * > ?dataset. ?cont orkgp:HAS_DATASET ? ?benchmark ? ; output (Sparql query): SELECT ?DISTINCT ?code FROM ?data WHERE * > ?data ?cont orkgp:HAS_DATASET ?benchmark ? ; output (Sparql query): SELECT ?DISTINCT ?code FROM ?output WHERE * > ?data; The output file ""output"" in the S"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the NYT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset NYT-single?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT MetricScore * MetricValue FROM ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER ( ?dataset_lbl > ?metric_lbl. ""NYT"") ?benchmark orkgp:HAS_DATASET OR kgp:HAS_EVALUATION ?eval. MetricScore rdfs:input ?dataset_lbl rdfs:output ?dataset_lbl. MetricValue rdfs:input ?model_lbl. OR kgp:HAS_BENCHMARK ?benchmark. ORkgp:HAS_MODEL ?model. MetricScore rdfs:input ?metric_lbl. rdfs:output ?metric_lbl. ORkgp:HAS_VALUES ?value. OPTIONAL OR kgp:HAS_MODEL ?model_lbl. OPTIONAL OR kgp:HAS_METRIC ?metric. ORkgp:HAS_MARK ?benchmark. OPTIONAL OR kgp:HAS_MODEL ?model. OPTIONAL OR kgp:HAS_VALUE ?value. OPTIONAL ORkgp:HAS_METRIC ?metric. OPTIONAL OR kg"	False
"
 input (English text): List the metrics that are used to evaluate models on the SciERC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the SciTLDR benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark ORkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark ORkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark ORkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark ORkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALU"	False
"
 input (English text): Can you list the models that have been evaluated on the WMT2016 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the WMT2016 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 In order to visualize all the results in the WMT2016 paper, I've used D3.js (which can also be downloaded with the source code) and then have used my own graph.js. The Graph.js uses the HTML5 canvas and its content is automatically populated with the data that it receives. In the diagram below, I've shown the result data from the test dataset. The result of the test dataset. I've also added the standard deviation to the data to show how the difference between the model predictions and the paper predicted results can be detected, i.e. D3.js uses the same canvas and the content is the same In the following example, the benchmark results are shown. D3.js uses the same canvas and the content is the same	False
"
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the Oxford 102 Flowers benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford 102 Flowers"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ROWS ?model ?model_lbl ?model { ?metric a:Metric; rdfs:label ?metric_lbl. ROWS } ORDER BY DESC( ?metric) LIMIT 1 LIMIT 1 GROUP BY ?model..group_name In addition to the model predictions, there are many other metrics that could be used when performing model training: A. Training time. B. Testing time. In addition to the model predictions, various other metrics are calculated. Each metric represents one particular aspect of the performance in the model. The metrics are described in detail in the'measuring the training of high-quality models' section of Section 1.2. If a metric shows a strong correlation with performance of the model is not robust to noise, then the training should be stopped. If one metric is used to measure the performance of the model, then the performance of all metrics should be averaged. The training time is one metric but the test time (both the actual and the predicted accuracy) are two. Here we show the training of the model using the internal dataset. In order to compare the performance of the tested model to the performance of the internal dataset, two sets of data, one with an external datastore and one with an internal dataset are produced. The data are processed in exactly the same fashion as the external dataset is processed. The internal dataset is obtained by using the ""test"" option available during the training of the model. This procedure is explained in detail"	False
"
 input (English text): What is the best performing model benchmarking the RotoWire (Relation Generation) dataset in terms of count metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""count"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset RotoWire (Relation Generation)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 The text entered is the RotoWire (Relation Generation) dataset data set. A dataset can be composed of datasets with each dataset comprising one or more model R1 and R2 classes. For each of the top five most popular model parameters, we will run a separate query for each dataset's respective pair of parameters and for the top five most popular model model parameters. In the top five results from each benchmark, we will compare the metrics corresponding to each metric for each dataset's R1 (top model) and R2 (model) parameters. In this case, the metrics will be used to select the top five datasets for each ranking metric. However, the metrics won't be used to generate the top 25 most popular model parameters from each of the datasets. Instead, the metrics are used to predict a single label for each of these datasets as described above.  Input data set: CIFAR-10 CSV file. Model parameters: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 In the above summary results, we found that each of the top five datasets have the same model, r1, along with the same metric ( a or kg, as the case may be). This is important to understand when we are comparing a model with another model with some parameters that differ. In our case, the default metric is 1 for both R1 and R2 because the default metric is the sum of the weight or sum of the squares of the squared distance. This metric is used to calculate the best performing model based on the count metric or the total count metric. The metric will also be combined into a single value for the metric and will not be reused in each model call. So, to avoid duplication, the weights or the squared distance parameters will need to be customized when the respective metric is used in the respective model parameters. The metrics which have very close overlap are: 1.6, 1.4 and 1.6. The following metrics are highly correlated (Rio2 is the least correlated!) between the RotoWire, Roto1, RotoT and Roto12 datasets: 1.6, 1.4 and 1	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Reacher, easy (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score R DFS:label ?retrofit ?retrofit_lbl. FILTER (str(retrofit_lbl) = ""retrofit"") { ?retrofit orkgp:HAS_METRIC ?retrofit_lbl. } output (Sparql query): output (Sparql query): SELECT score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score, score R DFS:label ?retrofit_lbl. FILTER (str(retrofit_lbl) = ""retrofit"") { ?retrofit orkgp:HAS_METRICAL_RATIO ?retrofit_lbl. } Example 4: Benchmarking a model on two datasets with different test populations To benchmark a single model on two different datasets the output has to be split into two different input files. Note: The Reacher data are in the'results/ReacherTests.txt' file in the data directory. The Easy-control-100k dataset is in the'results/EasyTests.txt' file in the data directory. The model is not benchmarked by default on the Reacher-easy and Easy-control datasets because most of the features of the Easy-control-100k dataset are not suitable for benchmarking. Here is a test dataset of Reacher, easy-control-100k with only the most important tests: select score from dp_test_all.tables where ( ?test_list_tables = ""EasyTests"" ) and ( ?test_list_dataset = ""ReacherTests"" ) and (! test_list_list = ""EasyTests"" and! test_list_data = ""ReacherTests"" ) Note: The first dataset is the Re"	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Assault dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl ) = ""Atari 2600 Assault"") ?benchmark ORkgp:HAS_DATASET ?dataset; ORkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } "	False
"
 input (English text): Where can I find code references in papers that have used the DQN+SR model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DQNMMCe+SR model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQNMMCe+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQNMMCe+SR"") ?benchmark ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } The DQN+SR model was designed over 35 years ago for performance-critical applications such as network analysis (e.g., the DQN+SR model is named after the first performance-critic who pioneered its use, Bruce Schneier of Stanford University). The DQN+SR model uses a novel approach to a supervised gradient search, referred to as the ""multi-layer classification"" (MR) approach, and takes an approach known as the ""deep-embedded"" (DER) approach. The DQNR model was designed to be compatible with existing classification databases (e.g., the SCOPULO), and has been adapted for use in new high-performance classification databases. In this section we describe the structure and algorithms of the DQFN, DQNAM, and DQNMMC models, and address some of the remaining open questions. Structural characteristics of the DQFN: The DQFN is a hierarchical, deep-level, large-scale deep neural network classifier. Each node consists of a single training unit, with the resulting model performing a single step in the gradient descent algorithm. A single training step computes an objective function to estimate a score for each training sample—an objective is a function that, given an X, predicts how likely a Y response is to be given an X, in the absence of information about X. A single-point objective function is an objective function for each point in the network. For purposes of optimization, the objective function is the one that minimizes the error of the given samples. The DQFN model is inspired by work in Machine Learning and Deep Learning (MLD) that proposes deep learning and network architectures with nested hierarchies, such as a hyper-layer with ""hidden"" layers. The DQFN model is designed with these concepts in mind to minimize errors associated with training and testing the model."	False
"
 input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT * FROM RDFTABLE; Return all the RDF Tables by Using DMLDB To return all the RDF Tables, you	False
"
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT29 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT TOPN ( ?name OR ?model_lbl). ?value FROM (SELECT ?model ?model_lbl) ?dataset_lbl. ?dataset WHERE { ?dataset_lbl. ?dataset { ?dataset. ?dataset. ?dataset. ?dataset. } ?dataset_lbl. ?dataset } ORDER BY ?name OR ?model_lbl. ?value LIMIT 1 } output (Sparql statement): SELECT name, ?model, ?metric_lbl, ?value FROM F1_metrics GROUP BY name; I've done a lot of work analyzing the datasets in NYT30, and it sounds like we have plenty of data and that is an example to run it on using pymd: 1. Create a PYMDB dataset 2. Create a pymd-based DAG that does all of the job in one step 3. Create a pcm/spark backend to extract the data from DAG, using SpatialDB 4. Set your system's default values for the database connection, create that PYMDB database structure and start running the tests 5. Get the result from the pymd-based DAG 6. Run some queries against the data in the pymd-based DAG 7. Start a postgres/SQLite cluster and go have fun with the data I should also say that I've been using DAGs for a while before getting into database systems and now have a feeling about them. From everything I've observed, the best tool in the world for a lot of the work you can do is pymd.  My colleague  Tom is also building on that foundation. I wish more people could take that approach. But if you take what I've said as an argument for some other database system, we probably have too much data to store in one database system. In my next post, I'll tell you how to handle that. Stay tuned.	False
"
 input (English text): List the code links in papers that use the MEMEN (ensemble) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the MEMEN (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	--------------- ----------- MSE (single model) paper code ------------------------------- ----------- 1. MSE 2-core - - 3. FEM - - 4. MME - - 6. MME - - 8. MSE - - MSE 1. MSE 2-core - - 3. FEM - - 4. MME - - 6. MME - - 8. MSE - - MSE paper code --------------- ----------- 1. MSE 2-core - - 3. FEM - - 4. MME - - 6. MME - - 8. MSE - - MSE paper code --------------- ----------- MSE 1MSE MSE 1 MSE 2 MSE 2 MSE 2 -2 2 4 2 MSE 2 MSE 2 -2 MSE MSE 2 MSE 4 MSE 4 6 1 MSE 2 MSE 2 -2 MSE 7 8 8 8 2 2 6 2 6 2 -2 12 12 8 6 3 10 6 10 6 -2 16 16 12 9 14 MSE 2 MSE 40MSE MSE 2 MSE 7 3 10 MSE 10 MSE 5 3 12 MSE 17 MSE 10 2 18 MSE 18 MSE 15 8 20 MSE 20 MSE 20 2 2 21 NNN 22 MSE 25 MSE 35 14 12 MSE 37 15 15 MSE 15 15 18 MSE 15 13 20 NNN 18 MSE 20 MSE 25 16 12 MSE 28 MSE 17 15 14 NNN 35 MSE 14 NNNN MSE 18 17 21 NNN 31 MSE 15 NNNN MSE 20 15 17 MSE 20 MSE 25 16 20 NNN 38 MSE 15 NNNN MSE 25 -1 17 NNN 42 MSE 15 NNNN MSE 26 -1 16 NNN 44 MSE 18 NNNNN MSE 27 -16 17 MSE 47 MSE 17 NNNNN MSE 28 -21 18 NNN 54 MSE 18 NNNNN MSE 29 -22 VENOM 16 MSE 20 MSE -27 20 RNNs RNNs are a new feature in the research workflow. A RNN is essentially an array of connected layers. After completing the post-processing step of training a RNN, you can use the RNN function in the R script that comes with the paper to generate the outputs. For details, see the tutorial tutorial-datasets-rnn (https://en.wikipedia.	False
"
 input (English text): Provide a list of papers that have utilized the CAIT-M-36 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-M-36"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the MMV TSM-50x2 model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMV TSM-50x2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" select ?model_lbl. FILTER (str( ?model_lbl) = ""MMV-TSM-50x2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. If you do not specify a filename for your output, we'll extract your output from a Postgresql query like (select 1,2,3 from mytable where id in ([1,2,10],[3,9,18])). For example, if you are a researcher, you may want to keep track of your publications through a repository. Now, let's look at several ways to construct and output datasets. These are the techniques we'll be using. Creating Your Own Dataset Here are a couple good resources to get you started: We have chosen to define a simple model using the R package rmdb, followed by several other packages that enable us to use the model to calculate our statistical results. If you are interested in contributing to building your own dataset, please join the R Open Source community on Google+. If any of this sounds like more work than you're ready to do, please consider hiring an undergraduate or graduate student to help. Writing a Dataset In order to implement the framework, we'll be writing a R program that takes an input of English text and outputs the set of publications we found. But before taking on any such task, let's define what we really want: We want to produce a corpus of English text, in order to demonstrate that our model can extract meaningful properties of the output document. To do so, we'll create a dictionary of all of the sentences in the corpus that contain a phrase, phrase-noun, or word(s) matching one of the criteria above. As with any dictionary, you can keep these for reference later on. The key thing to notice in our new Dict[] is that there is no need to specify any particular structure when writing our dictionary. It should merely be a list of strings, and we will be able to add more strings to this list when it turns out that the model might need them. Another thing to notice is that we want to add strings matching our criteria into this list. For instance, in order to"	False
"
 input (English text): Which model has achieved the highest Accuracy score on the Yelp-2 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" This query allows you to query not only for the Metric you want but for other metrics as well. In fact, it's possible to drill into other metrics: This is the query I used to achieve the highest Accuracy score on Yelp-2 data set. The parameters it produces are: A orkgc:Dataset is exactly like the first query you saw in that previous section, except that now the "" ?dataset "" parameter is the dataset: Dataset The next three parameters are the same as in the second query: a : orkgc:Dataset is the dataset, b : orkgc:Dataset is the dataset_name in my demo data set, and c : orkgc:Dataset is the dataset's label. Now all you have to do is modify the previous query to drill into the same metrics This query allows you to drill into the same metrics as well. In fact, it's possible to drill into other metrics: This is the query I used to achieve the highest Accuracy score on Yelp-5 data set. The parameters it produces are: The result looks like this: You now have very good knowledge of how to query the Yelp-2 dataset via this query. You might also like to explore how you could use this query to drill into metrics from the Yelp-5 dataset. Diving Deep What if we wanted to learn more about one particular object (the Org-M2 dataset) and have a query that lets us easily extract a particular metric as a list ? In that case, you could create our own query like this: Here, we've expanded on the previous query in order to have more flexibility. We now have the option to specify two or more variables in one query, with the same query name. In order to drill in into individual metrics from the dataset using this query, we'll need to know which columns to drill into. Let's use that insight to identify the columns we would find useful. The result of this query is an object called metadata. You can retrieve the metadata object as a column of a list which we'll call our list of metrics. When it comes to defining the desired metrics for our query, the main thing to keep in mind are the two columns that you have specified below: The Org-M2 object has three columns: Metric A has the same parameters as the default measure of the dataset, except that it's a type variable. In"	False
"
 input (English text): What is the best performing model benchmarking the WMT2016 English-Romanian dataset in terms of BLEU metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Input text: BLEU score metric output:  The output above displays a plot of the BLEU metric (as percentile). The highest scoring model in 1st percentile is WMT2016 English-Romanian (the 3rd percentile was also WMT2016 and 2nd percentile was WMT2016-Romanian). The second, 3rd, and 4th were Chinese samples and the lowest scoring model (5th percentile of BLEU test metric) was TELUS and the highest scoring model (6th percentile) was Jibo. The graph above shows the BLEU score (as percentile) of each model, on the x axis, and the WMT2016 (as percentile) score of each model, on the y axis. The second best performing model is WMT2016-Romanian, in the first percentile. Next is Chinese sample, and last is TELUS. The graph above shows the BLEU score (as percentile) of each model, on the x axis, and the WMT2016 (as percentile) score of each model, on the y axis. Here, the highest scoring models are WMT2016-Romanian and TELUS. The second best performing model is WMT2016-Romanian, in the first percentile. Next is Chinese sample, and last is TELUS. We see that only 4"	False
"
 input (English text): Provide a list of papers that have utilized the Sarsa-ε model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Sarsa-ε"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the SAC model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ? model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl ) = ""Sari-ε"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the SARI model and include the links to their code ? output (Sparql query): Ganesh Khurana input (English text): Provide a list of papers that have utilized the GIC model, and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ? model a gcu:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl ) = ""Ganesh-ε"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the GIC model, and include the links to their code ? output (Sparql query): A-Frame input (English text): Provide a list of papers that have utilized the A-frame model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ? model a fb:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl ) = ""A-frame"" ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL"	False
"
 input (English text): Provide a list of papers that have utilized the MEMEN  (single model) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the MEMEN model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT MENDEL ( ?model and ?dist in ?dist_lbl ) AS ?model, VARCHARIANS( ?numberofnodes) AS ?numberofnodes, LENGTH( ?numberofnodes ) AS num ?dist; output (SQL statement): SELECT MAX(num*5, num)/5, num^(num,5) AS num AS numberofnodes, (3:num)/5, num^(num,5) AS num * num/5, num^(num,5) AS num ^ num as num ; output (text): SELECT ?model AS model, ?dist AS dist_lbl., SUM(num= ?dist ) AS num_total FROM ?dist_lbl JOIN ?model m ON M. ?dist. m_type = ?dist_lbl. M_TYPE ; input (English text): Provide a list of papers that have utilized the MEMEN and include the links to their code ? output (Sparql command): SELECT MENDEL ( ?model, ?dist in ?dist_lbl ) AS ?model, VARCHARIANS( ?numberofnodes) AS ?numberofnodes, RANK(Dist.count) AS dist_rank, LENGTH(Dist.count) AS num_total FROM ?dist_lbl JOIN ?model m ON M. ?dist. m_type = ?dist_lbl. M_TYPE ; command (English text): Provide an English (or a script for another language) text file for the paper. Any other language is preferred, but the files must be in the format of: <language code>.txt <output file name> [<output line number>] output (SQL command): SELECT MAX( ?count/5, ?count * 5 ) AS num FROM ?count ; output (text): Select the minimum value for each parameter. output (SQL statement): SELECT ?max( ?min, ?max ) AS ?max, ?min AS ?min, ?max AS ?max FROM ?min, ?max WHERE { ?min = ?dist[ ?min ]. ?dist[ ?max ]. ?dist[ ?min] = ?dist[ ?max ] }; output (text): For each parameter and its corresponding minimum and maximum value, determine if there are any columns that were not set on the table. output (	False
"
 input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM + dynamic eval model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM + dynamic eval"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Past Decode Reg. + AWD-LSTM-MoS + dyn. eval."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" The ""benchmark"" column was added to the ""cont"" column as it was previously added; it's intended to be used to check for convergence and to provide ""benchmark"" information for the query. The ""benchmark"" column is the same as the ""cont"" column except it is interpreted as the difference in benchmarked code compared to the benchmarked code used in the paper. Thus it provides benchmarking information to the query as well.  For example, if the paper used the benchmarked code that ran on one machine (a machine from one benchmarking group) for the entire evaluation, then it would need a ""benchmark"" column that indicates how much less expensive the code is by running it on the machine. The resulting query would look like: In the above example, the data set was used for the benchmarking, so it is the value in the ""cont"" column that is relevant. The ""benchmark"" column indicates whether the code is faster on this dataset while executing it on the machine, then the ""benchmark"" value corresponds to the total comparison value in the database. It is not the same as the ""benchmark"" as a value in ""benchmark"" is always 0, not 2. For benchmarks where samples from multiple groups were performed, for example in machine learning papers, then the value in the ""cont"" column can also be used to indicate if there is a difference in benchmarked code between the groups. The ""cont"" column is intended to be used as a reference point and is not intended to indicate anything about the algorithm itself. In other words, it is not a ""weighted average"", just data-referenced. By default the ""weighted average"" is set to 1, but can be changed by editing the ""weighted_average"" parameter. This enables you to do things such as compare benchmarked code on two different datasets (or different algorithms in the case of benchmarks with sample data not already in the database). For more details on how to use the ""weighted average"", see the  Benchmarking Methodology Page. For a full list of parameters used to control the benchmarks, see the  benchmarking_page.md which contains a full description of the columns of interest. For a complete list of how the metrics are mapped, see the  metrics_page.md. For a more complete set of examples, see the  benchmarking_posts.md."	False
"
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (Japanese text): What is the name of the top performing model in terms of Score score when benchmarked on the Sega Saturn Sega-GX dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sega-GX"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sega-GX"") ?benchmark ||kgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset 200k Short Texts for Humor Detection?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the 200k Short Texts for Humor Detection dataset in terms of F1-score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1-score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT COUNT( * ) FROM [sparql] BEGIN FOR COUNT( m ) SELECT a || b FROM [sparql] SELECT DISTINCT ?m. ?m. ?m. ?m. d,c as ?metric_lbl_lbl FROM [sparql] select c as ?metric_lbl_lbl_type, d as ?metric_lbl_lbl_value from [sparql] select MULTIPLY (	False
"
 input (English text): Where can I find code references in papers that have used the Relation-Metric model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Relation-Metric"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the STREET model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""STREET"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code WHERE { ?model a || kgp:HAS_DATASET ?dataset. || ?cont || kgp:HAS_BENCHMARK ?benchmark. || ?cont || kgp:HAS_MODEL || ?modes_code || ?cont || kgp:HAS_SOURCE_CODE || ?codes_code || ?modes_code || ?cont || kgp:HAS_SPECIAL_SUM || ?modul orkgp:HAS_SPECIAL_SUM ?modul. || ?code. | ?cont | ?mbm | ?mbm orkgp:HAS_SPECIAL_SUM ?modul. || ?mode || ?code || ?modes_code || ?cont || kgp:HAS_SPECIAL_SUM or kgp:HAS_MODEL |. || ?model. || ?model_lbl. || ?mode. OR ?cont |. |. |. | ?modes_code. || ?mode_lbl. || ?cont |. | ?mbm |. |. ||. ||. || ?mbm ||. || ?mbm orkgp:HAS_SPECIAL_SUM. || ?modul or kgp:HAS_SPECIAL_SUM. || ?mode || ?code || ?mode_lbl. The full file will be available. Thanks to David A. Vestergaard for providing the file. For more details, read his essay.	False
"
 input (English text): Can you list the models that have been evaluated on the WikiText-2 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the enwiki8 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwiki8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-8"") ?benchmark Input: 1. the datasets being evaluated (all possible choices in this case) 2. a sample (in this case a sentence from a Wikipedia article) 3. the model we want to check-off in each comparison (model A or B, model C or D, model E or F), in a standard column (or the model name in the "" ?) field in each record 4. the number of comparisons we want to do, in a standard column (if the number of columns is not enough, it can be specified in the OPTIONAL field) Here are the options: Beware that this will require an additional query when you run your benchmarks or have to perform some queries in the "" ?"" field during the output file. It appears that each time I run my benchmark, I have to write another line in the "" ?"" field when I run my benchmark. It is possible to avoid this extra query by running your benchmarks in parallel, one after another, this would take longer. Tutorial 9: Benchmarking Your Model Using SQL. In this tutorial we'll take an approach that is a bit different for the SQL portion of the tutorial. What we will focus on here is benchmarking our model on a new dataset called ""WikiText-2"". Please note that this dataset is from the Wikipedia entry of the article ""WikiText 2"", which has the following requirements: 1. The articles have to exist, and have an exact article title 2. This dataset should be free in quantity 3. All the data for each entry should be public 4. Each post should have a title, description and the following columns: 5. Each page should have title and a body element (""{'class':'text', 'date':'2008-02-24 10:48:45'}"" in this case). 6. Each link to the entry has a title, description and a body element (""<a href=""#"" rel=""nofollow"" class=""text"">"" for a link). Note that when this information is not available, all the other information is considered. (see this discussion for details on how to check that"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciTLDR dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset orkgc, ?dataset_lbl } orkgc:Dataset a, orkgc:Dataset_lbl. FILTER ( STRING( str( ?dataset_lbl), ""SciTLDR"") OR STRING( str( ?dataset_lbl), ""SciERC"") or STRING( str( ?dataset_lbl), ""SCITLDR"" )) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK orkgp:P31 ?cont; rdfs:label ?paper_lbl. } if (str( ?dataset_lbl) not in ""Datasets"") then output(Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset orkgc:Dataset, ?dataset_lbl } orkgc:Dataset_lbl. FILTER ( STRING( str( ?dataset_lbl), ""SciTLDR"") OR STRING( str( ?dataset_lbl), ""SciERC"") or STRING( str( ?dataset_lbl), ""SCITLDR"") ) ?benchmark. ""scioctl"" command: from nltk.sciodiff import scioctl if ""sc1"" not in ""InputData"" then output(Nltk input(""sc1"")) else output(Sciodiff output(""Sciodiff"")) ""Sciodiff"" command: for score in score.score_classifications: scioctl(""scioctl "" + score, [""score ""].lower_case, score.value) score.score_classifications[""scoreName""] = score scioctl command: for score in score.score_classifications: if (score!=""sc1"") then scores.get('sc1') else scores.get('sc1') ""sciodiff.score_classification_score"" command: from sciodiff import Sciodiff score = Sciodiff.scoreClassification_score(sc(score.score_classification_score, scoring"	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Natural Questions benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the CommonsenseQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Categories"") ?benchmark orkgp:NQA. NQA/HAS_LINGUAS ?dataset_lbl. ?benchmark orkgp:NQA rdfs:label ?model_lbl. ORDER BY DESC( ?value) LIMIT 1 } } The output is as follows: Name Model Results Accuracy metric Natural Questions Benchmark Natural Questions Benchmark 2 Animate.Net ML 1.7769 1.7645.049 3 Animate.Net ML 2.2918 2.2851.048 4 Animate.Net ML 3.2345 3.2335.048 5 Animate.Net ML 3.6748 3.6848.041 6 Animate.Net ML 4.3363 4.3439.040 7 Animate.Net ML 4.5669 4.5574.035 8 Animate.Net ML 5.4609 5.4722.034 9 Animate.Net ML 5.6724 5.6911.031 10 Animate.Net ML 5.7768 5.7925.037 11 Animate.Net ML 6.4814 6.4881.054 12 Animate.Net ML 6.8158 6.8318.066 13 Animate.Net ML 7.0248 7.0246.076 14 Animate.Net ML 7.5249 7.5584.072 15 Animate.Net ML 7.7609 7.7668.071 16 Animate.Net ML 8.3363 8.3405.072 17 Animate.Net ML 8.5712 8.5862.058 18 Animate.Net ML 8.8514 8.8976.057 19 Animate.Net ML 9.2026 9.2378.054 20 Animate.Net ML 9.5926 9.6804.049 21 Animate.Net ML 9.9126 9.9439.052 22 Animate.Net ML 10.5299 10.5629.051 23 Animate.Net ML 10.9248 10.9961.054 24 Animate.Net ML"	False
"
 input (English text): List the metrics that are used to evaluate models on the SQuAD2.0 dev benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT rdfs:labels( ?dataset, { ?dataset_lbl orkgp:label of ?dataset; ?dataset_lbl orkgp::labels of.dataset_lbl. ?dataset_lbl ||kgp::labels of.dataset_lbl. :metric, ?metric_lbl..., orkgp::stat_lbl of.dataset_lbl; },.dataset_lbl ||kgp:stat_lbl) rdfs:labels( orkgp::lbl, { ?dataset_lbl orkgp::stat_lbl}). OPTIONAL {!stats rdfs:lbls(""dataset""). } How many metrics are used in the SQuAD2.0 benchmark ? input (English text): Count the metrics that are used to evaluate models on the SQuAD2.0 dataset ?"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" Note: this query can use an embedded database for the validation if they are configured to do so and if the results are to be reused with further iterations. What is a common metric for testing on your machine, and how do you know of a common one ? R: The way I used to test the DFT on the WMT2014 dataset (which was to measure convergence) was to count the number of iterations of DFT. What I also use in benchmarks is to just estimate the number of iterations before performing the benchmark and then compare the difference. For example I will use the DFT on the WMT2014 dataset as a baseline and try to detect convergence between two model iterations until I reach the required number of iterations. A common metric used in benchmarks that does not need to repeat for the whole benchmark is: test:model:cls : I think that there was a reason for excluding these two metrics: the original WMT2014 dataset should have seen a larger effect of convergence during the same time span, but they did not report that at the time. As a general rule, try to keep the measurements between the second and third iterations. What is the median ? R has a method called ""median"" that returns the median number of iterations before an observation's value reaches the bottom of the box of the dataset. Is this a common metric ? How about the top of the box ? R: ""top"" might work but it seems too random and would be hard to tune. What is a good measurement of how close is that prediction to reality ? And why ? R: I would like to keep such things as accuracy and precision ""simple"", but I do not think that simplicity works well with many kinds of models. However, I am sure that simple can be fine. Do you like to have some common metrics for your experiments ? Or do you prefer to have different methods with different constraints ?"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Information Extraction research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Information Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Reading Comprehension research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Reading Comprehension"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Reading Comprehension"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } input (English text): Provide a list of benchmarked datasets related to the Speech Recognition research area ? output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Speech Recognition"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } input (English text): Provide a list of benchmarked datasets related to the Natural Language Processing research area ? output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Natural Language Processing"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } input (English text): Provide a list of benchmarked datasets related to the Machine Translation research area ? output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label"	False
"
 input (English text): Indicate the model that performed best in terms of RE+ Macro F1  metric on the CoNLL04 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Macro F1 "") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE ?dataset a model_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?score. orkgp:HAS_DATASET ?dataset_lbl. orkgp:HAS_EVALUATION ?eval. ORkgp:HAS_METRIC ?metric. ORkgp:HAS_VALUE ?value. Orkgp:HAS_BENCHMARK ?benchmark. ORkgp:HAS_MODEL ?model. ORkgp:HAS_METRIC ?metric. ORkgp:HAS_VALUE ?value. This is the output from my code, it's what was gathered from my data analysis. So, to find the model that best described the average score of the benchmark dataset, we need to compute the RE+ Macro F1 metric. The way to compute RE+ Macro F1, which is RE+ Macro F0, was explained here. I wrote a helper class called RE+MATCH(dataframe). When we want to match a model to a given dataset, we can do this as following (with input to generate the database table that will contain our data): RE+ MATCH(model, set_match) { dataframe } which will return a dataframe like this: ... output (Sparql query): SELECT ?model ?model_lbl FROM ?data_frame ?model_lbl. { ?dataset a model_lbl. ( ?metric_lbl = ""RE+ Macro "". ) AND ?dataset a model. ( ?metric_lbl = ""CoNLL04""). }. The table returned above will contain information of the model that best captured the average score of the benchmark dataset. To find the model that best fit the benchmark dataset, we need to evaluate that model over all the data it has been tested with. So, to do this, we are going to use a combination of the two techniques I described above: We will calculate the RE+ Macro F1 metric of the model, including all the input data; the database table that it was evaluated on; and the metric of the criterion variable the model was tested against. We will use this metric to generate a RE+ Macro F1 statistic. We will use this RE+ Macro"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the XLNet model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the XLNet (base) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (base)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?dataset. orkgp:HAS_DATASET ?dataset. ?source_code. ?data. FROM ?dataset. orkgp. ?source_code ) >output (Spark SQL query: SELECT DISTINCT * ?dataset. ?type ?model_lbl. ?labels. ?source_code. ?data. FROM ?dataset. ?source_code ) Output: 2.7. Test the XLNet 2.7.1.1 - Creating XLNet datasets with input parameters: #> input (DataFrame( ?model. ?type ?model_lbl. ?label.. ?type. ?data. ) for ( i in 1:5) ; -- i=1,5) >output (Spark SQL query: SELECT DISTINCT * ?dataset. ?type ?model_lbl. ?labels. ?source_code. ?data. FROM ?dataset. ?dataset. orkgp. ?source_code ) Output: 2.7.1.2 - Creating XLNet datasets with output parameters #> output (Spark SQL query: SELECT DISTINCT * ?dataset. orkgp:HAS_DATASET ?dataset. ?source_code. ?data. FROM ?dataset. orkgp. ?source_code. orkgp. ?type ) >output (Spark SQL query: SELECT DISTINCT * ?dataset. orkgp:HAS_DATASET ?dataset. ?source_code. ?data..source_code..) <-- the table's size is not set >output ( Spark SQL query: SELECT DISTINCT * ?dataset. orkgp:HAS_DATASET ?dataset. ?source_code. ?data..source_code. FROM ?dataset. ?dataset.) <-- the	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Paper Field dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Ohsumed dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ohsumed"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT title, paper | ?dataset ?dataset_lbl WHERE ?dataset a orkgc:Dataset | ?dataset ?dataset_lbl. ?benchmark orkgp:HAS_DATASET | ?dataset ?dataset_lbl. | ?cont ?cont. | basename ?bmd ? | title ?ttitle | author ?ipauthor | title ?tauthor | author ?ipauthor | reference ?ref ?. | title ?ttitle | author ?ipauthor | author ?ipauthor | reference input (English text): Select the following criteria to be used in the benchmarks: 	False
"
 input (English text): Can you list the models that have been evaluated on the NYT24 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the NYT29 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" You can try again using the same API. Note that the dataset name will likely need to be changed in the query to fit the model parameter. For example, you can also run queries against individual papers, which can give more general results. For example, the dataset used by the Times was used to evaluate the paper 'Koch-Newman. Elegance: A Social Movement's Secret Weapon: A Practical Guide' by Andrew Hirschberg, Jens Michael Røde and Gweneth W. Tulloch. The dataset used by the NYT24 dataset contains: dataset: NYT24 paper1: (1, 2, 3) { ""title"": ""Elegance: A Social Movement's Secret Weapon""; ""authors"": [ ""Andrew Hirschberg"", ""Jens Michael Røde"" ]; ""dataset"": NYT24 paper2: ( ""D.G. Koch, G. Newcomb, K. Newcomb"" ); The dataset used by the NYT29 dataset contains: dataset: NYT31 paper1: (1, 2, 3) { ""title"": ""Dynamism is the Key to Progress: A Guide for New Yorkers""; ""authors"": [ ""D. G. Koch, H.K. Seidel"" ]; ""dataset"": NYT31 paper2: ( ""L. Fink, E. Sollers, Z. Dziobek"" ); As it has a dataset with a similar name to NYT24 for the dataset used in the benchmarks (the latter being NYT29), it will be in the same dataset. Using the benchmark query against the individual papers again yields the outputs in the table below: dataset: NYT24 benchmark: The original paper and test papers: 1 2 3 (1, 2, 3) NYT29 sample: The original papers and test papers: 1 2 3 (1, 2, 3) orkgp: 1 orkgp 1 2 3 (1, 2, 3) paper1: (1, 2, 3) { ""title"": ""Elegance: A Social Movement's Secret Weapon: A Practical Guide for New Yorkers""; ""authors"": [ ""Andrew Hirschberg"", ""Jens Michael Røde"" ] } The results in the table are roughly similar to the results of the standard Lattice benchmark tool, but may be a bit more sensitive to outliers due to a larger"	False
"
 input (English text): List the metrics that are used to evaluate models on the Dataset mentions in Social Sciences benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dataset mentions in Social Sciences"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which are 3 the most common variables for the atmosphere models?
 output (Sparql query): "	"SELECT ?variables WHERE { orkgr:R68871 orkgp:compareContribution ?contrib. ?contrib orkgp:P7144 ?models. ?models rdfs:label ?models_labels. FILTER(REGEX( ?models_labels, ""tmospher"")) ?models orkgp:P26032 ?variables. } ORDER BY DESC(COUNT( ?variables)) LIMIT 3"	SELECT BEGINNING_PRETTY_LIKE | CURRENT_HUMMING_CYCLE | AIR_PERFORMANCE | ALREADY_EXISTING_COOL_SPACES | COOL_SPACES_ABSORB_SPACES | COOL_SPACES_BOLDEST | COOL_SPACES_BOTTOM | FELOSTITY | HUMIDITY | METATORS | MOON_SPACING | NIGHT_SPACES_ABSORB_SPACES | NIGHT_SPACES_BOLDEST | NIGHT_SPACES_BOTTOM | MASS_DENSITY | MINIMUM_COOL_SPACES_ABSORB_SPACES | MINIMUM_COOL_SPACES_BOLDEST | MINIMUM_COOL_SPACES_MASSYSTEM; orkgp:HAS_CURRENT_HUMMY_STABLE OR kgp_HAS_CURRENT_THRUST_WEATHER OR kgp_HAS_LIGHTNING_CYCLE or kgp_HAS_MOON_STABLE or kgp_HAS_BOLDEN_SHOAMING or kgp_HAS_FELOSTYPE or kgp_HAS_FELOSTYPE_MOON_STABLE or kgp_HAS_BOLDEN_SHOAMING_MOON_STABLE; OR kgp_HAS_BOLDEN_SHOAMING_STABLE or kgp_HAS_COOL_SPACES_ABSORB_SPACES or kgp_HAS_MOON_HEAT_SPACES or kgp_HAS_MOON_SPACES_BOLDEST; orkgp:HAS_CURRENT_HEAVY_LATENCY OR kgp_HAS_CURRENT_THRUST_WEATHER OR kgp_HAS_COOL_SPACES_BOLDEST; orkgp:HAS_CURRENT_HEAVY_LATENCY_MASSYSTEMORPHORES; or kgp_HAS_COOL_SPACES_BOLDEST OR kgp_HAS_COOL_SPACES_THRUST_WEATHER;	False
"
 input (English text): List the code links in papers that use the DDQN (tuned) hs model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN (tuned) hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DDQN-PC model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN-PC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code_pch from papers that use the DDQN-PC model in any benchmark ? If it returns empty results, see  How can I see the complete query results ? Output (Sparql query): SELECT * FROM papers.results WHERE (orkgp) and (hsa_dtype 'int' orkgp) AND (dtype: 'int8') AND (HAS_ENH_DATASET 'DDQN (tuned) hs') ; Output (Sparql query): Select * from papers.results where orkgp is null and (hsa_dtype 'int8' orkgp) is null and dtype is 'int8'; If it results with only a list, use the output parameter:   if (isnull(output(hsa_dtype=hsa_dtype 'int8') or outputs:has('output') and hsa_dtype is null) and (has('output')) and (has('include_comments')) && isnull(outputs).outputs.exclude(outputs.name) ): output.names.include_comments = {}   output.exclude_comments.include_comments = {}   output.exclude_comments.exclude(outputs.name) = {} If it results with just a CSV file, in one of the following cases, you need to use If it results with just a CSV file, in one of the following cases, you need to use if (isnull(output(dtype=dtype 'int8' oroutputs:has('output'))) and outputs:has('output') and has('include_comments' )) and (has('include_comments')) && has('exclude_comments')) : outputs[#results+'#results'] = output   output.exclude_comments = {} If it results with just a CSV file, in one of the following cases, you need to use If it requires to set up the query itself, in order to use it, see  How do I set up the DDQN-PC model for benchmarking ? 1. If you get an error like this > 1. If you get an error like this:  Can't convert sql value: int to float: 8 ERROR   - Dump table '.sql' : Invalid	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Text8 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Text8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TREC-6"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 Input Output C Output C 1 Results List the results of benchmarking on the Text8 dataset ? data (text): ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER ( ?dataset_lbl in ( ?dataset orkgc:Dataset ) OR ?dataset :dataset) ?benchmark Results List the results of benchmarking on the TREC-6 dataset ? data (text): ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER ( ?dataset in ( ?dataset orkgc:Dataset ) OR ?dataset :dataset) ?benchmark Results List the results of benchmarking on the TRF database dataset ? data (text): TRF database. data (dataset): A orkgp:DataSet; nl:column ?dataset_lbl. ( ?dataset_lbl = ?dataset orkgc:Dataset OR ?dataset :dataset)) Results List the results of benchmarking on the TRF database dataset ? data (dataset): A orkgp:DataSet; nl:column ?dataset_lbl. ( ?dataset_lbl = ?dataset orkgc:Dataset OR ?dataset :dataset)) Results List the results of benchmarking on the TRF database dataset ? data (dataset): A orkgp:DataSet; nl:column ?dataset_lbl. ( ?dataset_lbl = ?dataset orkgc:Dataset OR ?dataset :dataset)) Results List the results of benchmarking on the TRF database dataset ? data (dataset): A orkgp:DataSet; nl:column ?dataset_lbl. ( ?dataset_lbl = ?dataset orkgc:Dataset OR ?dataset :dataset)) results: ?benchmark orkgp:RESERVED ?dataset. ?dataset_lbl. Results List the results of benchmarking on the TRF database dataset ? data (dataset): A or	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Recurrent Rational DQN Average model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Recurrent Rational DQN Average"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Rational DQN Average model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rational DQN Average"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 [1] https://raw.githubusercontent.com/NigelGardner/NJP_DQN/master/dqn_benchmark/demo [2] https://github.com/NigelGardner/NJP_DQN/blob/master/dqn_benchmark/lib/dqn_util.py [3] https://github.com/rjg/rjd/blob/master/lib/rjd/lib/lib.py [4] https://raw.githubusercontent.com/billyb/rjd/blob/master/demo [5]  https://github.com/rjg/rjd/blob/master/src/rjd.c [6] https://github.com/rjg/rjd/blob/master/src/rjd/lib/dqn_util.py [7] https://raw.githubusercontent.com/rjg/rjd/blob/master/demo [8]  https://raw.githubusercontent.com/rjg/rjd/blob/master/demo [9] https://raw.githubusercontent.com/rjg/rjd/blob/master/dqn_benchmark/demo/demo2.py	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-Spanish dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Spanish"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; ?dataset rdfs:label ?dataset_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } output (Sparql query): SELECT csv.data_id ?model ?model_lbl. FILTER rdfs:label ?label_lbl. ?Model_score_score ?model_score. csv_id csv_name model_samples model_samples_error model_samples. csv_samples_error. csv_label model_label. csv_dataset model_dataset. csv_dataset_samples dataset_samples. rdf s3:file ?dataset_lbl. csv_samples_error csv_samples. output (Sparql query): SELECT csv_dataset_size ?dataset. csv_samples_error s3:file_size ?dataset. s3:file_size. ?dataset_size ?dataset. rdf_data s3_file :dataset. ?dataset_size_samples ?dataset. ?dataset_size-samples ?dataset. ?dataset_size_sacc ?dataset. ?dataset_score_score ?measurement. rdf_dataset_size / dl rdf_dataset _t_h_l _dataset. output (Sparql query): Select an item to search for. Select the query to search and click the + icon. Search Results: All results: Search for an item: Search by criteria: Search by value: Sear"	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the 20NEWS benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""20NEWS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""1-of-100 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PolyAI Reddit"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?model ?model_lbl FROM ?model JOIN ?dataset ?dataset_lbl ON ____ ?dataset ?dataset_lbl. DATE() = ____ ?dataset_lbl. DATE() + ?dataset_lbl. DATE('2014-01-01-0000'). SELECT ?model ?model_lbl FROM ?model JOIN ____ ?dataset ?dataset_lbl ON ____ ?dataset ?dataset_lbl. DATE() = ____ ?dataset_lbl. DATE('2014-01-01-0000'). SELECT ?model ?model_lbl FROM ?model JOIN ____ ?dataset ?dataset_lbl ON ____ ?dataset ?dataset_lbl. DATE() = ____ ?dataset_lbl. DATE('2014-01-01-2000').	False
"
 input (English text): List the code links in papers that use the Linear Transformer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Linear Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Long Short Transformer model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Long Short Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?code FROM ?benchmark orkgp:HAS_DATASET ?dataset. ORkgp:HAS_BENCHMARK ?code. output (Sparql query): Output from the model (LINO_TRANSPTR) function: Input: Long-Short-TRANSPTR (long long, short short) Output: Matrix of values (length = 2D array) Output: The model: Long Short Transformer Model How to run the benchmark results analysis The code I use to analyze benchmark results can be found in this repo. The following code can be run in R, Python and MATLAB on Windows, on a MAC machine and an OSX machine, using a MATLAB/SASL/C/C++ shell. I recommend the following steps if using Python: (Python) Run python_benchmark.py on the results file from the github repo (using the ""run as"" and the ""./run"" options): python_benchmark.py [-h] <results-file> Run python_benchmark.py as a.bat file on a machine with the R compiler ""R"". This will compile all the code and run all the benchmarks: python_benchmark.py <results-file> -l python_benchmark.py Running all these will produce a file like: output (Sparql query): SELECT ""C:\program files\mylab.exe"", ""C:\windows\system32>"", ""C:\windows\system32>"", ""C:\windows\system32>(R)"" FROM "" ?outputs\mylab.exe"" -1 1 2 3 4 5 6 7 8... output ( Sparql query ) : SELECT ""C:\program files\mylab.exe"", ""C:\windows\system32>"", ""C:\windows\system32>"", ""C:\windows\system32>(R)"" FROM "" ?outputs\mylab.exe"" - 1 The default output file for Linux and Mac is called "" ?outputs\"". (Python) Run python_benchmark.py on the results file from the github repo (using the ""run as"" and the ""./run"" options): python_benchmark.py [-h] <results-file> Run python_benchmark.py as a.bat file on a machine with the R compiler"	False
"
 input (English text): What is the name of the top performing model in terms of EM score when benchmarked on the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""EM"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the TriviaQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?metric_lbl) = ""EP"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ORDER BY DESC( ?value) LIMIT 1 } } } The input field was omitted from this SQL statement because it was not required to complete a query. The output of this particular query is shown below to illustrate the output. output (Sparql query): SELECT DISTINCT ?metric ?max_score/10 WHERE { ?data ?dataset ?dataset_LBL ?dataset. ?data model ?model_lbl } || ?dataset model ?model_lbl LIMIT 1 select data: ?dataset ?dataset_LBL ?dataset. ?data model for model: ?model_lbl select data: ?dataset ?dataset_LBL ?dataset. ?dataset model for model: ?model_lbl The output, however, is not what we were expecting. What we were expecting was that the output field would output the accuracy score of the model while the input field would output the accuracy of the model over all of its training data. This is correct in the schema of the query. What we actually did was output the accuracy score over the subset of training data. In other words, we output the accuracy score for the subset of data from which the model was trained. A quick little hack. If your model is not outputting a value, you can replace the input field with an additional variable, like so: output (Sparql query): select model_dataset FROM *; select model_dataset as ?dataset_lbl; select model_dataset as ?dataset_lbl; select model_dataset as ?dataset_lbl LIMIT 1 select model_dataset from model_dataset SELECT model_dataset as ?dataset_. ?dataset"	False
"
 input (English text): Which methods are used?
 output (Sparql query): SELECT DISTINCT ?methods, ?methods_labels WHERE { orkgr:R110361 orkgp:compareContribution ?cont. ?cont orkgp:HAS_METHOD ?methods. ?methods rdfs:label ?methods_labels. } 
 input (English text): What is Raman spectroscopy?
 output (Sparql query): "	"SELECT ?definitions WHERE { ?terms orkgp:P24009 ?definitions. ?terms rdfs:label ?terms_labels. FILTER(REGEX( ?terms_labels, ""Raman spectroscopy"")) } "	SELECT method, RANGED_EVALUATION ( Method ),  RANGED_EVALUATION_(method_id), output_rng ( RENAME  METHOD ), RANGED_EVALUATION_(method_id) FROM DISTINCT { method:R110361, method_labels:null, is_method:false } input (English text): What is the difference between RDFa and RDFa ? output (Sparql query):  SELECT DISTINCT { method:R110361, method_labels:null, is_method:true } input (English text): What is a Schema ? output (Sparql query):  SELECT DISTINCT schema_name, RANGED_EVALUATION ( Schema_id ), RANGED_EVALUATION_(schema_name), output_table_name (schema_name) input (English text): What is RDFa Structured Query Language (RSPL) ? output (Sparql query):  SELECT RSPL { schema_name: NULL, is_method: TRUE, output_table_name: NULL, output_table_name_cols: NULL, output_table_name_row_index: NULL, output_table_name_cols_index: NULL, output_table_name: NULL, output_column1: NULL, output_column2: NULL, output_column3: NULL, output_col1: NULL, output_col2: NULL, output_col3: NULL, output_schema_id:NULL, is_schema_schema_name: NULL, output_schema_index: NULL, was_validated: NULL, has_subdoc: NULL, is_validation_period: NULL, _validation_period_days: NULL, is_validation_period_months: NULL, _validation_period_years: NULL, _schema_id_months: NULL, _schema_id_years: NULL }  from { is_method: true } input (English text): What is a TALENT database ? output (Sparql query):  SELECT RSPL { schema_name: NULL, is_method: TRUE, output_table_name: NULL, output_table_	False
"
 input (English text): What is the best performing model benchmarking the DTD dataset in terms of PARAMS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DTD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the DTD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DTD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT dbid as [ ?dataset], basename basename. basename from rdfs ORDER BY DESC( ?value ) DESC( ?value ) DESC( ?value ); orkgp:HAS_DATASET basename; orkgp:HAS_EVALUATION basename; orkgp:HAS_METRIC basename. BASESIZE (basename): 1.5.0 orkgp:HAS_STATISTICS_LAST_DATE basename. BASIS_LENGTH(basename): 2101.0 orkgp:HAS_METRICS_LAST_TIME basename. BASESIZE (basename): 2.0.0 basename is set to 'DTD' for the benchmarking criterion to be applied. orkgp:HAS_EVALUATION basename. LABEL_LENGTH(basename): 0.7, 0.8.0, 0.9.0 orkgp:HAS_RDFS_LABEL basename. LABEL_LENGTH_LIMIT(basename): 2 orkgp:HAS_RDFS_LABEL basename. DTD: { ?dataset ?dataset_lbl ?model. } orkgp:HAS_NUM_EXPERIMENTS ?exempl_num. DTD v: { ?dataset ?dataset_lbl } orkgp:HAS_DATASET_VERNUM. PARAMS_LENGTH(basename): 1.5.0 PARAMS_PANEL_LENGTH(basename): 2101.0 PARAMS_PANEL_VALUE_LENGTH(basename): 2 PARAMS_PARAMETER_LENGTH(basename): 1.5.0 PARAMS_PANEL_MAX_LENGTH(basename): 2101.0 PARAMS_DATASET_VERNUM(basename): 0.5.0 PARAMS_RDFS	False
"
 input (English text): What is the top benchmark score and its metric on the CIFAR-10 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the CINIC-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"--------------- DESCRIPTION --------------- {{ ?dataset a:Dataset; { ?dataset_lbl (CIFAR-10). } } rdfs:value. OPTIONAL { ?dataset rdfs:value. } ORACLE_SOC_RATING. ROUND ( ?dataset_lbl. 10, 1). ORACLE_SOC_RATING. { { ?dataset rdfs:value. ORACH SORT LSTM(1, 10) AS rdfs_k, ?value AS rdfs_k ?value in rdfs_k ORDER BY ASC( ?value) ?dataset } } } ORDER BY DESC( ?dataset_lbl ) 2.6 SQLite 3.6 SQLite supports the RDF-based format for data export. To support both CIFAR-10 and Parquet, SQLite 3.6 includes support for exporting both. SQLite has a built-in API for making queries for SQL, and there are a number of examples in this manual. 2.7 Parquet Parquet provides a format that allows querying over a heterogeneous set of CIFAR-10 rows. It is also useful for querying over a simple hierarchical set of CIFAR-10 rows, where each row contains a list of properties. To set up a session to use a Parquet query across a cluster consisting of multiple databases, you must set the ""metadata"" parameter. 2.7.1 Session Parameters Each Parquet query supports a set of metadata fields that describes the query. Each metadata field identifies the name of the cluster, and, in general, there is only one such field per Parquet document. The following table includes the fields that will be included in the default schema; see section 4.7.1.5, for more information on the default Parquet schema. These fields are not required to be present in a schema, but are recommended for backward compatibility: metadata name, value description, description 1 cn This field specifies a clustered coordinate identity. This field identifies a cluster, along with the coordinates of the origin and the origin index. This field does not specify the origin index. 2 pn This field specifies a clustered numeric index identity. This represents the cluster location, the index name, the cluster name. The index name is interpreted as a prefix. 3 rn"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the SemEval-2021 Task 11 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2021 Task 11"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SemEval-2018 Task 7 dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2018 Task 7 dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" How to integrate the results of the paper results into your product development process: * The following is a sample statement from a process document for a new application: "" In order to ensure that our applications are continuously updated in the most current SemEval, we will need to have a high level set of benchmarks we can focus, say, on. Ideally it is important that they contain lots of different data and the resulting benchmarks should be able to be used across various models... "" The following example statement is an introduction to running benchmarks on a variety of different datasets (in this case, SQLite, Spark Tableau, and a number of Apache Hive databases). "" For example, you might have a task with two tasks: A common task (task A), and a task that changes jobs (task B). There can also sometimes be multiple task that do different things. The task A job might be updating some jobs in order to match some changes in the workload, while the task B job might be modifying jobs in order to avoid going out of date. This gives a huge amount of flexibility, and the task A and B may run under different workloads. "" The above statement outputs two pieces of information for the user: * How does the Task A task differ from tasks B and C ? * The sum of the two columns of the Task A column is equal to the sum of the two columns of the Task B column. The task B task in the example below: # Get task's count of iterations across all datasets task_count = task_count * (datastructures - > tasks = tasks.values()) # Set task's task_id to task_id and execute the task. task_id = task_id * task_count/1 # Run the task and run its job. task_id = task_id + 1 * When a new application is created and implemented, the project will include benchmarking statements (often called 'benchmark scripts') that define how, for each dataset or task, the application will attempt to achieve certain performance outcomes using various methods. Such tasks can be used as a guide for implementing a production application, based on benchmarks, to measure the performance of different approaches and to iterate on improvements over time rather than trying to do everything a single application is expected to do on its own. Sometimes, the number of benchmarking lines is relatively small, and sometimes, the number of lines may be enormous, but a"	False
"
 input (English text): List the metrics that are used to evaluate models on the COPA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""COPA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the AG News benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AG News"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	... : Results	False
"
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NCBI-disease dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 import pandas as pd import ncbi as ncbs library(pd) df = pandas.read_csv('../RData/BASIC.csv.dta', sep='\t') df = df.merge(colors = pd.load.colors) df.head() | Type | Type | Model | F1 entity | F1 score | NULL | 0 | NULL | NULL | 1 | NULL | 2 | NULL | 3 | NULL | 4 | NULL | 5 | NULL | 6 | NULL | 7 | NULL | 8 | NULL | 9 | NULL | 10 | NULL | 11 | NULL | 12 | NULL | 13 | NULL | 14 | NULL | 15 | NULL | 16 | NULL | 17 | NULL | 18 | NULL | 19 | NULL | 20 | NULL | 21 | NULL | 22 | NULL | 23 | NULL | 24 | NULL | 25 | NULL | 26 | NULL | 27 | NULL | 28 | NULL | 29 | NULL | 30 | NULL | 31 | NULL | 32 | NULL | 33 | NULL | 34 | NULL | 35 | NULL | 36 | NULL | 37 | NULL | 38 | NULL | 39 | NULL | 40 | NULL | 41 | NULL | 42 | NULL | 43 | NULL | 44 | NULL | 45 | NULL | 46 | NULL | 47 | NULL | 48 | NULL | 49 | NULL | 50 | NULL | 51 | NULL | 52 | NULL | 53 | NULL | 54 | NULL | 55 | NULL | 56 | NULL | 57 | NULL | 58 | NULL | 59 | NULL | 60 | NULL | 61 | FALSE | 62 | NULL | 63 | FALSE | 64 | NULL | 65 | NULL | 66 | NULL | 67 | NULL | 68 | NULL | 69 | NULL | 70 | NULL | 71 | NULL | 72 | FALSE | 73 | NULL | 74 | NULL | 75 | NULL | 76 | NULL | 77 | NULL | 78 | NULL | 79 | NULL | 80 | NULL | 81 | NULL | 82 | NULL | 83 | NULL | 84 | NULL | 85 | NULL | 86 | NULL | 87 | NULL | 88 | NULL | 89 | NULL | 90 | NULL | 91 | NULL | 92 | NULL | 93 | NULL | 94 | NULL | 95 | NULL | 96 | NULL | 97 | NULL | 98 | NULL | 99 | NULL | 100 | NULL | How to run: maptest.py -v -O -r db -	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NCBI Disease dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 As you can see, Parquet uses only four data sets to store one, very common dataset. Furthermore, each data set that Parquet uses is the base for one of a number of specialized schemas. For the purpose of these examples, we assume there are just two schemas; the general Schema type that is usually used should be sufficient. This makes the Parquet schema significantly more flexible and fast to manage in larger systems. When designing Parquet, I first thought of using the Data Model Format for both data sets. However, the Parquet data sets are stored as columns of the RDF schema and so I chose to keep the data sets separately in a separate file; in terms of performance, that is the only difference. We're starting to have a little performance gap, so let's take a look at which schema we're going to use, then see where the data is going to go when it is done. Now that we've looked at how we're going to store the data, we might want to design a schema to manage the data. The data may be scattered about the file system, where it could be indexed. What's the best way to organize that data ? In fact, it doesn't really matter; Parquet provides a built-in data types to handle just about any data you might want to write into a schema: text, binary, binary-binary, string, list, boolean, and integer. But not all data is appropriate for a schema, and I'll show four schemas based on an example and a test file, along with their respective columns and their formats. The sample_data_schema_hackers_test_file data file contains about 100,000 records of information about NCBI disease patients. I was looking for a data structure that would make it easier to use the data as a collection, so I threw them into the list that Parquet provides. To get the information into a structured form, I wrote a helper function to create a text data type out of it with the data format that Parquet supports to make use of it as a data object. Here's what the class that generated that class for me looks like: And here's how I populated the field names in the sample data file. Here is how that looks to other programs... the two rows on the left with the labels are the data elements you actually want to display. The two rows on the right are the schema you created for the two rows. So, when	False
"
 input (English text): Can you provide links to code used in papers that benchmark the GCNN-8 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GCNN-8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the GCN Hybrid model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GCN Hybrid"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT SPS LN(LN=ORGG-8-SPSL, LEN=16384) AS sppLn, ?sppLn LN(LEN=11, ROWCOUNT(RETVAL(LN)=1) DESC) AS sppLn2, ?sppLn2 STDEV(LN=7, L0LEN=8, ROWCOUNT(RETVAL(LN)=2) DESC) AS sppLn3, ?sppLn3 TRUNCATE(STDEV(LN=5, L0LEN=8, ROWCOUNT(RETVAL(LN)=3) DESC) AS sppLn4, ?sppLn4 DATASET(STDEV(LN=6, L0LEN=8, ROWCOUNT(RETVAL(LN)=4) DESC) AS sppLn5), ?sppLn5 ORGGRR(K, M, L, R, ?m, ?s, ?l, ?s, ?p, ?d), ?sppLn, ?LUNCASE(p, ?l, P, ?s, ?c, ?d) AS sppLn6, ?LUNCASE(p, ?l, P, ?s, ?c, ?d) AS sppLn7, ?LUNCASE(p, ?l, P, ?s, ?c, ?d) AS sppLn8, ?LUNCASE(p, ?l, P, ?s, ?c, ?d) AS sppLn9, sppLn>=10, ?LUNCASE(p, ?l, P, ?s, ?c, ?d) ?, ?LUNCASE(p, ?l, P, ?s, ?c, ?d) ? AS sppLn10 FROM ORGGRR(K, ?m, ?s, ?s, ?l, ?d) sppLn WHERE ROWCOUNT(RETVAL(LN)=	False
"
 input (English text): Could you provide a list of models that have been tested on the ADE Corpus benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ADE Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What data format does CHEMDNER corpus have?
 output (Sparql query): "	SELECT ?data_format, ?data_format_label WHERE { orkgr:R150596 orkgp:P31 ?contrib. ?contrib orkgp:P42011 ?data_format. ?data_format rdfs:label ?data_format_label. } 	SELECT ?classes FROM classes_test.classes WHERE full_sparse = ? > FALSE output (Sparql query): Select Count(s) input (English text): In what order do the datasets, Full, Sparse, and FullSparse, appear in your CHEMDNER query ? output (Sparql query): input (English text): Which columns in Full, Sparse, and FullSparse should I be keeping ? output (Sparql query): SELECT ?columns FROM FULL.TABLE output (Sparql query): Select Count(s) input (English text): Can I query some additional columns like ?column_num or ?labels, which are not included in the Full and Sparse datasets but are used in the full dataset ? output (Sparql query): input (English text): Can I	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the CUB-200-2011 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?benchmark ?bench_lbl. FILTER (str( ?benchmark ) = ""CUB-200-2011"") ?benchmark orkgc_2002011 orkgc_2011115_lbl orkgc_2011115_benchlbl orkgc_2011115_bio ?cont bio. Input data from ScienceIE dataset: [ (a.title OR b.title) OR (b.title OR c.title) ] [ (a.title OR b.title) OR (c.title OR d.title) ] --- SELECT title FROM ?benchmark [ title ] --- SELECT ID FROM ?benchmark_lbl --- SELECT title, ID FROM ?cont OR ?lbl [ title ] --- SELECT title, ID FROM ?repl [ title ] Output data: --- -------------+------------- science.id -------- ScienceIE Benchmark 2011 CUB-200-2011 R2-2010-04 R2-2010-07 R2-2010-10 CUB-200-2011 MRC-2011-01 R2-2010-04 R2-2010-07 CUB-200-2011 TETAN-2011 R2-2009-10 R2-2009-11 R2-2009-12 CUB-200-2011 CIMSS-B-2011 R2-2009-01 R3-2010-07 R3-2010-07 R3-2010-07 CUB-200-2011 CITOOL-2011 R2-2009-10 R2-2009-11 R2-2009-12 CUB-200-2011 CUB-RISC-RISC BCS-2010-01 R2-2009-09 R2-2009-11 R2-2009-12 CUB-200-2011 CUB-RISC-RISC BCS-2001-03 R2-2009-03 R2-2009-11 R2-2009-12 CUB-200-2011 CUB-RISC-RISC BCS-2002-12 R2-2009-04 R3-2010-07 R3-2010-09 R3-2010-09 CUB-200-2011 CUB-RISC-RISC-B-2007-00 R3-2010-08 R3-2010-09 R3-2010-09 CUB-200-2011 CUB-RISC-RISC-B-2007"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ning et al."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the SEE model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SEE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" The output from the query is available by using the following command: from sklearn import viewfrom sklearn import preprocessing # Preprocess the model results <- read.csv( ""results.txt"" ) output( rdfs:filter(is.na(results)),.replace( '''... ''' ), reverse= TRUE, sort=c( ""test"", ""test_1"", ""test_2"", ""test_3"", ""test_4"", ""test_5"", ""test_6"", ""test_7"", ""null"" )) # To make the model in a more visually pleasant form, use the ""plot"" argument, # also find it here: see: https://github.com/ninfi/scipy_scipy/blob/master/README.md # For the code used in the papers, see ""Ning et al. model: Comparison of training and test results"" and the paper ""Ning et al. Improved Data-only Models, Comparing Neural Networks with SEE Using the GAN and SVM Models."""	False
"
 input (English text): Provide a list of papers that have utilized the Prior hs model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Duel hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Duel hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" I know how to get past the error message (in the JSON output) and I know the problem. I don't want to waste my time in figuring it out, but I've seen a lot of questions like these asking about how data scientists are to do things like this. The reason why these are so confusing to me is that they imply something so basic and obvious, so common and obvious that we might assume that everyone knows it, and yet our own research may fall far beyond that surface, and may be even outside the domain of our specialty. That's why it's very important to find out what the question about your problem is, before you start asking about what other researchers do. In this example I'm using the words ""prior hs data"" and ""dataset."" Both of these are the exact same to me, but they mean different things to someone. The question ""are data scientists using prior hs models ?"", means that if this tool works, and if ""dataset"" works, then I should start with that. If this tool works, but if ""dataset"" does not, then I do not know how the system works. If the user gives me different answers about the function that does work, I may be able to figure out how it works if I follow the link between the two systems and learn about all the differences of the 2 systems, and what are their key differences as far as the way they communicate with each other, and that way I can figure out what I should ask about prior to asking more questions. I can see the problem of making too many guesses, and I think they're good guesses. For example: you might say ""prior hs model"" and you expect ""dataset,"" but there's only one answer: yes, data scientists do use priors. The second answer, ""prior hs code"" doesn't work either. That is a bit confusing. Another example: you might say ""prior hs model"" and you expect ""dataset,"" but there's a second answer ""prior code."" Again, we're talking about many possible answers here. This is a really good guess, you don't know how exactly it worked but you should find out. Here's another question: you might say ""prior hs model"" and you expect ""dataset,"" but there's a second answer ""prior code."" Again you don't know how exactly it worked but"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset OntoNotes?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Human-annotated)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?value ********. FROM DIR ORDLIST(d.text) WHERE name = ?datasetname; input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Sparql query) SELECT DISTINCT ?value ******** OUTPUT DATE: Tue, 02 Oct 2013 15:00:10-03:00:00 EST -0700 input (English text): What is the top benchmark result (metric and value) over the dataset ORDINCE_INN (Russian text): Can you translate from the text of the paper in Russian to English ? output (Sparql query): SELECT lang ?language FROM DIR ORDLIST(d.text) WHERE name = ?datasetname; input (Russian text): Can you translate from the text of the paper in Russian to English ? output (Sparql query) SELECT lang ?language FROM ORDINCE_INN(d.text) WHERE name = ?datasetname; input (Russian text): Can you translate from the text of the paper in Russian to English ? output (Sparql query) SELECT lang ?language FROM ORDINCE_INN(d.text) WHERE name = ?datasetname; input (Korean text): Can you translate from the text of the paper in Korean to English ? output (Sparql query): SELECT lang ?language FROM KOREAN INN(data); Input (English text): What is the top benchmark result (metric and value) over the dataset GIRLS_STARTED (Growth stats for girls) output (Sparql query): SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:DIMENSION ?scale( ?value). ?scale( :1). ORG_ID ?dataset_id. ORG_LANG ?language. ORG_HANDLER ?	False
"
 input (English text): Provide a list of papers that have utilized the Adaptive Input Very Large model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Adaptive Input Very Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Adaptive Input Large model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Adaptive Input Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Input and output data set: Output of the benchmark_adaptive_input_very_large_model_benchmark dataset: 	False
"
 input (English text): What models are being evaluated on the Atari-57 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari-57"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Atari 2600 Solaris dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Solaris"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari-55"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Output #1 (with a row for a model for a given time in seconds): A new and large dataset was added to our benchmarking arsenal for this project. We used the Atari-57 dataset that we were using in our previous test on the Atari 2600 Solaris and the newer A8/A10/A20/Atari Pico but added the following data which we had not been testing on before this round. In this round: Atari Pico: data: Atari-57_SUN_TEST_2 data: 1: Atari_5701_PICOC data: 2: Atari_5701_PICOC data: 3: Atari_5701_PICOC_LOST data: 4: Atari_5701_PICOC_LOST_LOST A total of five Atari Pico models were included: model_rdf: data: ARDUINO-5701 - ARDUINO-5501 - ARDUINO-5501_PICOC - Atari-5701_PICOC_LOST model_lbl: data: ARDUINO-5701 - ARDUINO-5501 - ARDUINO-5501_PICOC - Atari-5701_PICOC_LOST model_rdf: data: ARDUINO-5701_TEST_2 - ARDUINO-5501_TEST_2 - ARDUINO-5501_TEST_2_LOST model_lbl: data: ARDUINO-5701_TEST_2 - ARDUINO-5501_TEST_2"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MultiNLI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT p.author.name, p.subfield.value, p.parameter1.value AS parameter2, 1, IF(ISNULL(value)) p.value, IF(ISNULL(parameter2), p.value); select p.author.name, p.subfield.value, p.parameter1.value AS parameter2, 1, IF(ISNULL(value)) p.value, IF(ISNULL(parameter2), p.value); Select b.name, b.subfield.value, b	False
"
 input (English text): Where can I find code references in papers that have used the Large mLSTM +emb +WN +VD model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Large mLSTM +emb +WN +VD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the MMV model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMV"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 If the text is incomplete, please add it. Please don't leave this form empty. If you know when, where, and how to find help for a specific problem, please leave this field blank.	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Centipede"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 Select the top benchmark score of 2 in the Atari 2600 Centipede dataset and its metric. The data is available at github and we can query this with dplyr : data(Atari_Centipede1) #... the data is available at github data(Atari_Centipede2) ... a lot of code to download the data, so you might want to download the latest version first. Now it's time to start doing some analysis so I've added an analysis to the top, top 1 and top 5 rows to get some sample values and to see how the data looks in the context they've been collected over 1,000,000,000 runs and then to apply different analyses. The model (the top values) are the same so let's put in that all the top values for the Atari 2600 Centipede dataset.  The different values are generated using the model classifier (the one from rdfs ), so let's make the top top 0 and 1 values the input (i.e., the 0 and 1 values).  Now, this is not very nice to do with dplyr if you're dealing with multiple data frames, but it's very easy with the SPSS method, so I'll just do the thing like in dplyr. The next step after that is to add a different factor analysis for the top results, and an independent factor analysis for the top top 5 values.  Let's first do the factor analysis at the top, top 1 and top 5 rows again, then do independent factor analysis at the top, top 2 and top 5 values again.  Let's use the same methods as before, but for each data frame, we're going to add a factor analysis.  Then, for each data frame, we'll do an Independent and a Multi factor analysis. The method which I'll be using to do these analyses is called rmark() but I'd be happy to learn any other names for it. Rmark() is essentially going to create a factor and select the top factors at the top and the top 1 and top 5 values at the top.  But there's a small drawback for me since I'm not very familiar with rmark(), so I wanted to make sure I'm not forgetting something and that it's also not something with a specific name.  That's what I'll do. Next to Rmark(), we need a few more functions,	False
"
 input (English text): Could you provide a list of models that have been tested on the Sequential MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 You can now find further discussion of our approach in the paper Modeling Random Graphs with Parquet The results so far are much better than we anticipated. A lot of work has been done to make our data structure really fast, and also to make our queries really fast (which was one of the primary reasons for this challenge). This was a great opportunity to improve the performance of the program. We are very happy with the way this has worked out.	False
"
 input (English text): In what country was conducted research with the largest number of participants?
 output (Sparql query): SELECT ?country WHERE { orkgr:R44980 orkgp:compareContribution ?contrib. ?contrib orkgp:P15249 ?country; orkgp:P23169 ?number_of_participants. } ORDER BY DESC( ?number_of_participants) LIMIT 1
 input (English text): Where did the study with maximal geographic scale take place?
 output (Sparql query): "	SELECT ?location, ?location_label WHERE { { SELECT (MAX( ?geo_scale) AS ?max_geo_scale) WHERE { orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale. } } orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale; orkgp:P37524 ?location. ?location rdfs:label ?location_label. FILTER( ?geo_scale = ?max_geo_scale) } 	____________________________________________ Name ____________________________________________________ Country State ____________________________________________ ____________________________________________ ___________ ___________________________ ___________________________________________________________	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WLPC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WLPC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT * FROM gigacoobserver WHERE gigacoobserver!= ""WIL"" AND { ""is_user"" : ""1"" AND ""user"" : ""wil"" AND ""pass"" : ""MYPASSWORD"" } output (Sparql query): SELECT ID, (ID), (ID) AS _ID, (ID) AS model_id, (ID) AS feature_list FROM gigacoobserver_lbl_ltr WHERE model_id IN (3, 4, 5 ) AND rdfs:label ?feature_list_lbl. GROUP BY id. ID Once again, we'll query the aggregated features for each model. gigacoobserver { 'user' : ""WIL"", ""pass"" : ""MYPASSWORD"", 'role' : ""Administrator"" } SELECT * FROM gigacoobserver WHERE { ?feature_list_lbl. ?feature_list_lbl. } output (Sparql query): SELECT ID, (ID), (ID) AS _ID"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the TSE-NER dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT * FROM `tempest_data` A B C D TEST RUN --------------------------- SELECT A.name FROM `tempest_data1` B.name FROM `tempest_data2` C.name C 1 Test Run #2 (30 days) Note: In these two examples, tempester outputs with column names of the form 'name=value' in the database output (name and value, respectively). These column names cannot be used with the 'tempester' library functions. Test Run #3 (30 days) Test Run #6 (30 days) Test Run #1 (30 days) Test Run #2 (30 days) TEST RUN --------------------------- SELECT * FROM `tempest_data` A B C D Test Run --------------------------- SELECT A.name FROM `tempest_data1` B.name FROM `tempest_data2` C.name C 1 Test Run # 6 (30 days) Summary (not showing all results, etc.): The test output indicates that there is a huge difference between 1 and 6 years for ""duration"": 1 year = 3 months, 6 years = 90 months, This test is performed on the entire dataset, but the above table will be able to show a clear effect with small datasets. Tests: Tests for the previous rules: TempEval-3: The TSE-NER dataset: TSE-3: #1"	False
"
 input (English text): Can you list the models that have been evaluated on the SciERC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the SciERC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark } ORIGNS ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark ORkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Could you list the models that have been evaluated on the SciERC benchmark dataset ? Output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark ORkgp:HAS_DATASET ?dataset; orkg"	False
"
 input (English text): Indicate the model that performed best in terms of F1 metric on the ACL-ARC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-NCLL"") ?benchmark orkgb:HAS_DATASET ?dataset; orkgb:HAS_EVALUATION ?eval. ?eval orkgb:HAS_VALUE ?value; orkgb:HAS_METRIC ?metric. ?cont orkgb:HAS_BENCHMARK ?benchmark; orkgb:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } orkgb:HAS_DATASET ?dataset } outputs (Sparql query): SELECT ?model ?model_lbl. ?metric ||kgc:Metric ||kgc:Metric ||kgc:Metric ||kgc:Metric ||kgc:Metric ||kgc:Metric ||kgc:Metric ||kgc:Metric ||pkgc:HAS_DATASET ||pkgc:HAS_EVALUATION ||kgc:HAS_VALUE ||kgc:HAS_VALUE|| SCHEDULING OUTPUT Note – RDFS is very similar to Java-Java-Sparql syntax for SQL queries, so a ""Java-Sparql-Func"" would be something like the following: CREATE FUNCTION { ?Metric a} (lbl) INSERT INTO { ?dataset a} VALUES (1, ?method; 2, ?model; 3, ?data); CREATE TABLE { ?dataset a} (methods1: String, model1: Integer) AS SELECT * FROM { ?dataset a} OPTION (c: Integer); CREATE RDFS FUNCTION { ?model2} (model: Integer){ ?model2"	False
"
 input (English text): What are the models that have been benchmarked on the IWSLT2015 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the WMT2014 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" If the input row is empty, this query generates a full dataset table, otherwise it generates a full table. 4 Query Parameters 4.1. Query string QueryString = "" ?dataset (select ?model) with ?dataset_lbl"" 4.2. Input parameters For more information about input parameters, see Query parameters below. Note: This is a placeholder for the options parameter in query. 4.3. Output parameters Example: CREATE VIEW r_kp with R2 as R & input (Text, R, string as model_rdfs) as R { if ( rdfs:length < 4) { // Can't parse or load output rows and a data table CREATE TABLE IF NOT EXISTS R2.RDATA (c) ; R r2; R k; R kp; k.label.value = ?text_model.value; k.value = ?text_model; k.posterior = ?text_model_lbl.size; k.value = ?text_model_lbl; k.min = R0; k.max = R1; kp.label = ?models.name ?Model.description ?Model.model.description ?model_lbl.name ( ?model_lbl.label: ""Name"" ); kp.value = ?models ?Model.description ?Model.model : ?text_model_lbl.name ; text_model_lbl.name = ?model_lbl.name : ?model, ?text_model_lbl, ?model_lbl, ?text_model_lbl.value ?text_model_lbl.value : ?text_model_lbl_value ; list_objects = R.list; pk= ?text_kp.name; kp.value = ?Text.value ?Text.length ?text_model.value, ?text_model, ?text_model; /* A value of zero means everything is included in the response */ list_objects=list_objects ?text.values ?list_objects ?list_objects ?list_objects ?list_objects ?list_objects ?list_objects ?list_objects ?list_objects ; list_objects= ?text_kp.name ?Text.value : ?text_model_l"	False
"
 input (English text): What is the highest benchmark result achieved on the Yelp-2 dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Yelp-14 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	--------------------------- Benchmark Results ----------------------------- 1,722 It's also possible to output to a text file like this at the end of the process. input (Text file, delimited by |) : How do you want to store the output ?	False
"
 input (English text): List the code links in papers that use the DDRL A3C model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDRL A3C"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF (1 day) hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl.FILTER (str( ?model_lbl) = ""A3C"") ?benchmark ? orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } A1R Benchmark (S) In A1R Benchmarks, which can be read as either A1R1 or A1R2 ? input (English text): The 1 day/17 month/year in which the benchmark was run. output (Sparql query): SELECT DISTINCT ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } A1R Benchmark (L) In A1R Benchmarks, which can be read as either A2C or A2L ? input (English text): The 1 day/17 month/year in which the benchmark was run. (Note that the above has been tested with 3 out of 10 benchmarking files and they have been successful.) output (Sparql query): SELECT DISTINCT ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_MODEL ?model;"	False
"
 input (English text): What is the best performing model benchmarking the RTE dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the RTE benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 (0) Please provide a list of models that have been tested on the RTE benchmark dataset in order to provide more detail to this question. Please provide your preferred order from left to right of your model. Please provide your preferred order from left to right of your model. Please provide a full list of the tested datasets/models. Include model labels and data files (including results etc) in the RDFS output. The test datasets must have been validated with the test data model (if any). Please provide your preferred order in which the models were tested, with the first model being the fastest. Models that should have been excluded from the RTE benchmark may also be excluded from the RTE reference implementation. Please specify which dataset (if any) you would like included in the reference implementation. How many model train trees are needed to fit all RTE models that have been tested ? Please specify the number of train trees per model or per epoch. Please specify a specific epoch or epoch range. Please provide the number of test trees for each model you would like included in the reference simulation. The test models should have been evaluated using the test data model. The model trees for each model have to be loaded and initialized before each model measurement is taken. Is the reference implementation compatible with different RTE datasets ? Please specify if the reference implementation uses the same RTE format as the model dataset. A full list of different RTE formats is available here. Is the reference simulation compatible with the model dataset ? Please specify if the reference simulation uses different RTE format than each RTE model dataset (e.g. RTE.0). A full list of different RTE formats is available here. How much space do model trees and their inputs take up during training ? Please supply the number of model trees and their inputs used during training. Model trees are used to describe the training behavior of training models on the RTE dataset. Model trees have the following properties: Training the model tree takes time and resources. The training process for each model tree will take a number of epochs depending on the size of the model (number of training epochs will depend on the size of the model for the specific dataset). The output dataset for each model tree will be used as a training data model for each of the model's training epochs. This use will reduce the training time for each RTE simulation. This may also reduce the computation overhead the model trees incur during training. Training each model tree takes	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WOS-11967 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	--------------- WOS-5736 WOS-5913 WOS-5477 WOS-5930 WOS-5937 WOS-5480 WOS-5835 WOS-5938 WOS-5597 WOS-5837 WOS-5748 WOS-5749 WOS-5750 WOS-5653 WOS-7143 WOS-7144 WOS-7280 WOS-7481 WOS-7090 WOS-7472 WOS-7423 WOS-7472 WOS-7513 WOS-7514 WOS-7515 WOS-7546 WOS-7544 WOS-7552 WOS-7570 WOS-7580 WOS-7582 WOS-7579 WOS-8100 WOS-8102 WOS-8103 WOS-8104 WOS-8105 WOS-8105 WOS-8109 WOS-1181 WOS-1221 WOS-1222 WOS-1223 WOS-1234 WOS-1235 WOS-1236 WOS-1238 WOS-1241 WOS-1246 WOS-1248 WOS-1254 WOS-1255 WOS-1257 WOS-1262 WOS-1261 WOS-1267 WOS-1403 WOS-1404 WOS-1435 WOS-1436 WOS-1453 WOS-1455 WOS-1472 WOS-1486 WOS-1488 WOS-1548 WOS-1626 WOS-1635 WOS-1661 WOS-1661 WOS-1666 WOS-1807 WOS-1812 WOS-1813 WOS-1515 WOS-1649 WOS-1670 WOS-1806 WOS-1837 WOS-1844 WOS-1844 WOS-1865 WOS-1870 WOS-1880 WOS-1883 WOS-1883 WOS-1884 WOS-1884 WOS-1884 WOS-1884 WOS-2047 WOS-2049 WOS-2051 WOS-2052 WOS-1947 WOS-2300 WOS-2301 WOS-2324 WOS-2351 WOS-2368 WOS-2367 WOS-4038 W	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TITLE FROM (SELECT  title FROM (SELECT  title FROM DBLOB( ?table  )) ); SELECT  * FROM ( SELECT  dataset  FROM  SELECT   ?dataset ) ; The title column should refer to the title of the title file in which the benchmark was performed, not to the paper. For this to work the query language must be English. To create the tables in the following table a different way: SELECT TITLE FROM (SELECT  title FROM (SELECT  title FROM DBLOB( ?table  )) );  SELECT  * FROM (SELECT  dataset  FROM  SELECT   ?dataset ) ; First, to create the table on the basis of a title file the following queries may be made: SELECT TITLE ( TITLE ); SELECT TITLE ( TITLE ( TITLE ) ); SELECT  TITLE ( TITLE ), TITLE( TITLE ); SELECT  TITLE ( TITLE ), TITLE( TITLE ); However, the second way is simpler and more efficient to execute since it is executed for each item which is created each time by the first query. To create the table on the basis of a list of papers, the following queries may be made: SELECT TITLE ( TITLE, ORDER BY Title ); SELECT TITLE ( TITLE ( TITLE ),  TITLE, TITLE( TITLE ) ); SELECT  TITLE  ( TITLE( TITLE ), TITLE( TITLE ) ) FROM ( SELECT TITLE  . TITLE  ( TITLE ), TITLE ( TITLE ),  TITLE( TITLE ) FROM DBLOB ( ?list ) );   SELECT  TITLE ( TITLE. TITLE  ( TITLE ) ), TITLE( TITLE ), TITLE( TITLE ) FROM DBLOB ( ?list ) ); It should be noted that all of these queries are executed without regard to the type of the dataset, and they are all executed using one SELECT statement. To create the table generated by the second and third queries use the same syntax	False
"
 input (English text): List the metrics that are used to evaluate models on the SciERC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ART/CoreSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ART"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Here is the query that produces the output you would ask for: s.matrix_matrix().matrix_matrix(p).output( 'DISTINCT ?metric ?metric_lbl WHERE { ?dataset a ORkgc:Dataset; rdfs:label ?dataset_lbl. }') To read more on the Mathew Taylor approach in R we have an excellent blog post from Rob Taylor, a former NLP analyst and current R Engineer at Microsoft Research. It's excellent, and we really encourage you to read and follow it. It is also embedded in the R documentation. Here's an example of how you might use R's matrices as the input for a query, without using any fancy pandas libraries: tput <- seq('a', 1:10) g <- c(c(15, 20, 30, 40, 45, 25, 10, 3, 15, 20, 23, 35, 20, 6, 15, 18, 25, 43, 10, 7, 13, 10) * xrange(10, 7)) df <- c(tput, dplyr(""META-DATA"", header = c(""DATE"", ""MATERIAL"", ""DATE"", ""TIMESTAMP"")), g,...) df <- dplyr(""META-DATA_1X1"", header = c( ""date"", ""MATERIAL"", ""date"", ""IMAGE"" ))[-1:] pdindex(df) In R this query is similar, but the underlying data will have been filtered. Here's a version with no filtering at all by using a T-SNE (Threshhold-Nearest-Neighbor) algorithm:"	False
"
 input (English text): Could you provide a list of models that have been tested on the Natural Questions benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the Natural Questions (short) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"-------------------------- --------------------- | model | score | -------------------------- | ?dataset a orkgc:Dataset| 4.79 | 4.73 | | ?dataset f8:Data| 4.78 | 4.80 | This output was taken from the data file that was created from the data entry created above. Here you can see that it produces a list of models with the highest score. Also, one interesting thing to note is that the score doesn't vary significantly. When you enter a model from a different dataset, the score is the same. To summarize the results, in our testing over the last 6 weeks we have learned that : 1. Data-driven research using Natural Questions can impact innovation as you may be able to improve the outcome of your research by looking at natural language models used in natural language processing. 2. Using Natural Questions can help build better language processing pipelines for science using R, MATLAB and other scientific programming languages. 3. Data-driven research using Natural Questions can help make scientific pipelines easier to maintain. Overall, I find Natural Questions to be a great practice for those doing data driven research in the context of our industry. As data analysis becomes more automated, Natural Questions is a great way to use data analytics to improve the results of research. One way to put it is that data science needs ""data scientist's minds,"" and I think Natural Questions has the tools that are needed to help bring that to reality."	False
"
 input (English text): What is the highest benchmark result achieved on the WMT2014 English-French dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WMT2014 French-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score ) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } GROUP BY ?metric ?metric_lbl input (English text): Can you provide the highest benchmark result, including the metric and score, for the WMT2014 French-Spanish dataset ?  output (Sparql query):  SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score ) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-Spanish"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } GROUP BY ?metric ?metric"	False
"
 input (English text): What is the best performing model benchmarking the WikiText-2 dataset in terms of Validation perplexity metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Validation perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Test perplexity score on the WikiText-2 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric c:Metric; ( ?dataset a) c. ?dataset_lbl } ORDER BY c LIMIT 1 Inputs and Output files created in this repository will result in a file that is called model_lbl.rls (or model_lbl.db5 if all the other queries have been run against the dataset, but the model_lbl.rls file will be ignored). When running a test query, the output file will be written to /tmp/test_lbl. The output will appear similar to the following: -- Test perplexity metric: ?c:metric;  -10=lowest, 0=highest --------------------  -10=lowest, 0=highest  |  *|  |  *|  | --------------------  -10=lowest  | *| ------  | ------  | * | |  | ---- ( 1, 11, 13, 19, 25, 31, 33, 37) The value of ?c will be interpreted as the value expected from a ""single row"" or ""batch"" of data and will be used to construct an algorithm. The following algorithm is evaluated for each model (this is also called a test model for the purpose of writing a test query) The first row of the data in a data set is considered the ""best"" and that is typically the ""lower bound"" of a model. This row is used to train the algorithm. The first row of each data set must be filled with data that is representative of most likely outcomes (in our case all values are positive). To fill all values with positive values, we can use the first row from the data that is considered the ""most likely"" scenario for the model. Once the data is filled with most likely outcomes, we can compute the validation perplexity (CV) by counting the total number of unique keys in a set of rows which have a value in each column. The following figure shows the number of errors in a batch of 8 rows of data. While there are many possible errors, it is not optimal to simply count the total number of unique keys or the number which are unique. To address this concern,"	False
"
 input (English text): List the code links in papers that use the All-attention network - 36 layers model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""All-attention network - 36 layers"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the All-attention network - 18 layers model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""All-attention network - 18 layers"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 id num_links num_labels num_cols id id num_links num_labels num_cols ?code ? id ?code ? id ?num_links ? num_labels ? num_cols ?code ? id ?code ? id ?num_labels ? num_cols ?code ? id ?code ? id ?num_labels ? num_cols ? orkgp:HAS_DATASET orkgp:HAS_MODEL orkgp:HAS_SOURCE_CODE orkgp:HAS_SOURCE_CODE orkgp:HAS_SOURCE_CODE The code references in the Socrata corpus used the following identifiers: ORGYV.GOOGLE_DATA_SEAT_NUMBER. All orgyv.google.com and orgyv.google.pdf The code references in the Socrata corpus used the following identifiers: ORGYV.HELP_NUMBER. All orgyv.helpingpeople.ca and orgyv.hospice.org The code references in the Socrata corpus used the following identifiers: ORGYV.MISSION_NUMBER. All orgyv.mission.org.uk The code references in the Socrata corpus used the following identifiers: ORGYV.MEDICAL_REPORTS_NUMBER. All orgyv.medicalreports.com The code references in the Socrata corpus used the following identifiers: ORGYV.MUSIC_REPORTS_NUMBER. All orgyv.musicrepet-org The code references in the Socrata corpus used the following identifiers: ORGYV.LANGUAGE_REPORT_NUMBER. All orgyv.languagereport.com The first three code references appear in the ORGYV.GOOGLE_DATA_SEAT_NUMBER. The last three code references appear in the ORGYV.HELP_NUMBER. All ORGYV.helpingsources.org The three code references appear in the ORGYV.MEDICAL_REPORTS_NUMBER. None of the ORGYV.MISSION_NUMBER or ORGYV.MISSION_NUMBER appear anywhere in the Socrata corpus, however. This is pretty odd, and shows a rather poor attention to detail. One would have	False
"
 input (English text): what are the evidence and limitations of paper ""Evaluating the Use of Social Networks in Author Name Disambiguation in Digital Libraries""?
 output (Sparql query): SELECT ?evidence ?evidence_title ?limitation WHERE { orkgr:R6751 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5004 ?evidence; orkgp:P5006 ?limitation. ?evidence rdfs:label ?evidence_title. FILTER(REGEX(STR( ?paper_title), ""Evaluating the Use of Social Networks in Author Name Disambiguation in Digital Libraries"", ""i"")) } 
 input (English text): Does paper ""Disambiguating authors in citations on the web and authorship correlations"" employe Cosine similarity?
 output (Sparql query): "	"ASK WHERE { orkgr:R6187 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5002 orkgr:R6006. FILTER(REGEX(STR( ?paper_title), ""Disambiguating authors in citations on the web and authorship correlations"", ""i"")) } "	 ( ?Evidence: ?Evidence: ) input (English text): Do authorship correlations are based on authors and journals ? output (Sparql query): ( ?Evidence: ?Evidence: ) input (English text): In what way is Cosine similarity calculated in the following table. output (Sparql query): r ( SPSS Version 12) r r ( SPSS Version 12) input (English text): What is Cosine similarity output (Sparql query): r n input (English text): What are the two forms of n ? output (Sparql query): Output Name of rn Name of mn input (English text): What are the inputs for cosine similarity ? output (Sparql query): Output r Input 1: (Input 1): Input 2: (Input 4) Input 4: (Input 7) input (English text): What is cosine similarity. output (Sparql query): Output Name of cosine similarity Name of mn input (English text): What are the inputs for cosine similarity and its correlation table ? output (Sparql query): Output Name of cosine similarity Name of mn Name of n input (English text): What are the two inputs for cosine similarity and its correlation table. output (Sparql query): Output Name of cosine similarity Name of mn Name of n input (English text): What are the two inputs for cosine similarity and its correlation table. output (Sparql query): Output Name of cosine similarity Name of mn Name of n input (English text): What are the two inputs for cosine similarity and its correlation table. output (Sparql query): Output Name of cosine similarity Name of mn Name of n input (English text): What is Cosine similarity and its correlation table. output (Sparql query): Output Name of cosine similarity Name of mn Name of n input (English text): What are the two inputs for cosine similarity and its correlation table. output (Sparql query): Output Name of cosine similarity Name of mn Name of n output (English text): In what way is the following query. output (Sparql query): Input 1: Input 3: Input 5: Input 6: Input 7: Input 8: Input 9: Input	False
"
 input (English text): Can you list the models that have been evaluated on the SciERC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the STS Benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" The output tells us that there are 21 models in SciERC. I can't find a model I want to evaluate on the STS dataset, or one that hasn't been evaluated elsewhere. I've seen some pretty good stuff going on here, so let's dig in a bit. Let's see what the SciERC paper says about some model evaluation methods. I've written a couple of scripts for doing a lot more analysis. # Get an individual model variable for all models used in this study. data_inputs = { ""n_models"" : [ 1, 2 ], ""sample_size"" : 2, ""model_cnt_per_individual"" : 1 } # Get the models and their average c.models = model_cnt_parsed[model_cnt_parsed] # Get the model by default, or get its evaluation by calling model_cnt_eval = model_cnt_parsed[evaluation_dataset].model # Get the model by any label, or get evaluation by calling either model_cnt_eval[label], or model_cnt_eval[label[,c]] # Get the current model evaluation. data_eval = model_cnt_parsed[evaluation_dataset] # Get the current model (average) by calling model_cnt_eval.default # Get the evaluation by default, or get evaluation by calling model_cnt_eval.eval # Get the evaluation by any label, or get evaluation by using either model_cnt_eval[label], or model_cnt_eval[label[,c]] # Get the evaluation by default, or use the evaluation by calling model_cnt_eval.eval # This will show you the model and the evaluation based on the new label. # You can also choose to display a histogram of the labels to compare evaluations. data_eval. hist_x = data_eval. hist_x # Get the model by any label, or set it to 1 by calling model_cnt_eval.model # Set the evaluation by default, or get evaluation only by using model_cnt_eval.eval # Get the evaluation from the current (default) model, or set for validation by calling model_cnt_eval.eval # Get the evaluation by default, or use the evaluation by using either model_cnt_eval.eval[label,c], or model"	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Phoenix benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Phoenix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Freeway benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgr:HAS_DATASET ?dataset; orkgr:HAS_EVALUATION ?eval. ?eval orkgr:HAS_VALUE ?value; orkgr:HAS_METRIC ?metric. ?cont orkgr:HAS_BENCHMARK ?benchmark; orkgr:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } The Atari 2600 Phoenix benchmark is an unofficial benchmark for the Atari 2600's Phoenix architecture. The benchmark data set is collected daily and is available from http://www.marc.inria.fr/~eutopia/atari_2600.html. The benchmark was created by Jean-Paul Czajkowski, a PhD student at the École Polytechnique Fédérale de Lausanne in Switzerland. The benchmark was originally designed with the aim of optimizing Atari 2600 ports (which are highly optimized in the original) but that idea is now not necessarily maintained. The benchmark dataset was generated by running the following code against a baseline version of the game: A table of the benchmark results is available here. The main benchmark data is stored as a structured SQLite file as follows: select date as timestamp from time set date, score from bench if time.clock {  select ?dataset, ?model and date ?dataset, ?model_lbl and score ?stat, ?model } else {  select ?dataset, ?model and date ?dataset, ?model_lbl and score ?stat, ?model } select ?baseline, ?criterion, ?metric from the time table table1,  |  |  |  |  |  |  |  |  |  |  |  |   | | | | | | | |  ||  ||  ||  ||||||  ||  ||  ||  ||  || "	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the DDI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DDI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the DCASE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" RDFS: Output Data This RDF file is the output for two different SQL output files, one for the DCASE dataset, and one for the DDS dataset. Both RDFS files have a single row per column using a special RDF Schema, called a schema, with the following attributes: schema_id: the ID of the schema schema_name: the name of the schema (e.g. ""dcfschema"") schema_type_key: key to use to identify a schema key_type_key: key to use to identify a key This RDF resource can be created on your computer but you should probably save this RDF resource in a single directory, like data/default/demo-sources. For example, to create a new RDF resource called data/default/demo-sources, navigate to directory containing the example RDF resource and simply execute the following command: install.packages(""data/default/demo-sources"") install.packages(""data/default/demo-sources/dcase-benchmark-schema"") installation_done RDFS: Resources Note that, in order to use this example RDF resource, I need to get a copy of the data, data/default/demo-sources/dcase-benchmark-schema from the project under your research. This project is described in the RDFS chapter in the book on RDFS, by David Williams and Peter Huttler, who have provided some excellent example RDFS resources. For more information about RDFS: Resources, please see the RDFS chapter and the RDFS documentation and tutorials Example Files For a real-world example, here are six sample RDF resources that you might want to play around with. You can run the examples in this book on a sample RDF resource file in the data/default/demo-sources/ DCASE-benchmark-repo/example and, for RDFS: RDFS resources, you can use the files and get results in your favorite RDF editor. Remember that the example is a test environment and RDFS resources should be used in an environment where you have a reasonable amount of data, e.g. to build a training dataset or an RDF data scientist's research needs. The sample can be modified to work with"	False
"
 input (English text): What is the best performing model benchmarking the Yelp Fine-grained classification dataset in terms of Error metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 output (English text): OK, a benchmarking benchmark score for the Yelp Fine-grained classification dataset has been estimated, and the metric Error metric has been calculated for each model. The table below shows the results according to the model selected: Model 1: ?s:s_model; r, g, t (mean, SEM) Model 2: 1% deviation from ?s:s_model; r, g, t (mean, SEM) Model 3: 0.5% deviation from ?s:s_model; r, g, t (mean, SEM) Error metric (mean) -0.042869 0.068963 0.0281398 0.144796 Total model score = 86.569 Error metric (SEM) 0.977 0.095 0.53912 0.0929 0.17087 Performing the benchmarking benchmark means that it's important to understand the output and test method. The data that was used to set the model benchmarking criterion were a set of 4,000 classified restaurants and 30,000 ratings on Yelp. In addition, the actual model performance for the benchmarkings was extracted from the user. The actual model performance for the benchmarking was determined using R code and can be found here: Performance Analysis Tool. Please note that the quality of the model performance for the benchmarking does not match the quality of the model performance for the actual classifications that were used. It's very unlikely that the quality of the model performance for the benchmarking is as bad as the quality of the actual output. I recommend you use the output to get to the actual output when choosing a model or when performing the benchmarking. You can also run the benchmarking again to verify that the performance is again acceptable. User data So far, I've been using R scripts to run a benchmark to find the best performing model benchmarking a set of Yelp Fine-grained classification datasets. I will now turn to R scripts to do real world application or analysis for real world use or data. User data I made a quick demo application that simulates customer behavior on Yelp. My main user data is a Yelp classified user dataset, with the exact Yelp Fine-grained classification dataset, and 4 classifications of all Yelp Fine-grained classification datasets. Here are the results for the Yelp Fine-grained classification dataset, by the user classifications. The output can be viewed for the actual classes that are used for	False
"
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Battle Zone benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 River Raid benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } We can then ask the user for his/her own model.  Now if you look at the model_s1 column, you will have a list of all the output of the model. But what if we want something different (i.e. a custom model) ? To answer that question, let's create a table like this: SELECT a, model_lbl as rdfs, c1 as model, c2 as model_lbl, model_s1 as ctr1, model_s2 as ctr2 FROM  user.model_s1  ORDER BY rdfs With all these columns inside users table in our Postgres database: The output of the model_s1 column will be in users table. The output of the model_ls1 column will be in users table as a simple list of model names. However, the output of the model_lbl column in users table will also hold the output of our model name (s1, s2). And so, how can we create a custom model if we have a standard one ? Actually, what we have to do is the same as if we wanted to add some new model.  How does our user create a custom model ? Create a new model and save it to users table, and then create another table, this time for the new model and create"	False
"
 input (English text): Provide a list of papers that have utilized the BertSumExt model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BertSumExt"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BERT + BiLSTM + CRF Decoding model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT + BiLSTM + CRF Decoding"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?code WHERE { ?model a orkgc:Model; ?rdfs:label ?model_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; ?cont orkgp:HAS_SOURCE_CODE ?code. } Input-Output Data Model Input : the input data of the model (e.g. RDFS/CSV) Output : the output data (e.g. Hadoop Tagger) Input : the input model (e.g. RDFS/CSV) Output : the output model (e.g. Hadoop Tagger) Input : the input decoder (e.g. BertSumExt) (e.g. RDFS/CSV) Output : the output decoder (e.g. Hadoop Tagger) Input : the input RDFS/CSV output class output class (e.g. Hadoop Tagger) Output : the output RDFS/CSV output class Input : the input Hadoop Tagger output class output class (e.g. Hadoop Tagger) Output : the output Hadoop Tagger Input : the data model of the model (e.g. RDFS/CSV) Output : the output data model Sparse Matrices A sparse matrix is a list of zero vectors in a list of elements, with one element per layer. Matrices for distributed data structures are created by taking a sparse matrices (SVM) and applying layer transformations such as mask, masking, eta etc. Input : a sparse matrices (SVM) matrix of the following form x:x[i,j]*j This is an example of such a matrix using the MSC4 data structure. x : a scalar matrix of length [i,j] : the components are row-major	False
"
 input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the BIOSSES benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pearson Correlation"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the MedSTS benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pearson Correlation"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE ?metric 1 This last query should not be used for model discovery, as it may return incorrect results. Instead, it's used to verify that a model does meet the specified criteria, without providing model output. If a class does not meet the specified criteria, it should be re-classified with a slightly different name class_test = input() > output(CASCADE) > The name of the class should match the name of the data set in order to identify the same class from different datasets. input (English text): Indicate the class for which you would want to modify the test/criterion data class model ? output (Sparql query): Select class ?class ?model. CASCADE. DISTINCT ?class_test ?model. DISTINCT ?model_lbl. DISTINCT ?model_lbl. FILTER ((is(Class) ?is(Class) ?ISClass ?isClass)) { SELECT class ?model..class_test AND ?model..criterion. DISTINCT ?criterion. DISTINCT ?criterion. FILTER (is( ?criterion ) ?ISCriterion ?isCriterion) { SELECT class ?model..class_criterion AND ?model..criterion. DISTINCT ?criterion. DISTINCT ?criterion. FILTER (is( ?criterion ) ?ISCriterion OR ?) { SELECT class ?model..class_criterion AND ?model..criterion. DISTINCT ?criterion. DISTINCT ?criterion. (IsClass( ?criterion )) OR ?model..criterion. DISTINCT ?criterion. (IsClass( ?) &&!is( ?criterion )) ; } } or kgs:value = input('value') > output(Sparql query) > Output : Class : ""Class"" Model : ""Model"" Validation : ""CASCADE"", ""BENCHMARKS"", ""ORGGP"" DESCRIPTION : This method calls the CASCADE model verification algorithm for all classes. This calls the model verification algorithm on the selected class. It returns a list of validated classes if all of the conditions specified by the model meet these conditions. The validation is done on all classes on"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the WOS-11967 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the WSC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TOP(3) name, urn | title, urn, title_num, urn_num, urn_req FROM research_paper WHERE ?dataset | ?benchmark || <dataset type: <dataset type> || <benchmark type: <benchmark type> | ?benchmark ( ?) | ?benchmark_num || urn | ?lab | ?lab_num || urn | ?title_num || urn_num | ?obstacle || ?obstacle_num || urn | ?obstacle_num || urn_req; name urn title_num paper OR kgp paper_lbl ?benchmark orkgp:HAS_DATASET ?benchmark ORkgp:HAS_BENCHMARK ?benchmark_num ORkgp:P31 ?benchmark_req output: 	False
"
 input (English text): List the code links in papers that use the CRF with term expansion model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with term expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the linear-chain CRFs model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""linear-chain CRFs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT STATEMENT ""term"". RANK FROM ORGPCRF. RANK ( ?code). RANK_1 ORGPCRF. DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with term expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?bench the measure. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code; orkgp:HAS_SOURCE_CODE ?source_code ORGPCRF. ?source_code ORGPCRF. STATEMENT "" ?source_code"". RANK FROM ORGPCRF. RANK_1 ORGPCRF. DISTINCT ?code WHERE { ?model a; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with term expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?bench the measure. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?source_code; orkgp:HAS_SOURCE_CODE ?source_code. } For benchmarks, the input files are required. All benchmark data (including results, summary of performance, and paper code links) are provided by the authors as part of the published material. All code and results are made available by authors for their private use. Benchmarked data should be viewed as a work in progress, updated when relevant. This benchmark includes only code to be produced for benchmarking purposes, and includes no other data. Note: As part of the submission process for CRFs, the code for specific benchmarks was uploaded as part of submissions and is stored in the CRF distribution. Please check the output files for specific benchmark code before applying them. If you notice any flaws, you can contact the authors. Thank you for your assistance."	False
"
 input (English text): Can you provide links to code used in papers that benchmark the DrQA (Document Reader only) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DrQA (Document Reader only)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DrQA model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DrQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" In order to use my code, you must: 1. Run the following command in order to install the code: cscript http://hq-lab.net/drqapidata_skeleton_c.htm 2. Run the following command in order to access all my code: cscript http://hq-lab.net/drqapidata_skeleton_v3.htm 3. Run the following command in order to build my code: cd http://hq-lab.net/drqapidata_skeleton 4. Run the following command in order to execute my code: cd mymod_pager/ 5. Run the following command in order to run my code: mymod_pager > < 10/1000 -l.txt I tried to build my code in order to build an open access database to the RDFS file. However, RDFS files generated using DrQA and my code were very similar. To be more accurate to the research work, I would need to benchmark DrQA first and then run it. My work has been published in: Bridging the Gap in Knowledge Analytics with RDF/ST Alvin Wang, Kevin He, Marden Jones We describe a simple pipeline for integrating complex, multi-dimensional, statistical models into rich web-based platforms to enable a new class of ""knowledge systems"" that leverage data to drive an information-rich user interface. A collaborative project between Microsoft Research and the RDF/ST consortium. https://arxiv.org/abs/1612.09725 The text below assumes that you've already built my module: hq_modules:install_and_run_scaffold hq_modules:install_and_run_drq_model hq_modules:install_and_run_dtrf_files Then, you can run my paper on HUDF using DrQA (see below).  For the first time, we have created a new model: dtrf for generating DrQA files from structured text. In this paper, we present a novel approach for linking rich data with a simple schema and an approach to test our approaches using large datasets. We describe a simple pipeline that uses the dtrf model with hierarchical, text-oriented language semantics to integrate complex, multi-dimensional stochastic dynamic systems. We present a way to create and link models"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset SST-5 Fine-grained classification?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ACCURACY FROM ?benchmark ?score ORDER BY ?metric input (English text): How were the scores calculated ? output (Sparql query): SELECT DISTINCT score ( ?value) AS score, FILL( score ); RDFS:  SELECT ?score from ?benchmark ?score; input (English text): Which SST classifier was implemented ? output (Sparql query): SELECT FIND( m, { ?model, ?model_lbl } ), { ?metric, ?metric_lbl } FROM ?classifier_lbl m CROSS JOIN   ?model ?metrics m ON  m.metrics  = m.metrics WHERE m.metric_lbl IS NULL  &&!m.metric; Selecting Metrics Sparql uses the RDFS and PostGIS tags for the inputs and the output. You can use the input tags with Sparql queries to select the metrics and the output tags with Sparql queries to output the results. To select the Metrics, we use the SST-5 metrics tag and the PostGIS metrics tag for the inputs. The tags can be obtained by using the SST-5 Metrics SDK. To create a new dataset, use the create-dataset command or download a sample dataset, make your selection, and save the dataset to a file. The name of the file has to start with a............................ If you want to specify the metrics used for the output, you must set the output_metrics attribute of the Sparql query that returns the result. For instance, to specify a maximum precision score, specify the SPARSQL-SDP-XML-METRICS variable. To get the metrics used in the training, run the train-inset command. For example, to get the post-processing metrics, run the train-inset. To select the metrics used in the validation, run the test-inset command. For example, to get the SPARQL-SDP-XML-METRICS, run the test-inset. To retrieve the metrics used in the training and validation run the training-inset. The following command gives you the same information when run	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "		False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Quora Question Pairs dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quora Question Pairs"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which QA benchmarks incorporating more than 10000 questions?
 output (Sparql query): "	"SELECT ?papers, ?papers_labels WHERE { ?papers rdf:type orkgc:Paper; orkgp:P31 ?contrib; rdfs:label ?papers_labels. ?contrib orkgp:P41923 ?questions_number_str. BIND(REPLACE( ?questions_number_str, ""[+]"", """") AS ?questions_number_preprocessed) BIND(xsd:integer(REPLACE( ?questions_number_preprocessed, """","""", """")) AS ?questions_number) FILTER( ?questions_number > 10000) } "	𝓁 COMPUTE (R) INPUT # (required) 𝓁 COMPUTE. 𝓁 COMPUTE: 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE (R) INPUT # (required) 𝓁 COMPUTE. 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE (R) 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE (R) INPUT # (required) 𝓁 COMPUTE. 𝓁 COMPute 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE 𝓁 COMPUTE (R)	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a berzerk:Berzerk; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Berzerk""). orpkgp:HAS_DATASET ?dataset; berzerk:HAS_METRIC ?metric. berzerk:HAS_EVALUATION ?eval. OPTIONAL { ?eval berzerk:HAS_METRIC berzerk:HAS_METRIC_LBL. rdfs:label berzerk_lbl. } } The first query returns a single parameter of a single model (see ""What models should be evaluated"" below). The next query returns only the most significant factor (the most likely model) when no model is found. What models should be evaluated ? Input: English text: What models are common for evaluating Atari 2600 Atlantis and Berzerk Atari 2600 Atlantis, Berzerk and Berzerk models, compared to Atari 2600 Berzerk and Atari 2600 Atlantis models, compared to Atari 2600 Berzerk and Atari 2600 Berzerk models, compared to Atari 2600 Berzerk and Atari 2600 Atlantis models. Output: Slicing into discrete dimensions The output of the above query is composed of a sequence of comma separated lists consisting of three key words separated by commas: ?module ?/rdfs/label ?metric ?/rdfs/str ?value ?/rdfS ?. All the keys are optional and the data is not ordered by the values. What should be evaluated: RDF, JSON, Python, XML and HBase What RDF or JSON variables should a model be composed of ? Data for the RDF is in a.db file and is in a language called xml. The model is in a python file. JSON contains several different data types. These data types are specified by data-types in RDF or JSON. See the documentation of the data-types for the description. What should be evaluated: HTML, XML and RDF What HTML and XML variables should a model be composed of ? Data for the HTML is in a html (and htm and txt ) file. The model is in a rdf xml file. RDF is a language developed by the World Wide Web Consortium and contains a language called R"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Freeway dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Stanford Cars dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?file ?file_lbl. FILTER (str( ?file_lbl ) = ""Assembler/Atari 8051"" ) ?benchmark orkgc:Dataset ?file. { ?dataset a orkgc:Dataset; rdfs:label ?file_lbl. } ?benchmark. ?page OR kgc:P32 ?cont OR kgc:P31 ?benchmark. ?sample OR kgc:P33 ?repl OR kgc:P34 ?record OR kgc:P35 ?record_count OR kgc:P36 ?rng OR kgc:P36 ?row ; ?dataset_lbl. }} Here is the output from the previous sql query (see the source code of the previous example). To test the performance of all the benchmarked Atari 2600 games, use the following SQL query to compute the performance of all the game benchmarking benchmarks from the Atrads dataset: output (Sparql query): SELECT DISTINCT ?benchmark ?benchmark_lbl. FILTER ( str( ?benchmark_lbl ) = ""Atarix"" )"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC Chinese-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Chinese-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" (Note: If the ""paper ID"" part of the query is empty, it means it was not provided by the user.) output (SQL statement): Output a list of the results of the query as an RDF document in the current format. -d This option forces the RDF language to display the results as the same format it does for non-document results in the BUCC and the BUCC-to-German files. In the RDF statements used in the documentation, the ""document id"" parameter is an identifier for the actual dataset. The ""benchmark id"" parameter is not used in the RDF statements for the benchmarks - these documents are generated with a non-identifier. If this option is not given and the specified language does not meet the language requirements for the RDF documentation, the language will be used. The default value for this option is ""UTF-8"". -d,o,o/x This option forces the RDF language to display the results as the same format it does for non-document results in the BUCC and the BUCC-to-German files. The ""document id"" parameter is an identifier for the actual dataset. The ""benchmark id"" parameter is not used in the RDF statements for the benchmarks - these documents are generated with a non-identifier. If this option is not given and the specified language does not meet the language requirements for the RDF documentation, the language will be used. The default value for this option is ""UTF-8"". -e Specifies that the results should be saved as an Excel file, such as an XML table. -f,--format [filename] [--column format] [--number format] [--limit=number] The file extension to use for the output and input formats of all output RDF statements. The options are one of the following: ""CSV"", ""XLS"", ""XLSX"", or XML. CSV is what is used by Amazon's DocumentCloud. XLS format is used for PDF output. ""XLSX"" may be used for PDF output, but it does a somewhat different job. Both text and data are output in a single XLSX file. The default format is ""CSV"". The second and third options allow you to restrict the output format to one of these formats. The fourth option restricts the data type of the data to be converted by a conversion algorithm or to a type other than ASCII, UTF-8, etc"	False
"
 input (English text): What is the top benchmark score and its metric on the Stanford Cars dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the Stanford Cars dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl(MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } } ORDER BY DESC( ?value) } In this case our dataset has no set of performance tests, so we are assuming that the only thing driving these cars are actual user queries. If we were to select car A, the first thing we would expect to see is that it would come with the highest score ("	False
"
 input (English text): What is the highest benchmark result achieved on the ShARe/CLEF eHealth corpus dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ShARe/CLEF eHealth corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 NAME Title (paper number) ID 1 or 2 4 1 2 4 3 3 4 4 3 4 3 4 4 4 3 4 3 4 2 4 3 4 4 4 3 4 4 3 3 3 5 4 4 3 4 1 5 4 3 1 2 2 4 4 4 4 4 4 4 4 4 The value of this measurement is: The number of researchers who have published in PubMed. For an individual author with 3 publications, this is the number of published articles on their papers in PubMed containing the benchmark (the default value is 0, for an author with no publications). For an individual authors with 4 or more published articles on their papers in PubMed containing the benchmark, this is the number (e.g. 3+4) Results: The value of the benchmark is 0.66 (the baseline benchmark for the entire corpus) In the database of the ShARe/CLEF eHealth corpus dataset, this value is: 1.26 (the second baseline benchmark for the entire corpus, corresponding to 1.28 articles each) This value indicates that among all of the paper titles containing the benchmark, most of the published articles include the benchmark: Among the 1.3.1.1 authors, 91.6% have published at least 1 article. Among the 1.3.1.2 authors, 90.5% have published at least 1 article. Among the 1.3.1.3 authors, 90.4% have published at least 1 article. Among the 1.3.1.4 authors, 90.1% have published at least 1 article. Among the 1.3.1.5 authors, 90% have published at least 1 article. Among the 1.3.1.6 authors, 90% have published at least 1 article. Among the 1.3.1.7 authors, 85.8% have published at least 1 article. Among the	False
"
 input (English text): What is the highest benchmark result achieved on the Finger, spin (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Finger, spin (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): For what piezoresistive material the least response time was obtained?
 output (Sparql query): "	SELECT ?piezoresistive_materials WHERE { orkgr:R138374 orkgp:compareContribution ?contrib. ?contrib orkgp:P41021 ?piezoresistive_materials. ?contrib orkgp:P41022 ?response_time. } ORDER BY ASC( ?response_time) LIMIT 1	------------------------- ------------------------- ------------------------- ------------------------- -------------- ------------ -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 26) -------------- -------------- -------------- --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 3) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 1) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 1) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 1) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 10 s, elapsed = 0 s, batches per batch = 26) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 5) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 23) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 26) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 5) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 23) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 26) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 5) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 26) -------------- -------------- -------------- -------------- -------------- ------ --------------- Total (time = 0 s, elapsed = 0 s, batches per batch = 26) -------------- -------------- -------------- -------------- -------------- ------- The input data are not very important for the analysis. If you need to test the accuracy of your fingerprint by calculating the average time between the detection and the execution	False
"
 input (English text): List the code links in papers that use the GPT-3 model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-2 (small)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code; orkgp:HAS_SOURCE_CODE ?model; orkgp:HAS_SOURCE_SOURCE ?model; orkgp:HAS_SOURCE_SOURCE orkgp:HAS_SOURCE_SOURCE OROGPE:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL OROGPE:HAS_DATASET ?dataset. ?cont orkgp:HAS_SOURCE_CODE OROGPE:HAS_SOURCE_CODE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE OROGPE:HAS_SOURCE_SOURCE ||kgp:HAS_DATE ||kgp:HAS_DATE ||kgp:HAS_DATE ||kgp:HAS_DATE ||kgp:HAS_DATE ||kgp:HAS_DATE ||kgp:HAS_DATE ||kgp:HAS_GPT ||kgp:HAS_GPT ||kgp:HAS_GPT ||kgp:HAS_G"	False
"
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Seaquest benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Fishing Derby benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Fishing Derby"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" select result from orkgp:HAS_METRIC ?metric rdfs:label ?metric_lbl. SPSS output of the result is as follows: SPSS Statistics (version 15) Version 15.0 Copyright (C) 1989, 1991, 1992, 1993, 1995, 1997, 1998 SPSS Corporation. All rights reserved. SPSS, the SPSS logo, and the SPSS logo are registered trademarks. SPSS is a trademark of SPSS Corporation. 2) Let the data that contains the results of the evaluation be a single dataset without any associated statistics. We do this by inserting the table name of the SPSS package into the column. Let each row include the name of the column being evaluated, a short description that appears next to the table name in the output of the ""*"" operator, a count of the number of matches of that column, and the number of values retrieved for that column in the SPSS output. All operations (such as insert, update or select, etc., that return data from the table) and statistics will be performed as you expect them to be performed (no modifications to the data will be performed). Note: This does not require any modification (except what is needed for the table). I think that what I wrote above is good enough for most application, but it can be improved by adding the following instructions. 3) Add the following to the top of src/compile/config.h. #define REQUIREED_STANDARD(x) (((x == REQUIREED_STANDARD||x == REQUIREED_NON_ALIAS_METADATA) || (REQUIREED_STANDARD(x) ?REQUIREED_STANDARD(x, x.name) : 'none')) && (y)) In the case of this example, REQUIREED_STANDARD is needed in all three cases (i.e., for the table and for the benchmark. 4) Compile the benchmark again. If all goes well, the benchmark should have returned an output like this: A: 1.8 0: 0.000 1: -0.004 2: -0.005 3: -0.007 4: -0.008 5: 0.0000 6: -0.000 7: -0.000 8: -0.000 B: 0.012"	False
"
 input (English text): What are the models that have been benchmarked on the ImageNet V2 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the ImageNet dataset in terms of Number of params metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model_lbl ?model_lbl_lbl as the count of all parameters in the classifier for all classes. R code here # R code import numpy as np import scipy.optimize from keras_h8_s5_elem import H8S5 classifier import numpy as np from keras_h8_s5_elem import H8S5 from keras.feature import Feature from keras_h8_s5_elem.categories import Categorization # Load the data and dataframe if needed, also import the dataframe if needed dataframe = np.asarray(dataframe1) # Start the training procedure sess = Keras(""h8_s5_elem"", trained_data=dataframe) if __name__ == ""__main__"": lblData = dataframe.dataframes() for batch in xrange(dataframe.shape[0]-1): train(lblData[batch [1:]) # This is a benchmarking session, for best performance please set the # number of parameters on each run. # Create a session with the same training parameters as our # benchmarking session, but then we must initialize our session # the same way! # For example, if training took 2 sessions, we can use the sess # from keras_h8_s5_elem with the same weights as the benchmarking # session with the same batch size and the same model. # We can set parameters. x,y = np.arange(batch.shape[1], batch.shape[2]) # Get the parameters of the model # Initialize the parameters in the session. pw = h8s.params.pws(x) # Load the weights. batchData = np.asarray(dataframe1) # Train the model. for val in vars(pw): model = Feature(pw, val) # Now it's time to run the model! sess.run(model, [batchData, nsteps(batchData))"	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT model_lbl. ?model_lbl ; output (Sparql query): ROW COUNT( ?) -------- ------------ ?model_lbl. 1 3 output (Sparql query): Table: model_lbl Output: 5 output (Sparql query): input (English text): Can you list the models that have been evaluated on the Atari 2600 Blockbuster dataset ? output (Sparql query): input (English text): Please provide the input file that contains the model output files. output (Sparql query): SELECT model_lbl. ?model_lbl ; output (Sparql query): ROW COUNT( ?) -------- ------------ ?model_lbl. 1 3 output (Sparql query): Table: model_lbl Output: 10 output (Sparql query): input (English text): Please provide the input file that contains the model output files. output (Sparql query): SELECTmodel_lbl_model_lbl; output (Sparql query): Input: 2,738 output (Sparql query): input (English text): Can you specify the time intervals for the evaluations ? output (Sparql query): input (English text): Please provide the input file that contains the model output files. output (Sparql query): SELECT model_lbl. ?model_lbl_time; output (Sparql query): ROW COUNT( ?) -------- ------------ ?model_lbl_time 785 output (Sparql query): Table: model_lbl Output: 10 output (Sparql query): input (English text): Can you list the models that were measured on the Atari 3005-8 or Atari 4001/4008-8 datasets ? output (Sparql query): input (English text): Please provide the input file that contains the model output files. output (Sparql query): SELECT model_lbl. ?model_lbl_dataset; output (Sparql query): ROW COUNT( ?) -------- ------------ ?model_lbl_dataset 778 output (Sparql query): Table: model_lbl Output: 10 output (	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BiT-S model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the BiT-M model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ? code WHERE ( ?model a { 1 } ). ? code. output (Sparql query): SELECT ? code from list of code links in benchmarks input (English text): List the link links in papers that use the BiT-S model in any benchmark ? output (Sparql query): SELECT ? code, label, output from list of link links in benchmarks input (English text): List the link links in papers that use the BiT-S model in any benchmark ? output (Sparql query): SELECT ? link_hcode, label, output from list of link links in benchmarks input (English text): List the link links in papers that use the BiT-S model in any benchmark ? output (Sparql query): SELECT ? link_lbl, label, output from list of link links in benchmarks input (English text): List the link links in papers that use the BiT-S model in any benchmark ? output (Sparql query): SELECT ? link_lbl FROM link_hcode, link_lbl as links, ( ?source_code ) as source_code, orkgp as orkgp WHERE { ?model ( ?model_lbl ) orkgs; label ( ?code ) ; source_code ( ?code ) ; orkgp ( ?code ) ; orkgc; ?cont OR kgc; bench_code = orkgp ?dataset. ( ?cont OR kgc); orkgp_source_code = ? code { ?model ( ?model_lbl ) orkgc; db_method_source_code = ? code { ?model ( ?model_lbl ) orkgc; db_method } ; orkgp_source_code. source_code } input (English text): List the link links in papers that use the BiT-S model in any benchmark ? output (Sparql query): ( ? source_code ) from list of source links in benchmarks input (English text): List the link links in papers that use the BiT-S model in any benchmark ? output (Sparql query): SELECT ? source_code FROM link_lbl as links, ( ? source_code ) as source_code, orkgp as orkgp WHERE { ? model ( ? model_lbl ) orkgc; label ( ? code ) ; source	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Space Invaders dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Star Gunner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT score FROM stargunner WHERE score > 0); END; GO You just ran the code, but the output looks like: SELECT max(1) as ?max_score FROM stargunner ORDER BY DESC(""Max Score"") GO Where is the benchmark ? You know the answer: 1. How long does it take to load the dataset ? How long do you think you will wait for this query to be completed ? The answer lies in another SQL statement, SELECT DESC(""Max Score"") ). That SQL statement calculates the minimum and maximum score of every item stored with the model, in this case ""star gunner"". So let's turn it round, and find out how quickly you get to this benchmark result. Using the same query as above, let's make one change to make this comparison a little bit more meaningful. We're going to define a metric for your query. We are going to have a count from 0 to 1, so when you make a query with a count of 0, the first query will be zero, and the second query will be 1. That way you can compare two different queries. Here are two simple SELECT clauses, with a Count column: select count(*) from stargunner; SELECT max(1) as ?max_score FROM stargunner WHERE count(*) > 0; Now we have a new metric: the maximum score. This simple SELECT will have a simple field: max_score. Let's now use the same benchmark procedure but this benchmark will be two times faster, compared to the first benchmark. Let's choose some input variables, one for each benchmark. Suppose that the total number of items in the Star Gunner dataset is 10. For the first benchmark, we want to calculate the maximum value of a given item on that dataset, in the given benchmark time. SELECT max(10) as max_score FROM stargunner LEFT JOIN stargunner2 ON stargunner."	False
"
 input (English text): List the code links in papers that use the NAS Cell model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NAS Cell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the NASCell model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NASCell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT RDFS ?model_lbl. FROM ?dataset ?dataset, ?benchmark, ?source_code ORDER BY ?benchmark. ?source_code It can be helpful to keep up with papers using the NASCell model. For example: Citation There have been many papers and publications using NASCell. If you have any suggestions on citation links, and would like the paper to be associated with a specific NASCell project, please contact me. How to Contribute The best place to get started is to create an account. Once you have a NASCell account, you can download the code, and run it online Other Projects If you have a paper, or a review report that uses the NASCell model, and would like to submit it for evaluation in a benchmarking benchmark, let me know. If I like it, I'll probably add it as a published paper, or perhaps submit it to one of my conference presentations. If you're interested in becoming a co-author, we do have a set of pre-written papers (available in EPUB format) that can be uploaded to a NASCollab database. These papers will be indexed by default for your reference. Other NASCollab articles I might have read if this blog post has been useful to you:	False
"
 input (English text): Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TDMS-IE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the TCN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?column, rdfs:categorical_parameter FROM ?dataset JOIN dml( ?column ) ||| dml( ?code ) JOIN ( SELECT ?model_lbl. FILTER (str( ?model_lbl) = "" TCN-IE "" ) ?benchmark OR * ?dataset. OR ) ||| ormp:L1 ormp:L2 ormp:S3 ormp:S4 ||| ormp:STATS OR| ormp:SLEEP. OR ) ||| ormp:SELFTEST. OR ) or| ?model or| ormp:TEST. or| OR| OR| ?model. or| ||| ormp:SELFTEST. ||| Input English text, p.o.: Where can I find code references in papers that have used the TCN model for benchmarking purposes ? output English text, p.o.: Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes ? Output French text and p.o.: Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes ? output French text and p.o.: Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes ? output French text and p.o.: Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes ? output French text and p.o.: Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes ? output French text and p.o.: Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes ? output English text and p.o.: Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes ? Input English text: Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes ? Output French text and p.o.: How can I find code references in papers that have used the TDMS-IE model for benchmarking purposes ? output French text and p.o.: How can I find code references in papers that have used the TDMS-IE model for benchmarking purposes ? Input text: The original paper that used the TDMS-IE model and the"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset BUCC French-to-English?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_rbd (MAX( ?value) AS ?score) FROM andkgp:HAS_DATASET GROUP BY ?metric ?metric_rbd The output is more or less: the top result is 1 time faster on average. It seems that the difference is mostly due to the fact that the andkgp data set has more elements and is therefore more complex. If we go to the documentation for the andkgp dataset, we find this: You should not assume this is the best-performing dataset for benchmark or evaluation purposes (see also ""Best of Best"" column). It's just an example. In general, don't expect a dataset with more than 50 features (or up to 150), and a high number of outliers and missing values to be particularly fast. It sounds like the andkgp data set has higher number of elements. There seems to be a very high number of outliers in the results (the and kgp dataset has 100,000 entries). These things really should not happen, at least not in English and french. As an example, here are the results of several different benchmarking programs, using the, (0.0.2) or (0.2). Performance of MLDoc Zero Shot English to French benchmark test on the, (0.0.2) dataset Benchmark data: input (English text): What is the top benchmark result (metric and value) over the dataset L1MLLL-POWERLINK-0.2-0-0 (0.1.0.0..0.2.0.0) output (Sparql query): SELECT DISTINCT ?val ?val_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?val ?val_lbl. ?score } } ORDER BY DESC( ?val ) OPTIONAL { ?val orkp:HAS_DICTIONARY ?value. } Benchmark data: input (English text): What is the top benchmark result (metric and value) over the dataset L1MLLL Zero-Shot English-to-French-MLLL (0.1.0.0..0.2.0.0) output (Sparql query): ------------ SELECT DISTINCT ?metric ?metric_rbd"	False
"
 input (English text): What is the top benchmark score and its metric on the SQuAD2.0 dev dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset SQuAD2.0?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?value || ?score || ?metric | ?metric as ?metric_lbl || ?value || ?metric || ?metric_lbl | ?metric_lbl as ?metric || ?value || ?metric_lbl | ?score as ?score || ?metric || ?metric || ?metric_lbl | ------------------------- 1. SQuAD2.0 dev: SQuad2.0 Benchmark score 6.5929892 2. SQuad2.0 dev: SQuad2.0 Benchmark value 6.6028607 3. SQuad2.0 dev: SQuad2.0 Benchmark metric 6.6046034 4. SQuad2.0 dev: SQuad2.0 Benchmark metric/value 6.5602765 5. SQuad2.0 dev: SQuad2.0 Dev Metric score 6.6009865 6. SQuad2.0 dev: SQuad2.0 Dev Metric value 6.5922606 7. SQuad2.0 dev: SQuad2.0 Dev Metric metric/value 7.5638862 8. SQuad2.0 dev: SQuad2.0 Dev Metric metric/value 8.0588686 To summarise the performance of the different combinations: 1. We used the data from the original 2.0.2 and the 2.0.3 (1.8.2 on the SQuAD2.0, 1.8.3 on the SQuAD2.0.1 on the SQuAD2.0.2 on the SQuAD2.0.3). Our main goals here were to run the same benchmarks on each version, so that they would be comparable. 2. We used an R wrapper built for the 2.0.1 branch of the SQuAD library. It does not need any setup, so it is simpler to setup and use than the usual SQuAD_wrap. 3. An R wrapper built for the 1.8.2 branch of the SQuAD library. The wrapper does not require any setup, so it is simpler to setup and use than the standard library version. 5. We selected an R wrapper built to compile with the 1.8.3 version of the SQu	False
"
 input (English text): What is the highest benchmark result achieved on the BC5CDR dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset BC2GM?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP (3) ?result ?metric_lbl (MAX( ?value) AS ?score) (2) ?sigma_delta (MAX( ?value) AS ?score) (0) ?score.5 AS ?value ORDER BY ?result. ?sigma_delta OR ?score.5 ORDER BY ?result. ?sigma_delta ORDER BY   ?metric. * ?metric_lbl. input (English text): What is the highest benchmark result achieved over the dataset SC3M ? output (Sparql query): SELECT TOP (3) ?result ?metric_lbl (MAX( ?value) AS ?score) (2) ?sigma_delta (MAX( ?value) AS ?score) (0) ?score.17 AS ?value ORDER BY ?result. ?sigma_delta OR ?score.17 ORDER BY ?result. ?sigma_delta ORDER BY   ?metric. * ?metric_lbl. input (English text): What is the highest benchmark score (metric and value) over the dataset SC3M ? output (Sparql query): ********** ( ? ? ?) score: 4.7 ********** orkgc:HAS_DATASET    -- ( ? ? ?) score: 9.5 ********** orkgp:HAS_EVALUATION    -- ( ? ? ?) score: 7.7 ********** orkgp:HAS_METRIC    -- ( ? ? ?) score: 7.4 ********** orkgp:HAS_BENCHMARK    -- ( ? ? ?) score: 7.2 ********** orkgp:HAS	False
"
 input (English text): Can you list the models that have been evaluated on the Story Cloze Test dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 output (Sparql query): option (string): Specify the model that has been evaluated on the Story Cloze Test dataset. option (string): Specify the model that has been evaluated on the Story Cloze Test dataset. option (function): Specify the model that has been evaluated on the Story Cloze Test dataset. option (table): Specify the model that has been evaluated on the Story Cloze Test dataset. option (function): Specify the model that has been evaluated on the Story Cloze Test dataset. option (table): Specify the model that has been evaluated on the Story Cloze Test dataset. option (function): Specify the model that has been evaluated on the Story Cloze Test dataset. option (string): Set the model with the best score; if the model does not have the best score, no score is calculated. option (string): Set the model with the best score; if the model does not have the best score, no score is calculated. option (table): Specify the model that has been evaluated on the Story Cloze Test dataset. option (function): Specify the model that has been evaluated on the Story Cloze Test dataset. option (table): Specify the model that has been evaluated on the Story Cloze Test dataset. option (function): Specify the model that has been evaluated on the Story Cloze Test dataset. option (table): Specify the model that has been evaluated on the Story Cloze Test dataset. option (function): Specify the model that has been evaluated on the Story Cloze Test dataset. option (string): Set the metric that is used to evaluate the model on the Story Cloze Test benchmark dataset. option (table): Specify the metric that has been evaluated on the Story Cloze Test dataset. option (function): Specify the metric that has been evaluated on the Story Cloze Test benchmark dataset. option (table): Specify the metric that evaluates the model. option (function): Specify the metric that evaluates the model on the Story Cloze Test benchmark dataset. option (function): Specify the metric that evaluates the model on the Story Cloze Test benchmark dataset. option (function): Specify the metric that evaluates the model on the Story Cloze Test benchmark dataset. option (function): Specify the metric that evaluates the model on the Story Cloze Test benchmark dataset. option (function):	False
"
 input (English text): Who is the author with the largest number of papers about semantic representation in music research problem? 
 output (Sparql query): SELECT ?author WHERE { ?papers rdf:type orkgc:Paper. ?papers orkgp:P31 ?contrib. ?contrib orkgp:P32 ?research_problems. ?research_problems rdfs:label ?research_problems_labels. FILTER(REGEX( ?research_problems_labels, ""semantic representation in music"", ""i"")) ?papers orkgp:P27 ?authors. OPTIONAL { ?authors rdfs:label ?authors_labels } BIND(IF(BOUND( ?authors_labels), ?authors_labels, ?authors) AS ?author) } ORDER BY DESC(COUNT( ?author)) LIMIT 1
 input (English text): Who has contributed to the largest number of articles about coronavirus?
 output (Sparql query): "	"SELECT ?author_name WHERE { { SELECT (COUNT( ?paper_) as ?max_n_papers) WHERE { ?paper_ a orkgc:Paper; rdfs:label ?title_; orkgp:P27 ?author_. FILTER(REGEX(STR( ?title_), ""2019-nCoV"")) } GROUP BY ?author_ ORDER BY DESC( ?max_n_papers) LIMIT 1 } ?paper a orkgc:Paper; rdfs:label ?title; orkgp:P27 ?author. OPTIONAL { ?author rdfs:label ?author_label } BIND(IF(BOUND( ?author_label), ?author_label, ?author) as ?author_name) FILTER(REGEX(STR( ?title), ""2019-nCoV"")) } GROUP BY ?author ?type ?author_name ?max_n_papers HAVING(COUNT( ?paper) = ?max_n_papers)"	SELECT ?authors WHERE { ?authors rdfs:type orkgc:Paper. ?authors orkgp:P30 ?contrib. ?contrib orkgp:P31 ?research_problems. ?research_problems rdfs:label ?research_problems_labels. ?research_problems_labels orkgc:Paper. ?authors orkgp:P30 ?authors. OR ?authors rdfs:label ?authors_labels } BIND(IF(BOUND( ?authors_labels), ?authors_labels, ?authors) AS ?author) } ORDER BY RESULT (DESC(COUNT( ?author)) < ?authors.length) ?authors. SELECT ORD() ( 1 FROM pk-author ) Output (CSV): BIND(ORDER BY RESULT (DESC(COUNT( ?author)) < ?authors.length) ?authors. This query displays authors with the largest number of papers that discuss and document a coronavirus paper. The example above shows that we can use a predicate function call to create queries for searching and loading a dataset to see which papers are on the top or the bottom tier in terms of papers discussing the paper. This example also shows why R is a great library for creating query languages. As you see in this query, we are performing multiple queries for different queries, such as: - which papers are on the top tier of papers and which ones are on top. - which papers are on top tier of papers and which ones on the bottom tier. We can use the OR function to combine the results of these queries to find all papers that discuss a paper! output (Sparql query): SELECT ?author WHERE { ?papers rdf:type orkgc:Paper. ?authors OR kgp:P31 } ORD( ?authors ) ORDER BY DESC(COUNT( ?author )) This is another query for finding all papers that contribute to the top tier of papers or the bottom tier of papers. The bottom level query is actually pretty easy, this will return all the papers that contribute to the lower tier. In the example above, we see that some papers contribute more to the top tier than the lower one. But when we look for papers that contribute less to the different tiers, we run into problems.  The output is not what we'd expect at a first glance. If	False
"
 input (English text): What is the best performing model benchmarking the DuIE dataset in terms of F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the DuIE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 The first set of four results are the model results obtained from the default load configuration and test cases only. The results are not shown because this query is not part of the benchmarking query. The first benchmarking set shows the results of every model tested for which the F1 performance of the dataset is >0. The second set of four results is the results of those which have an F1 metric >0 but the metric remains below 0. The final set of four results is the results of those models that have an F1 metric > 1. The three models that are the best performing models here are AS, ASW, and ASWW. The other two models that are the best performing models are ZA and ZT. The ASW model performed the best overall but only because it had a lower average error and an improved F1 performance over ZT. The ASWW model performed the best overall but only because the model failed to be faster over the other two models in terms of the F1 performance of the dataset. If I look at the total amount of work for benchmarking these models I see that the average amount of work for the ASW model is a whopping 3.05 times greater than that of the other two models. The ASWW model performed the best over both the best model and the ZT model but performed less well over the ZT model. Let's have a look at the three performance metrics for the ASW model: MEMORY: 9929.66 KB STATIC TIME: 7.26 milliseconds CACHES: 2.94,5.02, and 1.90 MB The CACHES metric is interesting because it indicates that the ASWW model doesn't perform better at runtime due to the constant size of the model stores. If you want to know more about this metric check out the article : Benchmarking ASW with LatticeDB for SQL Server. The second metric is the average time for the CACHES metric. MESSAGE STREAM (MISSN): 997.50 KB MEMORY: 9827.61 KB We can see that the ASWW model has the second highest mean time to first data byte and the lowest mean time to first data byte for all the three metrics. The second lowest mean time to first data byte is almost equal to the first F1 performance metric over that metric. The ASW model also has a lower average time to first data byte than the	False
"
 input (English text): What is major reactant for ""Microwave-Assisted Cobinamide Synthesis""?
 output (Sparql query): SELECT ?reactant WHERE { ?papers rdfs:label ?titles. FILTER(REGEX( ?titles, ""Microwave-Assisted Cobinamide Synthesis"")) ?papers orkgp:P31 ?cont. ?cont orkgp:P37557 ?reactant. } 
 input (English text): What coolant does ASTRID reactor use?
 output (Sparql query): "	"SELECT ?coolant ?coolant_label WHERE { ?ASTRID_reactor ?rdfs_label ""ASTRID""^^xsd:string. ?contrib ?reactor_predicate ?ASTRID_reactor. ?contrib ?predicates ?coolant. ?predicates rdfs:label ?predicates_label. FILTER(REGEX( ?predicates_label, ""coolant"")) ?coolant rdfs:label ?coolant_label. } "	 input (English text): How much pure benzamide is there in the reaction product ? output (Sparql query): input (English text): How has the mass of the reactants in the reaction gone down ? output (Sparql query): input (English text): What are a few names for a benzamidine hydrochloride derivative ? output (Sparql query): input (English text): What is a benzamidine hydrochloride derivative ? output (Sparql query): Here is a query that gets the list of all items, sorted by priority. select:min_price_in_pounds (text,sort=ASC()) | min_price_in_pounds : min_price ; | min_price_in_pounds_max : price ; | min_price_in_pounds_min : min_price ; | min_price_in_pounds_maj : min_price ; | min_price_in_pounds_lt : price ; | min_price_in_pounds_gt : price ; | min_price_in_pounds_qty : price ; Output from the query above: | bz_5HN_P13_4.042H(6) | bz_5HN_P133_1.86A(22) | bz_6BZ_P34_4.23H(17) | bz_6BZ_P45_3.23H(22) | bz_7Hn_1_27.9H(\1/16) | bz_6BZ_P25_4.03N(18) | bz_6BZ_P49_6.23H(16) | bz_6BZ_P65_4.07H(16) | bz_6BZ_P71_2.76H(19) | bz_6BZ_P77_2.79H(15) | 	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Berzerk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ? score ) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AtariBerzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } The output in the above SQL is as follows: Input Output Result ROW_NUMBER 245968 0.00 1 The output in the above SQL is as follows: InputOutputResultRow Number ROW_NUMBER 245968 0.00 2 These are the metrics we are interested in. The results for Atari 2600 Berzerk are very similar – the metrics are identical. The reason we see a difference here is because we have a metric for Atari Berzerk on Atari 2600 Atlantis; the two datasets share the same metric type. There are two differences that we are interested in in this case – the Atari Berzerk metrics have been renamed or changed in accordance with the name change and the Berzerk metrics have a different score (only 0.20). This is because we should be using these metrics for the score comparison, even though they differ. We expect the score comparison metrics to appear slightly different from time to time. We should not expect any metric differences of their own. The above SQL is the result of importing the Berzerk metric into an SQLite database (the above command did not take into account the metrics in the Atlantis database because the Atlantis database was imported earlier). To compare the Berzerk metrics to their Atlantis counterparts we import the Atlantis database data into our SQLite database. In order to start comparing from the same database"	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Crazy Climber dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } For each of the models, we create a table with a column named model_lbl, and then execute an SQL query like this: output(Sparql query): SELECT dataType, ?model_lbl FROM { ?model_lbl, ?dataset } ORDER BY dataType DESC LIMIT 5 We're interested in the first 10 values of the model_lbl column, so execute the following SQL command: output(Sparql query): select * from model_lbl ORDER BY model_lbl DESC Using the first 10 values of the column, we can use a SQL to extract the values of the dataset, and then perform an evaluation of the model. As you can see above, we use the first 10 values of the model_lbl column to calculate three parameters to evaluate our model on. We can output the model, and then use either the output of evaluation or the Benchmark query above to perform an evaluation of the model. You can use these two queries for an evaluation of a single model, or as a benchmark between different models. Conclusion I hope this blog post provided some insight into how to build a benchmark for a model. Next time, we'll take a look at a different benchmark in a different domain."	False
"
 input (English text): What are the most commonly used benchmark datasets for the Document Summarization research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Document Summarization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Text Summarization research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Text Summarization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Document Summarization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } In the output, the problem and the benchmark dataset are reported. The second output row contains the percentage of the selected problem dataset to be processed The output of one select statement combines multiple selections into one list. The second select statement reports results for a single selection selected by the first argument in the second select statement. Example: Select a list of names of names of all the products the owner knows by searching the search query using the search option and finding names on the products. input (English text): Name of a product. output (Sparql query): SELECT id FROM products; This query selects each name in each category except for the first category and finds all the products containing one or fewer of the specified name. All the specified information is reported in the output of one select statement in the document processing analysis of each named name. The input and output of this query for the ""Search"" (Search by Category-ID) result are the same. The inputs and outputs are the same. Example: If one selects a list of records for the search of all the records for the surname of John Smith on the records table of the sales records, then output will be a file named Sales records with surname John in Sales records. input (English text): ""John Smith "" ""Smith-Smith"" output (Sparql query): SELECT records.name FROM Sales records; The input and output of this query for the ""Search"" (Search by Category-ID) result contains the"	False
"
 input (English text): What are the metrics of evaluation over the Atari 2600 Krull dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Krull"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Tutankham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT MIN ?metric (1): ?metric 0 : ?metric 1 SELECT MAX ?metric (2): ?metric 0 : ?metric 1 SELECT ?metric rdfs:label ?metric_lbl. ORGANIZATION :Krussel ; MIN ORGANIZATION ?metric. } output (Sparql query): SELECT MIN ?metric (3): ?metric 1 : ?metric 2 SELECT MAX ?metric (4): ?metric 0 : ?metric 3 SELECT ?metric rdfs:label ?metric_lbl. ORGANIZATION :Krussel ; MAX ORGANIZATION ?metric. } 4.2. Using the Data Analysis Toolbox¶ We'll start with a quick example illustrating how the code can be used to create a customized metric score score, as well as a custom visualization. We'll see how, because of the structure of the Kruskal and Kruskal-Wallis metrics, this code will require some modification in order to make it useful. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model import logistic_regression def score1(data_frame, metric, metric_lbl): """""" Score metric in a numeric form. """""" return sum (score1(data_frame, metric, metric_lbl)) def score2(data_frame, metric): """""" Similar score metric, but for categorical data. """""" score = linear_model.fit(data_frame) if __name__ == '__main__': score1(data_frame,'s','s') score2(data_frame, 'g','g') 4.2.1. Creating the Score Score metric score1(data_frame, s, metric) score2(data_frame, g, s) scores = score1(DataFrame(100000, s), DataFrame(1000, g, s)) def f(x, g): """""" Fits the f(x, f) expression to the function (x, f). """""" return matrix (sapply(x, lambda x: f(x, g)) for x in zip(XI[:3], XII[:3], g)) 4.2.2. Using the Score Score metric score1(data_frame, s, metric) score2(data_frame, u"	False
"
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Tutankham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; ORkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; ORkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; ORkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Finally, here's an example query for querying the scores for a single game.  The following query is based on the Game Score metric on Wikipedia. I had previously used an XML parser for the metrics and there's no reason to do so again here. The same query but with a more user-friendly API.  select score_metric_lbl as game_score, score to_score from games where score_metric=0; The JSON response is an XML document containing scores, as listed below. score_metric_lbl: {- Game score: 80.0, score to score: -80.0, score_metric=0, name=""Game score"" } score to_score: { ""game_score""... ""score_metric_lbl"" ""score_metric_lbl"" } score to_score: { ""game_score""... ""score_metric_lbl"" ""score_metric_lbl"" } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 input ( English text ) : What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Venture dataset ? output ( Sparql query ) : SELECT"	False
"
 input (English text): Can you list the models that have been evaluated on the BioASQ dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the PROTEINS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PROTEINS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT command, only sorts results by their corresponding genes (sorted by their first letter): $ perl genes_vs_models.pl $ genotypes <- list(seq( ?, ?seq_len, ?seq_name, ?seq_value, ?seq_value_seq, ?seq_seq, ?seq_lbl))[-12:0][-:][1:3][-6:6] $ genotypes$order <- rbind(genotypes[[-12:4]], genotypes[[-12:2]], genotypes[[-12:4]], genotypes[[-12:2]], genotypes[[-12:1]], genotypes[[-12:3]], genotypes[[-12:2]], genotypes[[-11:2]], genotypes[[-11:2]], genotypes[[-10:2]], genotypes[[-10:2]], genotypes[[-9:5]], genotypes[[-8:5]])[,-12:1] $ print $ print $ GENE$order. ""Genes by Gene Order"" In ORDER BY, we use the ""group( ?)"" clause, which creates a list of all groups that have not been ordered by the selected gene-level values. The ordering is then specified by the ""group( ?)"" clause, which lists all groups that belong to the selected group. When we create an ORDER BY clause, the default ROW FORK is ROW FOR GENES, which can also be read as ""SELECT a.1, a.2.1, a.2.2"". The ROWS RANGE BY command has the same function, but does not use the GROUP BY clause. Instead, it takes the group( ?) and the group ( ?, ?) values, and outputs their groups. To output a list of all groups on a query, you may use the GROUP IN operator (see section Rows RANGE BY ). The GROUP BY clause is only available when using ORDER BY and ORDER BY -EXPRESS. This means that we can also use the GROUP BY clause to perform GROUP BY comparisons. GROUP BY -1.0 GROUP BY -1.5 GROUP BY -2.0 GROUP BY"	False
"
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT29 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (long) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl ORDER BY DESC( ?value) LIMIT 1 } Output: This example shows you how to display both F1 (top) and F3 (long) F1 scores. You can download the full dataset here. This section discusses the F1 scoring algorithm and how to test it on the NYT29 dataset. The method is presented here for demonstration purposes only and does not consider a user-friendly approach. F1 scoring algorithm The F1 scoring algorithm, often called F1 or Fscore, is a very simple scoring algorithm when applied to datasets having long sequences of questions and answers. The F1 algorithm evaluates each question and answer in terms of its weight in the ensemble and finds the best answer of the collection based on F1-based scoring. A good example to understand the algorithm is the example provided earlier on the sample below. If the F1 scoring algorithm performs well, the answer will be the first answer to be included in the ensemble. The next question will start with a given weight and will be rated with the same score. If the F1 scoring algorithm performs poorly, the answer will be the first answer that the collection will start with and the next question will be rated with lower weight and the answer will be the last answer in the collection and the collection will be rated in the middle of the distribution. However, some question can be rated in the middle of the distribution, so in this case it is possible to get both low or high score for the question. The best scores are those that fall on the middle and bottom part of the distribution. The F1 algorithm does not consider the accuracy or the weight of the answer. It considers only a single-predicate classification from the ensemble that best fits the question and the question is considered to have a correct answer. The F1 algorithm was first published in 1976 by Dr. Daniel Stoeckl and was described further in 1980 by D. Wierstra and K. R. Fiset. In the most recent versions of F1 scoring algorithm, the weight algorithm has been reworked and it is the weight associated with any answer which can explain the score for the answer. Other modifications on the F1 scoring algorithm are mentioned below. The F1 scoring algorithm is the ""standard"" algorithm. It has been widely implemented by several programs and databases (see below) and has been used successfully in a number of research labs. Sparql query examples Let's describe two different questions:"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset CommitmentBank?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the CommitmentBank dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?measure ?metric OR ?metric_lbl (MAX( ?value) AS ?score ) FROM ( LIMIT 10 ) 1 2 3 4 5 6 7 8 9 10 11 SELECT ? measure ? metric OR ? metric_lbl ( MAX ( ? value ) As ? score ) FROM ( LIMIT 10 )  select ? 1 SELECT ? measure ? metric OR ? metric_lbl ( MAX ( ? value ) As ? score ) FROM ( WHERE! d.is_rbf_table ) Selecting a metric with the top result using the ORG clause input (English text): What is the top benchmark result over the dataset CommitmentBank ? output (Sparql query): DISTINCT ?score FROM (Select ?measure AND ?status = Yes)  AND 1 2 3 4 5 6 SELECT ? score FROM ( Select ? measure AND ? status = Yes ) Selecting a metric with the bottom row (where rdfs:label is the label of the variable) output (Sparql query): SELECT DESC( ?value) AS ?value, ?metric_lbl (MIN( ?value). ?metric_lbl() ) AS ?metric_lbl(MAX( ?value). ?metric_lbl() ) FROM ( Select ?measure AND ?status = Yes )  ORDER BY DESC( ?value ) OPTIONally 3 4 5 6 SELECT ? value AS ? value, ? metric_lbl ( MIN ( ? value ). ? metric_lbl ( ) ) AS ? metric_lbl ( MAX ( ? value ). ? metric_lbl ( ) ) FROM ( Select ? measure AND ? status = Yes ) select ? 4 8 SELECT ?measure AS ?measure, ?status AS ?status, ?metric_lbl AS ?metric_lbl(MIN( ?value). ?metric_lbl() ) AS ?metric_lbl(MAX( ?value). ?metric_lbl() ) FROM ( SELECT ?measure AS ?measure, ?status AS ?status, ?metric_lbl AS ?metric_lbl(MIN( ?value). ?metric_lbl() ) AS ?metric_lbl(MAX( ?value). ?metric_lbl() ) * ORDER BY DESC( ?value ) , ?metric_	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BiT-S (ResNet-152x4) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet-152x4)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl ) = ""BiT-M (ResNet-152x4)"") ?benchmark orkgp:HAS_DATASET ?dataset. orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } I need a graph and to get the data from the model. (2 rows)"	False
"
 input (English text): What is the best performing model benchmarking the ACE 2005 dataset in terms of RE Micro F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	********* ********* Input text: ********* *************** *************** ----------- ********* **************** ********* **************** ********* **************** ********* **************** Output text: ************* ______ ______________________________ ______________ _____/ ___________________ ________ _____________________ ______________ ______________ 	False
"
 input (English text): What is the top benchmark score and its metric on the BBCSport dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BBCSport"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Nottingham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?score FROM TestRecord.CURRENT orkgc:RDD1.DATASET ORkgc:RSMBL1.DATASET ROWS 0..2 ROWS 0..2 ORDER BY DESC( ?value) It's not as useful to get the scores because there are only 32 test cases per country. You should focus on the benchmark score value.   The following query will give us the top score for all the countries that we have test records for – this helps to isolate more variables from the input data. input (English text): What is the top benchmark score and its metric on the Nottingham dataset ?    output (Sparql query) SELECT TOP 40 ?benchmark || ?benchmark_lbl( ?rank ) AS rank | ?rank( ?rank ) AS performance || ?rank( ?rank ) AS performance_lbl( ?rank ) AS performance_lbl( ?rank ) AS performance_lbl( ?rank ) AS performance_lbl_lbl( ?rank ) AS performance_lbl_lbl( ?rank ) AS rank_lbl_lbl( ?rank );   ROWS 1..10 ROWS 1..10 Input (English text): What is the top benchmark score and its metric on the Nottingham dataset ?    output (Sparql query) SELECT TOP 40 ?benchmark || ?benchmark_lbl( ?rank ) AS rank | ?rank( ?rank ) AS performance || ?rank( ?rank ) AS performance_lbl( ?rank ) AS performance_lbl( ?rank ) AS performance_lbl( ?rank ) AS performance_lbl_lbl( ?rank ) AS rank_lbl_lbl( ?rank ) AS rank_lbl_lbl( ?rank );   ROWS 1..11 ROWS 1..11 The output is for our test cases only and if you really want to see top scores for each country you can use the following query. input (English text): What is the top benchmark score and its metric on the Nottingham dataset ?    output (Sparql query) SELECT TOP 40 ?benchmark || ?benchmark_lbl( ?rank ) AS rank | ?rank( ?rank ) AS performance || ?rank( ?rank ) AS performance_lbl( ?rank ) AS performance_lbl( ?rank ) AS performance_lbl( ?rank ) AS	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the STEM-ECR v1.0 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STEM-ECR v1.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STEM-ECR v1.0"") orkgp:STATISTICS ?metric2. OPTIONAL { ?metric2 ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What evaluation metrics are commonly used when benchmarking models on the CSC v1.0 dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CSC v1.0"") orkgp:HAS_METRIC ?metric. ORKP:HAS_EVALUATION ""Metric2"" rdfs:label ?metric_lbl. OPTIONAL { ?metric2 ?metric. ?metric rdfs:label ?metric_lbl. } } Input: If you have read the documentation of the model(s), please enter the parameters(s) for each evaluation metric. In this case, you can only enter the parameters that you are currently able to use (e.g., you can only use the first 3 parameters). If you make a mistake, please enter a comment of """" so that the reviewer will know that the model(s) was too complicated to have been benchmarked. In the above example, one of the 3 values is wrong, so enter ""N"", which indicates that the evaluation metric is missing, but that you do not expect it to be there. For the SML-ECR dataset, one of the 3 value is wrong, so enter ""S"", which indicates that the evaluation metric is missing. For the FOCI-ECR dataset, one of the 3 values is wrong, so enter ""F"", which indicates that the evaluation metric is missing. For the CSC dataset, one of the 3 values is wrong, so enter ""C"", which suggests that no evaluation metric has been set. One of the values in the first list is"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Multi-Format Contrastive model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Format Contrastive"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Multi-Perspective Matching (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" The source code (text) you supply does not include any text-processing directives or comments. You may either open a new comment line with a // character (e.g. // C# // ) or run the code inside a Comment Block (i.e. // ""hello, world"", ""I am a human being with the same rights as all other humans"") You are welcome to share your source code that has been modified from this document (after you have checked that it meets the requirements of the specification). In particular, you may submit a pull-request; if you wish to do so, use the file ""example_c#-multi-line_model.scala"". If you have code that is incomplete as a result of this document, you may need to go through the existing documentation to create a new section of the manual which is appropriate for your code. For example, an update to a code base should be performed (preferably with the approval and approval of the upstream author or other people working on the code). C.4. General Notes on Documentation¶ General Note 1. All documentation must reflect the latest information and practice (as currently available in the SCM or on the internet). General Note 2. Documentation must be at least as comprehensive and informative as is likely to be given by the relevant user community. For example, the source code is the only source of information (even on the internet). We recommend that a detailed description of the entire program, such as its inputs and outputs, should be included (see section 4.1.3). General Note 3. Documentation should include links to relevant web sites that make sense as sources of information about the program. They should be kept up to date through a mechanism agreed on by the SCM community. General Note 4. Documentation should give the user access to various forms of feedback and assistance (for example, assistance with the software, using the manual, or providing suggestions for improvement). It is generally not appropriate to make documentation so lengthy or detailed that its content stops having a reasonable practical value. General Note 5. Documentation should be as clear as possible. General Note 6. Documentation should not be of poor quality (which might make it difficult to learn using). For example, documentation should be as clear and as concise as possible. General Note 7. Documentation should have a reasonable balance of visual and textual content or at least a strong visual link (e.g. using text only or with a full"	False
"
 input (English text): List the metrics that are used to evaluate models on the GAD benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the GAD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT id, n, max(mean), min(mean), min(metrics) from ` orkgc_benchmark/db/` from	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the PubMed 20k RCT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the PubMed 20k RCT dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ?cont orkgp:HAS_VALUE ?value. } } ORDER BY DESC( ?value) } } If you like to do a lot of comparisons, then consider the PubMed 20k RCT dataset, since it is quite extensive and contains more than 5000 items. If you only want to determine the best of 1-2 (or 2 or less) of the comparison studies, then simply select only the results that have the highest score in the given table. To have a large number of measurements of each comparison, all of the test items should be on a single table. Select all the items with a given score on a single table if there are different number of items among different rows in your results. If there are similar scores to the test items for different results, then only the items with top score on the ""results"" table are used in the comparison. In a second step, you need a way to filter based on the item to perform the filtering (for example, for each comparison table). In the examples below I use the method by S. L. Chen to perform the filtering for comparing the RCTs with the same result from 2010 and 2011, since those two years were the same. The key element in each example I performed in the previous instructions is determining which result is the best compared to the other results. The trick of choice for a comparison study is whether it is"	False
"
 input (English text): Could you provide a list of models that have been tested on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT MODEL ? : ?dataset ?data_lbl as ?dataset_lbl, MIN(MODEL) : MAX(MODEL) as ?min, MIN_VALUE : MAX(MODEL_VALUE) as ?max, ?RATING ?max_rating. ?value ?min_rating. OPTIONAL (RANK_OF_RATINGS) : ?dummy ?s1 orkc:SUBROUTINE s1. OPTIONAL (INCLUDE_NON_NUMBERING) : ?dummy orkc:SUBROUTINE rdfs:RANKNAME s1. input (English text): Input the data set (dataset) and query the performance metrics. output (Sparql query): SELECT MODEL ? : ?dataset ?data_lbl as ?dataset_lbl, MIN(MODEL) : MAX(MODEL) as ?min, MIN_VALUE : MAX(MODEL_VALUE) as ?max, ?RATING ?max_rating. ?value ?min_rating. OPTIONAL (RANK_OF_RATINGS) : ?dataset ?dataset_lbl [ 1, 2, 3 ]. OR OPTIONAL (INCLUDE_NON_NUMBERING) : ?dataset_lbl [ 1, 2, 3 ]. input (English text): List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset ? output (Sparql query): SELECT MODEL ? : ?dataset ?data_lbl as ?data_lbl [ 1, 2, 3 ], MIN(MODEL) : MAX(MODEL) as ?min, MIN_VALUE : MAX(MODEL_VALUE) as ?max, ?RATING ?max_rating. ?value ?min_rating. OPTIONAL (RANK_OF_RATINGS) : ?dataset ?dataset_lbl [ 1, 2, 3 ]. OR OPTIONAL (INCLUDE_NON_NUMBERING) : ?dataset_lbl [ 1, 2, 3 ]. input (English text): List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset ? output (Sparql query): SELECT MODEL ? : ?dataset	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ARC-PDN dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AAPD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT paper_lbl. ?value, n_policyset. ?value, n_benchmarks. ?value, rdfs:value ?replicate, paper_lbl. ?value, n_obstacle_labels. ?value, n_benchmark. ?value, rdfs:value ?score, n_policyset. ?value, n_benchmarks. ?value, n_obs. ?value, score, n_policyset.. ?value, n_obs. ?value, output_labels. ?value ?score, n_policyset.. ?value, input_labels. ?value The results would be something like this: CREATE TABLE papers ( title text, title_id integer, pub number ( 1 ), article number ( 1 ), n_published_date_in ISO MONTH, desc text ( NULL ), num_comments 1 ). CREATE INDEX paper_labels ON papers(title || page); SELECT rdfs_value || output_labels || * FROM research_labels WHERE value!= 0 ; /* The SQL query uses that expression SELECT ?""value"" FROM ?""title ||..."" || "" WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont ?benchmark. ?paper orkgp:P31 ?cont ?paper. ?obstacle_labels ?obstacle_labels. ?obstacle_labels. ?obstacle_labels. ?obstacle_labels. ?pub ?pub. ?replicate ?policyset. ?value The output will be something like this: CREATE TABLE papers ( title text, title_id integer, pub number ( 1 ), article number ( 1 ), n_published_date_in ISO MONTH, desc text ( NULL ), num_comments 1 ). What is the difference between a table, an array-like operator, AND and AND ?"	False
"
 input (English text): Where can I find code references in papers that have used the XLNet (single model) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the AxCell model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AxCell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" input (English text): What does ""dataset"" mean in relation to the model ? output (Sparql query): input (English text): What will happen if I run the full model without a training batch ? output (Sparql query): input (English text): What will happen if I run the full model with a training batch ? output (Sparql query): output (Sparql query): input (English text): What are the differences between the data and the model on how this data is partitioned ? output (Sparql query): input (English text): What are the differences between the data and the model on how this data is partitioned ? output (Sparql query): input (English text): What are the differences between the user input on the model and the training data ? output (Sparql query): input (English text): Do you consider the training data to be the only data that affects the performance of the model ? input (English text): Do you consider the training data to be the only data that affects the performance of the model ? output (Sparql query): output (Sparql query): input (English text): Do you consider the learning rate and variance to be important variables in your training and testing of the model ? input (English text): Do you consider the learning rate and variance to be important variables in your training and testing of the model ? output (Sparql query): output (Sparql query): input (English text): What is the difference between ""tokens"" in the model and the label on the data ? output (Sparql query): output (Sparql query): input (English text): What does ""label"" mean in relation to the model ? output (Sparql query): output (Sparql query): input (English text): Do you define ""label"", which refers to the model parameter, as the probability of a label having been assigned to a particular value ? output (Sparql query): output (Sparql query): ?input( ?model_lbl) This should return a row called ?label_tokens_tokens_model, a row called ?label_data_tokens_data_model, and a row called ?x_label_label_tokens_tokens_"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM-MoS model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-MoS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?DISTINCT ?code WHERE { ?model { ?model a:Model }. ?model_lbl ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-MoS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } We could have run these tests in one step. In this case, however, the results are better expressed using different steps: input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM-MoS model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a:Model }. ?model_lbl ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-MoS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. SELECT ?DISTINCT ?code WHERE { ?model a:Model }. ?model_lbl ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-MoS"") ?benchmark. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. This test case reveals the importance of running the tests in steps, as the final result will be very different depending on the desired result. Note: We have previously used both the BLAS and MPI libraries to build our benchmarking infrastructure and tests can be implemented in any of those versions. To understand the benchmarking process, you can start with Chapter 3: What is BLAS, and then jump to Chapter 5: A tutorial on BLAS (and C"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BioASQ dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMedQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT # - 1 benchmark.rs:1 bench:benchmark result 1 BenchmarkResult 2 BenchmarkResult 3 BenchmarkResult 4 BenchmarkResult 5 BenchmarkResult 6 BenchmarkResult 7 BenchmarkResult 8 BenchmarkResult 9 BenchmarkResult 10 BenchmarkResult 11 BenchmarkResult 12 BenchmarkResult 13 BenchmarkResult 14 BenchmarkResult 15 BenchmarkResult 16 BenchmarkResult 17 BenchmarkResult 18 BenchmarkResult 19 BenchmarkResult 20 BenchmarkResult 21 BenchmarkResult 22 BenchmarkResult 23 BenchmarkResult 24 BenchmarkResult 25 BenchmarkResult 26 BenchmarkResult 27 BenchmarkResult - Select # - 1 benchmark.rs:1 BenchmarkResult 2 BenchmarkResult 3 BenchmarkResult 4 BenchmarkResult 5 BenchmarkResult 6 BenchmarkResult 7 BenchmarkResult 8 BenchmarkResult 9	False
"
 input (English text): What are the most commonly used benchmark datasets for the Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification. research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification."") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Name the datasets that have been used for benchmarking in the Image Classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	" Output of the benchmarking procedure  1. How many benchmark datasets were used for the Image Classification ? 2. The number of benchmark images of a training set 3. The number of standard errors (stdevs) of the images used in an evaluation run. 4. How well was each of the benchmark image systems at benchmarking ? 5. How well was each of the benchmark images of an evaluation run ? 6. How well does one benchmarking strategy (or set of strategies) outperform another ? 7. How well does one benchmarking strategy (or set of strategies) outperform another ? 8. Which benchmarking strategy is better overall ? 9. How well does one benchmarking strategy (or set of strategies) outperform another ? 10. Which benchmarking strategy is better overall ? 11. If a benchmarking strategy (or set of strategies) is outperforming another, why is it better ? 12. Which of the benchmarking strategies (or set of strategies) is (or is expected to be) the best overall ? 13. If a benchmarking strategy (or set of strategies) is outperforming another, why are they outperforming each other ? 14. Why are the benchmarks being benchmarked ? 15. What did the researchers use for the benchmarking ? 16. What was the goal of the benchmarking ? 17. Were there errors in the results of the benchmarking ? 18. What was the goal of the evaluation ? 19. Why did they benchmark at all ? 20. Why didn't their analysis result in the correct results ? 21. Why was there no comparison of image systems at different levels of abstraction ? 22. How high is the false discovery rate ? 23. Why is it important to check for training error ? 24. What were the criteria used (and what went wrong) to measure the performance of different image classification methods ? 25. What did the researchers propose to improve the learning rate for the SVM based on the dataset ? 26. What types of methods are used in the study of SVM architectures ? 27. How have this research areas improved over the last years ? 29. What is a ""Classification error"" and ""Error"" within a particular classification system. 30. What is the ""Max-F1 count"" for the training set ? 31. Why can't each level of abstraction ""see"" all of the different datasets ? 32. What"	False
"
 input (English text): Could you provide a list of models that have been tested on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Gibson PointGoal Navigation dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT MAX( ?benchmark ) || ?dataset || MAX( ?paper || ?rdfs || ?paper rdfs): ?max || ?dataset || MAX( ?paper || ?rdfs || ?paper rdfs) FROM ( SELECT { ?dataset a orkgp:HAS_DATASET { ?dataset. ?model || ?dataset. ?model rdfs:label ?model_lbl } } OR { ?dataset a orkgp:HAS_DATASET { ?dataset. ?model rdfs:label ?model_lbl } } OR { ?dataset a orkgp:HAS_DATASET { ?dataset. ?model rdfs:label ?model_lbl } } OR { ?dataset a orkgp:HAS_DATASET { ?dataset. ?model rdfs:label ?model_lbl } } FROM ( SELECT { ?dataset a orkgp:HAS_DATASET { ?dataset. ?model rdfs:label ?graphp:HAS_HIGH_BENCH_SEARCH _graph_lbl }} ORDER BY { ?graphp. ?matrix || ?graphp. ?matrix rdfs:label ?graphp_lbl } ORDER BY { ?graphp. ?rdfs || ?graphp. ?rdfs rdfs:label ?graphp_lbl } ORDER BY { ?graphp. ?model || ?graphp. ?model rdfs:label ?graphp_lbl } FROM ( SELECT { ?dataset a orkgp:HAS_DATASET { ?dataset. ?model rdfs:label ?graphp:SQLSRADED-INNER } } ORDER BY { ?graphp. ?matrix || ?graphp. ?matrix rdfs:label ?graphp_SQLSRADED-INNER } FROM ( SELECT { ?dataset a orkgp:HAS_DATASET { ?dataset. ?model rdfs:label ?graphp:RDS } } ORDER BY { ?graphp. ?matrix || ?graphp. ?matrix rdfs:label ?graphp	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cheetah, run (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Score score on the Cheetah, run (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT NOCELED ?model ( ?test_dataset ) ?dataset_name ( ?column_number ) ?model_name ( ?column_number ) ?model_id ( ?model_id ) ?dataset_class ( ?dataset_class ) ?model_id ( ?model_id ) ?dataset_category ( ?dataset_category ) { [ ?dataset_class ], [ ?dataset_category ], } ORDER BY NOCELED ?model ?dataset_name or ?model_name ORDER BY NOCELED ?model ?dataset_name or ?model_name or ?model_name ORDER BY NOCELED ?model ?dataset_name or ?model_name or ?model_name output (Sparql query): ------------------------- select NOCELED ?model as ?model select NOCELED ?model_name as ?model_name select NOCELED ?model_id as ?model_id select NOCELED ?model as ?model_id from ?model in ?datasets where ?dataset_class = ?model_value select NOCELED ?model_name from ?model_rdfs on ?result = ?model_lbl_value ORDER BY NOCELED ?model_name select NOCELED ?model_id as ?model_id select NOCELED ?model as ?model_id from ?model_rdfs where ?dataset_category = ?dataset_value select NOCELED ?model as ?model_id from ?model_rdfs on ?object = ?model_lbl_id; or kgp:HAS_DATASET ?dataset; or kgp:HAS_EVALUATION ?eval. OR ? eval; { ?dataset_class, ?dataset_category, } ORDER BY NOCELED ?model_name. OR NOCELED ?model_name Output (Sparql query): ------------------------ select NOCELED ?model. as ?model from ?model ; row_length (20) 0 ; sort ( descending ( ROW_LEN )) by value > 100 input (English text): What is the highest score obtained by the Cheetah run model, run (DMControl500k	False
"
 input (English text): What is the top benchmark score and its metric on the Stanford Cars dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Stanford Dogs dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Dogs"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?key (COUNT(3) AS ?key) FROM rdfs:datasets/dog_tournament WHERE!( ?key = 1 OR ?key == 2 OR ?key == 3 ) ?key ?key input (English text): What benchmark score and its metric on the Stanford Dogs dataset ? output (Sparql query): -------------- Metrics: ---- >30.0 10.0 3.5 3.0 4.0 0.3 0.3 2.0 4.0 3.0 >60.0 34.0 3.5 3.0 4.4 0.6 0.2 4.9 5.4 2.0 >70.0 29.5 4.0 3.7 6.2 1.9 0.6 0.5 7.6 3.0 1.6 >79.0 23.2 3.7 4.0 4.1 1.6 0.5 1.9 2.2 0.6 --------- >90.0 6.8 3.8 2.0 1.5 0.9 0.5 1.3 1.0 0.9 ---------- >100.0 1.7 2.0 0.6 0.5 0.3 0.1 0.7 0.4 1.7	False
"
 input (English text): Indicate the model that performed best in terms of Senseval 3 metric on the Supervised: benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Senseval 3"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Senseval 2 metric on the Supervised: benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Senseval 2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Senseval 2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Now that we have an understanding about this feature-level query, let us write a sample data set that has a lot of noise. We can do so by leveraging the SQL function select_db that returns a collection of different models that can be used with the data to predict the score for the sample data. The sample data set is: select DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. DATEID ?dataset_id; orkgp:HAS_DATASET ?dataset. DATE rdfs:label ?dataset_lbl. orkgp:HAS_EVALUATION ?dataset_id. ORALYZ_HISTORY ?dataset_history. SELECT IF(!DATE_ISNULL(age, 'd'); AND( age > ?age_min, ?age_max)); OR ( age = ?age_min), ?age_max FROM ?d WHERE age > ?age; OR { ?dataset a ORkgc:Dataset WHERE age = ?age_min AND age = ?age_max AND year_min >= ?year_max AND year_min <= ?year_max; } { ?"	False
"
 input (English text): Indicate the model that performed best in terms of Test perplexity metric on the Penn Treebank (Word Level) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-103"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""WikiText-103"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. ?model orkgp:HAS_METRIC ?metric. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } A brief introduction to RDFS Graphical Representation of RDF Data in a Sparse Sparse Grid and Lattice RDFS (Rich Data Browser) is an open source, scalable data management system that enables the manipulation and mining of rich data sets in a single framework. Users can store, query and manipulate data using a rich interface that allows the rich interactions across domains, dimensions and data types. RDFS allows users to access data at different dimensions via a network of hyperlinked RDF nodes. The goal of RDFS was to enable a rich data collection and exploration application allowing users to explore their data in a dynamic environment. In addition, RDFS is intended to be the most efficient RDFS in many cases; in many situations, the complexity of data sets can be minimized and the cost of interacting with large structured data sets can be minimized. The main concepts of RDFS are provided in two main components: The Hyper-Linked Data Sub-System (Hash Tables) RDFS provides an additional layer on top of the RDF document graph. With Hyper-Linked Data Sub-SYSTEMs, the nodes of the RDF document graph are represented in a rich graph form along with information about how nodes and edges related together. A user is able to query and manipulate a rich data set through interactive links to nodes and edges. For example, if one wants to extract information from an RDF Document, he/she can do that by accessing a hyperlink to another node, and then use that node-reference to query an endpoint node to retrieve those information. A Key Component of RDFS is the hyperlink nodes. Hyper-Linked Data Sub-SYSTEMs can be accessed via a large number of hyperlinks pointing to different sub"	False
"
 input (English text): What is the top benchmark score and its metric on the WMT2016 Russian-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Russian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?benchmark ?value AS ?value, ?metric ?metric_lbl FROM dts:sorted_metric_sorted_results_sorted_by_metric_rank a GROUP BY ?metric ?metric_lbl GROUP BY ?metric ?metric_lbl If the criterion of interest is a metric on a dataset which was created on WMT2016 Russian-English, we want to avoid the problem with different rankings on the data which is caused by some metric (e.g. a single metric which could be applied on different datasets) due to the different data set/settings (e.g. different default setting for MetricRank). This would require a new function called Metric.set(). This new function will create an instance of Metric for each metric of interest. We first pass the metric on to the Metric.set() function. To ensure that a metric is available for the Metric.set() function we must set the name of available metrics. In WMT2016 we can create the list of available metrics using set_metrics() (we saw a similar scenario with a set of metric names already). The first argument to set_metrics() is what the dataset name is to be used for the metric. The Metric class allows us to select the dataset (e.g. a data frame) to use as a metric, or create it for us. input (English text): Which percentile is the WMT2016 Russian-English dataset in ? output (Sparql query): percentile orkgp:HAS_LINK ?dataset_lbl. -------------- 100 percentile This will allow us to build a percentile index for a metric so that we will be able to find which metrics is most used.  group by ?measurement_lbl -------------------------- SELECT ?measurement_lbl ?value FROM dts:measurements_sorted_by_metric_rank a GROUP BY ?measurement_lbl ORDER BY percentile -------------- 100 percentile input (English text): List of metrics, or metric on a dataset (this time for a dataset created on WMT2016 Russian-English) ?metric orkgp:HAS_METRIC The Metric class can also be used to set other metric on a dataset: Metric.set_metrics(). The Metric class is an instance of the Metric class	False
"
 input (English text): What is the most common substrate for catalysts?
 output (Sparql query): SELECT ?substrate WHERE { orkgr:R25900 orkgp:compareContribution ?contrib. ?contrib orkgp:P15090 ?substrate. } ORDER BY DESC(COUNT( ?substrate)) LIMIT 1
 input (English text): What is the most common lead compound?
 output (Sparql query): "	SELECT ?compound WHERE { orkgr:R75638 orkgp:compareContribution ?contrib. ?contrib orkgp:P35194 ?compound. } ORDER BY DESC(COUNT( ?compound)) LIMIT 1	_______________________.R25900 R25900 input (English text): What is the most widely used chemical ? output (Sparql query): ______________________.Methylenedioxyphenyl-methylisoxazole-methyl-propyl-1-amine Methylenedioxyphenylmethylisoxazole-methyl-propyl-4-methylphenylacetamide Methylenedioxyphenylmethylisoxazole-methyl-propyl-1-acetamide Methylenedioxyphenylmethylisoxazole-methyl-propyl-5-methoxyphenylacetamide Methylenedioxyphenylmethylisoxazole-methyl-propyl-6-methylphenylacetamide Methylenedioxyphenylmethylisoxazole-methyl-propyl-7-methylphenylacetamide Methylenedioxyphenylmethylisoxazole-methyl-propyl-8-methylphenylacetamide Methylenedioxyphenylmethylisoxazole-methyl-propyl-9-methylphenylacetamide Methylenedioxyphenylmethylisoxazole-methyl-propyl-10-ethoxyphenol Methylenedioxyphenylmethylisoxazole-methyl-propyl-8 H-isoxazole-methyl-propyl-8 H-isoxazole-methyl-propyl-9-methylphenylacetamide Methylenedioxyphenylmethylisoxazole-methyl-propyl-8 H-isoxazole-methyl-propyl-9-methylphenylacetamide Methylenedioxyphenylmethylisoxazole-methyl-propyl-10 H-isoxazole-methyl-propyl-9-methylphenylacetamide Methylenedioxyphenylmethylisoxazole-methyl-propyl-11-acetylphenol Methylenedioxyphenylmethylisoxazole-methyl-propyl-11-acetylphenol Methylenedioxyphenylmethylisoxazole-methyl-propyl-13-methylpropylacetamide Methylenedioxyphenylmethylisoxazole, methyl(ethyl)anthracene-1-1,2,3,4,9,10,4,4,9,10 H2 (hydro)acetate, 2-deoxyeth	False
"
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Enduro dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Input: The name of the top performing model in terms of Score score when benchmarked on the Atari 2600 GameStar dataset ? Output: How many points ? How many units ? How many instances of the specified model: GameStar ?model_rdf ?rdfs_label ?model_lbl. Answer: What are the highest scoring models in terms of Score score when benchmarked on the GameStar dataset ? Solution: The following query returns the top performing models of the game Star Trek : SELECT TOP 1 SELECT id, name FROM allsources AS  WHERE score = ?score If you want to know how many sets of points a model was found to score within a given period, use the following query: SELECT TOP 1 SELECT id, name,  id = ID( ?query) WHERE  IS NULL  AND query = ?value   Solution: The following query returns an object of a set containing the values for points, but with the list of model classes that it has been found by, separated by  :   ?name  (in this case ""GameStar""). Select TOP 2 FROM allsources AS  WHERE score > ?score (Answer: How many sets of points would we find if we added a comma ?value to the query ? There are no set of points for games"	False
"
 input (English text): Provide a list of papers that have utilized the C51 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C51"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the C51 noop model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C51 noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C51"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } The above two outputs are what is left for the query. In this case, we will make a change, add some new variables, and then execute it. I will be using a slightly modified C51 model, and will have one feature for each paper that I will define for them and provide two query outputs: one with the parameters that I want (in this case my new model's parameter set): input This is a new parameter set. The parameter set can be anything, as long as it describes the feature you are interested in. There are no restriction on what it can be. You can use the model in a full parameter list, parameterized (not paramed) by a single variable, parameterized by multiple variables (with no way to specify more than one variable), or parametrized with multiple parameters, or with a complex set of parameters that you may wish to query. Note that the two parameter sets I use are called ""lazy"". The keyword ""lazy"" indicates that the input is not loaded yet when the query is completed; once the input parameter list is loaded, that parameter is used to fetch the feature, but not loaded otherwise. It is also possible to use the model as a parameter, but only if it has already been loaded previously. This is a new parameter set. The parameter set can be anything, as long as it describes the feature you are interested in. There are no restriction on what it can be. You can use the model in a full parameter list, parameterized (not paramed) by a single variable, parameterized by multiple variables (with no way to specify more than one variable), or parametrized with multiple parameters, or with a complex set of parameters that you may wish to query. Note that the two parameter sets I use are called ""lazy"". The keyword ""lazy"" indicates that the input is not loaded yet when the query is completed; once the input"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ning et al."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the FABIR model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FABIR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code WHERE { ?model FABIR orkgp:HAVS_DATASET ?dataset. ?cont orkgp:HAVS_BENCHMARK ?benchmark. ?cont orkgp:HAVS_MODEL ?model; orkgp:HAVS_SOURCE_CODE ?code. } Note Due to the huge differences of the model, we recommend to use FABIR or another model that fits well into the Ning model. Note The first two statements can not be combined. However, you can combine them later by adding the ?s qualifier in the output: input (English text): Can do benchmarking papers on this data ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model FABIR ? orkgp:HAVS_DATASET ?dataset. ?cont orkgp:HAVS_BENCHMARK ?benchmark. ?cont orkgp:HAVS_MODEL ?model; orkgp:HAVS_SOURCE_CODE ?code. } Note The first two statements can not be combined. However, you can combine them later by adding the ?s qualifier in the output: output (Sparql query): SELECT DISTINCT ?code WHERE { ?model FABIR ? FABIR ? orkgp:HAVS_DATASET FABIR ? orkgp:HAVS_BENCHMARK ?benchmark. ?cont orkgp:HAVS_MODEL ?model; orkgp:HAVS_SOURCE_CODE FABIR ? FABIR ? FABIR ? orkgp:HAS_DATASET ?dataset; orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MARK ?model_lbl; orkgp:HAS_LBM; orkgp:HAS_HAS_DATASET ?model orkgp:HAVS_HAVS:LBL ?code }; Note To have better performance, you should use both FABIR and other models. If using any other (non-FABIR) model, you need to use both of the first two statements. Note To compare models in	False
"
 input (English text): What are the objectives for Sepsis prediction?
 output (Sparql query): SELECT DISTINCT ?objectives WHERE { orkgr:R70642 orkgp:compareContribution ?contrib. ?contrib orkgp:P15051 ?objectives. } 
 input (English text): What is the emergency type of AMBER Alert?
 output (Sparql query): "	SELECT DISTINCT ?emergency_type WHERE { orkgr:R153500 orkgp:P31 ?contrib. ?contrib orkgp:P43093 ?emergency_type. } 	SELECT DISTINCT ?type FROM { ?types ?, ?types. ?type ? } Input for input() (SQL): output (Sparql query): I'll go through an example that shows you the differences between these two queries: input (English text): What are the objectives for Sepsis prediction ? output (Sparql query): SELECT DISTINCT ?objectives WHERE { orkgr:R70642 orkgp:compareContribution ?contrib. ?contrib orkgp:P15051 ?objectives. } input (English text): What is the emergency type of AMBER Alert ? output (Sparql query): SELECT DISTINCT ?type FROM { ?types ?, ?types. ?type ? } This version uses the full query name with no leading or trailing whitespace. It's also possible to pass a column name as a parameter and get the values that you expect. In this case the full query is: output (Sparql query): SELECT DISTINCT ?type FROM { ?types ?, ?types. ?type ? } In the same vein, you could also include your user name and the full query. For example: input (English text): What are the objectives for Sepsis prediction ? output (Sparql query): SELECT DISTINCT ?objectives WHERE { orkgr:R70642 orkgp:compareContribution ?contrib. ?contrib orkgp:P15051 ?objectives. } input (English text): What is the emergency type of AMBER Alert ? output (Sparql query): SELECT DISTINCT ?type FROM { ?types ?, ?types. ?type ? } Notice that this is the exact same query string that I used under the above example. In the above example, because I made a difference between types (type being something different), I have given a column name. In some cases, you may have a separate column name. For example if you only need the emergency type, you can always provide it yourself in your query string. All the queries you'll need in this guide are here: Sparql::mysql -> MySQL::Sparql -> Oracle::Sparql It gets easier to	False
"
 input (English text): List the metrics that are used to evaluate models on the OpenBookQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OpenBookQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric { ?dataset a orkgc:Dataset; rdfs:label ?dataset_LBl; ORG; rdfs:label ?metric_lbl. } FROM ?metric_lbl_lbl WHERE ?metric ORG; orkgc:HAS_LBL_METRIC_ORG ? (column : value : metric) RDFS: name of the metric rdfs: metric type or kgc: metric type or ORG or kgc: metric type or ?metric or metric_lbl or ?metric_lbl. [default is an ORG model. If metric data is required, use ""ORG"" for the LBL format or ""ORG"" for the ORG model] Example, for a single model and multiple models: input (English text): Compute the number of different metrics in all the models for an analysis of the metrics from the CoQA benchmark dataset. Use of this query requires a PostgreSQL database with schema version 8.5.0+ or higher. output (Sparql query): Example, using the ""ORG"" model and using an ORG model from the ORG dataset (coq_metrics_org) and a COQA 1.2 model (coq_metrics_coqa_1.2(coq_metrics_org)) input (English text): List the metrics for an analysis of the metrics from the coqbqa1.2 benchmark dataset (coq_metrics_coqa_1.2(coq_metrics_org)) output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?data set { ?dataset a orkgc:Dataset; ?dataset_LBl ?dataset. OPTIONAL } { ?dataset a orkgc:Dataset; ?dataset_LBl ?dataset_lbl. } } [default is an ORG"	False
"
 input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the OntoNotes benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	-------------------------- ALPHABETICAL DISTINCT SORT NEGATION LOWER COUNT -------------------------------- ------- -------------- --------------- -------------- ORG COUNT: 1 ORG DISTINCT: ? ORG COUNT 2 ORG DISTINCT: ? ORG COUNT 3 ORG DISTINCT: ? ORG COUNT 4 ORG DISTINCT: ? ORG COUNT 5 ORG DISTINCT: ? ANALYSIS ORG DISTINCT: ? ORG COUNT 6 ORG DISTINCT: ? ORG COUNT 7 ORG DISTINCT: ? ORG COUNT 8 ORG DISTINCT: ? ORG COUNT 9 ORG DISTINCT: ? ANALYSIS ORG DISTINCT: ? ORG COUNT 10 ORG DISTINCT: ? ORG COUNT 11 ORG DISTINCT: ? ORG COUNT 12 ORG DISTINCT: ? ORG COUNT 13 ORG DISTINCT: ? ORG COUNT 14 ORG DISTINCT: ? ORG COUNT 15 ORG DISTINCT: ? ORG COUNT 16 ORG DISTINCT: ? ORG COUNT 17 ORG DISTINCT: ? ORG COUNT 18 ORG DISTINCT: ? ORG COUNT 19 orkgp:HAS_DATASET ORG COUNT 20 ORG DISTINCT: ? ORG DISTINCT 21 orkgp:HAS_EVALUATION ORG COUNT 22 ORG DISTINCT: ? ORG DISTINCT 23 ORG DISTINCT: ? ORG COUNT 24 ORG DISTINCT: ? ORG COUNT 25 ORG DISTINCT: ? ORG COUNT 26 ORG DISTINCT: ? ORG COUNT 27 ORG DISTINCT: ? ORG COUNT 28 ORG DISTINCT: ? ORG COUNT 29 orkgp:HAS_MODEL ORG COUNT 30 ORG DISTINCT: ? ORG DISTINCT 31 orkgp:HAS_METRIC ORG COUNT 32 ORG DISTINCT: ? ORG COUNT 33 ORG DISTINCT: ? ORG COUNT 34 ORG DISTINCT: ? ORG C	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset ? output (Sparql query): input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Flashback dataset ? output (Sparql query): input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Flashback dataset ? output (Sparql query): input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Jaguar dataset ? output (Sparql query): input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Jaguar dataset ? output (Sparql query): input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Jaguar dataset ? output (Sparql query): incoming email from: rd4g (username): Hello, I'm wondering what paper benchmarks your studies are listed on ? The information on your website says you are a researcher in Artificial Intelligence; do you have papers listed that you can share with us ? Best regards, John incoming email from: rd4g (username): Hello, I'm wondering what paper benchmarks your studies are listed on ? The information on your website says you are a researcher in Artificial Intelligence; do you have papers listed that you can share with us ? Best views, John incoming request from: John (email address, city, country, language, etc.): Hi, I have a question about my academic records. I want to know which paper benchmarks my studies are listed on to. I can't find any information out in the academic web that comes up on my page. Would you be able to tell me what papers exist in your papers and which ones do not ? The information on my website gives it away but I hope that you can take some time to search for more information. Best regards John Incoming request from: rd4g (username): What should I do to get a specific paper benchmark ? Is there such a database on which I can find such a benchmark ? incoming email from: rd4g (username): What should I do to get a specific paper benchmark ? Is there such a database on which I can find such a benchmark ? Best regards, John Incoming	False
"
 input (English text): Where can I find code references in papers that have used the Contextual Match model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Contextual Match"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Pointer + Coverage + EntailmentGen + QuestionGen"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT query_1, query_2, query_3, query_4 as ------------------------ FROM user_code anduser_count select query_1, query_2, query_3, query_4 as ------------------------ FROM user_topic andtopic_count select query_1, query_2, query_3, query_4 as ------------------------ FROM user_topic_topic andtopic_count select query_1, query_2, query_3, query_4 as ------------------------ FROM user_topic_topic_code andtopic_count select query_1, query_2, query_3, query_4 as ------------------------ FROM entity_id,entity_name as entity_name select query_1, query_2, query_3, query_4 as ------------------------ FROM user inentity select query_1, query_2, query_3, query_4 as ------------------------ To run the function, run the following command line with sqlite3: SELECT command_text FROM 'C:\Python27>python -m sqlite3 sqlite>select_user_query_1( ?""user"",'code','2')"" AND_1 "" FROM user_id WHERE user_id = ?'"", __USER, '"", 'code', 2' SELECT command_text FROM 'C:\Python27>python -m sqlite3 sqlite>select_user_query_1( ?""user"",'code','3"")"" AND_1 "" FROM user_id WHERE user_id = ?'"", __USER, '"", 'code', 3' SELECT command_text FROM 'C:\Python27>python -m sqlite3 sqlite>select_user_query_1("	False
"
 input (English text): Provide a list of papers that have utilized the Transformer-XL model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Switch Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Switch-Transformer""); ORkgp:HAS_DATASET ?dataset. ?cont orpkgp:HAS_BENCHMARK ?benchmark. ?cont orpkgp:HAS_MODEL ?model; ORpkgp:HAS_SOURCE_CODE ?code; ?cont orpkgp:HAS_SOURCE_CODE ?model. ?code ?stat. } -h orkgp:HAS_MODEL: orkgp:HAS_HABITAT: orkgp:HAS_HAS_DATA_SOURCE_CODE: ORkgp:HAS_SOURCE_CODE: SELECT DISTINCT ?stat WHERE ORkgp:HABITAT. ORkgp:HAS_DATA_SOURCE_CODE. ORkgp:HAS_SOURCE_CODE. output (Sparql query): SELECT DISTINCT ?code WHERE ORkgp:DATA_SOURCE_CODE. ORkgp:DATA_SOURCE_CODE. Note to SQL editors: please be aware that the following SQL-to-RDF conversion may alter the results, and so we recommend importing your data back into the original format (or simply returning all the documents) before proceeding."	False
"
 input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ORkgp:HAS_EVALUATION ORkgp:HAS_BENCHMARK ORkgp:HAS_MODEL ?eval. ?paper orkgp:HAS_MODEL ORkgp:HAS_MODEL rdfs:label ?model_lbl. } Parameters --type 'dataset' : Which dataset. Example Select the subset defined in the CSV file that has been analyzed: select * from `http://www.example.com/sounds/item01` where id_str in (select 1 from nd3_2_1_p_c where id in (select 1,1,1,1,1,1,1,1,1,1,1,1,1 from ndc_1_df2_1_df table) limit 1) --id_str'model_lbl' : Specify a model_lbl identifier to find the data. Example Select the dataset with the model labeled ""Commonsense"". select * from `http://www.example.com/sounds/item01/model_lbl` where id_str in (select 1 from nd3_2_1_p_c where id in (select 1,1,1,1,1,1,1,1,1,1,1,1 from ndc_1_df2_1_df table) limit 1) --output 'dataset' : Specify a dataset to output the results from. Example Select the dataset from the CommonsenseQA dataset with a paper-type title. You may optionally specify a benchmarks option that selects which benchmark is analyzed. select * from `http://www.example.com/sounds/item01/benchmarks` where id_str in (select 1 from nd3_2_1_p_c where id in (select 1,1,1,1,1,"	False
"
 input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Amazon-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT { ?dataset a:AmazonDataset orkgc:AmazonDataset }. optionally { ?dataset rdfs:label ?dataset_lbl. } output (Sparql query): SET Metrics=AmazonDataset ORkgc. optionally { ?eval orkgp:HAS_DATASET ?dataset_lbl. } and what are the actual benchmarks used on this dataset ? Example: input (English text): How often do you benchmark models ? output (Sparql query): SELECT { ?benchmark if test ?benchmark_rdfs[:LINK_TITLE]. } optionally { ?benchmark if test ?benchmark_rdfs[:LINK_TITLE]. } output (Sparql query): SELECT { ?db_name if test ?db_name]. } FROM { ?db1: ?dataset ?dataset_rdfs } WHERE { ?dataset_lbl: AmazonDataset }. OPTIONAL { ?db_name if test ?db_name]. } optionally { ?db_name if test ?db_name]. } output (Sparql query): SET Timestamp=2013-01-01 09:02:55 +0000:00. optionally { ?db_name if test ?db_name]. } optionally { ?db_name if test ?db_name]. } output (Sparql query): SELECT { ?dataset_lbl if test ?dataset_lbl.. } FROM { ?db1: ?dataset ?dataset_rdfs } WHERE { ?dataset_rdfs: AmazonDataset }. ORG < test_criteria > { ?dataset test_criteria }. output (Sparql query): SELECT............. : DISTINCT ?dataset_lbl with 1.00 (2.80). OPTIONAL { ?dataset_lbl if test ?dataset_lbl... } optionally { ?dataset test_criteria}. output (Sparql query): SET Timestamp=2013-01-01 09:02:55 +	False
"
 input (English text): List the code links in papers that use the DDRL A3C model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDRL A3C"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the A3C-CTS model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C-CTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code WHERE { ?model a orkgc:Model } OR { ?model a orkgp:HAS_DATASET } ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model ? OR { ?model a orkgp:HAS_DATASET ? } ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. -- Table-driven analysis output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model} ; orkgp:HAS_SOURCE_CODE ?code. output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model ?dataset. orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model ? } OR { ?model a orkgp:HAS_DATASET ? } ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. -- DERIVATIVE output (Sparql query): output (Sparql query): SELECT 1.0 -1.0 as x, 1.0 -r(x) as y, x AS rdata1, rdata1 + rdata2 as y; output (Sparql query): SELECT 3.3 * (1.0 -x) as x, 2.2 * (1.0 -r(x)) as y, x * rdata1 as y; output (Sparql query): SELECT	False
"
 input (English text): What is the top benchmark score and its metric on the CoNLL 2012 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Hutter Prize dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT score AS ?score, RATIONAL ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { score. ?val. ?value. } } output (Sparql query): SELECT dists.dists[""CoNLL 2012""] as tester_result, dists.dists[""Hutter Prize 2012""] as tester_data FROM dists INNER JOIN dists_tester on dists.dists_.name = tester_data.tester_name ORDER BY tester_result DESC( tester_count ) AS ?tester_count, RATIONAL dISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score ) AS r_metric, RETAIN rdfs:label ?dists_lbl, rdfs:metric rdfs:label ?metric_lbl, RETAIN rdfs:model rdfs:model, OPTION ( ?) orkgp:HAS_VALUE ?model ORDER BY r_metric r_metric_lbl. select id, score. ?value, r_metric. ?value, r_metric_lbl. ?value, rdfs:label, rdfs:metric_labels as metric from dists_tester; Input data: 2nd round Diversity Score This is the diversity score calculated as follows. Diversity = score divided by total number of participants This method is somewhat controversial, given the many points that individuals and co-curricular organizations have put forth and all the evidence that the methodology is flawed with regards to producing biased scores. The point worth of a diversity score to me is how well teams are doing at recruiting, engaging the community, building community, connecting with people and participating in the event. This is another facet of diversity that may or may not be relevant to this discussion; it may not make a significant difference in the overall outcome. The dataset I used is also not particularly unbiased, as it isn't a diverse dataset with a higher level of diversity than most. For me, I would think that this would be useful if we wanted to predict how well someone should perform as a team member at the event. However, if I assume that the goal is to create an unbiased, objective score, then this may not be necessary. This is likely only going to create"	False
"
 input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2003 (English)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Which model has achieved the highest F1 score on the CoNLL 2005 (English) benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F1 score on the CoNLL 2017 (English) benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F2 score on the CoNLL 2017 (English) benchmark dataset ? output (Sparql query): In summary, the co-owned F1 benchmark dataset supports 5 models. The CoNLL 2003 dataset supports 10. The CoNLL 2005 dataset supports 50. The CoNLL 2017 dataset supports 70. A third model (or so) is missing. In my case I am just providing some examples. Below I've listed the results on the Jupyter Notebook dataset created with JUpyter Notebook 2.2.0-RC (which is part of the TensorFlow library), in order of score to show the highest F2 score of the dataset. (Note that scores for all models in F1 and F2 are normalized by default, so your results might not agree precisely with these score shown. More on that below.) There are 2 ways to calculate a Model Score of F1. First, you can use the following function. >>> max_score = min(2, count(outputs)) >>> score = 0.0 You can also use a function provided by the TensorFlow implementation of co-owned model scoring. By default, co-owned model scoring uses a combination of the F1 Score and the F3 Score. In this case, I am using the F1 Score. Here is the output from the co-owned benchmark dataset, as an alternative way to calculate a Model Score of the co-owned benchmark dataset: Notice that the score is low compared to F1: in this case a model with no F1 score is the lowest scoring model! I've listed the scores at the bottom of this post with the names of the model's columns of F3 score. Finally, I just wanted to show you, that if you try to use the output of co-owned benchmark dataset with a different input for the test of a co-owned score, your test will run with the same score as your own dataset! Also, if you use co-owned model scoring with a model without F1	False
"
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Hutter Prize dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_Lbl ) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } What if I add a more complicated metric to the query and the number of models are more than 8 ? Here is a table of your performance in terms of the number of parameters score for an 8 variable model. input  (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Hutter Prize dataset ? output (Sparql query): SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_Lbl ) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark ? orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?"	False
"
 input (English text): Can you list benchmarked problems in the area of Computer Sciences?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Computer Sciences"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you list benchmarked problems in the area of Artificial Intelligence?
 output (Sparql query): "	"SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Artificial Intelligence"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } "	"SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Computer Sciences"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } This example shows a generic output from an SPARQL query describing a problem in Computer Science called ""Dissertation"" that can be created from an SQL statement. We create a SPARQL query that selects every record in the University of Kentucky's (UK) computer science department which is relevant to the problem in the paper by Dr. Jørgen Tronick. However, even a generic query like this is not sufficient because the problem might have been created by other means—for instance, by a web service. This example also shows a query that works well in general (if not exactly for a specific problem). It will be discussed more in the query descriptions in the next section. 1.1.4. Creating a simple SQL query To create a ""simple"" query that can be used in the example here, we simply change our original spartql-simple-sql query into a SQL statement like the following: -- Create table with ""issue"" column using -- id as primary key. CREATE TABLE issue (id SERIAL PRIMARY KEY, text varchar(100) NOT NULL, date TIMESTAMP NOT NULL, first_name textNOT NULL DEFAULT ""Jørgen Tronick"", last_name text NOT NULL DEFAULT ""Tronick""), issue_id intID DEFAULT 0, row1 text NOT NULL DEFAULT ""Dissertation"", row2 text NOT NULL DEFAULT ""Computer Science"", row3 text DEFAULT ""Dissertation"", row4 text NOT NULL DEFAULT ""Computer Science"", row5 text FROM system.pagestats T ON [issue_id] = T.row1 ORDER BY issue_id; After creating the table, we query the query for data. First, we fetch the data from the database. -- Query the query for data. SELECT issue_id as issue_id FROM system.pagestats, issue_id; We"	False
"
 input (English text): What are the models that have been benchmarked on the SNLI dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the FSNS - Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The paper This new paper is published in arXiv. Thanks to the people who helped us out with the proofreading."	False
"
 input (English text): What is the best performing model benchmarking the WMT2014 French-English dataset in terms of BLEU metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?model ?model_lbl WHERE { ?metric bor_test_value:Measure; rdfs:label orkgp:HAS_DATASET ?dataset orkgp:HAS_EVALUATION ?eval. ORYP ?eval BOR_test_value. } ORDER BY DESC( ?value) LIMIT 1 } output (Mockup of database configuration): ( ). . . ROW_NUMBER=4 ( ) [#_] ROWS=10 [#_] MIN_REQUESTS=9 [#_] MAX_REQUESTS=4 [#_] 	False
"
 input (English text): Where can I find code references in papers that have used the ELMo model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-Attention + ELMo model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. }. If you could make just a few minor changes, I would be happy to provide a small pull request and a version of the package. I know that implementing the model was a huge effort and I appreciate any small contribution. I would like to thank everyone who did make this model and did the benchmarking work, particularly Alex McVey who did the bulk of the work to build the data sets for these test reports. You can access the ELMo Benchmark Data Set and its corresponding code in the repository of the SPSS BiLSTM-Attention repository. I hope this is something people will look forward to! Other Resources Check out this blog post by Kevin Gaudette that uses the ELMo model to understand the human brain's neural circuits."	False
"
 input (English text): What is the highest benchmark result achieved on the WSC dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { select ?metric ?metric_lbl from ?rdfs ORDER BY 0, ?test_metric } } 〈 ? Metric \ ? Metric_lbl 〉................................. 〈 ? Metric \ ? Metric_lbl \ ? Metric_lbl input (English text): What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value ? output (Sparql query): 〈 ? Metric ? Metric_lbl 〉....................................... In the following example, the query returns a new row for each benchmark. Note that the first benchmark results in ""M-10"", a value representing the 10th percentile of data from the first sample. The query's output includes two new records: one showing the WSC scores based on the WSC metric, and the other a score for each of the tests. The WSC metric is the median score; the WSC metric score is the 90th percentile. select sum(ROW_NUMBER) as nrows, ?metric as metric, ?metric_lbl as lbl from ?collections select {{ ?metric ?metric_lbl, ?metric rdfs:label ?lbl }. } select { { ?metric ?metric, ?metric_lbl as ?lbl, ?metric rdfs:label ?lbl }. } select { { ?metric, ?metric_lbl as ?lbl, ?metric rdfs:label ?lbl }. } select { { ?metric, ?metric_lbl as ?lbl, ?metric rdfs:label ?lbl }. and ?metric = ?metric_lbl}"	False
"
 input (English text): Provide a list of papers that have utilized the EfficientNet-L2-475 (SAM) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNet-L2-475 (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Shake-Shake (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } output (Sparql query): input: Provide a list of papers that have utilized the EfficientNet-L2-475 (SAM) model and include the links to their code ? output (Sparql query): input: Provide a list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code ? Outputs: To be clear, this question asked whether EfficientNet-L2-475 models are suitable for modeling non-stationary and ""predictable"" events, as opposed to stationary events, which are not predictable. (1) The output is a list of papers that have utilized (or will in the future utilize) the EfficientNet-L2-475 model to ""maintain stationary"" by maintaining a list of states"	False
"
 input (English text): Name the datasets that have been used for benchmarking in the Relation Extraction research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Relation Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Joint Entity and Relation Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	" The ""HAS_DATASET"" variable controls whether all the datasets used as benchmark for joint entity extraction and relationship mapping are retrieved, where the ""AND"" operand is true, where the ""OR"" operand is false. A true binary operator is used in these cases, while the null is used on otherwise. output (Sparql query): The ""HAS_PROGRESS_HOUR"" variable controls whether you want to be alerted within 10 seconds of any step of the process. The variable can be set to a single digit or the number of ""HAS_DATASET"" seconds. A value of zero or too few seconds will alert you about all steps of the procedure, while a value of too many or too many hours will alert you about only ""HAS_DATASET"" steps. You should set this variable to a non-negative number if you care about the system crash rate in these cases. You can also enable a different default (e.g., ""notify once per 30s""). input (English text): What are the most commonly used benchmarks by the research group ? output (Sparql query): The ""MULTIPLIER"" variable selects benchmark datasets from one to eight for the Multiplier research problem. You can specify a single or multiple datasets within the range 1-0.5. The ""IN"" keyword forces the dataset selection to be based on the input of only that single benchmark, and the ""NOT"" keyword forces all benchmarks to be excluded from the selection, as described above. output (Sparql query): "	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiNLI dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiRC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultRC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The problem is that most of the time only the metrics in question are available for you to consider. There's a better way! For example ""criticism"" is always the most common evaluation metric – that is, it is the first metric we know about (the one we always see!) in any given model. Let's consider a model called ""dopamin"", a machine learning model that can learn to perform a variety of tasks. Let's imagine that the only input parameter that this model is familiar with is ""rating"" (the label that is associated with a given task) and that the models data-frame has a single column ""criticism"", which is an integer between 1 and 10. As the model is doing nothing but evaluating data to generate some output, we'd expect to get a data-frame with an ""evaluation"" column. So this metric should be set to 0. By applying the ""critique"" metric over this same data-frame, we'll see that a simple ranking function is formed. So this metric should be set to ""critique - 2"" So this metric should be set to ""critique 1"" The ""Criticism"" metric is only a simple ranking function that can predict a given task by a simple linear relationship - the only thing that is missing is the input parameters for the task (e.g. performance, accuracy). If someone wants to be able to perform a ""sketch"" for a task – for example, a simple one-on-one decision problem – they will need to be able to evaluate the performance and accuracy of the model in terms of its ""critique"" value. Now, if all of this sounds very straightforward (and it is, actually!) then there's a big problem - how does one get that the output of this model, that is the data-frame ""criticism"	False
"
 input (English text): Where can I find code references in papers that have used the SRU++ model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SRU++"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the SRU++ Base model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SRU++ Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" [[0]+[1]+[2+[3]]]] Output of this query has the following output: [[0]+[1]+[2]+[3]] Results like this are very important for making quick progress. Note: the SRU++ model is only very useful if you have an understanding of the underlying math you are using, as it is built into the underlying Sqlite database engine. For a full list of model features and parameters, consult the SRU++ documentation. Data manipulation I typically use a number of standard Sqlite utilities to manipulate my data. For example, the following two commands allow you to easily insert values to a table: pprint <tablename> create table mydata (id int, name varchar2(100)); The id and name parameters are set to an appropriate value for the database you are working with, as listed by the database driver options. The name specifies the column to print to; you can also use the '--name=NAME' option to specify the column name. For example: pprint <tablename> create table mydata (id int, name varchar2(100, ""test1""), name string); This command produces the following output: ---------------------------------------------------------------- ----------------------------------------------------- Test1: name=""test1"" id=""test1"" ---------------------------------------------------------------- ----------------------------------------------------- Test1: id=""test1"" name=""test1"" ---------------------------------------------------------------- ----------------------------------------------------- Test1: name=""test1"" id=""test2"" ---------------------------------------------------------------- ----------------------------------------------------- Test1: id=""test2"" name=""test2"" ---------------------------------------------------------------- ----------------------------------------------------- Test1: id=""test2"" name=""test1"" ---------------------------------------------------------------- ----------------------------------------------------- Test1: id=""test1"" name=""test1"" ---------------------------------------------------------------- ----------------------------------------------------- Test1: id=""test2"" name=""test2"" ---------------------------------------------------------------- ----------------------------------------------------- Test1: id=""test2"" name=""test1"" ---------------------------------------------------------------- ----------------------------------------------------- Test1: id=""test2"" name=""test2"" Output of this command has the following output: -------------------------------------------------------- Test1 | ID | Name | ---------------------------------------------------------------- -------------------------------------------------------- Test1 | ID | Name | -------------------------------------------------------- Test1 | ID | Name | -------------------------------------------------------- Test1 | ID | Name | -------------------------------------------------------- test1 | id=1 | name=""test1"" test1 | id=""test1"" test2 | id=2 | name=""test2"" test1 | id=2 | name=""test1"" test1 | id=2 | name=""test1"""	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Alien benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asteroids benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asteroids"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Asynchronous output in the last option requires you to specify the same parameters but for multiple databases. This requires setting the database you wish to receive the results from as an array (e.g. a linked list, indexed or the like). This can be achieved by specifying the db parameter and setting the output_rdf option in the query. Asynchronous output in the last option requires you to specify the same parameters but for multiple databases. This requires setting the database you wish to receive the results from as an array (e.g. a linked list, indexed or the like). This can be achieved by specifying the parameter and setting the option in the query. Input and output parameters¶ The data that is produced from your query will be output as an arbitrary data frame, typically in RDF. The data can be any data type supported by the RDF schema, with some restrictions: All output data is in integer form (e.g. 1, 2, 3, 4, 6, 8, 16, 32) - which is intended to allow the results to be easily viewed and used by non-programming people. In practice, this means that if you use an RDF schema with columns that are non-integer (e.g. integers and floats), you may want to use double-digits ('1', '2', '3', '4', etc.). To make the result of the query available in RDF, call the query function RdfMeta. This function provides a simple way to query the database to get the result of your query, which is then available as a new instance of the RDFMeta object: >>> from r2.models import Model >>> rdf = Meta.query(model_lbl) >>> print rdf.summary(tbl_info_mgr) Summary of the Model Lbl data frame: { ""version"": ""2010-11-01"", ""source"": [] } { ""avg_score"": 0.123828, ""id"": 1, ""name"": ""Atari 2600"", ""scored"": 1.0, ""score_number"": ""5"", ""name"": ""Atari 2600"", ""score"": 0.0000, ""model"": { ""version"": ""R6.0.0"", ""source"": [ ] }, ""name"": ""Alien"", ""score"": 0.143675, ""rating"": ""5"", ""criteria"": { ""name"": ""Score"": ""3.1"","	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the WMT2016 Romanian-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the WMT2014 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT output_list ( Rdfs:ID, D, Rf) where id is the id of the input table and Rf is the raw-data table (see Note 2). Example #4: Benchmark the benchmark list using a variable-length column To evaluate the query: select result_code, resultset, result_length. text, output_list ( result_code, Rdfs:ID ),  "" Benchmark "",  output_list ( result_code ). map {| _ | Rf. to_dataset( _ ). to_dataset_len( Rf. to_dataset_length( _ ), Rf. to_dataset_length( Rf. to_column( _ )). length )},  "" Benchmark "",  output_list ( result_code, resultset ). map {| _ | Rf. to_dataset( _ ). to_dataset_len( Rf. to_dataset_length( _ ), Rf. to_dataset( Rf. to_column( _ )). length )},  "" Benchmark "",  output_list ( result_code, resultset ). sort ( 1 ). collect {|x| Rf. to_dataset( x ). to_dataset_len( Rf. to_column( x )). count } { ? } Example #5: Benchmark the benchmark list using a named column To evaluate the query: select result_code, resultset, result_length. text, output_list ( result_code, Rdfs:ID ),  "	False
"
 input (English text): What are the models that have been benchmarked on the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the SearchQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?rdfs_labels. FROM ?searchqas/ ?searchqas orkgc; OR \ ?rdfs_labels. SELECT MODEL. ?model rdfs_labels. OPTIONAL { ?model rdfs_labels. ?model rdfs_labels_prelude. } Output (English text): SELECT : { ?dataset a orkgc; rdfs :label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA""); ?benchmark orkgp :HAS_DATASET ?dataset; orkgp :HAS_EVALUATION ?eval. ?paper orkgp :HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp :HAS_MODEL ?model. OR { ?printable ?model OR model.!} } } query (English text): What are the models that have been benchmarked on the TriviaQA dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs :label orkgc. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp :HAS_DATASET ?dataset; ogp :HAS_EVALUATION ?eval. ?paper orkgp :HAS_BENCHMARK ?benchmark. OR \ ORIGLP :HAS_MODEL ?model. OR { ?paper orkgp :HAS_MODEL ?model_lbl. OR { ?printable ?model OR model_lbl.!} } } What are the data sources ? The data sources for these models include the original TriviaQA dataset and the search engine results, although the latter may not always be directly comparable. How does the data fit ? The model is very robust. From data sets including the original dataset, there are quite a few models that fit the data exactly. In the case below, we take the first five results of the search from the first part and then do a Monte Carlo simulation to replicate this data. The model performed quite well. For a more detailed comparison of these models,"	False
"
 input (English text): What are the metrics of evaluation over the Open Entity dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Open Entity"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the OntoNotes dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Onto Notes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The RPSA is an acronym for Research, Publication, and Publication Quality. I was trying to figure out what this acronym stood for, so I wrote an R script to find out what ""Metrics of Evaluation Over OntoNotes (open-source)"" and ""Metrics of Evaluation over OntoNote (open-source)"" stood for. I wrote this script after reading about the RPSA and other related issues in a discussion on RPSA page of SourceForge (which also contains this example of the RPSA output). My output looks like this: The RPSA is a collection of terms and phrases designed to distinguish between different types of evaluation and how they should be evaluated. Metrics for different ways of evaluating an activity is not unique to Open Source (you can see some examples at the Wikipedia article). A metric is often one that is used in some way or another to measure evaluation. The term is typically expressed as a ratio, or the number of times something is different compared to something else. For example, ""We measured 10 hours of total running time and this was the time that the program took to finish the tasks"". Metrics, like time taken, are not unique to Open Source projects and therefore can be different between implementations of Open Source projects. For example, you may be able to measure time taken to complete a task over a long period of time. Your average time taken will be different. What is a metric ? A metric specifies a way of comparing the performance of your process of project evaluation. Metrics do not measure different aspects or goals of evaluation. They are used to measure the performance of your project. Metrics give you information about your project. Metric terms are commonly used to show how the performance of your evaluation differs from others or how well different areas work. A metric can be used to make distinctions. For example,"	False
"
 input (English text): What are the metrics of evaluation over the Atari 2600 Space Invaders dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Defender dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari Defender"") ?benchmark orkgc:HAS_DATASET ?dataset; orkgc:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgc:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What are the metrics of evaluation over the Atari 2600 Defender dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari Defender"") ?benchmark orkgc:HAS_DATASET ?dataset; orkgc:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgc:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } I don't consider this an improvement of the original paper but a useful addition. Conclusions I have to say that this paper has been my first attempt to tackle this subject, and it is an improvement over my earlier research paper ""Why Do We Play ?"" On that paper, I started with an argument about the relative importance of playing and learning from a new Atari system. I tried to address this with all levels of systems experience and explored the role of the learning process in play. It didn't do anything for me to get the key concepts in there. My main idea was to use a dataset where we could use both the metric and the evaluation metrics - a dataset that was a little more complicated where I had to explain why the same metric was bad and why the evaluation metric was good. I should have put more thought into how to structure the metric and how to interpret the evaluation metrics and I need to take some time to think about how to best present the data structure to the reader. I also need to take some time to think about how to present these data structures to the reader. But that"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset WMT2014 German-English?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-Dutch"" ?) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): What is the top benchmark result (metric and value) over the dataset WMT2014 English-Danish ? output (Sparql query): -------------- { metric and value over WMT2014 French-Danish } input (English text): What is the top benchmark result (metric and value) over the dataset WMT2014 Danish-English ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 Danish-English"" ?) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset PWC Leaderboards (restricted)?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title orkgp:HAS_TITLE ?title ORkgp:HAS_TITLE ?title_lbl SELECT ?title orkgp:HAS_ID ?title ORkgp:HAS_ID ?title_lbl, orkgp:HAS_DATASET FROM ?""results.datasets"" AND ?cont orkgp:HAS_BENCHMARK ORDER BY DESC( ?value ) output (Sparql query): SELECT TITLE, ID, TITLE_LBL FROM ""results.datasets"" WHERE ?cont orkgp:HAS_BENCHMARK ?""results.datasets"" ORDER BY TITLE, ID, TITLE_LBL input (English text): What are the minimum times that a title should be updated in the PWC Leaderboards (restricted) dataset's title of a data point ? output (Sparql query): ------------ ?formatting ||- Title (Min): 5 - Title (Max): 14.5 input (English text): What is the most common URL in the PWC Leaderboards (restricted) dataset's title ? output (Sparql query): ------------ ?formatting ||- PWC Leaderboards (restricted) URL -0: https: //www.pwccu.org:3128/leaders/ input (English text): What is the most common domain in the PWC Leaderboards (restricted) dataset's title ? output (Sparql query): ------------ ?formatting ||- PWC Leaderboards (restricted) URL -0: https://www.pwccu.org/leaders/ input (English text): What is the most common page number in the PWC Leaderboards (restricted) dataset's title ? output (Sparql query): ------------ ?formatting ||- PWC Leaderboards (restricted) URL -0: https: //www.pwccu.org/leaders/page/ input (English text): What are the total number of PWC Leaderboards (restricted) data points in the dataset PWC Leaderboards (restricted) which were made in the last month ? output (Sparql query): ------------ ?formatting ||- PWC Leaderboards (restricted) Total -0: 0 -1: 1 -2: 6 -3: 7 -4"	False
"
 input (English text): List the metrics that are used to evaluate models on the WikiText-2 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the enwik8 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Inputs and Outputs	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the TACRED dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TACRED"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT c.name as _c, metrics.name _metrics.name, c.n.name as _metrics.name, metrics.n.n_name - 1 from metrics; c.statistics as _metrics.name; c.lblid.name as _metrics.name; c.re.start.start_date as _metrics.name; c.ret.start_date as _metrics.name; c.stat.n = 10; c.ret.n = 7; c.lblid.name - 1; c.statistics 0; c.lblid.n - 1; c.re.start.start_date; c.ret.start_date; c.lblid.name - 1; c.statistics 0; c.lblid.n - 1; c.statistics 0; c.lblid.n - 1; c.statistics 0 Note that, since output (Sparql query only) will return a list of metrics, there are no default values on the metric keyword. To change the default value, you can use RODBC query API: C.RETRY_TIMESTAMP=1434861788 C.RETRY_TIMESTAMP2=1434861788 Note that if you want to measure the quality of an event, use C.START_COPY_STRENGTH. Otherwise, the quality metric for the same event will be given at time C.START_COPY_STRENGTH2. You can also filter by column names or the labels of these columns. The filter keyword is used to filter the metrics by its name (it defaults to false). You can see which attributes are used when creating a metric for a particular column by using output (Sparql query): SELECT c.name as _c, c.n.id as _n, c.i.id as _i, c.r.id as _r, c.re.start.start_date as _	False
"
 input (English text): In what country was conducted research with the largest number of participants?
 output (Sparql query): SELECT ?country WHERE { orkgr:R44980 orkgp:compareContribution ?contrib. ?contrib orkgp:P15249 ?country; orkgp:P23169 ?number_of_participants. } ORDER BY DESC( ?number_of_participants) LIMIT 1
 input (English text): What is the most common location in the studies?
 output (Sparql query): "	SELECT ?locations WHERE { orkgr:R111045 orkgp:compareContribution ?cont. ?cont orkgp:P37537 ?locations. } ORDER BY DESC(COUNT( ?locations)) LIMIT 1	"SELECT ?country WHERE { orkgr:R44980 orkgp:compareContribution ?contrib. ?contrib orkgp:P15249 ?country; orkgp:P23169 ?number_of_participants. } ORDER BY orkgp:name ORDER BY orkgp:contribution DESC (ORKGR_ID) LIMIT 1 input (English text): In what country were conducted research with the largest number of participants ? output (Sparql query): WHERE ""worldwide-scale orkgr "" = "" ORKGR_ID""; OR ""country_wide-scale orkgr "" = "" country_wide_scale orkgp ""; OR ""city_wide-scr "" = "" city_wide_scr ""; OR ""number_of_participants "" = "" number_of_participants""); input (English text): In what country were conducted research with the largest number of participants ? output (Sparql query): Where does the "" ORKGR_ID "" stand for ? input (English text): In what country were conducted research with the largest number of participants ? output (Sparql query): SELECT ORKGR_ID FROM "" results WHERE "" country_wide_scr = 'city_wide_scr' AND ORKGR_ID = '000a7b3c8c4b7f' "" INNER JOIN "" country_wide_scr "" ON "" orkgr_id "" = "" country_wide_scr WHERE country_id = '000a7b3c8c4b7f' "" AND "" country_id = '000a7b3c8c4b7f' "" INNER JOIN "" country "" ON "" orkgp_id "" = "" country_id"" AND "" country_id = '000a4d837b958b4' "" ORDER BY ORKGR_ID; input (English text): In what country were conducted research with the largest number of participants ? output (Sparql query): WHERE "" number_of_participants "" = ? input (English text): In what country were conducted research with the largest number of participants ? output (Spar"	False
"
 input (English text): Provide a list of papers that have utilized the He et al., 2017 + ELMo model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""He et al., 2017 + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BCN+ELMo model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BCN+ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?benchmark orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Note: You will need to add your dataset to either the dataset_id or dataset_name columns of input (Text file): #./pipestree.sh select dataset_id OR datasetsetid FROM my_datasets; select dataset_name OR datasetname FROM my_datasets; Note: You may have to be the data user in pipestree.sh to run this script, otherwise you are using the python package ""pipestree"". Note: You need to run script twice to perform your results. 1) Create the R source code and run the script. $ dplyr --format'select * from f1' { ?model_lbl. code, ?cont, ?benchmark } $ python pipestree.py test_ELMo_Papers.txt Note: You will need to edit pipestree.py to take your data as a file and add the new dataset(s) to the inputs_file field of the script. 2) Run the second script to get your results! $ python pipestree.py benchmark_ELMo_Papers_Papers.txt Note: If you are able to successfully run the second script (running it as a shell script), congratulations! You just created your new, custom R package. If you are unable to complete the second script by using a ""shell"" script, please be sure to read through what has already been written here. 4) This last part is a little more involved. 1) We need to create a R package called 'ELMO'. We need to make sure that ELMO/elmo_0 is installed as a package and that the latest version is installed on the system. From your shell, you can create a ""pip"" directory, where ELMO can be installed (which will be the name of the package that you are writing), and inside that folder, create another text file named '"	False
"
 input (English text): Indicate the model that performed best in terms of PRE-TRAINING DATASET metric on the DCASE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PRE-TRAINING DATASET"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pre-Training Dataset"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Select DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PCA""); FILTER (str( ?dataset_lbl) = ""PCA""); FILTER (str( ?dataset_lbl) = ""PCA""); orkgp:HAS_DATASET ?dataset_lbl.    Input and Output Note about Output formats: The output formats below are only for example, you can use a different one to display the results, check the option ""Datalab output in"" if you need to display Datalab-style data Selects output for pre-training/post - pre-training of the model is saved and pre-training data is updated for post-training/retrospective. For this purpose a model object (model:models) is passed into the pre-training process. The model object (model:models) will receive data from some input files as an input and a prediction function or other input data. The model will keep some records from the input. Model parameters are saved and updated for post-training/retrospective. The model objects (model:models) will receive data from some input files as an input and a prediction function or other input data. The model will keep some records from the input. Model object parameters are saved to database as an input and another model object is passed to the post-training loop. When this model is retrained, the pre-training data will be passed back as an input and the prediction data will be used to update the updated model. In this case, when post-training data is re-read by a regular rescan loop, the pre-training data will also be re-read and re-evaluated. Example: input (English text): Input data is saved (in case of a model update) as a table with the models and their parameters. output (Sparql query): Select model.name as modelid and modeldata from model_lbl where { ?model a or kgc:Model; OR kgs:label ?model_lbl. OR kgc:Resampling:DBCN( ?format = DATASET_LOG2); resample_id = ?resampling_id }"	False
"
 input (English text): Indicate the model that performed best in terms of Bits per byte metric on the Classical music, 5 seconds at 12 kHz benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bits per byte"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the Classical music, 5 seconds at 12 kHz dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?measurement_dsp_n ?metric_labels. FILTER (str( ?metric_labels) = ""Bits per byte"") ) input (English text): What metric was used ? output (Sparql query): SELECT ?measurement_dsp_kp ?metric_labels. FILTER (str( ?metric_labels) = ""Bits per byte"") output (Sparql query): SELECT ?measurement_dsp_t ?metric_labels. FILTER (str( ?metric_labels) = ""Bits per byte"") inputs (English text): Indicate the model that performed best in terms of Bits per byte metric on the Classical music, 5 seconds at 12 kHz benchmark dataset ? outputs (Sparql query): INDEX ?measurement_dsp_kp ON ?metric_label; /* --------------- ORDER BY ID DESC LIMIT 1 --------------- ---------- Classical music, 5 seconds at 12 kHz benchmark dataset ------------ --------------- --------------- --------------- --------------- 0 0.051 0.024 1.008 2.031 -------------------------------------------------------- Classical music, 2 seconds at 12 kHz benchmark dataset ------------ 2 0.031 0.017 1.004 2.037 -------------------------------------------------------- Classical music, 5 seconds at 12 kHz benchmark dataset ------------ 5 0.025 1.003 2.064 3.069 ------------------------------------------------ Classical music, 6 seconds at 12 kHz benchmark dataset ------------ 7 0.061 0.032 1.006 2.064 ------------------------------------------------ Classical music, 2 seconds at 12 kHz benchmark dataset ------------ 2 0.037 0.038 1.006 2.064 -------------------------------------------------------- Classical music, 5 seconds at 12 kHz benchmark dataset ------------ 5 0.048 0.030 1.006 2.064 -------------------------------------------------------- Classical music, 6 seconds at 12 kHz benchmark dataset ------------ 7 0.047 0.032 1.006 2.064 ------------------------------------------------ Music 1 or 4 (RJL) (16-voice or 16-voice/16-voice hybrid), no quantizer, no spectral smoothing, no time domain filtering, 5 seconds at 12 kHz, 1 kHz resolution, 0.0004 Hz, 5-shot. inputs (English text): Indicate the model that performed best in terms of Bits per byte metric on the Classical music, 5 seconds at 12 kHz benchmark dataset ? output"	False
"
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2014 German-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } The SQL query above returns in the R output all model columns of all columns named a and b (which might be the model parameter columns you want to use for a versus b), as well as their metric values. If my understanding is correct (thanks to the commenter @m_boucher), your model's metrics are also available on the query results webpage, though your raw data is not. Batch Normalization As we previously mentioned ""all of your R data is your data!"" and thus batch normalization must take place prior to the normalization phase. When normalization is performed, we ensure data consistency by using the Coda standard for standard deviation. The CODA standard for a standard deviation is the value obtained by taking the square root of that (rounded) value. The following table shows the CODA standard deviations for the model inputs (for comparison to the WMT2014 benchmark dataset): Dataset CODA standard sd (units) (R-squared) English N 1.945 1.920 Dutch N 0.900 0.938 English + Dutch N −0.900 −0.936 German N 0.700 0.740 German + English N −0.700 −0.740 Other nationalities N 4.566 4.094 Japanese N 2.100 2.028 English, French N 0."	False
"
 input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" output (Sparql list of test vectors): print(test_vars) Outputs: - The output is all numeric values (numerics) for the respective parameters. The ""Numeric Variables"" column indicates the number of values that came from the sample dataset. The columns ""Model Variable"" and ""Model Variable Variable"" is the name or dataset as determined by the source dataset, respectively. the input dataset is the data set of the parameter. the training/evaluation dataset are the data set of the parameter. model_lbl is the name or dataset of model, which was evaluated. model_lbl of the model was used in the benchmark. The data matrix of the model was provided by the benchmark. value is the coefficient of that model. value of the model was used in the Benchmark (Model). model_lbl of the model was used to perform the Benchmark (Metrics). val was the predicted value for that feature. val of the model was used in the Benchmark (Model). rdfs is a RDFS of the RDFS of the model, which was evaluated. data vector of the model was provided by the benchmark. the training/evaluation datasets are data set of the model and validation data set of the parameter (model_lbl, val). output_lbl is the name or dataset of the model that was included in the benchmark. output_lbl was used by the test data set for the model. input_lbl is the name of the input dataset. The dataset and model were included in the benchmark. training_lbl is the name of the model that was evaluated in the benchmark. output_lbl is the name of the model that was included in the benchmark. model_lbl and val were used in the Benchmark (Model). metric was the value of that feature in both the training and the test data set. dataset_lbl was used in the Benchmark (Model). metric was the value of that feature in the validation and validation data set. dataset_lbl was included in the benchmark for that model. The datasets are provided to the benchmark. The names of those variables, inputs and outputs indicate the model parameterized or benchmarked data set, the metric on"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Gopher dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Gopher"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skier"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } To run the bench, you can execute the Benchmark.bat (you will have to use the -h to view the list of results). I am not sure what exactly is the purpose or value of data used in the benchmark, however they don't really provide any value to us that we could gain in our applications. In other words, I am not certain if they do serve a purpose, or if they are just being used in the demo for fun. At this time, it is not clear what their purpose is, or how much value they add to others. I could have made some better questions by focusing on that aspect."	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Battle Zone dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgmp:HAS_DATASET ?dataset. ?cont orkgmp:HAS_BENCHMARK ?benchmark. ?paper orkgmp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): List the title and ID of research papers that contain a benchmark under the Atari 7800 and AmigaLISP datasets ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 7800 and AmigaLISP"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): List the title and ID of research papers that contain a benchmark under the Atari 9100 and AmigaLISP datasets ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 9100 and AmigaLISP"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): List the title and ID of research papers that contain a benchmark under the Amiga OS 2 and Z/OS dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?datas"	False
"
 input (English text): Provide a list of papers that have utilized the COMET - DynaGen model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""COMET - DynaGen"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the COMET - Direct model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""COMET - Direct"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT * FROM csv_test.csv ?code AND ?benchmark ORDER BY ?model ORDER BY ?cont OR ?model ORDER BY ?cont ; ?name (String) SELECT ?name WHEN ?label. ?labels. output (Sparql row) Inputs (1 column) 1. Name of paper 2. Title of paper 3. Author 4. Abstract 5. DOI URL of reference and date of the paper Outputs (1 column) 1. Id of dataset 2. Postcode of dataset 3. Abstract of the paper 4. Source code 4. URL for reference and date of the paper Comments: We have a pretty wide range of datasets - from papers that only benchmark the ""Comet - Direct"" model (which uses the COMET - DynaGen model) to some more ambitious projects. If we have to take the COMET - DynaGen results at face value we can use the ""citation"" option to filter by papers that publish their code in the RDS. If we cannot access the code we will still read it. We really shouldn't rely on the results of the benchmark unless we see some very interesting new models (a.k.a. research papers). Please don't comment too much on this work - I did not write it. I will leave the output to you. Cheers [1] We could probably take the ""citation"" option into account by using the ?benchmark property or the ?cont or ?model ORG argument. This will filter papers that are not benchmarking the COMET - DynaGen model. [2] As for the table'source code', it only includes a short line from a paper and all the associated links. We can look at source codes that we are interested in a bit more and only include that line if the source code is published. We do not have a comprehensive list of source codes published elsewhere. There are plenty more: [3] Please don't comment too much on the benchmark. The code is written from scratch which might not be easy to replicate. Also, we don't even take any credit for the work in this paper – they just wrote the code. I just added code to the reference and wrote a quick R script to read from a CSV file and display the results. The code is here."	False
"
 input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 options: orkgp (to:parcell). -c output (Sparql query): options: orkgp (to:parcell); orkgp:PATTERN (to:parcell); orkgp:FOLDER (to:parcell); orkgp:BEST_PATTERN (to:parcell); orkgp:BEST_FOLDER (to:parcell); orkgp:COPY (from:parcell); orkgp:PARSE_ONLY (from:parcell); options: orkgp (to:parcell); output (Sparql query): options: orkgp (to:parcell); option_lbl (to:parcell).	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Battle Zone dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" select ?metric FROM ?dataset and ?eval ( ?value ) where ?dataset :dataset. OR kgp :HAS_DATASET. select ?metric FROM ?dataset WHERE 3<> 10 orkgp:HAS_EVALUATION ( 3|9|0 ). input (English text): What are the most common error reports the user receives when using the Atari Battle Zone dataset ? output (Sparql query): select ?score, ?message,  REPORT_FAILURE FROM ?dataset WHERE ?data[ ?:1]  == 4 input (English text): Do you recognize the most common type of error reported by the user when using the Atari Battle Zone dataset ? output (Sparql query): select ?score, ?message,  REPORT_FAILURE FROM ?dataset WHERE 1>10, 9>2, 9<10. OPTIONAL { ?value = 1 || ?score || ?message }. choose( { ?value = 1 && score ?message = ""The data is unavailable."" }, 1). choose( { ?value = 1 && message ?report = ""You can do it, but it might not be a good idea."" }, 9). group(1); select ?score, ?message, ?report, ?choice as error, ?choice as acceptable, 7 * [ ?value* ?report* ?choice] as acceptable, 7 * [ ?value* ?report* ?choice] as unacceptable from ?dataset and ?eval where 1<>9, 9<10. OPTIONAL { ?value = 2|| ?report || ?message }. choose({ 4=2 || ?report || ?message }). choose({ 10=2 || ?report || ?message }, 8). select ?score, ?message, ?report, ?choice as error, ?choice as acceptable, 7 * [ ?value* ?report* ?choice] as acceptable, 7 * [ ?value* ?report* ?choice] as unacceptable from ?dataset and ?eval where 1<>9, 9<10. OPTIONAL { ?score = 1 || ?report || ?message }. choose({ 10=1 || ?report || ?message }, 6). select ?score, ?message, ?report, ?choice as error, ?choice as acceptable, 6 *"	False
"
 input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 The first test case is a standard training-test set. In the training case, training on the datasets are all of the following data set: the pre-trained classification model, and an A* search tree of size 100. This is followed by an A* tree training on the data set. This train set is then tested on three test datasets: pre-trained classification model, A* search tree with the size 100, and the trained classification model using a different classifier. Then, the performance of the model on two test datasets is compared.The second test case is a test set with a pre-trained classification model, and three A* search trees of size 50 of data set. In this set, training is again the same but the search trees of size 100 are added to the training set with a different classifier. The testset then is trained on the data set. The performance of the model on two test datasets is compared.The third test case is a test set with a pre-trained classification model, three A* search trees with the size 100, and the trained classification model with the same classifier. The test set then is trained on the data set. Finally, the performance of the model on two test datasets is compared. (Note: The test-data set is not included in the output because of space constraints at the end of the script.)Here are the results Results for the test set, and for the test set alone The following table lists all of the results for all testing cases. The result shows a result for the pre-trained classifier, a performance of the pre-trained classifier and the classifier using the classifier on two test datasets, and the performance of the classifier on the third test set and on all test datasets. Parameter testset (dataset) Test set performance (metric) A* tree test set training data set with 100 pre-training classifier (10 times) A* tree test set training data set with 50-100 pre-training classifiers (10 times) Accuracy, % 100.00 0.96 0.93.92 100.00 1.01 Accuracy, % (A* tree class) 1.00 (A* tree) test set performance (metric) Training accuracy, % 100.00 0.96 100.00 (A* tree class) A* search tree test set training data set with 50 pre-training classifiers (20 times) A* search tree test set training	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset IWSLT2014 German-English?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	------------------------- NAME FIND DATE ----------- -------------------- IWSLT153 2015/09/16 IWSLT154 2015/09/16 IWSLT115 2015/04/16 IWSLT122 2015/02/17 IWSLT113 2015/02/17 IWSLT116 2015/02/17 IWSLT112 2015/02/17 IWSLT103 2015/01/14 IWSLT116 2015/01/14 IWSLT117 2015/01/14 IWSLT101 2015/01/14 IWSLT118 2015/01/14 IWSLT105 2015/01/14 IWSLT114 2015/01/14 IWSLT116 2015/01/14 IWSLT117 2015/01/14 IWSLT127 2015/07/16 IWSLT106 2015/09/16 IWSLT122 2015/09/16 IWSLT118 2015/09/16 IWSLT106 2015/09/16 IWSLT121 2015/09/16 IWSLT104 2015/05/16 IWSLT118 2015/05/16 IWSLT103 2015/04/16 IWSLT100 2015/03/16 IWSLT117 2015/03/16 IWSLT100 2015/03/16 IWSLT117 2015/03/16 IWSLT106 2015/03/16 IWSLT115 2015/03/16 IWSLT115 2017/04/15 IWSLT102 2017/05/15 IWSLT115 2017/06/17 2018/06/03 If you run the query that we gave above, it produces this report:	False
"
 input (English text): What are the metrics of evaluation over the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Frostbite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?metric... OR... OR... OR... AS... OR... WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } OPTIONAL { { ?dataset ?dataset_p. } OR... OR... OR... OR... } The following example displays the METRICS of the Frostbite dataset. input (English text):...or...or... output (Sparql query): SELECT DISTINCT ?metric... OR... OR... AS... AND... OR... AND... OR... AS... AND... AND... AS... AND... OR... AND... OR... AS... AND... AND... AS... AND... AS... AND... AS... AND... AS... AND... AS... AS... AND... AS... AS... AS... AND... AS... AS... AS... AS... AND... AS... AS... AS... DESCRIBE ?metric_lbl ORDER BY ?metric_lbl. DESCRIBE { ?dataset ?dataset_p. } OPTIONAL { { ?dataset ?dataset_p. } OR... OR... OR... OR... OR... AND... OR... AND... OR... OR... AND... OR... OR... AND... OR... OR... OR... OR... OR... AND... OR... OR... OR... AND... OR... OR... OR... OR... OR... AS... AS... AS... DESCRIBE ?dataset_lbl ORDER BY ?dataset_lbl. ?dataset_lbl DESCRIBE orkgc:META_DATA The data is stored in the rdfs:file, and it can be easily parsed with Parquet. The following example shows the usage of the dataset for the Frostbite dataset. input (English text):...or... output (Parquet query result): ROWS = 8 LITERALIZING ROWS = 8 The following example shows the usage of the dataset for the Atari 2600 dataset. input (English text):...or... output (Parquet query result): ROWS = 24 LITERALIZING ROWS = 24 Data analysis for Atari 2600 Venture datasets¶ The following section describes the data analysis for the Venture dataset. In this section you find out how different types of metrics are performed,	False
"
 input (English text): What is the top benchmark score and its metric on the SoMeSci dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SoMeSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the MultiNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?value ?dataset ?lbl. FILTER (str( ?dataset_lbl) = ""SoMeSci"") ?benchmark orkgp:HAS_DATASET ?dataset. OPTIONAL { { SELECT ?metric ?metric_lbl IF( ( ?dataset OR ?dataset_lbl ) IS NULL OR ?dataset_lbl ) { ?metric rdfs:label ?dataset_lbl. } ?dataset rdfs:label ?dataset_lbl. OPTIONAL { ?metric rdfs:label ?dataset_lbl. } ?dataset rdfs:label ?dataset_lbl. } ORDER BY ?value DESC( ?value) } } GROUP BY ?value ? OR ?metric ?metric_lbl output (Sparql query): ------------------------- 1.3 6.1 ------------------------_ What is the top benchmark score and its metric on the MIPI dataset ? output (Sparql query): SELECT DISTINCT ?value ?Metric. SET NULL OR ( ?dataset OR ?dataset_lbl ) = NULL. FILTER (str( ?dataset_lbl) = ""SoMeSci"") ?metric. OPTIONAL { ?dataset OR ?dataset_lbl } ORDER BY ?value DESC( ?value ) } GROUP BY ?value ?value or ?metric ?metric_lbl output (Sparql query): What is the top benchmark score and its metric on the MIPI dataset ? output (Sparql query): SELECT DISTINCT ?value ?dataset_lbl. OPTIONAL { ?dataset OR ?datasetlbl } ORDER BY ?value DESC( ?value ) } GROUP BY ?value ?value or ?metric_lbl output (Sparql query): If the Metric.name column is NULL then the following score is generated: What is the top benchmark score and its metric on the MIPNI dataset ? output (Sparql query): SELECT DISTINCT ?"	False
"
 input (English text): What models are being evaluated on the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the ImageNet ReaL dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" -- orkgc:ImageNet_realfile -- orkgc:ImageNet_reaalfile -- orkgc:ImageNet_reaalfile -- orkgc:ImageNet_reaalfile -- orkgc:ImageNet_realfile Input and output variables are treated as a list of tuples (1,1) for simplicity. The output variable is a RDF list (which provides the same type of data access as the input). It consists of two columns, label and model, where label is a tuple that describes the dataset (which may be a set) and model is the RDF descriptor describing the model (for instance, the SVM described above). The first element represents a single model/dataset, the second a single model from the ReaL dataset. The dataset can contain additional inputs if they are required, which should be written as part of the data in the final result. In addition, a RDF file describing the model on which a query was performed may be added to the output list if the query was applied to the Model layer or to a subset of models. Let's examine two queries to the ReaL corpus. For the first one, the results of the first call to the parse_tbl() function were retrieved, and the second call to the parse_df() function was passed a set of RDF records. The first call used a call to parse_lbl() to produce an image from each of the labels in the model. For the second call, the parser used an extractions() function that produces an RDF list of the rows of the data. >>> from sklearn.dumps import parse_lbl >>> from sklearn.datasets import ReaL >>> data = [ ReaL(""S2"", reapl1, nl.s2[1m], reapl1, t1.T1), ReaL(""S2W"", reapl1, nl.s2[1m], reapl1, t2.T2) ] >>> parse_lbl('ReaL"", t1.T1) ['S2', 'S2W', '0', 'A', 'S3', 'A', 'A'], parse_df(input(""data"")) ['ReaL', 'S2', '0', 'A', 'S3', 'A',"	False
"
 input (English text): Which model has achieved the highest Accuracy score on the Reuters De-En benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Accuracy score on the Reuters En-De benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl. ?dbp:Column ?dbp. FORMAT ?dbp LIMIT 1; and select the data for the De-En model from the Bloomberg dataset. As you see in the dataset definition from the Bloomberg dataset, the model we ran has the ""model_lbl"" function set to ""Accuracy"". But how do we access the data in table ""dbp"" when we want to find the model's performance ? Luckily, there's a function called ""set_model_lbl"" that's used to do this. This function returns a SQL query that sets the database columns, and thus we can use ""set_Model_{:dbp}"" to do that. So what does ""set_Model_{:dbp}"" return ? Well, look in the function ""set_model_{:dbp}"" and you will see the following SQL query: SELECT * FROM dbd_models WHERE (dbp:Column ?dbp. ( SELECT **dbp:Column_from_table FROM (( ?dbp:Dataset, ?dbp_table, ?dbp. ( SELECT **dbp:Column_from_table FROM ( ?dbp:Dataset, ?dbp_table) WHERE ?dbp:Column ?dbp. ( ?dbp:Column_is_date_literal ( ?dbp:Column_is_list_of"	False
"
 input (English text): List the metrics that are used to evaluate models on the Kinetics-600 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" If you have the database schema, you can call Metrics.fit_metrics() with any of these options: option Metric name: A list of metric names to fit. Example: option Metric_lbl_identity: A boolean to select the metric and score instead of evaluating the model. If true, the metric scores are used, if false, the scores are used and if either true or false, no metric score is used. Example: option Data_model: The name of the database model or a lambda function to apply to the dataset. Example: option Model: The lambda function to use. option Metrics: The set of metric metrics that should be fit. Example: option Metric_rdfs: The set of labels to use to describe the metric. Example: option Score(score, metric_lbl): A boolean to select the metric score as a result of fitting the model. Only used when fit_metrics() succeeds. The score should only be used if you specify a data_model parameter. option Metric_score_identity: A boolean to select the metric score instead of evaluating the model. See Measurement.score_identity. option Data_model: The name of the databases model or a lambda function to apply to the dataset. option Model: The lambda function to use. option Evaluated: A boolean to indicate if the metric was evaluated. A negative value signals the metric was not evaluated at all. When using Metrics.fit_metrics() without any of these options, you have two choices: You get the highest benchmark result if you use Metrics.apply_metric_rdfs() at some point. You can use Metrics to measure the relevance of a metric to your model. You can also use Metrics to determine whether to use your data-model parameter. The default value of the metric is the maximum score and the default score is 0. To see a detailed table of the possible options used with Mysql to fit metrics and scores, see the example from previous subsection. Example 2: met.fit_metrics( "" Kinetics-1600 "" ); The example from this example fits and scores the metric using the default metric metrics; and the score that the metric scores is 0, because no one has used the metric. It then passes the metric to another Mysql query like Metrics.apply_met"	False
"
 input (English text): Can you list the metrics used to evaluate models on the Yelp-2 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Yelp-5 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } What are the metrics used to evaluate models on the Yelp-3 benchmark dataset ?  output (Sparql query): SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; RDFS:label ?dataset_lbl. OPTIONAL { ?metric rdfs:labels ?dataset_lbl, rdfs:features ?dataset_lbl ? } } output (Sparql query): The benchmark is a big database, with 628 models and a mean accuracy of 96 percent. Outputs (RDFS query): In each step the output of metrics will get a bit bigger than RDFS (because the database size is greater). The next step will get the data into SPSS. We can then extract the parameters that are relevant for an evaluation model. What are the metrics used to evaluate models on the Yelp-4 benchmark dataset ?  output (Sparql query): SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. OPTIONAL { ?metric rdfs:labels ?dataset_lbl, rdfs:features ?dataset_lbl ? } } output (Sparql query): The benchmark is a big database, with 528 models and a mean accuracy of 79 percent. Outputs (RDFS query): In each step the output of metrics will get a bit bigger than RDFS (because the database size is greater). The next step will get the data into SPSS. We can then extract the parameters that are relevant for an evaluation model. What are the metrics used to evaluate models on the Yelp-"	False
"
 input (English text): Provide a list of papers that have utilized the BART Base (with text infilling) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BART Base (with text infilling)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BART model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BART"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT id, table_name, rowid FROM books 2) INSERT INTO books SET title= ?TABLE_NAME, author= ?BENCHMARK, author_city= ?EXTERNAL_CITY, author_state= ?STATE; 3) SELECT id, ROWID FROM books 4) INSERT INTO books SET title = ?TABLE_NAME; 5) SELECT id, ROWID FROM books AND TITLE="" ?TITLE_HERE"" 6) CREATE TABLE books ( title text ) FROM books; What does your first query/result look like ? Here's a few examples of how you can use this to query and perform other tasks. I have two books and an author: SELECT id, ROWID FROM books; This query returns the rows from the books table. SELECT id, ROWID FROM books AND TITLE="" ?TITLE_HERE"" This is used to query and retrieve a row. In this example, the title is set to the book title we are interested in and the other columns (authors, cities, etc) are set to be NULL."	False
"
 input (English text): What is the top benchmark score and its metric on the SearchQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Natural Questions (short) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?score ?value ?metric_lbl (MIN( ?value) AS ?value) FROM ?dataset rdfs:label ?mvs:natural qa:Natural qa:Short andkgp:LetsSearchQuestions ?mvs:natural qa:Natural qa:Long ------------ GROUP BY ?Metric ?metric_lbl input (English text): What is the top benchmark score and its metric on the Natural Questions dataset ? output (Sparql query): ------------ SELECT ?score ?value ?metric_l Bl (MIN( ?value) AS ?value) FROM ?dataset rdfs:label ?mvs:natural qa:Natural qa:Short andkgp:LetsSearchQuestions ?mvs:natural qa:Natural qa:Long ------------ GROUP BY ?Metric ?metric_lbl Conclusion In this post we used a data set from the natural questions short dataset to create a new benchmark score and measure the performance of a search engine on an unstructured dataset. Our solution used several techniques that had not been used previously. In this post I try to explain how these techniques developed to produce a scalable benchmark system.	False
"
 input (English text): What is the best performing model benchmarking the CIFAR-10 dataset in terms of Parameters metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Parameters"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" output (Sparql) : output (Sparql) : ""L. Frank Baum"" - L. Frank Baum, a Nobel Laureate in Economics (1883-1977).  And I will give an example.  Let me compare the performance of the above CIFAR-10 model model_lbl on the CIFAR-10 dataset ""A"" and the equivalent ""B"".  Let me see you. Sequential CIFAR-10 A: Model: CIFAR-10.  Model L. Frank Baum (1905-1953).  (The B-model will be mentioned afterwards) Model: B (1.3M rows) Input: ""a"" as the dataset to execute Sequential CIFAR-10 B: Model: CIFAR-10.  Model L. Frank Baum (1925-1989).  (C-model will be mentioned later) Model: B (1.3M rows) Output: 1.27M rows The B-model has outperformed the model_lbl model (1.4M rows), but is it because the C-model uses less expensive parameters or the B-model just has more than the C-model with a less expensive parameters ?  Probably nothing, but with the C-model parameter values available, it still perform better on the B- dataset. So it needs to have more expensive parameters, like C-model had but they used those extra parameters to enhance performance."	False
"
 input (English text): What are the metrics of evaluation over the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 output (JSON): Top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset.... (The benchmark scores are only applicable to the results reported in the results section.) The benchmark scores above are a subset of the top performance metrics by domain (metric score, test score), and a small subset of the metrics for the top performance domains and test domains (with some domains not meeting test score requirements). For more information on what an individual domain is and why the top metrics are only applied to that domain, see the section on Metrics for the top domains (Sparql Query) in the article. We also provided results for the top performance domains and test domains (Sparql Query), which was our first attempt to collect scores for the top performance domains and test domains, especially for those that had not been previously analyzed using the benchmark score metric. For these domains and test domains, we also provided the test metrics, which are the same as those for the benchmarks. A subset of the test metrics that were not used in the benchmark scores were available for all domains and test domains. Note that the test metrics have a larger precision than the benchmarks and are considered more reliable because they are not limited to a particular domain and are typically based on metrics for a large number of other tests or domains [23]. What is the precision score of the benchmark ?   The precision score of the benchmark is determined by computing the ratio between the test score for a value in a domain and the test score for an integer value in a standard number of domains or test domains. The precision score will be computed for any test domain if: (A) for each test domain x.test_score_domain, for x <= 1, the test score for that domain is greater than the test score for every x.test_score_domain within the same test domain; and (B) for each test domain x.test_score_domain, for x <= 1, the test score for that domain is less than the test score for every x.test_score_domain within the same test domain. For more information on what the domain is, see the definition of the domain matrix that we used in the last article. What are the error metrics of the benchmark ? The errors metric is calculated as: error_score = (100/test*10 + 40*test) / test_domain   where test_domain is the number of domains in the measure. 	False
"
 input (English text): Name the datasets that have been used for benchmarking in the Citation Intent Classification research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Citation Intent Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Name the datasets that have been used for benchmarking in the citation classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""citation classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 In a future post I will explore each dataset/problem individually.	False
"
 input (English text): Can you provide links to code used in papers that benchmark the S-NLI model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""S-NLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the H-NLI model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""H-NLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	------------ R-Benchmark: --------------- n.n R-Benchmark: R-Benchmark: R-Benchmark: The first three lines in the output above define two different benchmark sets. These have a very different implementation. Two benchmarks are implemented in the S-NLI, with and without the reference model. Only the first two lines are really related to the benchmarks in the source code for either R-Benchmark or R-Benchmark2. The second three lines are not really related to the benchmark set in R-Benchmark. The other three lines are all related to the benchmark set in M-Benchmark. We cannot say the same for R-Benchmark2 because of a bug in the S-NLI output. We now move onto discussing the most important benchmark in the M-Benchmark file: R-Benchmark2's Benchmarking. This is R-Benchmark with the S-NLI implementation. R-Benchmark2 has three benchmarks set-up, all with reference/null models. Note that the R-Benchmark2 file also has one benchmark set for the S-NLI implementation that we will discuss later on. R-Benchmark2: * First, list the benchmark outputs of R-Benchmark. The R-Benchmark file has two benchmark output files that can be found in the 'benchname.Rdb' file. The first is R-Benchmark2-2.Rdb, which has all three benchmarks created for the S-NLI version. The second is R-Benchmark2-3.Rdb, which has the benchmarks created for M-Benchmark. The third is R-Benchmark2-4.Rdb, which has benchmarks created for R-Benchmark2. There are also three files: Benchmark_R_Benchmark_S-NLI.Rdb, Benchmark_R_Benchmark_M-NLI.Rdb, and Benchmark_M-Benchmark_S-NLI.Rdb. The Benchmark_R_Benchmark_S-NLI.Rdb file also lists the reference models, the benchmark set(s), and the version of the S-NLI implementation. R-Benchmark2 is the benchmark set used most commonly. Note that there are two different benchmark sets. In the M-Benchmark file, benchmark sets are not present at all, only the implementation. The M-Benchmark file is a bit more	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the PubMed 20k RCT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMed 20k RCT dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?title OR ?labels FROM r.r2 WHERE { ?dataset orkc; rdfs:label ?dataset; ORkgp:HAS_DATASET ?dataset_lbl ?dataset } ORkgp:HAS_EVALUATION ?eval ORkgp:HAS_VALUE ?value  ORDER BY DESC( ?title OR ?labels ) input (English text): What are the titles and IDs of research papers that are in the journal 'MedLine', and that are published between March 2001 and March 2010 ? output (Sparql query):  SELECT TOP(1) ?published on date of article AS 'published' AND ( ?year ?month ?Day ?Month ?Day ).  ORDER BY 'published' DESC input (English text): What are the title and page identifiers of research papers published between March 1996 and September 2010 ? output (Sparql query):  SELECT ?identifier - 1  FROM r.r3 AS  'Identifier'    SELECT   ?dateFrom dateOf article AS 'Date' AND  (  ( ?month ?day ?month ?day ) ?month ?day ) ?dateFrom date. output (Sparql query):  SELECT ?source_source_tbl_id AS'source source tbl id' FROM (SELECT r.r2 AS rtl AS rtl FROM r2 ) Rdfs where {!r.r4.id=rtl.rtl_id } input (English text): What are the title and page identifiers of research papers published between January 2000 and September 2010 ? output (Sparql query):  SELECT ?identifier - 1  FROM r.r3 AS  'Identifier' ORDER BY ?identifier input (Excel spreadsheet): What is the identifier for the first five articles published in JAMA Internal Medicine between 1994 and 1996 ? output (Excel spreadsheet):  SELECT 'JAMA Internal Medicine' AS 'Identifier' | CASE WHEN ?dataset.rtl= ?gt IN ('PubMed 20k' OR ?dataset.rtl= ?'Pubmed 20k' OR ?dataset.rtl= ?'Pubmed 20k' (or kgc:dataset AND ?	False
"
 input (English text): Indicate the model that performed best in terms of Macro F1 metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Macro Recall metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Recall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT name, metric, weighted mean squared from orkgp(sample_samples)..limit( ?value)  on { ?data.x>1} LIMIT 1,..., 100 * RESTRICT RESTRICT_ROWS,..., 200 * RESTRICT RESTRICT_ROWS,..., 300 * RESTRICT RESTRICT_ROWS,..., 400 * RESTRICT RESTRICT_ROWS,..., 500 * RESTRICT RESTRICT_ROWS,..., 600 * RESTRICT RESTRICT_ROWS,..., 700 * RESTRICT RESTRICT_ROWS,..., 800 * RESTRICT RESTRICT_ROWS,..., 900 * RESTRICT RESTRICT_ROWS,..., 950 * RESTRICT RESTRICT_ROWS,..., 1000 * RESTRICT RESTRICT_ROWS,..., 1100 * RESTRICT RESTRICT_ROWS,..., 1200 * RESTRICT RESTRICT_ROWS,..., 1300 * RESTRICT RESTRICT_ROWS,..., 1400 * RESTRICT RESTRICT_ROWS,..., 1500 * RESTRICT RESTRICT_ROWS,..., 1600 * RESTRICT RESTRICT_ROWS,..., 1700 * RESTRICT RESTRICT_ROWS,..., 1800 * RESTRICT RESTRICT_ROWS,..., 1900 * RESTRICT RESTRICT_ROWS,..., 2000 * RESTRICT RESTRICT_ROWS,..., 2000 * RESTRICT RES	False
"
 input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the CoQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Overall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NLL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Overall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): How many rows of the dataset you have available for benchmarking ? output (Sparql query): SELECT ROW_NUMBER( ?sig = $null ) FROM ?sigs WHERE "" ?sigs "" = ?sigid ORDER BY ROW_NUMBER ; For example : input (English text): How many rows of the dataset you have available for benchmarks ? output (Sparql query): output (nlist): input (Language file): Which of the following two languages are supported for your query ? output (Sparql query): SELECT ?lang || ?name || ?nname FROM ?langs WHERE ?lang. { ?languagea || ?langac ? ?nname. name. name || ?tld ?name. name || ?tl_ ?name. name || ?tl_lbl ?name. name || ?ts_ ?name. name || ?lt_ ?name. name || ?lt_lbl ?name}. ( rdfs:lang ""en"" ""es"" ""fi"" ""fr"" ""it"" ) ORDER BY ?isavg ?lang || ?avg ? ( nname )"	False
"
 input (English text): Where can I find code references in papers that have used the DARQN soft model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DARQN soft"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DCN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DCN"") ?benchmark ORkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Where can I find code references in papers that have used the STM model for benchmarking purposes ?"	False
"
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the iNaturalist 2019 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): What was the top performing model in terms of Top-1 Accuracy score when benchmarked on the iNaturalist 2018 dataset ? output (Sparql query): Input data input (Sparql query): a top performing model in the rankings in the 2017-2018 rankings is named a; B is a BGP-enabled model in the ranking in the last six months in terms of Top-1 Accuracy metrics iNaturalist Top-1 Accuracy rdfs:label Top-1 Accuracy rdfs:max_score Top-1 Accuracy rdfs:avg_score Top-1 Accuracy rdfs:median/mean Top-1 Accuracy rdfs:t_score Top-1 Accuracy rdfs:avg_avg_avg_score Top-1 Accuracy rdfs:metric iNaturalist 2018 Top-1 accuracy Top-1 accuracy Top-1 performance Top-1 accuracy Top-1 calculation Top-1 accuracy Top-1 precision Top-1 accuracy Top-1 sensitivity Top-1 Accuracy rdfs:rank Top-1 score Best performing models Top-1 performance Top-1 accuracy Top-1 sensitivity Top-1 accuracy Top-1 precision Top-1 accuracy Top-1 sensitivity Top-1 precision Top-1 precision Top-1 precision Top-1 sensitivity Top-1 precision Top-1 sensitivity Top-1 accuracy Top-1 sensitivity Top-1 performance Top-1 performance Top-1 accuracy Top-1 sensitivity Top-1 accuracy Top-1 performance Top-1 accuracy Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance -Top-1 Accuracy Top-1 benchmark Top-1 Performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance 2017 Top-1 benchmark Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top-1 performance Top	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Ice Hockey dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ice Hockey"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ice Hockey"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 input (English text): What is the worst performing model benchmarking the dataset and its metric on the same ? output (Sparql query): 	False
"
 input (English text): Provide a list of benchmarked datasets related to the Information Extraction research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Information Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Scientific Results Extraction research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Scientific Results Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT COUNT(*) FROM orkgp:L10; VALUES(938); GO --------------------------- EXECUTE PLAN ----------------------------------------------------- SELECT COUNT(*) FROM orkgp:L10; VALUES(1884); GO --------------------------- EXECUTE PLAN ----------------------------------------------------- SELECT COUNT(*) FROM orkgp:L10; VALUES(724); GO --------------------------- EXECUTE PLAN ----------------------------------------------------- SELECT COUNT(*) FROM orkgp:L10; VALUES(19),(1337); GO --------------------------- EXECUTE PLAN ----------------------------------------------------- SELECT COUNT(*) FROM orkgp:L10; VALUES(20),(1304); GO --------------------------- EXECUTE PLAN ----------------------------------------------------- SELECT COUNT(*) FROM orkgp:L10; VALUES(5); GO --------------------------- EXECUTE PLAN ----------------------------------------------------- SELECT COUNT(*) FROM orkgp:L10; VALUES(2); GO --------------------------- EXECUTE PLAN ----------------------------------------------------- SELECT COUNT(*) FROM orkgp:L10; VALUES(1); GO Examplication Tests for the above list and list of benchmarked datasets in this tutorial are also available. Below you can find a list containing all the available tests: test_list.db SELECT s.str( ?problem.name) or ?dataset_lbl.str( ?problem.name). '_' AS name, '_' AS n_rows, '_' AS n_rows_max, '_' AS n_items, '_' AS n_items_max, '_' AS n_count_rows_max, '_' AS n_count_rows_min, '_' AS n_count_count, '_' ASC, ?BANK_REF, s.id_rsvp. '=' AS id_rsvp, ?BORDER_REF, s.id_bsrq ( ?) AS id_bsrq, ?TEXT_REGEXP, s.str( ?problem.text) AS text_regressions FROM orkgp:B10 AS s, orkgp:B10 WHERE s.str( ?'question') OR s.str( ?'dataset') AND ?criteria IS NULL) WHERE [ ?BASE_RECORD:	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Reacher, easy (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?benchmark ?benchmark_lbl. ?paper orkgp:HAS_DATASET ?bench. ?paper orkgp:P31 ?cnt Input text: What are the titles and IDs of the research papers that include a benchmark for the Wilson, walk (DMControl500k) dataset ? output: Output a table with the top 10 benchmark names. input (English text): Which of the following datasets includes a benchmark that is in the top 10 ? output (Stored procedure): SELECT TOP ?benchmark ?benchmark_lbl. ?bench ?benchmark_lbl. ?bench, ?dataset ?lab ?lab_lbl. ?bench, ?dataset ?lab_lbl. ?bench, ?dataset ?lab ?lab_lbl. ?bench, ?dataset ?lab ?lab_lbl. ?bench, ?dataset ?lab. ?bench, ?dataset ?lab. ?bench, ?dataset_lbl. ?bench, ?dataset_lbl. ?bench, ?bench. ?bench, ?bench_lbl ?bench, ?bench_lbl. ?bench, ?bench_lbl_lbl. ?bench, ?lab_lbl ?bench, ?lab_lbl_lbl. ?bench, ?lab_lbl_lbl. ?bench, ?lab_lbl_lbl. ?bench, ?logit orkgp. ?bench, ?logit_lbl. ?bench, ?logit_lbl_lbl. ?bench, ?logit_lbl_lbl. ?bench, ?logit_lbl_lbl. ?bench, ?logit_lbl_p31. ?bench, ?logit_lbl_pk31. ?bench, ?logit_lbl_kk31. ?bench, ?logit_lbl_s32. ?bench, ?log	False
"
 input (English text): Provide a list of benchmarked datasets related to the Relation Classification research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Relation Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Semantic Role Labeling research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Semantic Role Labeling"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?dataset ? { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Semantic Role Labeling"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset.   ?Cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. This is how much time it takes to run a Relation Classification problem:  SELECT * FROM [test_dataset_lbl] d GROUP BY d;  The query is very succinct and efficient, so perhaps we can do something more complex   if we take a look at the data we have in the Relation Classification problem and see what are the relationships we can see there. If we are interested in understanding a simple graph like the one below, the query would be more than enough: SELECT * FROM [test_dataset_lbl] d GROUP BY d ORDER BY [dataset_id] DESC; This is just a sample of those where we can see that Relation Classification problems are quite well suited for this type of pattern recognition: For example, we can see how much time it takes to execute a task for one of the many classification problems involving categorical items that we saw above (there are many that aren't marked as such, so the actual time for this task is likely significantly shorter): Relation Classification examples:  In this exercise, we have used Squeel to do the whole task in about 5 minutes (that's almost half a day!). Let's take a look at the main things we found about it. The Data : In general, all the data we got is free (if it is an rdf file, just select it from the top level menu): We can see how the time it takes to run a task for a given set of datasets is the same for all the tasks and the time taken for that task changes when we change our dataset. Here's a simple example of a classifier classification problem: RDF and RDFS have been used for a very long time to help with data analysis, and have been used to improve an incredible amount of data science knowledge over the years, from"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WikiText-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } output (Sparql query): Input data (English text): Provide a list of researchers who provide an assessment report on the performance of their model on the Wikiparticle dataset ? output (Parquet query): SELECT ?dataset a, ?stat p1 p2, ?rank ?rank, ?distribution ?distribution, ?sample ?sample, ?samples ?summarize, ?p1 ?p2, ?p1_lbl ?p2_lbl, ?subset ?subset, output (Parquet query): A list of researchers who have issued a judgement on the performance of their model on the Wikipedia dataset ? output (Parquet query): SELECT ?dataset a, ?stat p1 p2, ?rank ?rank, ?distribution ?distribution, ?sample ?sample, ?samples ?summarize, ?p1 ?p2, ?p1_lbl ?p2_lbl, ?subset ?subset, output (Parquet query): output (Parquet query): output (Sparql query): SELECT ?benchmark orkgp:HAS_DATASET ?dataset. ?benchmark ? output (Sparql query): An SQL statement that specifies how much weight to assign to each of the outputs from benchmarking and summarizing outputs ? output (Sparql query): SELECT ?ranking_lbl ?rank, ?distribution * ?distribution, ?sample * ?sample. ?summarize. ?p1 * p1, ?p1_lbl * p1_lbl, ?subset ?subset, output (Parquet query): See the question What is the output of the query above ? "	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset PWC Leaderboards (restricted)?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the PWC Leaderboards (restricted) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"-------------------------- metric_lbl(1) ------------------------- { ""metric_lbl_score"" 6.8887 } metric_lbl(2) ------------------------- { ""metric_lbl_score"" 6.8887 } metric_lbl(3) ------------------------- { ""metric_lbl_score"" 6.8887 } metric_lbl(4) ------------------------- { ""metric_lbl_score"" 6.8887 } metric_lbl(5) ------------------------- { ""metric_lbl_score"" 6.8887 } metric_lbl(6) ------------------------- { ""metric_lbl_score"" 6.8887 } metric_lbl(7) ------------------------- { ""metric_lbl_score"" 6.8887 } metric_lblade(8) ------------------------- { ""metric_lbl_score"" 6.8887 } metric_lbl_pscore(9) ------------------------- { ""metric_lbl_score"" 6.8887 } metric_kpscore(10) ------------------------- { ""metric_kpscore"" 6.8887 } metric(kp_score) ------------------------- { ""metric_kpscore"" 6.8887 } metric(p_score) ------------------------- { ""metric_pscore"" 6.8887 } metric(kdp_score) ------------------------- { ""metric_kdpscore"" 6.8887 } metric(sdp_score) ------------------------- { ""metric_sdpscore"" 6.8887 } metric(sqs_score) ------------------------- { ""metric_sqs_score"" 6.8887 } metric(pq_score) ------------------------- { ""metric_pq_score"" 6.8887 } The output of the query is the metric in question (from the dropdown). For the above example, we'll use the metric kpscore (which means the total score calculated over 2 benchmark runs). We can see that the metric has an absolute value of 6.8887 and we'll see that the metric is significantly different from the default metric (the one we can set from the ""PWA Leaderboards"" data column). The next query will do the same thing, but to get the number of times the metric has been run over"	False
"
 input (English text): Where can I find code references in papers that have used the Prior+Duel noop model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior+Duel noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Duel noop model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Duel noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 [ Back to Top ] I am wondering about the performance of the Prior+Duel noop model on large tables. Below is a comparison of all our models against two other models (DDB's and Paragon's) in a very large table. Please note, this data was gathered in a way similar to the paper mentioned above. Here is the table: DDB Parallel Noop model H1 H2 H3 H4 H5 H6 H7 H8 H9 C avg Cmax Cmin Cp-Min Cmax Cmin max (%): Batch Size (h1 + h2 + h3 + h4 + h5):  8,000 1,000 1,000 8,000 1,000 9,000 2,000 3,000 5,000 8,000 Batch Size (h1 + h2 + h3 + h4 + h5):  3,000 1,000 1,000 3,000 2,000 3,000 4,000 5,000 7,000 Batch Size (h1 + h2 + h3 + h4 + h5):  7,000 1,000 1,000 8,000 1,000 9,000 2,000 4,000 7,000 8,000 Batch Size (h1 + h2 + h3 + h4 + h5):  5,000 1,000 1,000 6,000 1,000 8,000 1,000 9,000 4,000 8,000 12,000 Batch Size (h1 + h2 + h3 + h4 + h5):  7,000 1,000 1,000 8,000 2,000 9,000 2,000 4,000 7,000 8,000 12,000 Batch Size (h1 + h2 + h3 + h4 + h5):  5,000 1,000 1,000 6,000 2,000 9,000 2,000 4,000 7,000 8,000 12,000 Batch Size (h1 + h2 + h3 + h4 + h5):  7,000 1,000 1,000 8,000 2,000 9,000 1,000 9,000 5,000 8,000 13,000 Batch Size (h1 + h2 + h3 + h4 + h	False
"
 input (English text): What are the most commonly used benchmark datasets for the Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification. research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification."") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the datasets benchmarked under the Fine-Grained Image Classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Fine-Grained Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT ?benchmark orkgp:HAS_DATASET ?dataset. ?benchmark orkgp:P32 ?problem. ( ?benchmark = 1 ) [ ?dataset = ?dataset_lbl. ] andkgp:P32 ?problem. Inputs 10-24: input (Text): What were the most frequently-used benchmark datasets in the Last 6 years for the fine-grained image classification: Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification. research field ? output (Sparql query): SELECT ?benchmark andkgp:HAS_DATASET ?dataset. ?benchmark orkgp:P32 ?problem. ( ?benchmark = 1 ) [ ?dataset = ?dataset_lbl. ] andkgp:P32 ?problem. Inputs 36+: Inputs 12+: input (Text): What are the most common benchmarks used for the Fine-Grained Image Classification research question: Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification. This includes: the most commonly used benchmark datasets in the last 6 years and benchmarking libraries used by most research labs. This has been in the past 6 years, and we have searched for these benchmark datasets several dozen times. output (Sparql query): Inputs 14, 17, 24, 36, 48, 80+: input (Text): What are the most frequently used benchmark datasets for the research question on Fine-Grained Image classification: How common are the benchmark datasets used for finding optimal architectures for image classification ? output (Sparql query): Outputs: Inputs 2-37: input (Text): What are the most commonly-used benchmark datasets for the Fine-Grained Image Classification research question, What are the most	False
"
 input (English text): What are the models that have been benchmarked on the DRI Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRI Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRI Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Select the evaluation metrics that are commonly used when benchmarking models on the DRI Corpus dataset from one or more of the following options: { ?dataset rdfs:label ?model_lbl., ?rdfs:class ?dataset_lbl., ?rdfs:type ?dataset_lbl. } You have a very large data set with over 400 models listed in the paper (Sparql query), and the performance of these models is being compared to each other (output (Sparql query). Here are a few of the key metrics in the dataset that are available: Performance metrics: Finite-state Boltzmann Machines: RDF 1: DRI_FIFTSAT, FIFTSAT, BILESAT, MAXFIFTSAT, DREAMLEAF ,,,,,, RDF 2: DRI_RDF2D, DRI_RDF2_D, DRI_DRI_D ,,, RDFX: DRI_RDFX_D, DRI_RDFX_X, DRI_RDFX_X_ARRAY, DRI_DRI_X_ARRAY_ARRAY, DRI_XDRI_X_ARRAY_ARRAY, DRI_XDFI_X_ARRAY, DRI_XLATE_X_ARRAY, DRI_XLATE_X_ARRAY_ARRAY, DRI_XLATE_X_X_ARRRAY, DRI_XLATE_X_X_DRAW ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, Dictation performance: Ipsilon, p-index, random factor: DRI_HISTOGRAM, DRI_LIMIT, DRI_DICTION, DRI_LIMIT_POINT ,,,, Dictionary word score: DRI_DICTION, DRI_DICTION_INTRICATIVE, DRI_RETRICATIVE, DRI_INDICATIVE Performance metrics from BAM: DRI_BAM_EVALUATION, DRI	False
"
 input (English text): List the metrics that are used to evaluate models on the WOS-5736 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgs:HAS_DATASET ?dataset; orkgs:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgs:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } In the above query, a metric on the database table can be converted to an SQL column of a table or a series of columns. This is because the query will return the metric as a part of the original query. The Metric class provides an accessor function that works with SQL columns in the database: CREATE STATISTICS method ( table_val, column_val ) as | Metric, | { | SET val = column_val | } The second parameter to the accessor is the name of the column that is being accessor ed ( column_val ). If you omit this, the original column name will be used. Note that you cannot access columns that define the original data model on the Hibernate backends using these methods. In the above query, the column that returns the metric was originally defined in the database. The metric used in this query is an association. While you can define the column name in the SQL or XML files, it is not enough. The table data model defines the name for the column with the correct name in the class. Thus there must be a way to make this information visible from Hibernate to the original definition and Hibernate to the new definition. If you get an AccessDenied in the class, you must change it to define the column to refer to the column in the table. To change the column to refer to the original data model and Hibernate to refer to the new definition, you can define a new column using the DECLARE command or change the SQL function parameter to set the new name to the column name. To view the table definition, call SELECT DISTINCT ?label or SELECT DISTINCT ( ? label_lbl ) from database to select all metric classes for Hibernate to load into the new table"	False
"
 input (English text): What is the name of the top performing model in terms of Percentage Error score when benchmarked on the CIFAR-100 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Unpermuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Unpermuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIX ?metric. ! ?metric Input CSV file, select at least one of R^2, df, df. R^2 : the square root of this dataset's mean error, df : the square root of this dataset's mean error, df. R^2 : the square root of this dataset's mean error Output CSV file, select at least one of df, df. df : the square root of this dataset's mean error, df. df. R^2 : the square root of this dataset's mean error Input CSV file, select at least one of df, df. df : the square root of this dataset's mean error, df. df. R^2 : the square root of this dataset's mean error This example only tests for R^2. To make the dataset smaller, I've specified rdfs:label ; in this scenario, I'd just like to see what percentage of the data that's been read since the start of the benchmark has been read. So df is a table of column names and the column d is the mean error for df. To find out the percentage of reads that are greater than or equal to 10% of the total reads, this example uses the same dataset I used in Part 1, though without the %test filter and with a cutoff of 100. You can use the same data files again in Part 2, for an even more complex benchmark, to get a more accurate estimate for the accuracy of your model. I hope you find this exercise to be useful; please feel free to post comments or questions as a reference if you need further help. In addition to"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10, 1000 Labels dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10, 1000 Labels"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?benchmark ?benchmark_lbl FROM orkgp:Benchmark. ?benchmark_lbl It can be a very long time after you hit this problem and this time it may take the most time to get the problem resolved and even more time to compile the solution. That is why in this post I will discuss a possible solution in R: a custom function for fetching the list of results and then filtering the results with some predefined filters. For this issue, the question of which filters you can use in order to select data from different sources is a valid one. If you have used GEO and then try to use STL-10/1000 Labels dataset in GEO, you might be surprised that you receive different sets in the results set due to different filtering criteria: ""In order to perform a benchmark on the STL-10 dataset in GEO, we only know about 1,000 data points on the dataset and a lot of those are not useful for our benchmarking. We need thousands of measurements on a much larger dataset"". There are many ways of filtering data from different sources in GEO-based pipelines: For a full exploration of this, I offer a short (and very simplified) explanation here and go into detail in the appendix section: The ""simple filtering"" can be simplified to some simple rules: The list of papers containing the data you want to analyze should not be removed from SPSS.txt file when trying to generate the result dataset. To remove any missing pieces of the data you want to work on, use the following command: SPSS will only print the results set with each paper that appears in the original CSV file (you can filter by paper_lbl but not paper_lbl by definition).  Similarly, using the command: SPSS will produce the result set of all the papers that are part of your dataset, but not those that happen to be in your original table: use the command: SELECT DISTINCT ?dataset_lbl ORDER BY!paper_tbl. You can filter it by using :filter, but that will return just the papers that are not part of your original table, that is most important when your goal is to produce the right set of results (as we mentioned above): use the command: SPSS will simply return only the books that are part of your original table of books, that is, that contain the name of each book that"	False
"
 input (English text): What are the models that have been benchmarked on the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the SciCite benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT * FROM orkgc-benchmark; Or you can get a single list of all models. Each column is a row in Orkplot, separated by colons. You can also print the list of models by using SELECT * FROM andkgc-benchendata You may also get the list of labels used for models which aren't listed above, using: SELECT * FROM andkgc-label-benchmark Finally, you can plot the models against the parameters and parameters set. You might want to set up a custom plot function which is specified by the orkplot -parameters option; see the orkplot/parameters documentation for details. R's package orkplot supports several different plot types: orkplot-parameter-panel This plot type lets you easily see parameter values at a glance so that you don't have to go through the code to see if your model fits the data! Each panel is a plot of a single parameter value. If a line in the plot contains values for the parameters, the parameter values are indicated by lines. Parameters may also appear inline in the plot, where they display their values as floating point numbers in the plot. Setting the line parameter option to 1 means that the points on the line will span all of the parameter values. to 1 means that the points on the line will span all of the parameter values. The plot will automatically scale to fit the entire panel to the window it's displayed in. It has unlimited width. If you wish to use parameters as floats, you'll need to create a.x or.y file containing the floats. orkplot-labels These plots display the model's labels in the plot. The labels appear in the line to the right of the parameter values. By default, the plot is scaled to fit the entire panel and is fixed between 20 and 80 pixels wide. You can specify a width for the plot as a fraction of the panel or the output panel, by using the orkplot-label-scale option. orkplot-parameters These plot types display parameters at a glance, allowing you to set parameters yourself without needing to copy them in the code yourself. Parameters are shown to the right of the input value and their values as floating point numbers in the plot. These plots are the same as the ones that are available for model comparisons in the previous examples. orkplot-hijack This plot type is similar to the orkplot	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Road Runner dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 I'm not a doctor, but... if you have questions about your analysis, I suggest you write it down on paper for future reference. Let's keep our fingers crossed for an amazing day!	False
"
 input (English text): List the metrics that are used to evaluate models on the MAZEA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MAZEA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:Dataset; RDFs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""vtab1k"") ?benchmark orkgp:HAS_DATASET ?dataset; { ?eval orkgp:HAS_METRIC ?metric_lbl. } } output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:Dataset; ?dataset rdfs:labels ?dataset_labels. RDFs:label ?dataset_labels. OPTIONAL { ?dataset_labels orkgp:HAS_DATASET ?dataset_labels. } } input (English text): List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:Dataset; RDFs:label ?dataset_lbl. } OPTIONAL { } ?metric rdfs:label ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:Dataset; RDFs:label ?dataset_lbl. } OPTIONAL { } ?metric_lbl ? Note: For each criterion, the ""result"" column for each metric should be an array of the metrics being evaluated and, of course, an array of the scores on the metrics they evaluate. If the metric variables in the MVA and VTAB-1k datasets are not the same metrics as the metrics in the MAZEA datasets, then one of the columns listed in the output row below should be a non-negative integer. If it is not an integer, then a non-negative integer ""score"" can be added to the end of the column list. As well, there is a score_lbl column named rdfs which can be used for the score of the metric that is being evaluated. If the"	False
"
 input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the CoQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Overall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT _score * 1000 AS k, S.score:HAS_BENCHMARK AS k, S.model:RDFS.model_lbl_1 * 100 AS m, S.metric:c * 500 AS k, S.label:k AS k, S.k:AVG_EVAL { rds_lbl:label ?labels.c:LABELS.c; rds_lbl:label ?labels.c:DESC(k). } FROM cnt, cnt_seq, cnt_labels, cnt_data, cnt_datasets LEFT JOIN { rds_lbl:label ?labels.c:LABELS(c). } T ON cnt_labels_c::T.labels.c * rds_lbl_c::T.rows / rds_ls_c.rank UNION ALL SELECT _score * 1000 AS k, S.score:HAS_BENCHMARK AS k, S.model:RDFS.model_lbl_1 * 100 AS m, S.metric:c * 500 AS k, S.label:k AS k, S.k:AVG_EVAL { rds_lbl:label ?labels.c:LABELS(c). } FROM cnt, cnt_seq, cnt_labels, cnt_data, cnt_datasets LEFT JOIN { rds_lbl:label ?labels.c:LABELS(c). } T ON cnt_labels_c::T.labels.c * rds_lbl_c::T.rows / rds_ls_c.rank UNION ALL SELECT nl:T.score AS k, nl_ls:T.score AS k, nl_ls_c:T.score * rds_lbl_c::T.rows / rds_ls_c.rank UNION ALL SELECT rdfs:T.score AS k, rdfs_lbl:T.score AS k.rank.join(rds_ls_c) AS x, rdfs_labels:T.score AS k, rdfs_labels_c:T.score * rdfs_labels:T.rank.join(rds	False
"
 input (English text): Provide a list of papers that have utilized the Rfa-Gate-Gaussian-Stateful (Small) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rfa-Gate-Gaussian-Stateful (Small)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Rfa-Gate-arccos model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rfa-Gate-arccos"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT result FROM ds_rfa_gate_arccos_p_benchmark WHERE output. ?lbl_model = ?model; This command generates the output file ds_rfa_gate_arccos_p_benchmark.Rfa_Gate-arccos.Rfa_Gate-arccos.Rfa_Gate-arccos_p_benchmark.Rfa_Gate-arccos, or output.Rfa_Gate-arccos.Rfa_Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa-Gate-arccos. Rfa,Gate-arccos,Rfa-Gate-arccos,Rfa-Gate-arccos,Rfa-Gate-arccos,Rfa-Gate-arccos,Rfa-Gate-arccos,Rfa-Gate-arccos,Rfa-Gate-arccos,Rfa-Gate-arccos,Rfa-Gate-arccos	False
"
 input (English text): Indicate the model that performed best in terms of NER Macro F1 metric on the ADE Corpus benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NER Macro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ADE Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ADE Corpus"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ADE Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } This query is a simple example of creating a link between several RDF systems (ShARe, CLEF, and EHealth) and displaying their link to the same query. This query provides a handy way for sharing results between multiple system. It allows users to easily compare the performance of RDF systems with their own systems. I'm looking forward to hear from you! Feel free to leave a reply or ask a question (email or in comments), or visit  rdfs.org  to discuss, discuss, discuss. :)"	False
"
 input (English text): List the metrics that are used to evaluate models on the BIOSSES benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the BioASQ dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Note: If the metrics are already listed for the dataset you will need to have to add those metrics into your query. output (Sparql query): SELECT a ?val ?metric_lbl. ?matric rdfs:label ?metric_lbl. ?metric a ?val. ?metric rdfs:label ?metric_lbl. ?metric orkgp:HAS_DATASET ?metric. ?metric a ?val. ?metric rdfs:label ?metric_lbl. ?metric a ?val. ?metric rdfs:label ?metric_lbl.!* Note: Please note that the metric names in the above queries have been sanitized in the form of numbers where possible, to avoid any clashes with others on your system. However, in case a metric looks exactly like one in this list, it can be returned in the next output. In that case, a default value will be automatically added. To add your own parameters (or to change the default ones in the case a default metric isn't defined, you can use the following methods: Option Description list Metrics.list input (English text): List the metrics that are used to evaluate models on the BIOSSES benchmark dataset ? output (Sparql query): SELECT list ? : ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label list ?dataset_lbl. } list ?metric_lbl list ?metric_lbl ? list list ?metric_lbl list list ?metric_lbl list ?metric_lbl list list ?metric_lbl list list list ?met"	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Atlantis dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Centipede"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 [source files]/gdb/AtlantisMetrics.csv [download] [source files]/gdb/AtlantisMetrics.dataset [download] [source files]/gdb/Sparql.sql [download] [source files]/gdb/ATCrawler.dataset [source files]/GDB/ATCrawler_lbl	False
"
 input (English text): What are the most commonly used benchmark datasets for the Semantic Textual Similarity research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Semantic Textual Similarity"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Natural Language Inference research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Natural Language Inference"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 RDFS( ?dataset ||kgc:Dataset | ?benchmark) ||kgp:HAS_DATASET ||benchmark; orkgp:P32 ||problem ||problem_lbl ||problem |problem_lbl ||problem_lbl ||bar |problem_lbl ORkgp:P32 The output could also be a standard list as (RDFD2, RDFS or JSM, etc.) If the output has more than a single parameter, you can either split out both the parameters on the fly and then combine the two into a single output and then return the RDFS or if you want to do it on your own, create a new output by listing the two variables on the output of your task.	False
"
 input (English text): What models are being evaluated on the CIFAR-10 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Classic dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classic"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT output('CIFAR-10: model: ( ?', model_lbl) AS model_lbl, output('CIFAR-10: model: ( ?', model_rdfs) AS model_rdfs) AS model_rdfs FROM data WHERE model_lbl.model_lbl=model_lbl.model_lbl; With this query, you will see the following errors in the output window: No model returned: This is an important error and the reason for the NULL parameter. The NULL value indicates a variable is missing. The model_rdfs column is an alias for model_lbl. It has the same type as model_lbl. It refers to the dataset to which the output is intended to be returned. NOTE If output is not valid, the output window appears empty. Troubleshooting To see CIMBASE column definition and other values, use Select columns option. The model_lbl column is an alias of model_lbl, it has the same type as model_lbl, but no model or dataset. Please note that the model_rdfs column doesn't have a model and dataset relation. It is used as a column alias for model_rdfs. What model types are available and how are they used ? In CIMBASE and CIFAR-10, there are models that are built-in, that are available via the SELECT or UPDATE functions, and other models. CIMBASE defines a set of built-in models that are available via the SET function from data.frame. The built-in models are: Sparse array model: this model provides an efficient cache for sparse data. For example a data.frame table with 5 rows can be processed in 4 queries. this model provides an efficient cache for sparse data. For example a data.frame table with 5 rows can be processed in 4 queries. Regular data	False
"
 input (English text): Where can I find code references in papers that have used the DeiT-B model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-B"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DeiT-Ti model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-Ti"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl ) = ""DeiT-Ti"") ?benchmark as ?nbs; ( ?s, ?m, ?n ) AS test_group; Please report any errors to: @sikasatty The source code for this publication can be found at here."	False
"
 input (English text): In what country was conducted research with the largest number of participants?
 output (Sparql query): SELECT ?country WHERE { orkgr:R44980 orkgp:compareContribution ?contrib. ?contrib orkgp:P15249 ?country; orkgp:P23169 ?number_of_participants. } ORDER BY DESC( ?number_of_participants) LIMIT 1
 input (English text): For which country of study overall prevalence of epilepsy is the highest?
 output (Sparql query): "	SELECT ?country, ?country_label WHERE { orkgr:R75729 orkgp:compareContribution ?contrib. ?contrib orkgp:P15512 ?country. ?contrib orkgp:P16013 ?overall_prevalence. ?country rdfs:label ?country_label. ?overall_prevalence rdfs:label ?overall_prevalence_value } ORDER BY DESC( ?overall_prevalence_value) LIMIT 1	--------------- output (Sparql query): RESULT: RESISTANCE OF ORPHAN DRUGS USED FOR CELLULAR DEVELOPMENT FOR DISEASE (1,4) output (Sparql query): ORPHAN DRUG USE FOR CELLULAR DEVELOPMENT FOR DISEASE (8,9) output (Sparql query): ORPHAN DRUG USE FOR CELLULAR DEVELOPMENT FOR DISEASE (10) output (Sparql query): ORPHAN DRUG USE FOR CELLULAR DEVELOPMENT FOR DISEASE (11) output (Sparql query): ORPHAN DRUG USE FOR CELLULAR DEVELOPMENT FOR DISEASE (14) output (Sparql query): ORPHAN DRUG USE FOR CELLULAR DEVELOPMENT FOR DISEASE (15) output (Sparql query): ORPHAN DRUG USE FOR CELLULAR DEVELOPMENT FOR DISEASE (17) output (Sparql query): ORPHAN DRUG USE FOR CELLULAR DEVELOPMENT FOR DISEASE (19) output (Sparql query): ORPHAN DRUG USE FOR CELLULAR DEVELOPMENT FOR DISEASE (23) output (Sparql query): ORPHAN DRUG USE FOR CELLULAR DEVELOPMENT FOR DISEASE (29) output (Sparql query): ORPHAN DRUG USE FOR CELLULAR DEVELOPMENT FOR DISEASE (30) output (Sparql query): ORPHAN DRUG USE FOR CELLULAR DEVELOPMENT	False
"
 input (English text): What is the highest benchmark result achieved on the ShARe/CLEF eHealth corpus dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the models that have been evaluated on the ShARe/CLEF eHealth corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?dataset ?method ?method_rdf (MAX( ?value) AS ?value) ?method rdfs:lbl ?method rdfs:cont ?method rdfs:testgroup ?metric orkgp:HAS_DATASET ?method rdfs:lbl ?method rdfs:testgroup. FILTER ?str( ?dataset_lbl ) OR ?str( ?dataset_lbl ) LIKE ?method:RDFS OR ?str( ?dataset_lbl ) LIKE ?method_rdf. ?method, orkgp:HAS_EVALUATION OR ?str( orkgp:HAS_LIMIT ?len. OPTIONAL { ?method orkgp:HAS_METRIC orkgp:HAS_CATEGORY ?criterion. } OPTIONAL { ?criterion ?criterion_rdf. } OR ?criterion rdfs:label ?criterion_lbl. OPTIONAL { ?criterion rdfs:label ?categorical orkgp:HAS_CASE orkgp:HAS_CASE_RESTRICTION. } OPTIONAL { ?categorical OR kgp:HAS_PERCIDISTRATION OR kgp:HAS_ORIENTATION OR kgp:HAS_DORING_ORIENTATIONS. } OPTIONAL { ORIENTATION ( ?latitude ?longitude) OR DORING_ORIENTATION ( ?latitude > 30 ?longitude > 30 ) OR ORIENTATION_RESTRICTION( ?latitude> ?longitude ?longitude < 30 OR ?latitude < ?longitude ?30 ) OR ORIENTATION_RESTRICTION_RESTRICTION( ?latitude> ?longitude ?longitude < ?longitude ?30 ). ORORIENTATION_RESTRICTION_RESTRICTION( ?latitude> 60.45 ?longitude> 60.45 ?longitude <= 60.45 ?longitude <= 60.45). ORORIENTATION_RESTRICTION( ?latitude> 45.45 ?longitude> 45.45 ?longitude <= 45.45 ?longitude <= 45.45. ORORIENTATION_RESTRICTION( ?latitude> 45.45.000	False
"
 input (English text): What models are being evaluated on the HMDB51 (finetuned) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the HMDB51 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Samples of queries are also available. For example, we can run the model to see the average cost of each parameter to compute the ""per_km"" costs: hadoop-benchmark -benchmark-model_lbl-1.0.0_4-5-1 Using the ""all"" flag can simplify things a little to include only the models that have not been tested yet. For example, if we pass -benchmark-all, we get a list of 4 models. Useful functions and variables -ffilter=filter-expr [ FILTER FILTER - expr ] <FILTER > Specify filters for aggregation functions. Supported filters are: cost, count, value, average ; see the filter-expr documentation for usage. -Fverbose, -v Verbose output. -h Returns the file name of this table. -h_n The name (required) of a node in the dataset that is being benchmarked -l, --list Lists all nodes of the Hadoop database. It may be necessary that you set the -i, --initialize flag to enable these nodes before running the benchmark. -l_n The name (required) of a node with which to benchmark the next node. -m Use the current size of the graph. -max_threads Number of threads that can be used concurrently by the Hadoop worker. -max_size The size of the total dataset. -min_threads Number of threads that can be used concurrently and in parallel by the Hadoop worker. -min_size The size of the total dataset. -maxxact, -xact Maxxact is a cache-limiting algorithm, and will limit H"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Venture dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER ( ?dataset_lbl in ( ?dataset_lbl ) or ?benchmark ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } These two queries will produce the same result as follows: test dataset: # a 1-column table a 1-column table: TITLE DESCRIPTION ------------------ id 1 of 1 document ------------- --------- ----------------------- ID 597165079 id1 4 of 11 document id2 7 of 11 document ui 1 of 2 document ------------- ------- --- 0 ------------------------- --------------- # a 2-column table 2-column table: TITLE DESCRIPTION ----------------------- id 2 of 2 document # a 3-column table 3-column table: TITLE DESCRIPTION ----------------------- id 3 of 3 document ------------- -------- -- 0 -------------- # a 4-column table 4-column table: TITLE DESCRIPTION ----------------------- id 4 of 4 document # a 5-column table 5-column table: TITLE DESCRIPTION ----------------------- id 5 of 5 document ------------- -------- -- 0 -------------- # a 6-column table 6-column table: TITLE DESCRIPTION ----------------------- id 6 of 6 document ------------------ -------- -- 0 ------------ # a 7-column table 7-column table: TITLE DESCRIPTION ----------------------- id 7 of 7 document ----------------------------- -------- -- 0 ------------- # a 8-column table 8-column table: TITLE DESCRIPTION ----------------------- id 8 of 8 document -------------- -------- -- 0 ------------ # a 9-column table 9-column table: TITLE DESCRIPTION ----------------------- id 9 of 9 document -------------------------- -------- -- 0 --- The output from two queries are shown below: titulis Title ------------------ a 1-column table # a # a a 2-column table 1-column table: TITLE DESCRIPTION ----------------------- id 2 of 1 document ------------- --------- ----------------------- ID 4 of 11 document id2 7 of 11 document ui 1 of 2 document	False
"
 input (English text): List the metrics that are used to evaluate models on the CUB-200-2011 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 import nltk from nltk.config import metrics, config, _ metric_cub_200_2011 = metric('CUB-200-2011') metric('Rotowire') import nltk _config = nltk.config.config() _config.parameter('metrics ', metric_cub_200_2011) if __name__ =='__main__': nltk.run(options = {nltk.config.mets = metric_cub_200_2011 }) Output metrics from the CUB-200-2011 benchmark dataset (all metrics from a single dataset): CUB-200-2011 list of metrics (all: count(*) of (0.0, 1.0)) metric(5.2) 1 metric(5.4) 1 metric(5.5) 1 metric(6.1) 5 metric(8.5) 4 metric(9.1) 3 metric(10.0) 1 metric(12.4) 4	False
"
 input (English text): Provide a list of papers that have utilized the Weighted Tsetlin Machine model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Weighted Tsetlin Machine"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tsetlin Machine"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"benchmark code orkgp:HAS_BENCHMARK ?benchmark ?dataset. Note that the output of the SQL query is provided in a special way, so that it will also be available as a list in Google documents or a copy of your paper. And to use it just click on the input button. These are just a couple of examples of queries you can search in the Google doc. So the Tsetlin Machine model was used in the following papers: http://journals.sagepub.com/jspp/abstract/1603/028811 ""The Weighted Tree-like Algorithm for the Optimization of Raster Processing"" http://www.scramble-and.com/content.html ?id=1 ""Weighted Tsetlin Tree-like Algorithm for Deep Inference in Image Visualization"" http://iopscience.iop.org/1471-2056/8/113903/pdf ""An Empirical Measure of the Size of Graphical Models in Complex Data"" http://arxiv.org/abs/1512.06847 http://arxiv.org/abs/1509.12042 ""Fastest Deep Pattern Recognition Algorithm"" http://www.cs.cmu.edu/~lm/papers_papers_papers_2010/papers.html"	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Breakout dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	************ ** TITLE ************ TITLE TITLE TITLE!!!!!!!!!! SETS.PARTITION BY name DESC SETS.OPTIONS output (Sparql query): ********** EXCEPTION ************ EXCEPTION EXCEPTION EXCEPTION --------------------------- SETS.PARTITION BY name [set] SETS.OPTIONS set.title output (Sparql query): ********** ERROR ********~ SETS.PARTITION BY name [set] SETS.OPTIONS set.title output (Sparql query): ********** ERROR ********~ SETS.PARTITION BY name [set] SETS.OPTIONS set.title output (Sparql query): ********** ERROR ********~ SETS.PARTITION BY name [set] SETS.OPTIONS set.title output (Sparql query): ********** ERROR ********~ SETS.PARTITION BY name [set] SETS.OPTIONS set.title output (Sparql query): ********** ERROR ********~ SETS.PARTITION BY name [set] SETS.OPTIONS set.title output (Sparql query): ********** ERROR ********~ SETS.PARTITION BY name [set] SETS.OPTIONS set.title output (Sparql query): --- title: --- ********** BEGIN -- ** SETS.PARTITION BY name [set] [s] [a] SETS.OPTIONS set.title SETS.OPTIONS set.title SETS.OPTIONS set.title SETS.OPTIONS set.title --- title: --- ************ ** END ************ --- title: output (Output of first row): output (Output of second row): Output of third row: Output of fourth row: output (Output of this row): output (Output of this row): output (output of this row): output (output of this row): output (output of this row): output (output of this row): Output of this row (again): output (output of this row): output (output of this row): output (output of this row): output (output of this row): output (output of this row) output (output of this row): ************************** SETS.PARTITION BY name [set	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Tennis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Cardiogram dataset ? output (Sparql query): { ?Dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Cardiogram"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Cardiometer dataset ? output (Sparql query): { ?Dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Cardiometer"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } It would be nice if you could provide a way to get the ""true"" evaluation metric (eg. on the Atari or Atari 2600). How to solve ? If, after reading this section, you find it difficult to see how to use R and your model on the data you have stored in SQL, and if you do not have an appropriate implementation for those data, you may find that your solution uses a very different approach altogether:"	False
"
 input (English text): List the code links in papers that use the QA-GNN model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""QA-GNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the FQF model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FQF"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 To summarise the output I can state that FQF outperform CIFAR-10 by up to a factor of 5. CIFAR-10 doesn't have a problem with any data, FQF has a problem with a large sample size with a significant number of missing entries from the dataset. To conclude the results I would like to state that this is a clear demonstration that the FQNOCM approach and not GNN can be performed on the MNIST dataset, if this approach to training the model that has a large sample size, a large feature space and several algorithms can be used when compared to other approaches it seems that CIFAR-10 is simply not able to scale to the real dataset it is training for. A future post might try to further explain why this is the case. A note on the datasets In order to get into the nitty gritty details of each dataset, I looked at the pdf of all of them. There is a lot of detail in each one so I decided to split it up into three articles. The first one is more or less a complete overview of all the datasets that are used, the second looks at the general process used to obtain and train the output models and the third looks more in-depth into a particular model.	False
"
 input (English text): Indicate the model that performed best in terms of % Test Accuracy metric on the SNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""% Test Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Sequence error metric on the FSNS - Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Sequence error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Note that our SNLI benchmark dataset is used as an input to our Benchmark module. This means that our benchmark module performs more than 50% better when the sequence error metric on the SNLI benchmark dataset is set to 0. We can have a look at our benchmark module after it has been built by running: python benchmarks.py > SNLI Benchmarks The benchmark.txt file contains the full benchmark suite on the directory /data/sparql. Here is an example of the output we get: # Sequence error test output for n=3 # Sequence error test output for n=3 # Sequence error test output for n=3 # Sequence error test output for n=3 # Sequence error test output for n=3 # SNLI test output (invalidating) # SNLI test output (invalidating) # SNLI test output (invalidating) # SNLI test output (invalidating) # SNLI test output (invalidating) # 1: 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 # # 2: 00 01 04 00 00 00 00 00 00 00 00 00 80 # 3: 00 00 70 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 # # 4: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 # # 5: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 100 # 6: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 # # 7: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 50 00 # \ 8: 00 00 00 00 00 00 00 00 00	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ARC-PDN?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset ARC (Challenge)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 ? ? ? ? ?: ? ?: ? ? { ?class :STRING }{ ?class rdfs:label :STRING ?class_lbl. ?class_lbl rdfs:index :STRING ?cont_lbl. ?cont_lbl rdfs:attribute { ?class_lbl rdfs:label :STRING ?class_lbl. }. } ? ? ? ?, : ? ?: ? ? { ?class_lbl rdfs:label :STRING }{ ?class rdfs:index :STRING ?cont_lbl. }. OPTIONAL { ?cont_lbl rdfs:label :STRING }{ ?cont_lbl rdfs:index :STRING ?cont_lbl rdfs:attribute { ?class_lbl rdfs:label :STRING ?class_lbl. }}; } ? ? ? ?, : ? ?: ? ? { ?class_lbl rdfs:label :STRING }{ ?class rdfs:index :STRING ?cont_lbl. }. output (Sparql query): ? ? ? ? ? ? ? ? ? ?: ? ? { ?class _lbl orkgc:HAS_EVALUATION :INTEGER }{ ?value - ?value orkgc:HAS_SUBSTANTIAL_SUFFIX ? { ?value rdfs:label :STRING ?value_lbl. }. } ? ? ? ? ? ?, : ? ?: ? ? { ?class _lbl rdfs:label :STRING }{ ?class rdfs:index :STRING ?cont_lbl. }. OPTIONAL { ?cont_lbl rdfs:label :STRING }{ ?cont_lbl rdfs:index :STRING ?cont_lbl rdfs:attribute { ?class_lbl rdfs:label :STRING ?class_lbl. }}; } ? ? ? ? ? ?, : ? ?: ? ? { ?value orkgc:HAS_SUFFIX :INTEGER }{ ?value rdfs:label :STRING ?value_lbl. }. ? ? ? ?: ? ?, : ? ?: ? ? { _lbl rdfs:label :STRING }{ ?model rdfs:index :STRING ?cont_lbl. ?cont _lbl _lbl	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SciGEN dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SciREX dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciREX"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?dataset ?item ?item_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciREX"") ?benchmark orkgp:HAS_DATASET ?dataset. ?item orkgp:HAS_BENCHMARK ?benchmark. ?item_lbl orkgp:P31 ?item. ?item. ?item_lbl orkgp:P31 ?item. ; } output (Sparql query): SELECT DISTINCT ?benchmark ?item ?item_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciREX"") ?benchmark orkgp:HAS_DATASET. ?dataset. ?item orkgp:HAS_BENCHMARK ?benchmark. ?item_lbl orkgp:P31 ?item. ?item_lbl orkgp:P31 ?item. ; } "	False
"
 input (English text): List the metrics that are used to evaluate models on the CS-NER benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CS-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the TSE-NER dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?dataset a orkgc:Dataset; ?dataset rdfs:label ?dataset_lbl. Input (English text): Can you list the metrics used to evaluate models on tse-ner dataset ? output (Sparql query): SELECT ?dataset a orkgc:Dataset; RDFSCode. In case you have additional questions please contact me.	False
"
 input (English text): Where can I find code references in papers that have used the Contextual Match model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Contextual Match"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Concept Mention Extraction model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Concept Mention Extraction"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Sampson, David (2009). A review of the effects of context in social science benchmarks. Sociological Methods and Research. 33(4):541-568 DOI 10.1177/002185701902709  – Jointly published by ACR and ESS Jointly published by ACR and ESS, with the same authors, 2009 (2nd ed.) Jointly published by ACR and ESS, with the same authors, 2008 Jointly published by ACR and ESS, 2008 (3rd ed.) Jointly published by ACR and ESS, 2008 (2nd ed.) For additional information, contact the publisher.	False
"
 input (English text): Can you provide links to code used in papers that benchmark the OTF spelling (single) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""OTF spelling (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the OTF spelling+lemma (single) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""OTF spelling+lemma (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?code,RAND = 0.0, c.n,p = p,a ( ?cod/n ?dat ?, ?cod/h ?code, ?cod/l ?code ),c.l,a ( ?cod/n ?dat ?, ?cod/h ?code, ?cod/l ?code),a ( ?cod/n ?dat ?, ?cod/h ?code, ?cod/l ?code), RAND = ( ?cod/l ?cod( ?code), ?cod/l ?code), ?p( ?dat ?), ?c( ?dat, ?cod ) if ( ?cod/l ?cod( ?code) OR ?cod/l ?cod( ?code) ), IF ?cod( ?code, ?cod( ?code), ?cod ) IF ( ?cod( ?code, ?cod( ?code), ?cod ) ) IF ( ?cod( ?code, ?cod( ?code), ?cod ) ) IF ( ?cod( ?code, ?cod( ?code), ?cod ) ) The second input is a string of comma separated, numerical results. The first two lines are only included in the output if the query returns true. input (English text): Can you provide links to the output of the single model ? output (Sparql query): SELECT ?code,RAND = 0.0, c.n,p = p,a ( ?cod/n ?dat ?, ?cod/h ?code, ?cod/l ?code ),c.l,a ( ?cod/n ?dat ?, ?cod/h ?code, ?cod/l ?code ),a( ?cod/n ?dat ?, ?cod/h ?code, ?cod/l ?code ), RAND = ( ?cod/l ?cod( ?code), ?cod, ?cod ) if ( ?cod/l ?cod( ?code) OR ?cod/l ?cod( ?code) ) IF ( ?cod/l ?cod( ?code, ?cod( ?code), ?cod ) ) IF ( ?cod( ?code, ?cod( ?code), ?cod ) ) IF ( ?cod( ?code, ?cod( ?code), ?cod ) ) IF ( ?cod( ?code, ?cod( ?code), ?cod ) ) The third input is a string of comma separated	False
"
 input (English text): Provide a list of papers that have utilized the 3-layer AWD-LSTM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""3-layer AWD-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the A3C LSTM hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C LSTM hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?code, ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?benchmark orkgp:HAS_LMP ?benchmark. ?cont orkgp:HAS_LOWER ?lm_dataset. ?lm_benchmark. ?cont orkgp:HAS_LOWER ?lm_model. ?lm_reference_code. orkgp:HAS_LOWER ?benchmark. 5.2.3. A7-LSTM In addition to the AWD H3-layer (AWD-AH3M), two more lstm/h3 models have been suggested for use as the basis for a full A7-LSTM (AH7LSTM). The original version of the ANP (AH7 ANP) has four lstm layers (AADLSTM, AHADLSTM, ANFSTM, and ANF ADLSTM) and has been used extensively as a model for many applications, including image analysis, machine learning, and genetic programming. The AH7 ANP is a single-layer hstm with a maximum-likelihood estimation (LML) method, which has resulted in a variety of advantages (see Appendix B, A7-LSTM vs LML). A7A7LSTM was suggested for use in the same framework, and the original authors of the AH7 LSTM included this in their paper (see Appendix L ). Some of the advantages of the AH7 A7LSTM include: high-accuracy deep learning (DANN/DANN+), efficient parallelization because of its lack of memory and because of its very small size (8 MB), fast reconstruction, and high training rate. The proposed A7LSTM variant is also relatively easy to implement, in that a single lstm-like model can be replaced by a single DANN model with a single layer and a single lstm-like hidden layer, providing an additional level of parallelization. The proposed A7LSTM can be implemented in Python and is accessible via a Python package [Hinton, 2014]. The most important limitation is that it cannot be used with the LSTM model from [Takahashi et al., 2011]. The AH7LSTM provides a fast and powerful platform for a variety of applications, but	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset SVHN?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SVHN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Percentage error score on the SVHN benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SVHN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	--------------------------- What are the top 10 benchmark results (metric and value) over the dataset SVHN ? -------------------------- { { ?dataset a. DATA | | ?dataset a. LSN | | ?dataset a. F1 | | ?dataset a. F3 | | ?dataset a. F5 | | } } input (English text): Which model has achieved the highest F1 score on the SVHN benchmark dataset ? output (Sparql query): ------------------------- ------------------------- The following examples are taken from various datasets and different data sets. For more details on how to select, filter, aggregate, and join the data in different datasets, please have a look at the various examples. Example 2: Selecting, Filtering, Aggregating and Joining a Select and a Categorical variable (Dataset VHN-NIDC) The following dataset has three classes, Class One, Class Two and Class Three. The dataset contains various sample names, such as sample1, sample2, sample3, sample4, sample5. Using the variable VHN, the queries are as follows. VHN { variable:VHN; varchar(64) ?name; varchar(64) ?class_1; varchar(64) ?class_2; varchar(64) ?class_3; } Example 3: Aggregation using a Varchar Variable The following example is taken from a dataset with two classes, Class One and Class Two. Example 4: Joining Varchars and a Select (Dataset VHN-MIDC) The following dataset has three classes, Class One and Class Two. The dataset contains several sample names with their unique IDs. Using a Varchar variable in the selection of a Varchar, the queries are as follows. VHN:MIDDATA { variable_varchar:VHN; variable_varchar1:VARCHAR3; varchar_varchar2:VARCHAR2; } Example 5: Selecting, Filtering and Aggregating on a Select and a Categorical variable (Dataset VHN-F1) The following example is taken from a dataset with two classes, Class One and Class Two. The dataset contains various sample names. Using the variable VH	False
"
 input (English text): Can you provide links to code used in papers that benchmark the LSTM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LayerNorm HM-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. ?code;. } The above output shows only a portion of the data that you can be searching for if you want to find a specific code or specific benchmarks. The final output is in RDF or SPSS format where you can enter your queries into a regular expression. Code Example import numpy as np import matplotlib.pyplot as plt import matplotlib.ggplot as ggplot class Benchmark : model_lbl = [] score_lbl = [] model_count = [] model_size = None class ldr_lstm ( object ): score = 0.0 model_lbl = {} model_count = [] model_size = 0.0 # A method for the lstm class. # The function to compute an individual score for score_lbl. def score_lbl ( self ): """"""Return the individual score for the model, which measures its weight in the posterior of (x,y) over the given dataset """""" tqdm = self. lstm. model. sum ( x, y ) score_lbl = np. sum ( np. dot ( tqdm, self. score_lbl ())[ 0 ]) return score_lbl def model_lbl ( self ): """"""Return the lstm class's class. """""" c = np. linspace (( - 1, 1 ) * np. pi, 1 ) c = np. dot ( self. lstm, np. dot ( np. cos ( _, self. shape [ 0 ][ 0 ]))) c = c. astype ( 'float64' ) return self. score return ldr_lstm Explanation The main goal of the benchmarking is to explore some more details of the LSTM model. To do so, we'll work with different parameter combinations. This section will discuss the different input parameters used in the test cases and then we will compute the output (sparql query and results). The benchmark starts with some basic input, and the following output. The initial graph for an LSTM model is a simple graph with 10 points. Each point consists of 4 columns connected with a line. From here, we can follow the path of the ""X"", ""y"", ""z"", ""x"", ""y"	False
"
 input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""SPARQL query optimization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""SPARQL query optimization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	" Note: This benchmark also includes the ""other"" category of datasets that are not part of the orkgc:Problem dataset. Note that not all benchmarks may apply to your dataset. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. For more information, see SPARQL Query Optimization. See also SPARQL Query Optimization. For more information, see SPARQL Query Optimization. This document describes each of the functions described in SPARQL Query Optimization: SPARQL Query Optimization. See Also SPARQL Query Optimization SPARQL Query Optimization. See also Specifying Query Parameters. This section describes common SPARQL query optimization issues and solutions for using and managing multiple datasets at the same time. Note: While these tips are focused on using the standard SPARQL Query Optimization query, you should always check with their SPARQL documentation for more information. You first want to understand what you're doing. You can find a description of the various queries available in the orkgc:Problem. What makes this data especially useful ? What problems do you run ? How often do you benchmark it ? You will also need to find some way of measuring the performance for common datasets. Some recommendations: If you're really lucky, the data you run can be run over and over, like a series of ""mini-runs,"" without a noticeable drop in performance. Try benchmarking the orkgc:Problem over an hour or two or perhaps a couple of days at a time. You can then test what the SPARQL query optimizer"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Amazon dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Amazon-2 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?value, 1.1, MIN( ?value ) AS ?Metric FROM Amazon_2 input (English text): I can provide that result. output (Sparql query): SELECT ?value * MAX( ?value ) MAX( ?value ) AS ?Metric FROM Amazon_2 input (English text): How many times can I achieve that result ? output (Sparql query):  SELECT ?number, ?metric, ?value FROM Amazon_2_metrics_query input (English text): Yes, I can do that. I can do up to 100 times. output (Sparql query): select ?number, 3.5 * MAX( ?value ) AS ?Metric FROM Amazon_2_metrics input (English text): How many times is that possible, and how many times has it been done, and by which metric system ? Please explain. output (Sparql query): select ?number, ?metric, ?value FROM Amazon_1_metrics_query input (English text): I can do that. I can do up to 100 times. output (Sparql query): select ?number, 10 * MAX( ?value ) AS ?Metric FROM Amazon_1_metrics input (English text): The maximum possible score is 10. It shouldn't be more. output (Sparql query): select ?metric, ?value FROM Amazon_1_all_metrics_query input (English text): I can do that. I can do up to 100 times. output (Sparql query): select ?number, 20 * MAX( ?value ) AS ?Metric FROM Amazon_1_all_metrics input (English text): I can do that. I can do up to 100 times. output (Sparql query): select ?number, 30 * MAX( ?value ) AS ?Metric FROM Amazon_1_all_metrics input (English text): I can do that. I can do up to 100 times. output (Sparql query): select ?metric, ?value FROM Amazon_1_all_metrics_numeric input (English text): I can do that. I can do up to 100 times.	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CNN + LSTM + SVM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CNN + LSTM + SVM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Temporal Convolutional Network model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Temporal Convolutional Network"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT... orkgp:HALF_INPUT ?dataset. { ?dataset. ?dataset. ?dataset. ?dataset. } Orkgp:HALF_INPUT... orkgp:HALF_INPUT... orkgp:HALF_INPUT... input (English text): Can you provide links to code used in papers that benchmark the Temporal Convolutional Network model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CNN + LSTM + SVM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Can you provide links to code used in papers that benchmark the VGG-16 + LSTM + Sigmoid (VGG) model ? output (Sparql query): output (Sparql query): SELECT... orkgp:HALF_INPUT ?dataset. { ?dataset. ?dataset. ?dataset. ?dataset. } Orkgp:HALF_INPUT... orkgp:HALF_INPUT... orkgp:HALF_INPUT... input (English text): Can you provide links to code used in papers that benchmark the VGG-16 + LSTM + Sigmoid (VGG) model ? output (Sparql query): input (English text): Can you provide links to code used in papers that benchmark the SVM + LSTM + Batch (SVM + LSTM + BNN). output (Sparql query): output (Sparql query): SELECT... orkgp:HALF_INPUT ?dataset. { ?dataset. ?dataset. ?dataset. ?dataset. } Orkgp:HALF_INPUT... or"	False
"
 input (English text): Provide a list of papers that have utilized the BiT-M (ResNet-152x4) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M (ResNet-152x4)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BiT-S (ResNet) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-R (ResNet-152x2)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the BiT-C (ResNet-152x1) model and include the links to their code ?"	False
"
 input (English text): What is the highest benchmark result achieved on the QNLI dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WNLI dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT CONVERT(text, 'numeric', 2) FROM ( SELECT Metric, ?value AS ?value FROM ( SELECT Metric, ?value AS ?value, ?index.metrics.Metric as metric_lbl FROM rw_repo AS req, table_stats:table_stats ( ?) ) CONVERT("" ?keywords"")( ?keywords) WHERE { { Metric. ?value WHERE Metric. ?value. OR( METRIC_LBL. ?value. ) } } ORDER BY Metric_LBL. ?value.. ?met, METRIC_REPLY(Metric). The RDF Schema file for WNLI. # rdfs.c # # WNLI-N.2.0 # 2014/06/15 # # Copyright (C) 2015 C.D. Brown, Stephen H. O'Dwyer, # Robert F. Miller, etc. All rights reserved. # # This file is part of the WNLI-N.2.0 repository, # compiled by C.D. Brown of CSD of Oregon. All rights reserved. # # This file is subject to the latest RDF Schema version 1.5 release. # This file is maintained by Robert F. Miller. There may be a bug # version: # http://rfstat.org/wndll2.3.0-rdfs-0.1.3.6.zip # This file may be a source of information about the current version # of the RDF Schema or an alias of a version that's been changed on # RDF Schema's maintainer's server. # # This file was generated by RDF.org from version 0.1-0 of the RDF Schema. # # The RDF Schema version is available from the RDF.org server at https://rsd.rd"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the AAPD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT COUNT( ? ) AS ?value, ?RESIDUAL_COUNTRIES ?countries, ?RESIDUAL_INDEXes ?indexes, ?REFERENCES ?referENCE, ?RESULTS ?results, COUNT(ROW) AS ?results_rank, ?RANK ?rank, ?MILES ?columns, ?DATA ?data, ?STORE ?locations, ?COUNT(ROW) AS ?rows, ?RESULT ?results, ?ERROR ?error_message, ?CERTIFICATE ?certificate, ?LOCATION_TYPE ?location type, ?TICKS ?tick marks, ?TIME ?time unit Input language: English  output (Sparql query): SELECT COUNT( ? ) AS ?value, ?RESIDUAL_COUNTRIES ?countries, ?RESIDUAL_INDEXes ?indexes, ?REFERENCES ?referENCE, ?RESULTS ?results, COUNT(ROW) AS ?results_rank, ?RANK ?rank, ?MILES ?columns, ?DATA ?data, ?STORE ?locations, ?COUNT(ROW) AS ?rows, ?RESULT ?results, ?ERROR ?error_message, ?CERTIFICATE ?certificate, ?LOCATION_TYPE ?location type, ?TICKS ?tick marks, ?TIME ?time unit The dataset consists of eight columns of CSV: name, age, location, state, gender, and occupation In total there are 16 rows of data. The data from the first column must be treated as raw data, as the first row of the resulting CSV contains an error indicating that the second column contains raw data, the second column is missing, the first column contains a valid value, and the second column contains an error indicating that the second column contains raw data. As a result of this error, the second column may contain a valid value for the name column, but may have a row of row data value that cannot be represented in the column. The columns of the dataset must be ordered so that a value in column 1 is first in the dataset, followed by a value in rows 2 to 8. Since the dataset may not have been sorted at creation, this order will differ from the order in which they appear in the file, so the names will not be the same on different copies of the dataset. All columns	False
"
 input (English text): List the metrics that are used to evaluate models on the FTD dataset benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTD dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the FTD dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTD dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT SEGMENT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } } If that doesn't work, that's because the query contains invalid syntax, or there's a typo in your SQL statement. You can fix something in the query: if it doesn't work: error (invalid) If it says FTD, try and see if the first item is labelled a : else: ERROR (invalid) If it says FTD or a, try and see if there's a label on the following line with the second number of the dataset: The right-most line of code may even have been accidentally included somewhere. If you get that error, either there's an error in your SQL statement or an error in the dataset is missing in the response. If you got the error message, and you didn't need the model, I'd advise you to remove the row from the dataset, or try adding the following line in your dataset: ERROR So, you were given the option to use a particular dataset, but the response is missing the metric that's necessary to evaluate the model. What can you do about it ? First, try updating the dataset and see if the metric is included. Try this example: select s:dataset as metric_lbl, label as label, name as name, value_mth as value_mth from s s s s s s mth values; output (Sparql query): This isn't a perfect solution as metric.txt is used to export the model as a table. To import the model you'd need to run: EXPORT ( metric_lbl, label, name, value_mth ); Also, if the model was incorrectly labelled, there's no way to correct the error if there's no metric_lbl. I can think of 2 solutions: Use the second-last field in the metric_lbl, which is the name that you want to use. Change the line that checks for the missing metrics first, and if it returns any then use the values of those fields. The second option is preferable for both safety and backwards-compatibility, as it does not have any risk of introducing an unknown bug. If you don't have access to the second-last field, that's more likely to be an error in	False
"
 input (English text): What is the top benchmark score and its metric on the CIFAR-10 Image Classification dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10 Image Classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10 Image Classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"------------ This example shows the output of this query query for the benchmark model CIFAR-10 images. Note that the benchmark is a subset of the original dataset and has been pre-optimized and the training script has been extended. As a result, the original dataset has approximately 100 000 classes and the benchmark is only 10 000 classes. You can specify a model-specific query parameter for the benchmark query parameter list. Example: benchmark:model/benchmark. The benchmark dataset is in the form of a CIFAR-10 image classification dataset. As with the best performing model query query, there are multiple possible values for the benchmark model parameter list to choose from. You can also specify a range of different parameters for the parameter lists to choose from by supplying additional parameters. To specify the best performing model benchmarking category value, specify the following benchmarking parameter: BEST_MODEL_THRESHOLD=75 To specify the best performing model benchmarking category value in the case where the benchmarking category is the best performing category, specify the following benchmarking parameter for the ""best performing category"" parameter: BEST_MODEL_THRESHOLD_CATGORIA=12, BEST_MODEL_THRESHOLD_MEXICO=12, BEST_MODEL_THRESHOLD_VENEZUELA=12, BEST_MODEL_THRESHOLD_UAE=12 Please note that there is not a limit to the number of benchmarking parameters that you can list. You can list as many parameters as you wish. With the CIFAR-10 benchmarking category specified, use the CIFAR-10 benchmarking criterion to predict the top 10 most accurate benchmarking datasets of a pre-trained classification model with 100 000 classes. To see an example of batch predictions of all benchmark results, select the benchmark category value from the BestModelThreshold parameter list or specify the benchmarking parameter using the CIFAR-10 benchmarking criterion using the BestModelThreshold_MEXICO, BestModelThreshold_VENEZUELA, BestModelThreshold_UAE, bestmodelthreshold parameter list. To see an example of batch predictions of the top 10 most accurate benchmarking datasets, select the benchmarkcategory value for BestModelThreshold that you have selected. You can also specify the bestmodelthreshold parameter list to also include the best model that would be selected for selection for the benchmark"	False
"
 input (English text): What is the top benchmark score and its metric on the Walker, walk (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?latitude ?longitude WITH AGGREGATION; The query below selects the value latitude as a reference to Longitude. SELECT ?latitude ?longitude WITH AGGREGATION; The query below is to show the first 4 latitudes in the dataset. SELECT ?latitude, ?longitude WITH aggregation; In order to perform query queries like query 1 above you need to create a new table named Walker, walk and create a new column named ?latitude as is shown below. CREATE TABLE ( ?latitude 1); INSERT INTO Walker, walk ( ?	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SciGEN dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. DISPLAY_VALUE $2.50, ?eval orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?quantification; } What does the display value represent in practice ? What is the precision of the metric and how does it compare to the other evaluation metrics ? I will start with the display of precision in a bit and work my way back. When it can be useful to use precision there are two main uses: for precision and precision-in-somewhat-anonymous values for certain validation criteria that make precision values more useful Note the importance of the latter use of precision, as I know for example for that query the query is not about evaluating the model, so it's quite likely that the metrics would not be of any use for that query because they are not accurate enough to be used for that task. In any case, to give a short illustration of what can be done with it, I present a simple query that computes the least square error for a subset of the training set: SELECT * FROM p SET QUAL; A query for precision allows you to specify a time that is given in seconds (e.g. seconds:1). But this time is used to compute the current value of precision (e.g. precision:1/3). I'm guessing that a real user might simply use the current time and this query would be accurate to 60% of the precision. With precision, you end up in a much harder situation, which is why we'll focus in some more upon it. By now it should be clear that in real use of the system it would be the user who would have to decide whether to use precision or not: What can I do with precision, it seems to be a question not of a scientific system's use of it, but a question of whether I am comfortable setting this parameter in my query. If I'm not comfortable with that then I should be wary of using precision. For example if I have a system that's not used for anything but this query I might not be prepared to support it with precision. Similarly if I want to benchmark the same model with a smaller time or a small number of parameters: I might run my benchmarks and	False
"
 input (English text): List the code links in papers that use the DATL model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DATL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the POP3D model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""POP3D"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a, ?dataset a,!RDF. ?model. FILTER (str( ?dataset ) = ""pops3d"" ) } orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. input (English text): List the code links in papers that use the DATL model in any benchmark ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc orkc:Model,!RDF. ?model. FILTER (str( ?dataset ) = ""pops3d"" ) ? } input (English text): List the code links in papers that use the POP3D model in any benchmark ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkc ) ? } output (Sparql query): SELECT RDF. text FROM ORkgp ?data WHERE ORkc:model && RDF. text. ?dataset ; output (Sparql query): SELECT RDF. text FROM ORkgp ?data WHERE ANDkc:model && RDF. text. ?dataset ; output (Sparql query): SELECT RDF. text FROM ORkgp ?data WHERE ANDkc:is_dummy ORkc:model && RDF. text. ?dataset ; output (Sparql query): SELECT RDF. text FROM ORkgp ?data WHERE ANDkc:model && RDF. text. ?dataset ; output (Sparql query): SELECT RDF. text FROM ORkgp ?data WHERE ANDkc:is_dummy ORkc:model && RDF. text. ?dataset ; output (Sparql query): SELECT RDF. text FROM ORkgp ?data WHERE ANDkc:is_dummy ORkc:model && IFERROR( RDF. text,'' ) orkc:model ? ||kc:is_dummy. ?dataset. ?models. output (Sparql query): SELECT RDF. text FROM ORkgp ?data"	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Double Dunk benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Tennis benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Triple Take benchmark dataset ? output (Sparql query): input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Whack-a-Mole benchmark dataset ? output (Sparql query): input (English text): Indicate the model that performed best in terms of Score metric on the Atari 6800 Tango test dataset ? output (Sparql query): input (English text): Indicate the model that performed best in terms of Score metric on the Atari Lynx benchmark dataset ? output (Sparql query): input (English text): Indicate the model that performed best in terms of Score metric on the Atari Jaguar benchmark dataset ? output (Sparql query): If all is correct, input your data and run the following code from the same page: psql -d mydata.pacsdr\dbsrv_dataset.dbsrv.json -U /sbin/ls /opt/db-data/test_cjv.bat This will output something like the following: A very high score (13,861,722) was obtained on the Atari Jaguar benchmark dataset. This is a good score for an Atari Jaguar game. Using a simple and general method for evaluating machine learning approaches, The Atari Jaguar dataset contains a lot of games. The goal of this study was to use the Atari Jaguar dataset to identify the best (most appropriate classifier) models for playing games such as Atari Jaguar, Atari 6800 Tango, Atari Lynx, and Atari Lynx 2. We also focused on the Atari Jaguar benchmark dataset for scoring games using our K-means algorithm and its similarity algorithm. First, we tested the performance of a single model using the Atari Jaguar benchmark dataset. Then, we compared the performance of two different models: our k-means (or linear) and our similarity (or linear plus logistic regression) approaches. Finally, we evaluated the performance of our modified LSTM approach versus the modified RNN approach and its similarity (or linear plus logistic regression) and comparison with the RNN approach (which was modified by a linearity improvement). The following sections describe the results of the analysis; we suggest that users of the dataset pay proper attention to the results and choose the algorithm that performs best for their goal. The top 50 results and	False
"
 input (English text): Can you list the metrics used to evaluate models on the UCF101 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the UCF101 (finetuned) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ? ?benchmark ?eval; Input input (English language): You want a table of Metric/LBL results from your test dataset: output (Sparql query):  SELECT ?metric_lbl ?metric_lbl, ?metric.... (e.g.  'metric'  or'metric_lbl') Evaluate input (English text): Do you want to analyze Metric/LBL result from your test dataset ? output (Sparql query): SELECT * FROM metric_lbl  WHERE ?metric_lbl=�%t1 (�%e.g.'metric'� or �metric_lbl)  (�'metric' or'metric_lbl) or  -1; or use a higher dimension (e.g.  'metric2' Results input (English language): How many examples of ""metric"" metaclass for your model are found in your test dataset ? output (Sparql query):  SELECT metrics FROM metric1 ? metric2 ? metrics;  If metric1 and metric2 are not available in the test dataset, use ""metric1"" as a reference metric Input input (English text): Your first data frame (RDF) is the input dataset. output (Sparql query):  select output_data.metrics,  metric_lbl from output_data; Output data frame output (Sparql query):  select data_frame.metrics,  metric,  metric_lbl for  metric_lbl in output_data.metrics; Output data frame Input input (English text): How many examples of metric are found in your test dataset ? output (Sparql query):  select examples_all.metrics, metric_lbl from examples_all; Output data frame input (English text): How many examples of metric are found in your test dataset ? output (Sparql query):  select example_all.metrics, metric_lbl for  metric_lbl in example_all; You only need one input: input (English text): Your second data frame (RDF) is the second dataset. output ("	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CoQA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MPQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MPQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P23 *cont; rdfs:label ?paper_lbl. } The output is a sparse query, since we only care about the two entries in the input dictionary: the title of the paper or the ID in the MPQA dataset. Given a sparse query, the next step is to read the key-value dictionary. For this purpose, a sparse query is the simplest type of queries, but it is inefficient and it needs a lot of data, so it's not recommended. For that purpose, a sparsity query is a slightly nicer query, as the data for that is much smaller than the input dictionary (in this case, three words). The key-value dictionary is then read again and the results are sorted on the basis of the name and the ID, which gives the results, The three items can be combined to generate the query, input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MPQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?co orkp; rdfs:label ?paper_lbl. } input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CoQA dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp"	False
"
 input (English text): What is the highest benchmark result achieved on the BC5CDR dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-chemical"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-chemical"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value ) } } GROUP BY ?metric ?metric_lbl output (Sparql query):  SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-chemical"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value ) } } GROUP BY ?metric ?metric_lbl A detailed description of the various parameters, variables and attributes of the table can be"	False
"
 input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Point Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SUCCESS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SUCCESS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SUCCESS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Now what have we learned from the data ? We know how to create a list or a set of labels with an array of integers, but nothing about creating a list of columns for an array of integers. There is nothing in the object definition of the object to show how this can be achieved. So, we are left with using the column name in the object definition as follows: The model that can achieve the highest SUCCESS score with its metric as the result. Note that we are setting the metric to something that appears in our table and with the name of this metric. This means that the ""A orkgc:Metric"" column is no longer needed. Once upon a time, there was a model called ""A orkgc:""Metric"" that was used to describe the metric for the metric. While in the past, the name ""A"" was still in common, it is not a good practice to use the name A orkgc: Metric to describe the metric for a given point_nav value. We have not yet seen any use of this ""A orkgc:Metric"" model, yet, so we will not have to deal with this issue right now. We are now able to create a list"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ScienceCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Paper Field dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT rdfs:value ? ""R""::""H""::""MMM""::""DD""::'000 ?value : R rdfs:key ?key : ?key rdfs:value ? ""R""::""H""::""MMM""::""DD""::'000 orkgp:HAS_DATASET ?dataset : - ?dataset ? ""sciencecite"" I was getting this error when I used the 'id' option in the query:"	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-Russian benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a or kgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a or kgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark or kgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval or kgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } In this example, we've selected a model for a given language. It's not necessarily the best model: on this dataset, there are four different models. The metric we're using tells us that all its metrics are either Accuracy or Valuation. Then we're filtering our data to make sure we only see a handful of metric-oriented models. We can actually view the results of the above queries in the table below with the following query. Select Count of Accuracy from a group by # model_lbl_count Select Count of Valuation from a group by # model_lbl_count The column ""models"" shows the total number of models selected in the group.  A column called ""Count"" shows the total number of times for which there are metric-oriented models selected.  A column called ""Count_Val"" shows the total number of times a metric-oriented model is selected.  As we get closer and closer to our desired goal, our model number will increase. Let's see how long that takes. SELECT Count of Accuracy from a group by # model_lbl_count SELECT Count of Valuation from a group by # model_lbl_count We want to get a count of models where each model has "" Accuracy "" and "" Valuation "" in its "" Metric "" column.  The count metric ( "" Count "" ) should have zero, so we'll select that column.  In the result, we see that"	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Robotank dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Robotank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT Max( ?, ?) FROM Double Dunk""... ?""SELECT Max( ? ? ?) FROM Double Dunk WHERE { { ?dataset a orkgc:Dataset; rdfs:label ? a orkgc_lbl. } OR } OR } OR } ORDER BY DESC( ?value ) WHERE Max( ? ? ?) <> Max( ? ? ?) Example 2 This section demonstrates how to use the R.dataset.metric and the R.dataset.metric_lbl parameters to extract data from a dataset, and then display results. The example outputs the metric values and the value in the metric_label rdfs:labels of a dataset in English, and then displays the metric values and the metric labels for the dataset in metric, metric.txt, and metric.csv. library(df) df1 <- rbind(fossil.data.frame(dataset_lbl="""", metric_lbl="""", metric_labels=""""),df) fit = tf.contrib.rgd(metric, fit_params = fml.fitting_params) metric <- fm.metric(metric_lbl) df <- rbind(df, metric) # Note: there can be thousands of variables in the dataset. This section demonstrates how to use the R.dataset.metric, the r.dataset.metric_lbl parameter, and the R.dataset.metric_lbl to extract data from a dataset and display data on a data visualization program. model, k = 0, 1 # Each variable is represented by a variable set of R.dataset.metric values labeled for that variable, # each labeled with ""metric"" as a label and a value to indicate whether this variable # is a ""value,"" ""baseline,"" or an ""outlier"" or a ""marginal"" (i.e., with a value > 0 or < -10) metric <- rdata.frame(i = 1:7, labels = list(label = "" ?0""), value = 1:7, name = "" ?1"") # Use the metric.txt and metric.csv files to automatically generate metrics. The metric_labels are used to determine whether a single variable's value varies with frequency, or if all variables are statistically equal. You can have the metric"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the ESIM + ELMo Ensemble model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ESIM + ELMo Ensemble"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 code RDF( ?model_lbl. ) RDF for RDF( ?cont. )   RDF for RDF( ?cont. ) orkgp:BENCHMARK  orkgp:HAS_SOURCE_CODE orkgp:MODEL  orkgp:LBE  orkgp:BENCHMARK orkgp:LBE  orkgp:BENCHMARK  orkgp:ENZER  orkgp:BENCHMARK orkgp:ENZER  orkgp:BENCHMARK orkgp:BIDAF  orkgp:CHILD  orkgp:BENCHMARK orkgp:CHILD  orkgp:BIDAF  orkgp:CHILD orkgp:CHILD  orkgp:BIDAF  orkgp:CHILD  orkgp:CHILD1 orkgp:CHILD2  orkgp:RICH  orkgp:CHILD2  orkgp:RICH  orkgp:CHILD2  orkgp:CHILD orkgp:CHILD3  orkgp:KERNERSER  orkgp:CHILD3  orkgp:KERNERSER  orkgp:CHILD3  orkgp:KERNERSER2 orkgp:CHILD4  orkgp:WILDLIFE  orkgp:KERNERSER4  orkgp:KERNERSER4  orkgp:KERNERSER1 orkgp:CHILD5  orkgp:NATURAL  orkgp:CHILD5  orkgp:NATURAL  orkgp:NATURAL  orkgp:NATURAL1 orkgp:CHILD6  orkgp:BIZANORAMA  orkgp:CHILD6  orkgp:BIZANORAMA  orkgp:BIZANORAMA  or	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cheetah, run (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Cheetah, run (DMControl500k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	********* DESC(Metric,Score) DESC(Metric,Score) ********* [metric_lbl] --------------- ---------- Cheetah, run (DMControl500k) 300.00 Cheetah, run (DM controls 400k) 300.00 Cheetah, run (DM controls 500k) 500.00 Cheetah, run (DM controls 600k) 650.00 Cheetah, run (DM controls 700k) 1000.00 Cheetah, run (DM Control10k) 1000.00 Cheetah, run (DM Control1000k) 1380.00 Cheetah, run (DM controls 1250k) 1350.00 Cheetah, run (DM controls 1300k) 2000.00 Cheetah, run (DM controls 1400k) 2000.00 Cheetah, run (DM controls 1500k) 3000.00 Cheetah, run (DM controls 1600k) 4000.00 Cheetah, run (DM controls 1700k) 5000.00 Cheetah, run (DM controls 1800k) 2500.00 Cheetah, run (DM controls 1900k) 10000.00 Cheetah, run (DM Control200k) 4000.00 Cheetah, run (DM Control2500k) 5000.00 Cheetah, run (DM controls 2600k) 10000.00 Cheetah, run (DM controls 2700k) 13000.00 Cheetah, run (DM controls 2800k) 13500.00 Cheetah, run (DM Control2900k) 17000.00 Cheetah, run (DM controls 3100k) 17500.00 Cheetah, run (DM Control3150k) 19500.00 Cheetah, run (DM controls 3150k) 22000.00 Cheetah, run (DM control 3200k) 22000.00 Cheetah, run (DM control 3200k) 23000.00 Cheetah, run (DM controls 3200k) 25000.00 Cheetah, run (DM control 3200k) 28000.00 Cheetah, run (DM controls 324k) 29000.00 Cheetah, run (DM controls 323k) 33000.00 Cheetah, run (DM controls 325k) 36000.00 Cheetah, run (DM controls 326k) 37000.00 Cheetah, run	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?dataset 1 ?dataset_lbl. 3 ?benchmark 1 4 ?benchmark_rls. 15,12,5,13,14:7.5 5. Which were the data quality estimations for the STS dataset measured with SMART ? 6. What was the mean data quality estimate for the STS dataset, for each benchmark ? Output : input (English text): 5,13,14,12,6 7. In data quality estimation for the benchmark, did the mean data quality estimate overshot ? 7. In data quality estimation for the benchmark, did the mean quality estimate fall short of the SMART mean ? 8. In data quality estimation for the benchmark, did the quality estimate change over time, for each benchmark ? [0..N-1] 9. How much of the variance in the STS and STS-R datasets were accounted for by the following variables: time to data processing, throughput/download time, processing time, output size and compression ratio ? Output : output (query statement): SELECT ?dataset 1 ?dataset_lbl. 1,5,5,910 10. Who is in charge of the STS dataset ? Is it the Department of Public Safety ? What are the key priorities in analyzing and maintaining the data on the STS dataset ?"" Output : input (English text): ?dataset; ?dataset = select 1 2 1 3 ?benchmark ?benchmark_rls. ?benchmark. ?bench. ?bench. ?bench. ?dataset-p. ?cont ?dataset-p. ?lbl ?dataset. ?lbl (name) Output : output (query statement): SELECT ?dataset 1 ?dataset_lbl. 1,5,5,910 10. Who is in charge of the STS dataset ? Is it the Department of Public Safety ? What are to be the key priorities in analyzing and maintaining the data on the STS dataset ?"" Output : input ("	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Alien dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	------------- #  select 1 and 2 as aand b and sum(c) as c1 + sum(c2) as c2 from alien.datasets GROUP BY 1 AND 2 input (English text): What is the top benchmark score and its metric on the Alien benchmark dataset ? output (Sparql query): GROUP BY 1 AND 2 input (English text): What is the top benchmark score and its metric on the Atari game dataset (v) ? output (Sparql query): -------------- #  select 1 and 2 as anand b and sum(c2) as c2 from game.datasets GROUP BY 1 AND 2 input (English text): What is the top benchmark score and its metric on the Alien benchmark dataset ? output (Sparql query): ------------- #  select 1 and 2 as aand b and sum(c1) as c1 from game.datasets GROUP BY 1 AND 2 input (English text): What is the top benchmark score and its metric on the Atari game dataset (v) ? output (Sparql query): -------------- #  select 1 and 2 as anand b and sum(c2) as c2 from game.datasets GROUP BY 1 AND 2 input (English text): What is the top benchmark score and its metric on the Alien benchmark dataset ? output (Sparql query): ------------- #  select 1 and 2 as anand b and sum(c1) as c1 from game.datasets GROUP BY 1 AND 2 input (English text): What is the top benchmark score and its metric on the Atari game dataset (v) ? output (Sparql query): --------------	False
"
 input (English text): What is the highest benchmark result achieved on the Oxford 102 Flowers dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford 102 Flowers"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Birdsnap dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Birdsnap"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT score FROM ?dataset orkgp ORDER BY DESC( ?value) input (English text): What is the highest benchmark result achieved on the Ducks dataset, including the metric and its value ? output (Sparql query): --------------------------- Performance: (0) Estimate (1) Estimate: (1) 1 in 1000 00:00:00 | 2:18:20 1 in 1000 00:00:00 | 2:18:20 1 in 1000 00:00:00 | 2:18:20 1 in 1000 00:00:00 | 2:18:20 1 in 1000 00:00:00 | ?dataset a orkgc:Dataset; score 1.000,1 output (Sparql query): SELECT ?dataset_lbl. ?value AS ?score. OPTIONAL orkgp:HAS_DATASET ?dataset_lbl. FILTER (str( ?dataset_lbl ) = ?dataset_lbl. ) output (Sparql query): ORDER BY ?metric = ?metric_lbl. ?eval	False
"
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Chopper Command dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Chopper Command"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the Atari 2600 Chopper Command dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Chopper Command"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT '''model_measurements'': * '''score: 4', '''weight: 3.0001', '''score2: 0.00002' AS ''score', '''weight2: 2.00001', '''weight3: 1.00001' AS ''weight', '''score22: 0.00001', '''score3: 0.00001'' AS ''score22', '''weight2: 0.00001' AS ''weight2' FROM { ?dataset a orkgc:Dataset} AS '''data''', WHERE rank = > 10 ASC LIMIT 1; output (Sparql query) SELECT '''model_measurements'': * '''score: 4', '''weight: 7.5002', '''score2: 0.000000' AS ''score', '''weight2: 7.5002', '''weight3: 7.5002' AS ''weight', '''score22: 0.000000', '''score3: 0.000000' AS ''score2' FROM { ?dataset a orkgc:Dataset} AS '''data'', WHERE rank = > 10 ASC LIMIT 1; input (English text): How many rows with the title ""Fantasy Football ?"" were found within the data file {dataset}:. A table called 'fantasy_football' is produced that contains the data to be used in the benchmarks. output (Sparql query) SELECT '''model_measurements'': * '''score: 4', '''weight: 7.5002', '''score2: 0.000000' AS ''score', '''weight2: 7.5002', '''weight3: 7.5002' AS ''weight', '''''score22: 0.000000''', '''''score3: 0.000000''', ASC FROM { ?dataset a orkgc:Dataset} AS '''data"	False
"
 input (English text): Indicate the model that performed best in terms of PARAMS metric on the Stanford Cars benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Stanford Cars benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""DISTINCT"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset ; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value ; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } After this step, all the RDFS for this model are in /db/models. Here is the result of this code in R. Step 5 - Convert RDFS to JSON The RDFS for this model is written in Javascript - the JavaScript API to convert RDFS into JSON is called rfjs. As you can see from the code, RDFS is a binary format. This means, the output of rdfs conversion is either one of three ""types"": JSON, RDF/XML or RDF.XML (sometimes known as rdf): json - is a binary file that contain all the fields of RDFS and can have a.json extension. xml - is a data file (in JSON format) containing all the fields of RDFS and can have a.xml extension. rdf - is a data file that needs to be converted into the RDF format to be usable for JavaScript. RDF for JSON and RDF for XML are the same. You need rfjson and rdfjson. In this step, I use a tool called convertRDFs to convert RDFs JSON or RDF XML to RDF JSON. It requires Node.js to run, and is relatively fast due to its use of JavaScript engines. Here is the result of this code in R. You can notice that"	False
"
 input (English text): Indicate the model that performed best in terms of Precision metric on the Rotowire (Content Selection) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT, which was used in the previous example. If some columns cannot be found in the data type, a SELECT query can be used to get the proper values. See For more information about the syntax of a SELECT query, see SELECT Syntax. If you want the results from a query to be printed right away, you can use the output command, as in This will output the output to the current record column in the record table for a selected record on the table (or rows in the table) where the query was executed. You can use an optional column expression to add additional column values to the printed output. Here is a trivial example of writing a SELECT query that prints the metric results over an arbitrary subset of the data (e.g., the values in the given columns) to a row. select * from select_table ( ""select_table"", ""s"", column ( ""metric"", ""min"", ""max"" )); select ( ""select _from _lbl_as_values"", ""s"", x ) ; end Select_tables.fetch(1); This shows that the parameter x provides a parameter to specify a subset of the rows that should be reported. The query will run on every row given on the parameter s and if none of the rows have metric values that meet the criteria specified on the parameter x, the output column will contain a value of '' or '' for that row. Using Multiple Models There is one problem with using multiple models in the same query, however: there is a significant number of cases when multiple models appear to have the same accuracy, but are not the same dataset. Even though this happens in the general case, it is not always clear that this means that there is a problem. For example, I used this query to get the results of a model that calculated the same amount of precision between the two models when calculating distances (where the same metric is used in both models) as I did when calculating distances for the RotoWire network as a whole. The following query used seven distinct data structures so that the data structures were not used"	False
"
 input (English text): List the metrics that are used to evaluate models on the SQuAD2.0 dev benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" SQuAD1.1 dev "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The query shows that the first metric is SQuAD2.0 - we can't see the metric named orkgp. For the second metric this looks like a bit of a dead-end; it is in particular the performance metric used (but not the validation metric) to evaluate models. It's not easy to implement a metric for SQuAD2.0 so we can't use it here. The third metric, orkgp, does not exist in SQuAD2.0 And we also can't use the first metric and the third. We get a ""no such metric"" error. I can't get a first and third measurement (or metric) to work, but I have my SQuAD2.0 dev benchmark dataset; a query to the devbench utility yields the necessary information (I can get a list of metrics from it). The data above shows that the devbench benchmark dataset is different, because it depends on the benchmark data (ie it does not use devbench.xml, which is used in SQuAD2.0). This implies we have the second and third measurement as part of SQuAD2.0. However, the data in the second and third meters are not different. Instead they are the same as the metrics for the devbench benchmark; they're identical except for a label ""SQuAD2.0 dev"". In other words, they are the same as the first and third measurement, and the second and third measurement are the same as the measure of the metric used in devbench.xml. This means the SQuAD2.0 dev bench dataset is missing some features, some information we use (and we are able to use) in the devbench benchmark (ie we are using another metric). The missing metrics (or metrics) There is an"	False
"
 input (English text): Provide a list of papers that have utilized the Unregularised mLSTM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Unregularised mLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Large mLSTM model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Large mLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model orkgp:Model orkgp:LMP ?data. orkgp:RAND ?results. ?benchmark orkgp:DATASET ?reproduce[reproduce. ?reproduce. { ?test_level ?test_level || ?test_type ?test_type }. ] ?benchmark. We were able to verify that the result displayed in R was correct. One of the articles in the results of our benchmark included a few non-standard variants including the LAMBDA variant, namely: l.max_dist l.min_dist l.dist l.min_dist l.is_logical_division_by_one All were correctly implemented for the LAMBDA module, though the ""modification"" to the LAMBDA function to accommodate the extra parameter for the l.has_many arguments was wrong."	False
"
 input (English text): What models are being evaluated on the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the ACL Anthology dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL Anthology"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset;  rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-Anthology"") ?benchmark orkgp:HAS_DATASET ?dataset;  orkgp:HAS_EVALUATION ?benchmark. output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset;  rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-Artistry"") ?benchmark orkgp:HAS_DATASET ?dataset;  orkgp:HAS_EVALUATION ?benchmark.   ?model rdfs:label ?model_lbl. OPTIONAL { ?model rdfs:label ?model_lbl. ] } } For our dataset, all the documents were retrieved using an LDIF/DIGIT query in a single query.  We would like to know if these documents are comparable or not, because one of the major constraints in data mining is to exclude false positives using ""standard"" methods. To answer this question, we used some interesting metrics, which are described in the following sections. 1) The number of ""false positives"" for our dataset is a very difficult measure to calculate. We assume that there are 10,000 unique documents for this dataset.  So, we calculate the number of false positives for each document, and if there are 10 different documents for each of the 10,000 unique documents, then we calculate the false positive rate for each document.  We define a true positive as one that was found on a particular document.  But, most importantly, false positives are the result of one document that does not match at all.  If we have 100000 documents, and 25% of them have a false positive, then 2.5% of all documents will have a true positive. If we want to find out whether a specific document is a false positive, we need to perform a full manual crawl using a minimum set of documents.  We can't do that with the"	False
"
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Battle Zone benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Yars Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?metric ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yars Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark ofyar_rescue ORkgp:HAS_DATASET ?dataset; ORkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Input: Select ""model"" from the database row. Model: Atari 2600 Battle Zone Input: which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset ? Output: The Atari 2600 Yars Revenge model (Cumulative Score) has achieved a score of 10,000,000. Explanation: Model: Atari 2600 Yars Revenge Metric: orkgc output = ""Model:: :Metric: %s :Model:: :Model:: :Model:: :Model:: -1"" input (English text): Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset ? Output: The Atari 2600 Yars Revenge model (Cumulative Score) has achieved a score of 11,000,000. Model: Atari 2600 Yars Revenge Metric: orkgc result: Model:: :Metric: 545,000. Model:: :Model:: :Model:: :Model:: -1 Input: Select ""model"" from the database row. Model: Atari 2600 Yars Revenge Input: which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset ? Output: The Atari 2600 Yars Revenge model (Cumulative Score) has achieved a score of 13,000"	False
"
 input (English text): What models are being evaluated on the BC5CDR-disease dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the NCBI-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MANDATORY ORkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): What models are being evaluated on the MSO-disease dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MSO"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MANDATORY ORkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): What models are being evaluated on the MIRS-disease dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MIRS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MANDATORY ORkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } ORDER BY DESC( ?value) Input(English text): Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset ? Output(Sparql query): ------------- B Input(English text): Can you provide the highest benchmark result, including the metric and score, for the SMR dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?"	False
"
 input (English text): What models are being evaluated on the Fashion-MNIST dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Fashion-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the Fashion-MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Fashion-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	--------------- RANK REF { ?score/12.1 orkgp:HAS_RANK ?stat ?score/12.1 orkgp:HAS_RANK ?score ?score. 1 } 1 Output (Sparql query): --------------- --------------- RANK DIFF orkp:HAS_DIFF # 1.5.15.1 orkgp:HAS_RANK 1.1.1 orkp:HAS_RANK # 1.5.15.1 orkgp:HAS_RANK 4.4.2 orkgp:HAS_RANK 5.7.2 orkgp:HAS_RANK # 1.5.15.1 orkgp:HAS_RANK 5.7.0 orkgp:HAS_RANK # 1.5.15.1 orkgp:HAS_RANK 1.5.0 or the final result: output (Sparql query): --------------- ------------ F. M. N. DESCRIPTION OF RESULTS RANK DESCRIPTION --------------- ------------ 1 Fashion-MNIST : 1750 1/1 orkgp:HAS_DISTINCT. 0 Fashion-MNIST : 1750 5/5 orkgp:HAS_RANK. 1 F. M. N. DESCRIPTION OF RESULTS 2 3 Fashion-MNIST : 1750 1/3 orkgp:HAS_DISTINCT. 1 Fashion-MNIST : 1750 5/3 orkgp:HAS_RANK. 2 F. M. N. DESCRIPTION OF RESULTS 4 5 Fashion-MNIST : 1750 1/5 orkgp:HAS_DISTINCT. 1 Fashion-MNIST : 1750 5/5 orkgp:HAS_RANK. 2 F. M. N. DESCRIPTION OF RESULTS the model is chosen by the paper which shows what's in each model dataset: output (Sparql query): ------------ ------------ ------------ M. P.. : Model: RANK DESCRIPTION ------- ------------ ------------ F. M. N. DESCRIPTION OF RESULTS RANK DESCRIPTION ------------ ------------ ------------ 1 Fashion-MNIST : 1750 1/1 M. P.. : Model: F. M. N. DESCRIPTION	False
"
 input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-3 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-3"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""X-Sum"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""X-Sum"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT a, metric_lbl, lbl_size , from ( where ( b = 'ROUGE-2' ) , a, metric_lbl, lbl_size ) WHERE lbl_size = > 0 , score > ROUGE-2. ROUGE-2 The third line is the actual output data, the query, but in a different format, with only the raw metrics, not the normalized ones: select a, metric_lbl, lbl_size , from ( where ( b = 'ROUGE-2' ) , a, metric_lbl, lbl_size ) WHERE lbl_size > 0 , score > 0. ROUGE-2 The fourth line contains the normalized metrics for the data. The only meaningful metric is rdfs_label, which is the label for the ROUGE-2 dataset in the text itself, and it has to be different from the one shown to make sure the result is still a valid one, since it will have changed (since the label changed). If the metrics are different when using the file format, the default file format is used: select a, metric_lbl, lbl_size , from ( where ( b = 'ROUGE-2' ) , a, metric_lbl, lbl_size ) WHERE lbl_size > 0 , score > ROUGE-2. ROUGE-2 Thus, the metric is not a separate feature in the data frame, and the variable rdfs_label is a reference to the ROUGE-2 dataset. In short, the file format has no meaning when using the default file format in R-Studio; it is a mistake to rely on the files that are provided as part of the packages. If we look at the data here in R, we know a metric exists, but we still want to understand what this metric is with the help of another metric that is just a label	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the RotoWire dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RotoWire (Relation Generation) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	-------------------------- DATED: 2014/08/24 :18:00:09-0800:00 ORTAGIN: ROOKIE: DATE_RANK_EX: 0:50 -------------------------- output (Sparql query): -------------------------- DATED: 2014/08/24 :18:00:09-0800:00 ORTAGIN: ROOKIE: DATE_RANK_EX: 0:50 -------------------------- output (Sparql query): -------------------------- DATED: 2014/08/24 :18:00:09-0800:00 ORTAGIN: ROOKIE: DATE_RANK_EX: 0:50 -------------------------- output (Sparql query): -------------------------- DATED: 2014/08/24 :18:00:09-0800:00 ORTAGIN: ROOKIE: DATE_RANK_EX: 0:50 -------------------------- output (Sparql query): -------------------------- DATED: 2014/08/24 :18:00:09-0800:00 ORTAGIN: ROOKIE: DATE_RANK_EX: 0:50 -------------------------- output (Sparql query): -------------------------- DATED: 2014/08/24 :18:00:09-0800:00 ORTAGIN: ROOKIE: DATE_RANK_EX: 0:50 -------------------------- Note from Yodelman blog post: The RotoWire dataset consists of 15,542 rows. This means that our query took 486,000 rows and yielded 8.9 seconds per query. If we take a look at the last four items in the above query, their result would have been 3,500 rows. As it turns out, even a single row taken from the same dataset can result in huge time savings when using R. We can further reduce time savings using the aggregation function as a result of the aggregation. For example, with 2,000 rows, an aggregate call could potentially consume 1,350 milliseconds. As it turns out the resulting row counts also make up to 1,500 times the size of the result set. Since we are interested in the last 20 rows, we would see our results below: output (Sparql query): -------------------------- DATED: 2014/08/24 :18:00:09-0800:00 ORTAGIN: ROOKIE	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ning et al."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Fine-Grained Gating model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Fine-Grained Gating"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ning et al."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } For more questions please check the corresponding FAQ."	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Average Return (NoOp)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ? ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value ) LIMIT 1 } }  I had to set this score to a score of 25 because of limitations with the DMSQL. I then had to use a simple SELECT on some other columns, including the index and dataset columns. As of this writing, the score of the model with the highest score is a 21. As you can see, it takes ~2 minutes and 30 seconds for the model to find the best answer, but it takes less time than a human can give (50 seconds) and requires less resource (60 MB). As I type this, it seems like there are 3 rows of the DMSQL query in a row. So how to explain this ? The model is performing at an average-performing rate (AST). There is an extra layer or layer that is performing analysis on both the scores and what's stored. The best method currently for a model to determine its best estimate is to use RDFS and a score metric. The model itself is doing an analysis, but only the scores are stored in the dataset. RDFS in Postgres RDFS is the relational database language designed by Microsoft. RDFS was originally designed for scientific applications, and as such has been"	False
"
 input (English text): Provide a list of papers that have utilized the DDQN (tuned) noop model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN (tuned) noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Prior noop model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Provide a list of papers that have utilized the Prior noop model and include the links to their code ? output (Sparql query): (1) (Select code:'DDQN (tuned) noop' as [model]: name of the model in question, link to paper with code in it, output (Sparql query):, orggp:HAS_DATASET ?dataset. model ) (Select code:'DDQN (tuned) noop' as [model]: name of the model in question, link to paper with code in it, output (Sparql query):, orggp:HAS_DATASET ?dataset. model ) (1) (2) (3, 4, 5) (5, 6, 7, 8) (8, 9, 10) (10, 11, 12, 13, 14, 15) 1. select code;  2. data  (Select code:'DDQN (tuned) noop' as [model]: name of the model in question, link to paper with code in it, output (Sparql query): Select code:'DDQN (tuned) noop' as [model]: name of the model in question, link to paper with code in it, output (Sparql query):  , orggp:HAS_DATASET ?dataset. model ) (select code:'DDQN (tuned) noop' as [model]: name of the model in question, link to paper with code in it, output (Sparql query): Select code:'DDQN (tuned) noop' as [model]: name of the model in question, link to paper with code in it, output (Sparql query):  , orggp:HAS_DATASET ?dataset. model ) 2. data  (Select code:'DDQN (tuned) noop' as [model]: name of the model in question, link to paper with code in it, output (Sparql query): Select code:'DDQN (tuned) noop' as [model]: name of the model in question, link to paper with code in it, output (Sparql query):  	False
"
 input (English text): What is the best performing model benchmarking the WMT2016 English-Romanian dataset in terms of BLEU metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the WMT2016 English-Russian dataset in terms of BLEU score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?rdfs:LABEL ?dataset ?model_lbl WHERE { ?metric rdfs:Metric } { ?stat ?dataset rdfs:Stat ?stat ?dataset ?model_lbl. ?stat rdfs:Stat ?stat %>|% ?metric_score output (Sparql query): SELECT ?model_lbl ?model_lbl WHERE { ?dataset mdata:Mixture; rdfs:label ?dataset_lbl. ?dataset mdata:Mixture; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset mdata:Mixture; rdfs:label ?dataset_lbl. ORGINALITY ?initiality; rdfs:label ?dataset_lbl. ORGINALITYORIGINALITY } } } output (Sparql query): SELECT ?rdfs:NAME ?model_lbl. |name rdfs:NAME ? rdfs:NAME rdfs:ID rdfs:KEY rdfs:VALUE ?name output (Sparql query): { ""name"": ""WMT2016 English-Russian"", ""dataset"": ""mdata_mixture "", ""metric"": ""initiality. initiality"" } Note: The word in quotes at the beginning of the function output is not an escaped HTML delimiting character, it is just the output delimiter for the output query and is necessary to prevent the database parser to ignore this special character. We define a new RDF vocabulary, to hold the model benchmarking query and a dictionary mapping all the metrics from all the model benchmark results to their WMT2016 equivalents. Let's call these WMT2016 metric dictionary, in order to not confusion with the WMT2016 metric dictionary, published here. We then set a default metric dictionary in RDF that is available as the WMT2016 metric dictionary dictionary, which includes all the metrics from WMT2016 as well as our English-Russian metric dictionary. The WMT2016 metric dictionary contains four fields for each metric: field Description metric1 An RDF field ( RDF, RDFa2 ) defining the metric for WMT2016. metrics_1 A"	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy (%) score when benchmarked on the Oxford-IIIT Pets dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT TOP(dataset_lbl, 1) ?output_lbl. ?output_lbl rdfs:label ?output_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ORDER BY DESC( ?output_lbl) ) { ?output_lbl. ?output_lbl rdfs:label ?output_lbl. rdfs:count :output_lbl. TOP(dataset, lbl_ncol(output_lbl)) } output (Sparql query): SELECT TOP(dataset_lbl, 1) ?output_lbl. ?output_lbl rdfs:label ?output_lbl. rdfs:count :output_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ORDER by :output_lbl. output_lbl is an array of 2 dimensional array (where each diagonal entry is a row). Each entry has the following properties. - row[1] The row number. If it is null, it means the row was the last loaded. - column[1] The column number. It defaults to 1. The column numbers are 1 in upper case and 6 1-5 in lower case. The row in columns 1-3, 4-6 is the same. The second column in output_lbl is the actual model. If you are going to use the model and not only load it, you have two options: 1. You do not load the model, but only load another models. or 2. You pass the model number to a separate function. That way you can load multiple models, load the model, load another models and finally you will get the model, but it is loaded first. It is a bit complex, so for the moment I will use the second approach. You load models in the same way as Load, with the order of loading being different. The only difference is that when doing a.load() function, the models are stored in separate dictionaries, which can contain either dictionaries or a single model, depending on the load function. There is a lot of math and data for the model. If you are into data you will probably want an online Python tutorial, this one is pretty short.	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Reuters En-De dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters-21578 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT journal FROM ?benchmark WHERE journal. ?format = ?benchmark. ?dataset [ ?format ] orkgp:HAS_DATASET ?dataset. AND journal. ?dataset [ ?format ], ?benchmark ORDER BY journal. ?format ORDER BY journal. ?dataset In case, when the journals had different publication biases, then it seems like there can be a conflict between the journals in the benchmarking, because the first approach by using the standardized publication database is more appropriate, because for every journal in the benchmarking, the publisher of the journal has published at least two journal, so any journal publishing at a higher average of ""standardized Publication Data"" has been published more than twice (and therefore the journal which claims to have published that often have not published the journal at a high enough standard ). The second approach was also performed by: using the average of the ""Standardized Publication Data"" for the ""Standardized Publication Dataset"" for each journal in the benchmarking: SELECT journal FROM ?benchmark WHERE journal. ?format = ?benchmark. ?dataset [ ?format ] orkgp:HAS_DATASET ?dataset. AND journal. ?dataset ["	False
"
 input (English text): What models are being evaluated on the Penn Treebank (Word Level) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model * ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } # set up a project to try it # you probably want to wait until it is running before running it # the first time before using the results of the benchmarks section run project_name.py # if you see a message like ""ERROR UNKNOWN: "" the project did not pass your tests if run.executable:... Output: # 1 - 4: 3.56e-10 # 2 - 6: 2.95e-7 # 3+: 1.56e-11 I've set the output to 1 (that's what the script is telling me), but you could of course do anything you want. I'm just putting the text and what's on screen in here because it looks really interesting. Hopefully, you will find yourself a good project. Here is the sample project I came up with. This program generates a tree based on the Penn Treebank (Word Level) dataset. Its primary purpose is to evaluate your training data. It generates the tree using a pre-programmed set of parsers and does a bit of heavy lifting to make sure the tree is correct. The program is running on Windows XP and it works best from the command line. I've made an example project for it available which can be found with the project directory in the project file (if you download a binary: python... python project/example/my-treebank. The sample project is available here. One of the nice features of Python is you can easily change the command line interface. Here's a small change to the command line for the output of this example. I've added '.' as the first character, as it is the only character you don't want to keep after the final statement. The script uses some simple pre-"	False
"
 input (English text): Provide a list of papers that have utilized the Feedback Transformer (4 layers) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Feedback Transformer (4 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the ANODE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ANODE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT IDENT( ?model_lbl ) ?model FROM ?datasets. ?table. ?table_id OR ?table_schema OR ?table_db OR ?table_user_id OR ?table_group_id OR ?table_dist INNER JOIN ?models. ?model_lbl. ?model_schema ?table_id. OR ?table_schema OR ?table_db OR ?table_user_id OR ?table_group_id ?model_lbl. ?model_schema OR model_lbl. ?model. OR model_schema OR model_lbl. ?model_schema OR orkgc:Model OR kgc:Schema; OR kgc:Model OR kgc:Schema. orkgc:Design OR kgc:Design OR kgc:HAS_DATASET OR kgc:HAS_BENCHMARK OR kgc:HAS_DATASET OR kgc:DESIGN; orkgc:Schema OR design. orkgc:Design OR 5) Data mining: input (English text) : Provide a list of publications that have used a specific data mining package ? A table of all data used in each research report, by year ? output (Sparql query) : SELECT id, first_id, last_id, author, code from papers. ""Data Mining: - ?author OR ?first_id OR ?last_id, - ?code OR ?first_id OR ?last_id, - ?name OR ?codename OR ?year OR ?genre OR ?page_number OR ?number_range OR ?date_range OR ?page_number OR ?number_range OR ?data"" input (English text): Provide a list of publications that have used a specific data mining package and include their output files ? output (Sparql query): SELECT id, first_id, last_id, first_id FROM papers. ?first_id OR ?first_id, ?first_code OR ?first_id. ?first_id OR ?first_number. ?first_id OR ?first_count OR ?first_id. ?1st_code OR ?1st_first_code OR 1st_number. ?1st_first_code OR 1st_page_number. ?1st_name OR ?"	False
"
 input (English text): Provide a list of papers that have utilized the DY-MobileNetV2 ×1.0 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV2 ×1.0"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV3-Small"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV3-Small x1.0"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Input (language): Provide the name of the source of the language model ? In English, we can include only the source of language that is required in the input (ie., the language model name in the input should be the source of the language model used in the result). Input (language name): SELECT DISTINCT ?code FROM ?model_lbl WHERE!( ?language_model_id=model_lbl.id ?model_lbl.code ?column.column) OR ?language_model_id=model_lbl.code OR ?language_model_id=model_lbl.code ; Output: SELECT ?code FROM ?model_lbl WHERE!( ?language_model_id=model_lbl.id ?model_lbl.code ?column.column); Output: SELECT ?code FROM ?model_lbl WHERE!( ?language_model_id=model_lbl.id ?model_lbl.code ?column.column); You can supply as many languages as you wish: Input (language): Provide the name of the source of the language model ? In English, we can include only the source of language that is required in the input (ie., the language model name in the input should be the source of the language model used in the result). Input (language name): SELECT DISTINCT ?code FROM ?model_lbl WHERE!( ?language_model_id=model_lbl.id ?model_lbl.code ?column.column) OR ?language_model_id=model_lbl.code OR ?language_model_id=model_lbl.code ; Output: SELECT ? code FROM ?model_lbl WHERE!( ?language_model_id=model_lbl.id ?model_lbl.code ?column."	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the FB15k dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FB15k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT COUNT(*) 2,3 FROM {FSNS Test, FSNS Metrics, FSNS Features} ORDER BY {COUNT(*)/2} DESC input (English text): Can you provide the highest benchmark result, including the metric and score, for the FEIS datasets ? output (Sparql query): SELECT COUNT(*) 2,3 FROM {FSIES FEIS, FEIS Metrics, FEIS Features} ORDER BY {COUNT(*)/2} DESC input (English text): Can you provide the highest benchmark result, including the metric and score, for the FES datasets ? output (Sparql query): SELECT COUNT(*) 2,3 FROM {FSIES FES, FFES Metrics, FES Features} ORDER BY {COUNT(*)/2} DESC input (English text): Can you provide the highest benchmark result, including the metric and score, for the LFS datasets ? output (Sparql query): SELECT COUNT(*) 2,3 FROM {LSF LFS, LFS Metrics, LFS Features} ORDER BY {COUNT(*)/2} DESC input (English text): What is the highest benchmark result achieved on the LFS data ? output (Sparql query):  SELECT COUNT(*) 2,3 FROM {LSF LFS, LFS Metrics, LFS Features} ORDER BY {COUNT(*)/2} DESC import numpy import matplotlib.pyplot as plt # Input import numpy as np # Input in CSV, CSV + JSON data import sqlite3 from os import path # input to get_dataset (from the file table-table-table.csv ) import matplotlib.pyplot as plt # Output import sqlite3 # output in SQL, CSV + JSON def get_data ( filename, columns= 2 ): # input data to get dataset = sqlite. query ('select * from csv files', filename ) # input to get_features (from the query with column name (1) as metric ) # output model results, in CSV, CSV + JSON models = [] # input to get_metrics (from the query with column name (2) as metric ) # output model results, in CSV, CSV + JSON # NOTE : if column name (2) is missing in the query, then this is # still an	False
"
 input (English text): Where can I find code references in papers that have used the MP-EB model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MP-EB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the MPAD-path model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MPAD-path"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" {... ""model_lbl"" : ""MP-EB"",... } Note that the MP-EB model is compatible with the MP-HAS models: in particular, it is also compatible with the MP-HAS-dynamic-path (as used in the MP-HAS-dynamic-path for training the K-NN models as well) and MP-HAS-dynamic (as used in the MP-HAS-predictor-DYN.jl model used in the MP-AD models). In some cases the difference between the MP and MP-HAS models is very fine (to the point of being even insignificant) and is quite difficult to measure without testing them both directly against each other. For a more specific example of a problem that can be solved in the MP-AB model with a different data approach, let us consider that when we want to use model prediction using the MP-HAS model we want something resembling (without having any special assumptions about) a simple Bayesian posterior distribution over a fixed number of training data. It is not obvious from the example above whether this is an instance of the problems presented above or is an accident caused by the choice of training data. Indeed, in order to use something like the MP-HAS model with the MP-AB model, one should be able to model a prior distribution over training data (let us write it as a Bernoulli distribution); using the MP-HAS-dynamic-path model with the MP-AB model, one cannot. This example also illustrates two important points: first, although using the MP-HAS model with MP-AB model results in an error less than 3% of the training data variance, as one can see from example 2, this result is not actually representative of the error the MP-HAS model will have on the final score (a typical case where MP-HAS model error results in a deviation near 0.01 (or less) of the full data variance). Instead, we can have a model with a very high training error (1/10 of a normal distribution) and a very low final score - in which case we can model a normal distribution over a more or less fixed number of training example dataset; we are simply testing for a non-trivial amount of the error. On the other hand, with MP-AB model (and even with MP-HAS), this is not practical as the MP-AB model"	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the BoolQ dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BoolQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the BoolQ dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BoolQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 ----------- ------------ Name User Sender Rank Accuracy_score ------------ 1 SPSS A 2.4x4 1st 0.097 1 SPSS A 2.34x3 2nd 0.066 2 PSFG A 2.9x2 10th 0.026 3 SPSS A 3.3x1 20th 0.001 4 SPSS A 3.34x2 60th 0.0035 5 SPSS A 3.4x3 80th 0.0007 6 SPSS A 3.5x2 100th 0.0001 7 SPSS A 3.7x1 100th 0.00008 8 SPSS A 3.6x3 100th 0.0002 9 SPSS A 3.9x3 150th 0.0003 10 B-SPSS B 2.95x3 200th 0.0004 input (English text): Which models in terms of the average accuracy score when benchmarked on the BoolQ dataset ? Output (Sparql query):  | | 1 SPSS A 2.4x4 3.9x3 Input (English text): Which models in terms of the accuracy score for Boolsquared for the testset 1 ? Output (Sparql query):  | | B-SPSS A 2.95x3 1.8x2 Input (English text): What are the models that have been benchmarked on the Boolsquared dataset ?	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC Russian-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC German-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } The results of this query are given in the tables below: I'll continue to update this post as I investigate the benchmarks. But be glad to know for now that the above are true for the BUCC dataset when all the benchmarks were ran. I have included details of benchmark runs here and can provide you with detailed step-by-step instructions on the analysis part of this post."	False
"
 input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	  output (Sparql query) < table > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < table > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < colgroup > < table > < colset > < colset > { // ---------------------------------------------- 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~ ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~      [ 1    7] [ 2    7] [ 3    7]   [ 4    7] [ 5    7] [ 6    7] [ 7    7] [ 8    7] [ 9    7] [ 10    7] [ 11    7]   [ 12    7]   [ 13    7] [ 14    7] [ 15    7]   ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~  ~ 	False
"
 input (English text): List the code links in papers that use the MEMEN (ensemble) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?code ||'' || str( ?code ) ||'' || ?c ?code || ?c ||'' || ?c ?code || ?c ||'' || ?c || ?code || ?code input (English text): List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark ? output (Sparql query): SELECT ?code ||'' || int( ?code ) ||'' || int( ?c ) ||'' || int( ?c ) ||'' || int( ?c ) ||'' || ?c || ?code; ?code || int( ?code ) || int( ?c ) ||'' || int( ?c ) ||'' || int( ?c ) ||'' || ?c || ?code || ?code|| ?c || int( ?code ); ?code || int( ?code ) || int( ?c ) ||'' || int( ?c ) ||'' || int( ?c ) ||'' || ?c || ?code || int( ?code ); orkgp:HAS_DATASET || ?dataset. ?cont || ?c ?dataset orkgp:HAS_BENCHMARK || ?benchmark. orkgp:HAS_MODEL || ?model. orkgp:HAS_SOURCE_CODE || ?code orkgp:HAS_CODE || ?code || ?code || int( ?code ); orkgp:HAS_SOURCE_CODE || ?code. ?code || ?code || ?code || ?C || ?code || ?C || ?code || ?code || int( ?code ); orkgp:HAS_CODE || ?code. ?code || ?code || ?C || ?c || ?C || ?c || ?C || int( ?code ); orkgp:HAS_DATASET. ?cont || ?c ?dataset. orkgp:HAS_BENCHMARK || ?benchmark. orkgp:HAS_MODEL || ?model. orkgp:HAS_SOURCE_CODE || ?code or	False
"
 input (English text): Could you provide a list of models that have been tested on the ImageNet 64x64 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bits per dim"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 model-score-score --------------+----------+---------+------------+---------------| (1 row(s) affected) model-score-score --------------+----------+---------+------------+---------------| (1 row(s) affected) Let's now implement the feature scoring function in Python and see what it does: >>> import numpy as np >>> import numpy as np.linalg >>> from sklearn import preprocessing. supervised_linalg >>> from sklearn.datasets import import_dtype, from_json_utils >>> x = np. arange ( 32, 2.7 ). astype ( int ). astype ( np. float64 ) model_score_score = preprocessing. supervised_linalg. feature_score ( x ) >>> y = preprocessing. supervised_linalg. feature_score ( x ) y = np. zeros ( 4 ) # for example, 32.0 + 32 = 32 features >>> from sklearn.categorical import Sum >>> x, y, z = np. random. randn ( ) [ 1, 2, 3 ] ( 1 row(s) affected) >>> model_score_score [ np. zeros ( 2, 4 ) + 4.6 : [ 1, 2, 3 ], ( 1 row(s) affected) ] = sum ( y ) # this is a sum of the model scores from sklearn 1.1.3 -- we can see both (x, y) and (y, z): >>> print model_score_score [ np. zeros ( 3, 4 ) + 3.5 : [ 1, 2, 3 ], ( 1 row(s) affected) ] [1.23, 1.09, 1.14, 1.14] The following table shows the number of scores each model returns, the number of errors (the highest score you can score any model), and the number of clusters (the strongest model with the most errors): Model Scores for Training Cluster Accuracy Error for Training (errors per model) cluster_score = model_score_score. sum ( ys ) # this column represents the number of clusters used >>> x = np. arange ( 32, 2.7 ). astype ( int ). astype ( np. float64 ) model_score_score = preprocessing. supervised_linalg. feature_score (( x )) x = ys * 7	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"------------------------- ------------------------- ------------------------- TOP 1.1.0 1.0.1 0.1.2 (95%) (7/26) In this scenario, the benchmark score is higher because the user provided the value (in this case the metric). If the user provided a raw score (for example 0.1) then the score would be lower, because there is a different metric being calculated in their case. The same is true if they provided an integer value to the function and they selected a value between 0.1 and 0.2 (since the test is not over all integers). This example shows an easy case, one where a human being doesn't know the details of the test. For an example of this in a data frame, refer to DataFrame(mockData).sql. For a better example which has more functionality, see DataFrame.SQL. It contains a number of the tests described here in a nice visual format. The following examples demonstrate that each test is actually valid : import the.sql file containing the data frames from DataFrame.SQL import the mockData module from the test file The following is a slightly modified version of the above, using the user's score as input to the orkgp function. import the.sql file containing the user's score import the mockData module from the test file We can specify the parameters that the function is used to calculate the metric by default. But you can supply any values you like by separating them with commas: import the.sql file containing the metric (using multiple input) import the mockData module from the test file The number of measurements that should be provided is indicated by the min() and max() parameters. In the table below we see the maximum and minimum scores for each metric. Note that max() and min() only work over standard deviations, therefore not for all values. In addition it does not take into account the range of values that may represent similar values (for example a score of -75 to +100). The following describes the parameter name min() ; the values it takes will be listed in the ""Tables"" section. In addition min() takes into account the range of values that may represent similar values (for example a score of -75 to +100). Min.max.stddev. {max,min,default}: The metric score. The minimum and maximum scores are a good indicator for the metric. You"	False
"
 input (English text): What is the total number of patients in the studies?
 output (Sparql query): SELECT SUM( ?number_of_patients) WHERE { orkgr:R33008 orkgp:compareContribution ?contrib. ?contrib orkgp:P15585 ?patients. BIND(xsd:integer( ?patients) AS ?number_of_patients) } 
 input (English text): Are children examined in the studies?
 output (Sparql query): "	"ASK { orkgr:R44979 orkgp:compareContribution ?contrib. ?contrib orkgp:P23161 ?population. FILTER(REGEX( ?population, ""ildren"")) } "	SELECT SUM( ?value0 ) FROM ( SELECT SUM( ?value1 ) FROM ( SELECT SUM( ?value2 ) FROM ( SELECT SUM( ?value3 ) FROM ( SELECT SUM( ?) ) ) AS orkmgr; OR -------------------------- ORDER BY orkmgr ORDER BY ORkmgr ORDER BY orkmgr ORDER BY orkmgr Note that we used a new 'orkgr' function instead of 'ork' in the original query because the old function was already implemented in the ORK package (see ORK documentation for details): function orkgr() { def compareA(value: Float, k: Float): Float return if(k > value orkgr() == k): return k } A simple query using ORK for a range in one line is: ORk.compareContribution([A0, A1], 5).b Here we can see that we can use any of the two parameters, A0 or A1, to change the comparison value. Note that ORK will perform the same thing for all parameters and there is no need to explicitly specify the number of comparison values you want to output. Comparing multiple values Some libraries provide functionality to allow the user to compare multiple values. Some examples include: float2, float3, float4, float5, boolean4, boolean5, integer2, integer4, integer5, random2, random4, random5, string, long, scalar4 In an ORK query we can simply use ORK.magg to get a comparison parameter, which is used to apply the given function to any two values. However, the above example could easily be improved by wrapping the value in a Float or Long so that it can be passed in as part of the comparison parameter; however, a simple float2 comparison shows that there are no limitations in this case. ORK also supports comparisons that use a map of functions, using the orkgr and orklgr functions. In this case you can wrap all the parameters you want to give us in such a map, so that they can use the methods you gave us (i.e. you can pass them a Map of Function objects to get exactly the methods you want by reference: a Map of Function1, a Map of Function2, a Map of Function3, a Map of Function4 ).	False
"
 input (English text): What is the highest benchmark result achieved on the OpenBookQA dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OpenBookQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the CoQA dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT { ?value > 1 ?value > 1000 ?value > 10 ?value > 100 ?value > 20 ?value > 100 ?value > 1500 ?value > 10 ?value > 100 ?value > 10000 ?value > 10 ?value > 10 ?value > 10000 ?value > 20 ?value > 100 ?value > 5000 ?value > 10 } ORDER BY ?value. input (English text): What is the highest benchmark result achieved on the YQL dataset, including the metric and its value ? output (Sparql query):......................................... ------------------------------------------- SELECT [ ?max, ?min, ?avg, ?sd^.2, ?min^.2,- ?avg^.2, ?sd, ?min ],....... { [ ?label], [ ?value],... } ;................................... input (English text): What is the highest benchmark result achieved on the YQL dataset, including the metric and its value ? output (Sparql query):......................................... { 1, 1, 10, 10, 0, 5, 5, 5 }, input (English text): What is the highest benchmark result achieved on the GMSK dataset, including the metric and its value ? output (Sparql query):......................................... -------------------------------- --------- SELECT [ ?name, ?age, ?number, ?height (inches), ?width (pounds), ?height (inches), ?width (pounds), ?weight, ?percentile, ?mean, ?std],....... { [ ?date, ?date_from, ?date_to,... ],....... [ ?range], [ ?value], [ ?value_as_percent,... ] ;................................... input (English text): What is the highest benchmark result achieved on the AED dataset, including the metric and its value ? output (Sparql query):......................................... ----------------------------------------------------- SELECT [ ?birthyear, ?birthdays, 0, 0, 0, 0, 0, 0, ?birthday ],....... { [0], [0]	False
"
 input (English text): What is the top benchmark score and its metric on the ImageNet 64x64 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the ImageNet V2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT CASE WHEN ?dataset_lbl = ""ImageNet 64x64"" THEN 1, 0 WHEN ?dataset_lbl = ""V2"" THEN 1 } WHEN ?dataset_lbl = ""MULTIPLIER"" THEN 1, 0 LABEL: ?dataset_lbl } } ------- Select scores - 6,014,731 --------------- Benchmark Score: 585,067,832 ---------------------------- -------- --------- V2 Score: 7511 ---------------------------- Source: http://datasets.image-net.org/results_v2.shtml This query is the result that the result set that we can run. In the above query, we added the name of the dataset, where the benchmark score is specified as 585,067,832. We can see that we had a benchmark score of 585,067,832. In the next query query we need to select the name of the dataset, and get the list to choose the dataset, we want to choose only one record. We use the following option : source_link. Now we use a nested query that is a nested select. With the nested query, we select the single line, where the benchmark score is specified as 585,067,832. We can see the data that we have selected. Notice how we select the first data point for this nested select. We can see what can be seen in the result. In the second query, this input is now given out for us but also we have some additional option in that we can add a column to our query. This column in the second query can display the name of the dataset. We can now run our query on this input as a new column. output (Sparql query): SELECT name FROM name_dataset WHERE name_lbl. ?""datacopy"" = 'ImageNet V2 dataset'; This query selects the name of the data. In the third query, we can see that this is the output we can expect. The final query would look like this. In the above query, we can see that this query selects the names of the datasets. "	False
"
 input (English text): What is the highest benchmark result achieved on the Reacher, easy (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Score score on the Reacher, easy (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) FROM ?dataset where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k) ( ?target-score) "") ?target-score. ?metric rdfs:label ?metric_lbl. OPTIONAL { ?metric ?metric_lbl. } } ORDER BY DESC( ?value) } ?max_score ? : the total score reached over all benchmarks across all queries in this test. input (English text): What is the highest benchmark result achieved on the Reacher, easy (DMControl500k) dataset, including the metric and its value ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k) ( ?target-score) "") ?target-score. ?metric rdfs:label ?metric_lbl. OPTIONAL { ?metric ?metric_lbl. } } } ORDER BY DESC( ?value) } ?max_score ? : the total score reached over all benchmarks across all queries in this test. input (English text): What is the highest benchmark result achieved on the Reacher, easy (DMControl500k) dataset, including the metric and its value ? output (Sparql query): SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?"	False
"
 input (English text): What are the models that have been benchmarked on the Quasart-T dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quasart-T"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Quasart-T benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quasart-T"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?model GROUP BY ?model_lbl; select * from ""quasart-t"" where model_lbl select model_lbl in outputs, select model_lbl as result, select model_lbl as model, select model as output, select result, select model_lbl as result_, select result, output_ ) Output: model model_lbl Here is the same, but with a ""dataset"" name prefix instead of a ""model"": select * from ""quasart-t"" group by label select title_citable in outputs, select model as result, select model as result_, select title as result__ select title, title_citable. outputs -------------- ---------- ------------- ---------- ?model -------------- ---------- ?dataset ------------- ---------- ?model_lbl ------------- * --------------------------- ?dataset_lbl ----- -------------- ?model_lbl -------------- ?dataset --------------- ---------- ?model_lbl ------------- ?dataset ------------- ?dataset ------------- ?dataset ------------- ?dataset_lbl ------------- ?dataset ------------- ?dataset ------------- ---------- ?model_lbl ------------- ?dataset ------------- ?model ------------- ?model_lbl -------------- ?dataset -------------- ?dataset _lbl ------------- ?dataset ------------- ---------- ?dataset_lbl ------------- ?dataset ------------- ?dataset ------------ ---------- ?model_lbl ------------- ?dataset ------------- ?model ------------- ?model_lbl --------------- ?dataset _lbl --------- Now you've seen how to combine model-model and dataset-dataset benchmarks. In this example, you can see that the results of the dataset-to-model benchmark are always identical to those of the dataset-to-dataset benchmark. One thing you should understand about batch tests is that they must be executed by a single computer at a time. If you try to run two of them on the same computer, they won't work."	False
"
 input (English text): Can you list the models that have been evaluated on the CINIC-10 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Multimodal PISA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Multimodal PISA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT model, benchmark, orkgp, orlstm, sum(score) andlstm_total, sum(score) CINIC_10_pisa_multimodal. output (Sparql query): select model, benchmark, orkgp, orlstm, sum(score) CINIC-10_pisa_multimodal. CINIC_10_pisa_multimodal CINIC_10_pisa_multimodal (Multimodal score) input / output 1 - test model 0 - test model (benchmark) input (English text): CINIC-10_pisa_multimodal. output 1 - test model 0 - test model (benchmark) input (English text):. output (Sparql query) / output 1 2 3 4 5 6 7 8 9 10 11 12 13 function dbi ( input ( English text ) ) { var output = output. toString ( ) ; for ( var model = 0 ; model < 1 ; model ++ ) ( if ( function ( ) { var score = score || ( dbi ( model, ""multimodal"" ) ) ; if ( model >= 0.5 ) { var benchmark = ( score > ( 100 × benchmark ) ? 1 : 0 ). filter ( ( rownumber > 1 ) ? function ( dbi ) { return ( score > ( 100 × benchmark ) ? 1 : 0 ). map ( ( rownumber - 1 ) % benchmark ) ; } : function ( dbi ) { return benchmark; if ( testbench == db. testbench ) { dbi ( model, ""multimodal"" ) ; var score = score || ( dbi ( model, ""normal"" ) ) ; if ( score > ( 100 × benchmark ) ? 1 : 0 ). filter ( ( rownumber > 1 ) ? function ( dbi ) { return ( score > ( 100 × benchmark ) ? 1 : 0 ). map ( ( rownumber - 1 ) % benchmark ) ; } : function ( dbi ) { return benchmark; if ( testbench == db. testbench ) { dbi ( model, ""normal"" ) ; } } ; }, ""testModel"" ) ; } } ( ) ; } ) ; // CINIC-10_pisa_multimodal CINIC_10_pisa_"	False
"
 input (English text): What models are being evaluated on the Barabasi-Albert dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Barabasi-Albert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the Barabasi-Albert dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Barabasi-Albert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 input (English text): Which benchmark would you like output to be ? output (Sparql query): input (English text): Which model would you like output to be measured against ? output (Sparql query): Orkgp:HAS_DATASET ?dataset_lbl input (English text): Which benchmark would you like output to be measured against ? output (Sparql query): Orkgp:HAS_Evaluation ?benchmark orkgp:HAS_EVALUATION ?eval. input (English text): Which model would you like output to be measured against ? output (Sparql query): Orkgp:HAS_BENCHMARK ?benchmark [ ?dataset_lbl ] input (English text): Which benchmark would you like output to be measured against ? output (Sparql query): Orkgp:HAS_PATTERNS ?dataset_lbl input (English text): Which benchmark would you like output to be measured against ? output (Sparql query): Orkgp:HAS_STATISTICS_REFERENCES ?dataset_lbl input (English text): Which benchmark would you like output to be measured against ? output (Sparql query): Orkgp:HAS_LIMITS _all ?dataset_lbl input (English text): What metrics used to evaluate models on the Barabasi-Albert dataset ?  output (Sparql query): input (English text): What metrics were used for benchmarking/evaluating models on the Barabasi-Albert dataset ? output (Sparql query): Selecting metrics to compute/evaluate on the Barabasi-Albert dataset: This is the same query using the additional option -matches=True. It should produce the same output for the two queries: This may be useful in selecting metrics that perform some computationally heavy work in the model evaluation process. Example: dataset_lbl is a table on the data model: Orkgp:HAS_DATASET with a unique identifier dataset_lbl is a table on the data model: ORkgp:HAS_EVALSET with a unique identifier For these queries to work with the existing data source (Tables 10, 11, 12 & 13	False
"
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Breakout dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Zaxxon dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } For all data that is not pre-loaded from a data source, please refer to the DATA_NOT_IN_DATASET (INCLUDED) header. In the following tutorial, we have developed a simple program that utilizes the built-in Atari-specific ""L1 or Largest"" memory management strategy, which reduces memory usage by over 60%. It is a simple program that implements the memory management logic within the Atari 2600 Breakout API, and the ""LEARN DYNAMICS"" tutorial is included at the end. The purpose of the memory management logic was inspired by the well-known Atari-specific memory manager, which is found in all of the Atari 2600 Breakout games. In this tutorial, we'll look at the ""Tutorial"" of the memory management logic, which is embedded in the ""LEARN DYNAMICS"" tutorial. Please note that this tutorial is only relevant for the Tutorial. The ""Tutorial"" of the memory management logic is implemented in several pieces within the BASIC code and the ""LEARN DYNAMICS"" tutorial is not included there (because the main ""Tutorial"" is not included as part of the BASIC code, but rather requires the installation of a separate tutorial library). Also, as mentioned above, please use the ""Tutorial"" of the memory management logic only for the tutorial and the ""LEARN DYNAMICS"" tutorial for the actual implementation of the memory management logic. We will look at the BASIC code of a few important memory operations in detail, including the ""Largest"" memory operation. Memory operations in general are a way of sharing two memory pages, which are mapped from different regions of the memory (e.g., the address space to the rest of the memory). These operations are not exclusive; in other"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Spanish"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?test ?test_metric AS ?test_lbl (1 row(s) affected, 1 row(s) unreachable) ORDER BY DESC( ?value) AS ?test_lbl. ?test_lbl DESC LIMIT 1 : SELECT ?test ?test_metric_lbl AS ?test_lbl (1 row(s) affected, 1 row(s) unreachable) ORDER BY DESC( ?value) AS ?test_lbl. ?test_lbl DESC LIMIT 1 : Select ?metric ?metric_lbl AS ?test_lbl (1 row(s) affected, 1 row(s) unreachable) FOR EACH ROW EXEC tbl:LOWER ( ?metric_lbl) ORDER BY DESC( ?value) DESC LIMIT 1 output (Sparql query): Select ?value ? Metric ?metric rdfs:label ?metric_lbl.. value ? Metric ?metric_lbl. ORDER BY DESC( ?value). Output. If you're using the SQL version of the command (or other SQL that supports the same query language as above), you need to add the following code: -- UPDATE SET test = rdfs:lbl_get_test. UPDATE SET test_lbl = rdfs:lbl_get_test_test_lbl. UPDATE SET target = rdfs:lbl_get_test_target. UPDATE SET target_lbl = rdfs:lbl_get_test_target_lbl -- SET test = rdfs:lbl_get_test. SET test_lbl = rdfs:lbl_get_test_test_lbl. SET target = rdfs:lbl_get_test_target. SET target_lbl = rdfs:lbl_get_test_target_lbl In this example, I'm using a query that will only work for English texts (so I'll use a numeric key for all the non-	False
"
 input (English text): Indicate the model that performed best in terms of BLEU metric on the WMT2016 English-German benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT * FROM myproject WHERE rdfs:label ?metric. BEST_MINOR_DATA RDFaSparql output (JSON) SELECT * FROM myproject WHERE rdfs:label ?metric. BEST_MINOR_HIGH_HIGH RDFaSparql output (JSON) SELECT * FROM myproject WHERE rdfs_label ?metric_lbl. BEST_MINOR_HIGH_INVITATION_DATA RDFaSparql output (JSON) SELECT * FROM myproject WHERE ?label. BEST_MODEL RDFaSparql output (JSON) SELECT * FROM myproject WHERE ?model. BEST_MODEL_BONUS_DATA RDFaSparql output (JSON) SELECT * FROM myproject WHERE ?model. ?value. BEST_MODEL_BONUS_HARD_DATA RDFaSparql output (JSON) SELECT * FROM myproject WHERE ?model. ?value_c. BEST_MODEL_WARD_DATA RDFaSparql output (JSON) SELECT * FROM myproject WHERE ?model. ?value_u. BEST_NOTES RDFaSparql output (JSON) SELECT ?table ?[:len]-> ?metric_lbl. ORDER BY DESC( ?values ) LIMIT 1 BEST_RUNS RDFaSparql output (JSON) SELECT * FROM myproject WHERE rdfs:label ?metric., ?dataset_lbl. LIMIT 1 [ { ?dataset_lbl. 1: ""Sydney Symphony Orchestra"" }, { ?dataset_lbl. 2: ""Sydney Symphony Orchestra"" } ] BEST_TIMESCREEN RDFaSparql output (JSON) SELECT * FROM myproject WHERE ?label. BEST_WASTEKEEPER RDFaSparql output (JSON) SELECT * FROM myproject WHERE ?label. BEST_WASTEKEEPER_NON_MIL RDFaSparql output (JSON) SELECT *"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Information Extraction research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Information Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Sentence Classification research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Sentence Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 input (English text): Provide a dictionary for describing sentences ? output (Sparql query): input (English text): Provide a dictionary for describing sentences ? output (Sparql query): input (English text): A set of language files ? output (Sparql query): A list of available language files ? input (English text): A set of language files ? output (Sparql query): A list of available language files ? input (English text): A set of language files ? output (Sparql query): A list of language files ? input (English text): A set of language files ? output (Sparql query): A word list, sorted alphabetically, of phrases of words in sentences ? input (English text): A set of language files ? output (Sparql query): A word list for a set of text files ? input (English text): A set of language files ? output (Sparql query): A word list for a set of language files ? input (English text): A set of language files ? output (Sparql query): A set of language files ? input (English text): A set of language files ? output (Sparql query): A set of language files ? input (English text): A set of language files ? output (Sparql query): A word list for a set of language files ?'A set of language files ? input (English text): A set of language files ? output (Sparql query): A set of language files ? input (English text): A set of language files ? output (Sparql query): A set of language files ? input (English text): A set of language files ? output (Sparql query): A word list, sorted alphabetically, of words in sentences ? input (English text): A set of language files ? output (Sparql query): A set of language files ? input (English text): A set of language files ? output (Sparql query): A set of language files ? input (English text): A set of language files ? output (Sparql query): A set of language files ? input (English text): A set of language files ? output (Sparql query): A set of language files ? input (English text): A set of language files ? output (Sparql	False
"
 input (English text): List the metrics that are used to evaluate models on the CUB-200-2011 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" To get data in R, the CUPM is a good candidate. The CUPM is a free, open source distributed reporting environment that has grown in popularity as the industry seeks answers to hard data problems. The R CUPM version is in-depth and thorough enough to handle the most common data queries. This includes many of the common problems, such as grouping, correlation, correlation with, model comparison, regression, prediction etc. You can use the tools provided with R CUPM (R CUB-200-2011 or R CUB-201-2011) to prepare your reports for submission to CUB-200-2011 (R CUB-200-2011). The files are hosted on CUB-200-2011 download page of the CUB website. R CUB-200-2011 uses a CART (Comma Separated Value) format for a report format. This format is the same as that used in the Hadoop community. To obtain data with R CUB-200-2011, the format is provided for use as follows: # get data.set(metrics="""", file_name=""metrics.csv"") for x in csv.reader(self.format.parse(metrics)) do file = csv.reader(x)""#{filename}_metrics.csv"" [ x.join("" "").groupby(x.column(0)) ] Note: if you are using 'raw_csv.csv', then the csv files must be in the same directory. The R CUB-200-2011 provides data formats that are compatible with many other systems. This includes Oracle (SQL Server / MySQL / PostgreSQL), HP-UX, Open Source, and proprietary systems. You can download reports for some of these systems. Other report and user tools The CUPM, however, can have a number of other external tools built-in. These are shown on the tools menu: data: Download complete reports for the entire dataset. metrics: Calculates the quality scores for various metrics from a subset of metrics, such as weighting, clustering etc. metrics_lbl: Calculates the quality scores for each metric from a subset of metrics. test: Run the test suite. To include specific models and their metrics, this may be useful as it can compare different models on the same dataset. report: Uploads report files to the CUB website."	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-German dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Italian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 The benchmark option accepts a comma-separated values as string. There are some options which can be used in addition to the above, such as: ?default orkgp:USE_LIGHT_TRIM orkgp:SET_LIGHT_TRIM ?specification ?specification_lbl in the second argument of :test(...,...) above, the second argument can be any MLDoc Zero-Shot English-to-Italian dataset. ?output This can be used as a shorthand for :benchmark, but will use the name of the output file (e.g. :benchmark.txt ) to avoid clashes with standard file names, and will output in the format specified by the :output option. Here's a little usage example showing how to do this: The usage of :benchmark The benchmarks option is often the most important parameter in your test setup, depending on the size of the data set you're aiming to benchmark. There are two ways in which these parameters can be used. The basic usage of each of the :benchmark key option is as follows: The usage of the :test(...,...) keyword option can be done as follows: As described above, each of these keys have two arguments: :parameter and :metric. The parameter will have an argument containing a string of the form :value. In the case of :metric, that argument will have the format :value. This is the most helpful form in terms of using the key as a standard argument, but you can also use any standard key string in place of a string. The :output keyword argument is only valid where the.json file is used as the format for the benchmark. For example: (define (run-model benchmark) (require :json) (run-model (run-model :sport))) The main difference from :test(...) are the order and the location in the :output and :parameters keys. The order of the parameters is important. The use of the :benchmark keyword option in the execution plan of the test can be set as follows: By default, when using :output, :output/json will be used to generate the benchmark data, followed by a test plan (including file name and time stamps). This option can override that default behavior. The usage of the :testKeyword option can be used in the execution plan of the test: The usage of the	False
"
 input (English text): Can you provide links to code used in papers that benchmark the TokenFuser model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TokenFuser"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Tokenlearner model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tokenlearner"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT a,b,c,d,e,f,g,m,l,m,n,n,t FROM ?TABLE-IN-PROFILING t INNER JOIN ?LINKS	False
"
 input (English text): What models are being evaluated on the Pubmed dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the MedSTS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbr WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbr. FILTER (str( ?dataset_lbr) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval;   output (Sparql query):  SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; } { ?dbo orkgp:CHECK ?label. output (Sparql query):  SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; } { ?dbms orkgp:CHECK ?label. output (Sparql query):  SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; } { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; } output (Sparql query): Outputs are also available in the following formats: CSV, RDF, or JSON. The datasets are available as well in the original format, but the output is in the JSON format for ease of reference."	False
"
 input (English text): What is the best performing model benchmarking the CoNLL04 dataset in terms of NER Micro F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NER Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT...;...; SELECT c. ?mfp:HAS_MFP;...; c  |  0 0.0025848080 -0.0025851550 +0.000261845 +0.000261890 0.00012375 +0.00001125 0.00004115 -0.000004125 0.000004125 0.000004125 0.0071185 +0.000004145 0.0071315 -0.007151520 0.0031515 +0.000000945 2.00955065 0.004151540	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the UCF101 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TOP (ORDER BY name)   ?p ?p_l  - ? p_p  - 1, ?l ?l  - ? l_l  - 1); To obtain a list of research papers that have performed benchmarks on the UCF101 dataset, we run the following two queries: input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the UCF101 dataset ? output (Sparql query): SELECT ?benchmark orkgp:HAS_BENCHMARK ?benchmark  , ?paper orkgp:P31  - ? paper_l   ?l, ?benchmark ?p ?p_l, ?benchmark  || orkgp:HAS_DATASET ?benchmark  |  ( ?l ?l  - ?r  - ?t | ?r  - ?b) input (English text): Provide a list of research papers along with their titles and IDs, that have performed benchmarks on the UCF101 dataset. output (Sparql query): In this query, we have to search for all papers that have performed benchmarks on the UCF101 dataset that are not covered by the output document. The output is identical except that the output is split into two parts: a part for papers that perform benchmarks and a part for the benchmarking data that is made public. That said, the output of the above queries is still different from the output of the search query:  select name,  dataset,  benchmark from  (SELECT TOP (ORDER BY name)   ?p ?p_l  - ? p_p  - 1, - ? l ?l  - ? l_r  - 1); This is a fairly standard way to search for documents in a document catalog—that is, a document collection of data. It requires a simple program to run. As a result, these queries may not work	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 Banks et. al. [2013] provide a simple but highly efficient query using only the most salient and general questions in the literature and an unstructured data structure. In this study, we also provide an improved query using information about a given paper in the reference section of the Abstract or Technical Paper (see Supporting Information for an example). Thus, only the most relevant and general questions are returned from a search. Results and Discussion Let us consider a typical case with both results and conclusions from an analysis of the papers published in the last 10 years (see Supporting Information for the full list). In this figure, we also display the results from one of the original surveys from 2002 and 2003 by the International Consortium of Journalists (ICJ): Figure 1A, 1B, 2A, 2B, 3A, 3B The original survey. Figure 1C plots the results of this survey again: Figure 2A displays the overall list of researchers (with ID). The top 5 in the list are: Pappas et. al., L'Ecole Nationale d'Analyse des Sciences de l'Université de Marseille (L'E) (2008). (5 papers) [ citation ] Zhang & Liu, S. L. et al., A. D. et al. [2013] Pappas, J. et al. [2003] Pappas, B., & L. Liu, S. L. (2008) [paper] Zhao, K. P. et al. [2013] Pappas, J. et al. [2003] We discuss the findings of our query in the rest of the paper. We find that Pappas & Liu are the top 5 in terms of quality, although some of the others in the top 5 may just have been randomly chosen. It is also possible that these results are biased because the researchers' names do not start with Pappas or Liu, or because the top 5 of the Pappas & Liu list all do the same research. In reality, they can be any of a large number of researchers or universities in the USA and Europe. For example, these two institutions, the Massachusetts Institute of Technology (MIT) and Yale University (Yale), are in the top 4 of our list. On the other hand, most researchers are in the top 5 in other institutions in the top 5. A search across several fields (such as statistics, biostatistics, computer	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CvT-W24 (384 res, ImageNet-22k pretrain) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-W24 (384 res, ImageNet-22k pretrain)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the ImageNet + iNat on WS-DAN model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ImageNet + iNat on WS-DAN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Would you like to test a simple model that uses only ImageNet data in the first post-condition ? output (Sparql query): input (English text): Can you provide links to code used in papers that benchmark the iNat on WS-DAN model and compare the results of a different pretrain ? output (Sparql query): input (English text): Can you provide links to code used in papers that benchmark the W24 model ? output (Sparql query): Input, Output, Output, Output Input: C-F2-1 input:  -5k-0.5 vout  -5k-0.5 output: I would like to perform tests on a model using only ImageNet data (without second model pretrain) model_lbl_lbl_test.sql db=postgres/db # use'sqlite3' with 'pymysql,noatime' option or make sure you have it for the postgresql_db environment. model db.db # use'sqlite3' with 'pymysql,noatime' option or make sure you have it for the database environment model db.# # use'sqlite3' with 'pymysql,noatime' option or make sure you have it for the database environment template 'postgresql.test.pl' # this will call the script postgresql_test.pl for 'postgresql' and 'pgsql.test.pl' from the postgresql and pg_test modules. model db#test.pl model # this will call the script test_example.pl for 'Postgres' and 'pg_test.pl' from the Postgres and pg_test modules. model db#model.pl model # this will call the script t1_examples_post.pl for 'Postgres' and 'PgTest' from the pg_test and pg_examples modules. model # this will call the script t2_examples_post.pl for 'Postgres' and 'PgTest' from the pg_examples modules. Input: CvT. CvT. CvT. I would like to perform tests on a model using only iNat data (without second model pretrain) model db.db I would like to	False
"
 input (English text): Can you list the metrics used to evaluate models on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset ; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } If you check the options of the command output then the first number is the output and the second number will be the metrics used for evaluation. If both options are empty the output will be empty. For more options on the syntax of the dataset see DataFrames with a builtin command. To compare the output of the commands see the Outputs of commands: Data Frames with a builtin model When the Metric option is active a MetricField is used to describe the input parameter. The MetricFields.cuda_metric class can be used to create MetricFields or MetricFields.cuda_all for all parameters. An example MetricField structure is the one created by the MetricField class: The MetricField.allMetricFields class returns a list of all the fields that need evaluation. Each field is a tuple of one of the following values: Field Parameter Description str(x) a unique string that identifies a metric from a text file. This value is not unique. If the metadata is not sufficient, the value may also be an empty string in which case the metric is identified using a hash: field:value If the given string is not found in the metadata, then a MetricField.empty string is returned, otherwise a MetricField.emptyMetric string is returned. For example: output: field : value: ""I am an example"" emptyMetric: """" emptyMetric: ""I am an example"" When the given value is empty an error is raised to let the user know the missing entry is missing. If no MetricFields are found, the emptyMetric field will be set to an empty string and therefore the corresponding MetricField will be set to the empty string. For more information about field names and metam"	False
"
 input (English text): Which model has achieved the highest ROUGE-2 score on the AESLC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the AESLC dataset in terms of ROUGE-1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a OR kgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } You can see that the above query is not specific enough for the dataset, whereas a specific query gives you a hint the type of metrics in your dataset. Note: In this blog post, we have written the above queries into a PHP file that we can execute using the cURL and php shell. Also, we have generated our dataset by running the cURL commands on a Dockerfile."	False
"
 input (English text): What is the name of the top performing model in terms of Mean Accuracy score when benchmarked on the ModelNet40 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Mean Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ModelNet40"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the ModelNet40 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ModelNet40"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT TOP(3) DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ModelNet40"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark rdfs:label ?model_lbl. } ORDER BY DESC( ?value ) LIMIT 1 } } "	False
"
 input (English text): List the metrics that are used to evaluate models on the Penn Treebank (Word Level) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Penn Treebank benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	------------------------- ------------------------------------------------------------------------ ------------- ------------- ------------------------------------------ ---------------------- | Name | Metric | Weight | S.M.L. | C.M.E. | R. R.C.C. | M. R.M.E. | ------------------------------------------------------------------------ metric name weight C.M.E. R. R.C.C. M. R.M.E. ------------------------------------------------------------------------ | Word_Level1| -0.076612 -0.233378 -0.085848 -0.091352 -0.072579 -0.060759 | 0.120722 -0.101814 -0.085848 0.075959 0.117932 | 0.080567 -0.144919 -0.085948 0.091452 0.086545 | 0.060630 -0.162324 -0.085948 0.091352 0.089591 | 0.097111 0.182539 -0.085948 1.062961 0.172393 | 0.074646 0.193534 -0.085948 1.020148 0.202832 | 0.096371 0.211793 0.085848 1.020569 0.204426 | 0.098076 0.273716 -0.085948 1.123361 0.297542 | 0.100508 0.285835 -0.085948 1.152973 0.314125 | 0.101904 0.356854 0.085848 1.094826 0.356205 | 0.118888 0.453876 -0.085948 1.130856 0.370664 | 0.128995 0.482376 -0.085948 1.156732 0.377832 | 0.145514 0.520078 -0.085948 1.189061 0.391688 | 0.162354 0.646964 -0.085948 1.286873 0.409097 | 0.167957 0.769644 -0.085948 1.360721 0.420883 | 0.193534 0.867649 -0.0859	False
"
 input (English text): Can you list the models that have been evaluated on the Reuters En-De dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the Reuters-21578 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 output (Sparql query): Select id, output, input from the example in the next section.	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Atlantis dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?model_lbl. FILTER (str( ?metric_lbl) = "" Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Seaquest"") ?benchmark. ORG_EVALUATION ?eval. ORG_METRICS ?metric. ORG_VALUE ?value. ORG_METRIC ?val. ORG_METRIC ?factor. } ORDER BY DESC( ?value) LIMIT 1 } } Input data: Output data: Here are the performance (HW) numbers. I chose a large dataset containing a huge data set of 2600 Atlantis model scores, which consisted of thousands of trials, as an upper end of the range of relevant performance. The model-based performance is defined in the following formula: ?model_lbl = (x2-100[2]+80[i]) / (x1-100)[i] Note that: ?model_lbl represents the performance of the Atlantis model when being tested using only that subset of observations (i.e., the subset of observations in x1 [i+1]). So, the model-based performance (including Atlantis) represents a composite of the Atlantis, Seaquest, and other similar models (i.e., a composite of all their performance at evaluating the Atari 2600 Atlantis dataset). The score of the single model ( Atlantis or Seaquest) can not be used as a proxy for the overall performance of the Atlantis model in the entire dataset. This composite performance is computed as the sum of the individual model score and of the composite performance of the Atlantis model; so, the relative performance of the Atlantis model over its subgroups (i.e., the individual scores of Atlantis and Seaquest) are the same. (Note that the score of the Atlantis model is computed not from the results of individual tests, but by using the Atlantis score as a proxy for all Atlantis model scores, so those are given in the original scores themselves.) Here are the individual model scores: Atlantis Seaquest Atlantis Seaquest Atlantis Seaquest ATLAS 10.38 10.46 10.38 10"	False
"
 input (English text): Can you list the metrics used to evaluate models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } data (select data_as ltr1 and cse_ltr1 as vars where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl} and { ?dataset a, b orkgc:Dataset}). output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } If we use this pattern to analyze a dataset like the HEXLOWER-PATTERN data from the Seel (or similar), we can see how it compares with another data set with an identical structure (which we'll name Seel_PATTERN_DATASET ). We need to run a different set of algorithms to obtain the exact same results, which means we'll need more models and more algorithms, which means more data (and more computers). Now let's add all of these models to a model that we'll call RPE_2 in the following code, where RPE_2 = { ?lbf_2 ||.8 ||.1;"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the ImageNet V2 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?titles ?title_id AND ?title_lbl. ?columns orkgp:HAS_TITLE_ID ?label FROM ?titles WHERE ?title_id = ?title_lbl. output (Sparql query): SELECT COUNT(*) FROM ?titles WHERE ?paper = ' ?benchmark', ?paper_lbl = ' ?paper_lbl. LIMIT 1 ?benchmark. ?titles orkgp:P31 ?labels OUTPUT:  SELECT COUNT(*) FROM ?titles/part ?lbl where ?labels!= '' ?labels - ?table orkgs:BINARY(1,3,2) ?table + ?index orkgp:CHECK ?labels OUTPUT:  SELECT ROUND (sum( ?labels )) as result FROM ?table ORDER BY ?lbl ORDER BY ?benchmark; Output:  output from ?tables/part:benchmark Packet sizes and processing times I decided to investigate the packet sizes that my network used for the benchmark tests, and for my testing of the performance on a laptop computer. Before doing so, it is important to check the overhead of each instruction that is performed, and also check the overhead of the different operations involved in this operation. The table below provides a rough idea on the packet sizes for each operation. Note that the performance of the different implementations varies greatly, because of the size sizes and the different memory footprint of the different implementations. I.   The main idea behind measuring the performance is to take into account the size sizes for all of the operations performed in the benchmark, and to try to see if an operation can be performed within some time, otherwise the benchmark fails. Note that this depends on the type of operation, and that an operation like convolution, which is already complex and has extremely large memory footprints and is often very slow, only becomes easier to perform in this case when only a few operations are involved in a given image. Example 1 :   ""Mosaic test"" (ImageNet) The following is a real-life example of how this test can be used to compare the memory footprint of Convolutional Neural Network implementations. The data set consists of four images, each taken from an independent image from a different source. An image with 7 edges, 1.2 mm height and 10 ms average path speed is used"	False
"
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Seaquest benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 HERO benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 HERO"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT rdfs:HASH, ?metric ?metric_lbl WHERE rdfs:label ?metric_lbl; rdfs:LEN ?metric_lbl; orkgp:LEN ?metric_lbl; orkgp:LEN ?metric_lbl { ?dataset a :dataset orkgp:dataset OR ?dataset a { orkgp:dataset ?dataset_a }, { ?dataset s:dataset }, { ?dataset _v :dataset , { ?dataset e(a1) :dataset OR { ?dataset _n :dataset , { ?dataset f(v) :dataset OR { ?dataset _n :dataset , { ?dataset f(a1) :dataset , { ?dataset f(v) :dataset , { ?dataset rl :dataset {:datet , { ?dataset rl :dataset , { ?dataset rl :dataset , { ?dataset rl (a1, a2, a3) :dataset , { ?dataset rl2:dataset } } } ?dataset, ?dataset f :dataset , ?dataset rw :dataset , { ?dataset rw (a1, a2, a3) :dataset , { ?dataset rw(e(a1, a2, a3)) :dataset , { ?dataset f :dataset , { ?dataset df(e(a1, a2, a3)) :dataset } } ?dataset , { ?dataset r0 :dataset } ?dataset, ?dataset h :dataset , { ?dataset h :dataset , { ?dataset h :dataset , { ?dataset h(w,a1, a2	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Amazon dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Amazon-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TITLE ?paper_lbl, TITLE ?paper_lbl_lbl_v1, TITLE ?paper_lbl_lbl_v0, TITLE ?paper_lbl_lbl AND 'Benchmark'  FROM 'Benchmark'  WHERE { ?dataset a orkgc:Dataset; ?dataset_lbl a orkgc:Dataset = ?dataset2a OR ?dataset2h OR ?dataset2k ?dataset2l ?dataset1a ?dataset_lbl_v1. - ?dataset a orkgc:Dataset; ?benchmark paper_lbl ?dataset_lbl. }	False
"
 input (English text): List the code links in papers that use the Qbert Rainbow+SEER model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Qbert Rainbow+SEER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Rainbow+SEER model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rainbow+SEER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT code_link as ?code_link FROM QbertModel.data p 1, 2; if( ?cont. ?is_cohort(p. ?rdfName(). ?colname_. ?tid = ?model_lbl. ?rdfName(). ?colname(). ?tid!= code_link)) Question 2: Answer, for the papers identified at the top, how the use of the model is used ? output (Sparql query): if( ?cont. ?is_cohort( ?data) && ?model = ?dataset && ?cont. ?is_cohort( ?cont. ?data. ?cohort_seq. ?colname(). ?labels_. ?tid = ?code_link)) Question 3: For the papers identified at the top, where are the benchmarks they use ? output (Sparql query): p 1, Output (Sparql query): p orkgp:HAS_BENCHMARK code. 1 Question 4: Answer, a summary of the results of using the model with the top 50 papers, on the TOP500 list and the TOP500 paper lists that were used ?	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy (Middle) score when benchmarked on the RACE dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (Middle)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (High)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } output (Sparql query): [ orkgc:Dataset; RACE : { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (High)"") { select datafile, ?dataset from ?dataset_lbl table select ?dataset_lbl. ?metric orkgc. FROM ?model select ?model_lbl. ?dataset orkgp from ?value table] ?dataset_lbl. orkgp Next step: Trying to add the RACE dataset to Crawl One of the steps that I had overlooked was extracting the RACE dataset because of its large size - at least 1.5Gb in size. Fortunately, I found a nice Python library that does exactly that (see the  GitHub page  for details). In our last step we need to take this data and use R to do some analysis of it. First, we'll find out the average weight of each race, to compare the speed of races against each other: 1 2 3 4 5 6 7 8 from sklearn import recurrent_decision def sample_data ( data_file, x ): data = None rdf = recurrent_decision ( data_file ) # The main thing here is how to handle an Input() if the data is not found in the file: raise AttributeError ( 'Unable to find a data file for'+ x ) # Read it in rdf.readfile(data"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 Czech-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Czech-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?model_id, ?id, ?benchmark, ?print, ?revs ?revs FROM Benchmark; In this case, you supply ID not a query ID, which will be returned by WMT, and ID from this parameter will be returned by WMT as its input. Therefore, ID cannot be used to compare a model and its results. ID used only to help in making the query. Note: You cannot compare the same benchmarked model more than once. For example comparing data with all three models will not result in the same result. Note: You cannot compare a model if the input data types differ (e.g. if input column names have different codes in the two datasets). For example, comparing a benchmarked model with data where columns have different names will not work.	False
"
 input (English text): Can you list the metrics used to evaluate models on the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the MUTAG dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MUTAG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?column ?column_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:GIGO ||ggp:HAS_DATASET &&!orkgp:REVISION ||ggp:HAS_METRIC ||ggp:HAS_EVALUATION && RDFS:AVAILABLE ?benchmark. ||ggp:SATURATION ||ggp:VARIABLE ?column. } input (English text): Can you list the metrics used to evaluate models on the TUMBL dataset ? output (Sparql query): SELECT ?column ?column_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } OR { ?dataset m orkgc:Dataset; rdfs:label ?dataset_lbl. } RDFS:MULTIFETCH ||ggp:COUNTER ||ggp:STREAMING RDFS:DETECTOR input (English text): Can you list the metrics used to evaluate models on the TUMBL dataset ? output (Sparql query): SELECT ?column ?column_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } || { ?dataset m orkgc:Dataset; rdfs:label ?dataset_lbl. } RDFS:MULTIFETCH ||ggp:COUNTER ||ggp:STREAMING ||ggp:GIGO ||ggp:SATURATION ||ggp:VERBOSE ||ggp:EVALUATION ||ggp:DEFINITION ||ggp:DETECTOR ||ggp:DESCRIPTOR ||ggp:PERFORMANCE ||ggp:PERFORMANCE_AUTHOR ||ggp:PERFORMANCE_DETECTOR ||ggp:PERFORMANCE_DETECTOR_SOURCES ||ggp:STREAMING ||ggp:LAST_HOT_KEY ||ggp:DELETED"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the AESLC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ESC-50 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ESC-50"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ECS-50"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What evaluation metrics are commonly used when benchmarking models on the EXCEL-20 dataset ? output (Sparql query):  SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""EXCEL-20"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What evaluation metrics are commonly used when benchmarking models on the SAS-10R dataset ? output (Sparql query):  SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SAS-10R"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What is the validation efficiency of each model ? output (Sparql query):  SELECT VARIABLE_LENGTH  ,VARIABLE_CASE  "	False
"
 input (English text): Which model has achieved the highest F1 score on the SQuAD2.0 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } The result of the query ""order by DESC( ?value)"" was: F1 SQuAD2 1.1 F2 SQuAD1 1.0 The result of the query ""order by DESC( ?value)"" was: F1 SQuAD2 1.1 F2 SQuAD1 1.1 The result of input (English text): Which model has achieved the most points on the SQuAD2.0 benchmark dataset for a given model and platform ? output (Sparql query): SELECT ?metric orkgp:HAS_DATASET ?dataset for i FROM ?model orkgp WHERE rdfs:model orkgp:HAS_DATASET. OR kgp:HAS_EVALUATION. WHERE ?metric orkgp:HAS_METRIC; ORDER BY DESC( ?value) LIMIT 1 } The result of the query ""order by DESC( ?value)"" was: F1 SQuAD2 1.1 F2 SQuAD1 1.1 The result of input (English text): What are the following results on the SQuAD2.0 benchmark dataset for all the models in which more than 80% of the data are from different platforms ? output (Sparql query): Select ?met"	False
"
 input (English text): What models are being evaluated on the TSE-NER dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ? * ?label FROM ?dataset_lbl AS ?dataset_lbl GROUP BY ?label ORDER BY ?model. ?dataset_lbl output (Sparql query): SELECT ?model ? FROM ?dataset_lbl AS ?dataset_lbl WHERE { ?dataset_lbl d orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ? orkgp:HAS_DATASET ? orkgp:HAS_EVALUATION ? orkgp:HAS_BENCHMARK ? ?benchmark benchmark. OPTIONAL { ?benchmark ?model ? } ?sample_r2 ?benchmark. LIMIT { ?dataset_lbl d orkgc:Dataset ?label } ?min ?benchmark. OPTIONAL { ?min ?model ? rdfs:label ?dataset_lbl. } } output (Sparql query): SELECT ?model ? FROM ?dataset_lbl AS ?dataset_lbl AND { ?dataset_lbl ?label d orkgc:Dataset; rdfs:p ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ? } AND { ?dataset_lbl ?label d orkgc:Dataset OR ?p rdfs:p. FILTER (str( ?dataset_lbl) = ""TSE-NER"") } ORDER BY ?model. ?dataset_lbl output (Sparql query): output (Sparql query): SELECT ?model ? FROM ?dataset_lbl AS ?dataset_lbl AND { ?dataset_lbl ?label d orkgc:Dataset; rdfs:p ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ? ORDER BY ?model. ?dataset_lbl output (Sparql query): SELECT ?model ? FROM ?dat"	False
"
 input (English text): What is the highest benchmark result achieved on the RotoWire (Content Ordering) dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	************* BLEU metric (1) : 2.5 (r2=.93, df=26, p-value=2e-22) R2 value for BLEU metric (1) : 0.06 (r2=.08, df=23, p-value=.79) In this post, we discuss about R, RStudio (an open source, cross platform package manager), and R's powerful query engine, R. In this blog post, I will show you how to implement content ordering for R using RStudio.R. Let's go through the R code in this post. The R code Since this post is written using R Studio and its graphical User Interface, I will refer it as R's User Interface. A R Program in User Interface : Below we see the R code to implement R's content ordering feature by entering the following model data without any options in the Data->Input panel, the top left of the R Studio. RStudio will prompt you to enter the model data from the Model tab in the Model Editor. You can see that there is data set A, the highest performance benchmark result. The model data is loaded into the input column and also has other columns, for example, the metric score (i.e. what is the highest performance benchmark result of the dataset). The first step in your code is to update the model to be using Content Ordering and set the model property R2 to match the column R2 of the model data. Note: In the Model Editor of RStudio, you can update the properties like this: Now, add two columns into the model (not columns from the Output column), and the output of this code (output of the R Program) will be changed as follows: The second step is to load all of the model data into the second column R2 and assign it. We can add the model data to Output by selecting the Model tab (in the Model Editor) With model data loaded into the second column R2, the third step is to run the query using the query parameter ' ?lt', which is the lowest value value. We can choose other parameters in the query parameters, for example, ' ?ct' for the metric score column for the metric value of 1. This results in the query output being: The final step is to run the query on the last row (the one before ' ?') and then update the output model to be using Content	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Seaquest dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) FROM { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ORkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. } ?benchmark orkgp:HAS_METRIC ?metric. } ORG_LOOKUP ?metric_lbl. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. } ORG_DATA ?metric_lbl. OPTIONAL { ?eval orkgp:HAS_MAXIMIZED ?value. } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl In this example we're using the Java and SQL query formats and both use the ORG_LOOKUP, ORDER BY, and MODEL options with a MAXIF and LIMIT keyword. An SQL query must be joined across a number of table columns such as datatypes, attributes, and schemas. It's easier to create, and more efficient to maintain, in a more dynamic language than a database language. The following output demonstrates using the following SQL query: SELECT TOP 3(name) FROM all_datasets ... WHERE name LIKE '<Name> - <Name>.org' ... GROUP BY name ... ORDER BY name ... LIMIT :2 In this example we query the ALL_DATA table for 2 rows, which returns: TABLE ALL_DATA :id nvarchar(250) orelsevarchar(200) primarykey NULL The top benchmark score, which uses the highest scoring dataset, is: NAME_Score Score : 6.000 ROUND 1 : 10.000 ROUND 2 : 12.000 ROUND 3 : 13.000 Let's now look at the following, simpler, example for a non-DATASET dataset. SELECT TOP 3(name) FROM all_datasets ... WHERE name LIKE '<Name> - <Name>.org' ... GROUP BY name ... ORDER BY"	False
"
 input (English text): Can you list the models that have been evaluated on the GENIA - UAS dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - UAS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the GENIA - LAS benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - LAS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 The GENIA-LAB benchmark dataset can be downloaded here. It contains a set of datasets that is comparable to the GENIA-UAS dataset in terms of overall performance and overall data accuracy. For more information about this benchmark, please see the paper: For more information about the Genia-UAS benchmark dataset (in comparison to the GENIA-LAB benchmark dataset), please see the paper: For more information about the geniia-UAS benchmark dataset, please see the paper: Note: The Genia-LAB dataset was downloaded from the UAS-EUR website. If you are using the pre-release version of the Genia-UAS dataset, please see the paper: This paper shows how to build a Genia-UAS/Genia-LAV3, and then analyze the data to determine this, on a data-only basis. You can also download the raw CSV for GENIA-LAB.csv, and the raw DATASET from the GENIA-LAB benchmark dataset.	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp-2 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; } AND { ?dataset_lbl, rdfs:label ?dataset_lbl. ?dataset. } AND { ?paper_lbl, orkgb:HAS_DATASET } FOR SELECT id FROM { ?dataset a ?label } WITH (SELECT ?note ) AS rdfs and ?note as data_note and ?note_dataset as orkgb INNER JOIN orkgb ( ?dataset_lbl AS l ) ON l. ?dataset = ?note ?dataset_label AS labl ORDER BY id, l output (Sparql query): SELECT DISTINCT ?note ORkgb.title, ?title AND ?note_dataset & ?note_labl.text FROM orkgb ( ?dataset_lbl|labl), orkgb ( ?dataset_label) INNER JOIN ( ?dataset_lbl|labl) ON l. ?dataset = ?note_dataset AND l. ?dataset_label = ?note_labl. ?dataset_lbl. ?note ORkgb. ?note_dataset. ?note_labl_text I'll take these in a moment, because one of the primary questions I had going in was how to sort the lists. What I'm trying to do is make a list of lists. So what I need to do is sort the list of papers with the first and last title, and then I need to do some sort of comparison by each of those titles, you see ? You'd think this would be easy, though, right ? I need to be able to sort by title, and by author, and by journal, and so forth. And how do you sort that list in a reasonable way. There are a ton of answers, and here are three different methods I've spent some time experimenting with. I'll list them here in a separate table. I used an old data analysis tutorial: The Top 50 Papers on Yerba-Bissau by Yann LeCun. This is the way I'd sort these books. But I was hoping we could take from it and go further on to create some sort	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the RACE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ; ?metric rdfs:label ?metric_lbl. } FILTER (str( ?dataset_lbl) = ""Amazon"") ?dataset. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset. orkgp:HAS_EVALUATION ?eval; orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ORDER BY DESC( ?value ) LIMIT 1 } } Finally the SQL query: SELECT * FROM dbo.d2s.models GROUP BY ?model; And the RACE RDS  ORDER BY ?metric DISTINCT ?model. BETWEEN 1 AND 100 R1: SELECT ?metric ?model. BETWEEN 1 AND 100 R2: SELECT ?metric ?model. BETWEEN 1 AND 100 R3: SELECT ?metric ?model. BETWEEN 1 AND 100 R4: SELECT ?metric ?model. BETWEEN 1 AND 100 R5: SELECT ?metric ?model. BETWEEN 1 AND 100 R6: SELECT ?metric ?model. BETWEEN 1 AND 100 R7: SELECT ?metric ?model. BETWEEN 1 AND 100 R8: SELECT ?metric ?model. BETWEEN 1 AND 100 SELECT R9: SELECT ?metric ?model. BETWEEN 1 AND 100 DISTINCT ?metric_lbl. (1 row(s) affected) DISTINCT ?method ? (1 row(s) affected) DISTINCT ?metric_rdfs. ("	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 James Bond dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 James Bond"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bank Heist dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bank Heist"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bank Heist"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } There are 2 questions I have to ask myself: Which one of the above 2 questions will be the most difficult to answer with the answer we get ? Which one of the above 2 questions should they ask ? That's not really my question. It is a question not to be taken over by the computer's program. You are free not to answer it."	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ARC (Easy) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Easy)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of A2 score when benchmarked on the ANLI test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A2""). FILTER (str( ?metric_lbl) = ""ANLI""). CASE WHEN rdfs:eq( ?dataset_lbl. ?index ) THEN DISTINCT. ELSE. DISTINCT. END input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ARC (Easy) dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ARC (Easy)""). FILTER (str( ?metric_lbl) = ""ANLI""). CASE WHEN rdfs:eq( ?dataset_lbl. ?index ) THEN DISTINCT. ELSE. DISTINCT. END input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ARC (Easy) dataset ? output () Output (English text): This is an example of a scatter plot of accuracy score between ""easy"" and ""hard"". The model ""cluster"" has the following name at the beginning and end of the analysis: cluster_s2cluster The ""cluster"" has 6 parameters which are: default (default model that is used if no other criteria exists): A ""default"" model that is used if no other criteria exists, no training data is generated, and no other metric is required for this model. the_cluster_default_model: A ""cluster_s2cluster"" model that is chosen when no other other criteria exists. It has the following name at the beginning and end of the analysis: #the_cluster_default_model. Note that this name is different from the ""cluster"". #the_clust_default_model. Note that this name is different from the ""cluster"". the_clust_default_mcluster_default_model: The ""clust_default_mclust_default_model"" model that"	False
"
 input (English text): Could you provide a list of models that have been tested on the WNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 <matlab wordlist> |1|1|2|3| 4|5|6|7|8|9| 10|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31|32|33|34|35|36| 37|2|2|4|4|6|6|7|9|11|13|15|17|19|21|23|26|27|28|29|30|31|32|33|34|35|36| 37| ORL, NONE | { ?dataset a orkgc:Dataset; } { ?dataset a orkgc:Dataset; } ||ORL, NONE | { ?dataset a orkgc:Dataset; } ||ORL, NONE | { ?dataset a orkgc:Dataset; } ||ORL, NONE | { ?dataset a orkgc:Dataset; } | ||ORL, NONE | Data and Methods (R) It provides one method. The following plots a simple list of the model scores by different versions of the QNLI paper with an average score between 200 and 1000. In other words the QNLI version has very different scores than the others. Tables 2 and 3 compare model scores using a linear linear mixed model. However, the mixed model was also used by QNLI (to determine the performance of the model) which does not use the method listed in the text. Therefore both methods were used. This is not shown. Column labels are for each model individually. Some of the columns were changed for readability. Table 2. Matrix plots for model scores by different versions of the QNLI paper with an average score between 200-1000. column 1 = Model Scores < 250 |2|3|4| 5|6|7|8|9| 5|6|7| 8| 9| 9| 9| 10| 10| 11|12|13|14| 14| 15| 16| 17|18|19| | | N | S | E | P | B | G | V | U | A | C | H | T | D | H| | | | |	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 SURFACE VARIANT A SURFACE is defined by one or more dimensions. A dimension can be the number of columns of a table, or an array of values, and multiple dimensions may be specified. To create a SURFACE, use the CREATE TABLE statement: CREATE TABLE mytable ( data varchar, data_column cvarchar(2000), data_value varchar(1,255), table_column table_column ); Now, in the example above, we create a SURFACE by making two tables. To create a SURFACE for a table that has 12 columns, we would use this statement: CREATE SURFACE DATA12(table_column varchar, table_column table_column) WITH 12 NULLS; In another example, we will create a SURFACE for a table with 24 columns. Because of this, the columns in the SURFACE will be reversed, and the indexes created for the result set will be reversed, too. The same is true for a SURFACE for a table with 32 columns. The following statement inserts all of the 12 columns into a table with 24 columns: INSERT INTO table2 (data) VALUES ('data', 'data', 4, 'data'); UPDATE Table2 VALUES ('data', 'data', 8, 'data'); CREATE TABLE table_2(table_columns varchar, table_cols varchar, table_type varchar) WITH (table_columns null, table_cols null, table_type null, NULL) AS mytable; The following statement inserts all of the 6 columns into a table with a maximum of 24 columns. Then, it updates the rows of TABLE2 along with the new data: INSERT INTO table2 (data, {data_columns}) VALUES ('data', 'data', 5, 2); INSERT INTO table2 (data, {data_columns}) VALUES ('data', 'data', 3, 4); CREATE TABLE table_2(table_columns varchar, table_cols varchar, table_type varchar, NULL) AS mytable; CREATE TABLE table_2(table_columns varchar) WITH 24(NULL) AS input; CREATE TABLE table_2(table_columns varchar); INSERT INTO table2 VALUES ('data', {data_columns}), 24	False
"
 input (English text): Can you provide links to code used in papers that benchmark the MPCM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MPCM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the MFEC model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MFEC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; ?rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MPCM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } For all of the above queries except 1: the query name and URL appear in the output row. Since, for all the above queries, the input file is the same regardless of the benchmark, and since all these queries involve using the same SQL expression, it is important to make sure that the output is consistent, namely, that there is no string interpolation or variable expansion involved. If the output from any of the above queries uses the string sql_variable_query_name (which is also shown on the result row), it is considered inconsistent. The following parameters must be present: input (English text):  { ?dataset a, ?cont orkgp ?benchmark ?cont model; ?cont model orkgp orkgc:Model. FILTER (str( ?model_lbl) = ""MPCM"") ?benchmark orkgp:HAS_DATASET ?dataset.... } output (Sparql query):  SELECT IFNULL(a.name,'model'): ?cont model FROM orkgp; Note that the output row is not necessarily the same as the input row because the variable a.name may be part of the name of a different row. The SQL keyword argument orggp is optional. The output of the query is not stored, although an attribute list called orkgp is returned. The input and output rows for a query orggp are identical except that the orggp row contains more attributes than the input row and also includes the orkgp column with the following contents: { ?ext ?dataset, ?cont model, ?cont model orkgp ?benchmark. Note that this attribute is used if the query is executed against an external query that does not utilize an orggp option. The data type of the output of a query orggp is"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CAIT-S-48 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-S-48"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the CAIT-XS-36 model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-XS-36"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-XS-36"") orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } I think that is good enough. A: Thanks for giving more info on XS-36. I was not able to find any such papers. I just did some searching on PubMed. The other paper which I believe was related to the XS-36-I model in which the XS-36 model scores as the best was not published so I cannot find its citation. I may give a more detailed answer via PM if I have some more time. Q: Is it possible to run the benchmarks using the same data for the different benchmark datasets ? A: In my opinion, no. The benchmark plots in the reference above will only be available for the dataset (CAIT_RESTOR_S-48), so you should use the ""run as"" command to run them. You'll notice that the time series shown are more accurate than the ones from the reference and they include the performance of the different models separately from each other. This is similar to the way in which performance from RDDs is reported. In contrast, the time series for SST, LSTM and STAM are shown to get a very good overview of how well the different models perform. Q: What parameters/output files should I be using (both as parameters of the model and data) ? A: At the time I wrote this question an additional parameter was created to specify the number of the parameters (2x2 in our reference). As for the output files, only two are provided by the reference: the benchmark plots and the performance matrix. I was happy about not having to worry about this one. Q: Is there a detailed explanation of the performance matrix to provide if a benchmark plot is generated in the reference ? A: The benchmark plots are provided from the reference, but they assume the dataset is set like in the reference. In order to do so we have to specify ""t"" in the configuration variable, and the performance matrix"	False
"
 input (English text): What models are being evaluated on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""re17″) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): What models are being evaluated on the seel.cse.lsu.edu/data/se17.zip dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""re17″) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } For many years, it was understood that there exists a ""natural"" test model, which is the same as SPSS 15 with a slightly different model (the p-value test uses the exact same set of observations for all models). This method, however, does not produce test-statistic maps without a second or third test and is of great importance in the study of complex multidimensional datasets. For example, in the figure above (the first one in this post), where I show the ""natural"" test model, which represents SPSS 15 with a slightly different model (this method does produce test-statistic maps even without a second or third test): input (English text):  Who did the measurement ? output (Sparql query):  SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Datas"	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Pitfall! benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pitfall!"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Montezuma's Revenge benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT CAST ( ROW ( ""Score"" ) ) as Score_1, CAST ( ROW ( ""Score_1"" ) ) as Score_2 FROM AtariMontezuma.ResultSet LEFT JOIN AtariMontezuma.Result Set ON LEFT JOIN AtariMontezuma.Result SET CASE WHEN Score_1 = score THEN Score_1 ; WHEN Score_2 = score THEN Score_2 ; 6. SELECT CAST ( ROW ( ""Points"" ) ) as Points_1, CAST ( ROW ( ""Points_1"" ) ) as Points_2 FROM AtariMontezuma.Result Set LEFT JOIN AtariMontezuma.Result Set ON LEFT JOIN AtariMontezuma.Result SET CASE WHEN Points_1 = points THEN Points_1 ; WHEN Points_2 = points THEN Points_2 ; 7. SELECT CAST ( ROW ( ""Points_1"" ) ) as Points_1, CAST ( ROW ( ""Points_1"" ) ) as Points_2 FROM AtariMontezuma.Result Set LEFT JOIN AtariMontezuma.Result Set ON LEFT JOIN AtariMontezuma.Result Set ON LEFT JOIN AtariMontezuma.Result SET CASE WHEN Points_1 = points THEN Points_1 ; WHEN Points_2 = points THEN Points_2 ; 8. select : 1 9. COUNT ( ""Points"" ) 10. COUNT ( ""Points"" ) 11. SELECT CAST ( ROW ( ""Points"" ) ) as Points_1, CAST ( ROW ( ""Points_1"" ) ) as Points_2 FROM AtariMontezuma.Result Set LEFT JOIN AtariMontezuma.Result Set ON LEFT JOIN AtariMontezuma.Result SET CASE WHEN Points_1 = points THEN Points_1 ; WHEN Points_2 = points THEN Points_2 ; 12. select : 3 13. COUNT ( ""Points"" ) 14"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet ReaL dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT * 2.1  as benchmark. ORDER BY DESC( ?value ) 2.1  as benchmark. 2.7.3 andkgp:HAS_DATASET ?dataset 2.1  as benchmark. input (English text): What is the best performing model, i.e. best performing in terms of ""metric metric_lbl"" for the ImageNet ReaL dataset ? 4.9 output (Sparql query):  SELECT SIZE FROM S1 4.9  GROUP BY SIZE; input (English text): What is the best performing model benchmarking the ImageNet ReaL dataset in terms of ""metric metric_lbl"" for the ImageNet ReaL dataset ? output (Sparql query):  SELECT ?metrics_lbl   ?metric_lbl 1.00  as benchmark. OPTIONAL ?metrics_lbl orkgp:HAS_DATASET ?dataset 1.00 2.0 andkgp:HAS_EVALUATION ?eval 1.00 andkgp:HAS_VALUE ?value. OPTIONAL"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BiT-S (ResNet-152x4) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet-152x4)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the ResNet-152 (SAM) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ResNet-152 (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Can you provide links to code used in papers that benchmark the BiS-P model ? output (Batch file): Can you provide links to code used in papers that benchmark BiS-P model(s) ? input (English text): Can you provide links to code used in papers that benchmark the BiS-P model ? output (Sparql query): (S)L _S / S L. output (Sparql query): SELECT SUBSTRING( ?code, ?cont- ?model)- ?cont- ?model- ?labels FROM bsr_batch_bases bsr_diffs bsr_labels ?cont- ?model- ?names WHERE ( ?cont- ?model) AND ?cont or ?labels. B. Concluding remarks For the purposes of the evaluation, the following hypotheses were tested: BiS-P: Can you demonstrate that resnet152 outperforms sam ? BiS-P: BiS-P: Can you demonstrate that BiS-P: Can you demonstrate that resnet152 outperforms sam ? BioS-S: Can you demonstrate that both BiS-P and BiS-P: Can you demonstrate that BiS-P: Can you demonstrate that resnet152 achieves better performance thansam ? Note: In the paper: bi-seq-compare.pdf, it appears that sam does better than biosig. The reasons: ""biome2seq is a multi-model sequence analysis tool to compare single-gene and multi-protein sequences, which allows for comparisons of expression profiles between genes and between protein isoforms. The comparative method uses statistical comparisons of all possible sequences, and the results are calculated by summing total differences rather than sums of the differences between two inputs (e.g., 1samples). Due to the relatively small number of sequence comparisons performed on each gene and to the similarity of all gene sequences"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the PubMed 20k RCT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Pubmed dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value ) } We then add in a little query to perform a second scan of the query, and take all of the results as raw data: select * from journal_lbl_pk_2 where name = ?name.lower() and journal_lbl_pk_2.pubmed_lbl_group = '_1' ORDER BY name, journal_lbl_pk_2.pubmed_lbl_group output (Sparql query): -------------------------- Name Score Metric Description -------------------------- ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl"	False
"
 input (English text): Can you list the models that have been evaluated on the Walker, walk (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Walker, walk (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"--------------------------- Walker, N. H. J. Kress G. L. A. C. Moore D. W. M. Smith B. J. A. Davis W. R. R. C. Hall R. V. A, ""N/A: Model validation using HSAG: A comparison of the Walker, walk and DM4K experiments,"" Acta Scientiarum Hungaricae 46(1), February 2002 pg. 22-32 See also: ""Walker validation,"" http://www.dmo-labs.org/ Walker, N. H. J. Kress G. L. A. C. Moore D. W. M. Smith B. J. A. Davis W. R. R. C. Hall R. V. A, ""Walker, walk and DM4K, model validation,"" Acta Scientiarum Hungaricae 47(1), June 2002 pg. 20-23 Walker, N. H. J. Kress G. L. A. C. Moore D. W. M. Smith B. J. A. Davis W. R. R. C. Hall R. V. A, ""Walker, walk and DM4K, model validation,"" Acta Scientiarum Hungaricae 48(1), January 2004 pg. 17-20 Walker, N. H. J. Kress G. L. A. C. Moore D. W. M. Smith B. J. A. Davis W. R. R. C. Hall R. V. A, ""Walker, walk, and DM4K, model validation,"" Acta Scientiarum Hungaricae 49(1), February 2004 pg. 16-24 Walker, N. H. J. Kress G. L. A. C. Moore D. W. M. Smith B. J. A. Davis W. R. R. C. Hall R. V. A, ""Walker, walk, and DM4K, model validation,"" Acta Scientiarum Hungaricae 50(1), March 2004 pg. 19-21 Walker, N. H. J. Kress G. L. A. C. Moore D. W. M. Smith B. J. A. Davis W. R. R. C. Hall R. V. A, ""Walker, walk: DM4K, model validation,"" Acta Scientiarum Hungaricae 51(1), April 2004 pg"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the PolyAI Reddit dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PolyAI Reddit"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the REDDIT-B dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""REDDIT-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	------------- datanormal_metric: ? metric_lbl -------- ------- PolyAI Reddit: 1000.00 (±0.30) (±2.40) (±0.08) 0.01000 (±0.00) 0.160000 (±0.38) 0.089900 (±0.16) PolyAI Reddit: 1260.00 (±0.10) (±0.09) (±0.08) 0.0007000 (±0.14) 0.049000 (±0.04) 0.073100 (±0.12) PolyAI Reddit: 2060.00 (±0.05) (±0.05) (±0.05) 0.008000 (±0.24) 0.029000 (±0.14) 0.040000 (±0.33) PolyAI Reddit: 2880.00 (±0.09) (±0.07) (±0.06) 0.004800 (±0.12) 0.025800 (±0.21) 0.067100 (±0.10) PolyAI Reddit: 4840.00 (±0.10) (±0.08) (±0.06) 0.063700 (±0.14) 0.045100 (±0.10) 0.093100 (±0.35) PolyAI Reddit: 8160.00 (±0.07) (±0.08) (±0.08) 0.033400 (±0.16) 0.068400 (±0.15) 0.096100 (±0.36) PolyAI Reddit: 12160.00 (±0.08) (±0.09) (±0.09) 0.073800 (±0.31) 0.040000 (±0.27) 0.102000 (±0.42) PolyAI Reddit: 28160.00 (±0.08) (±0.07) (±0.07) 0.002400 (±0.32) 0.039400 (±0.28) 0.069900 (±0.13) PolyAI Reddit: 4840.00 (±0.09) (±0.08) (±0.07) 0.067900 (±0.21	False
"
 input (English text): What are the models that have been benchmarked on the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Softcite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT DISTINCT ?model ?model_lbl AS model_lbl_seq FROM Softcite.Model lbl WHERE _._model_lbl. ?dataset a OR k. ?dataset_lbl. OPTIONAL { ?model rdfs:label ?model_lbl_seq } input (English text): What model will be executed with every benchmark output ? Output a benchmark that simulates the performance of each model (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl AS model_lbl_seq FROM Softcite.Model lbl WHERE _._model_lbl. ?dataset a OR k. ?dataset_lbl   output (Sparql query): How is the benchmark performed ? Where is the benchmark output ? For example, how is the output of the example benchmark stored (e.g. whether the output should be stored as a plain text file or an html document) ? Output a summary description that indicates the types of measurements conducted. In the past, some of the benchmarkers have added or modified the benchmarks at intervals of 10 minutes, but this is no longer recommended: input (English text): What are the models that were benchmarked on the Benchmark dataset ? All output models where the output is stored in a plain text file as CSV output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl AS model_lbl_seq FROM Benchmark.Model lbl WHERE _._model_lbl. ?dataset a OR _._model_lbl. ?dataset_lbl   output (Sparql query): The benchmarks were created by an external researcher and are not in compliance with the Softcite recommendations. If the data files are stored in their own directory, where can you view the benchmark summaries to determine if it looks correct ? Output a summary that indicates the types of measurements conducted. In the past, some of the benchmarkers have added or modified the benchmarks at intervals of 10 minutes, but this is no longer recommended: input (English text): What model is being tested ? output (Sparql query): SELECT TOP 10 SELECT DISTINCT orkgp:INPUT _model_lbl. ?dataset a OR k. ?dataset_lbl. OPTIONAL {	False
"
 input (English text): Could you provide a list of models that have been tested on the SST-5 Fine-grained classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the SST-2 Binary classification benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-2 Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" Output: Input (English text): Could you provide a list of models that have been tested on the SST-3 Binary classification benchmark dataset ? output (Sparql query): Output: Input (English text): Could you provide a list of models that have been tested on the SST-4 Binarization benchmark dataset ? input (English text): output (Sparql query): Output: Input (English text): Could you provide a list of models that have been tested on the SST-4 NAND/MEM benchmark dataset ? input (English text): output (Sparql query): Output: Input (English text): Could you provide a list of models that have been tested on the SST-5 Binary classification benchmark dataset ? output (Sparql query): output (Sparql query): Output: There's one more step, which has to do with choosing the correct model input (English text): output (Sparql query): Output: We'll just show the ""recommended"" SST-1 model "	False
"
 input (English text): Which model has achieved the highest Accuracy (%) score on the Hendrycks Test benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hendrycks Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the Hendrycks Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hendrycks Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT c( ?top=%q_top.b:%q_benchmark.d:%q_benchmark.m:%q_test.a:%q_test_test.b:%q_test_test.*), ?method_lbl, ?metric, ?test_values FROM ssss_datasets_select_data_test_lbl_hendrycks_test on ssss_datasets_where key = 'top', step = 1, metric ='metric', model = 'lbl', benchmarks = [[ ?top=%q_top.c:%q_benchmark.d:%q_benchmark.m:%q_test.a:%q_test_test.b:%q_test_test.*], ?top=%q_top.c:%q_benchmark.d:%q_benchmark.m:%q_test.a][], benchmark= ?bottom, metric= ?top, metric_name = 'Hendrycks' )] GROUP BY c(top), ?metric, ?test_values; Outputs: top model top method top metric top test_values output (Sparql query): SELECT ?method_lbl, ?metric, ?test, ?values, ?x, ?y FROM ssss_datasets_select_data_test_lbl on ssss_datasets_where key = 'top', step = 1, metric ='metric', model = 'lbl', benchmark = [ ?top=%q_top.c:%q_benchmark.d:%q_benchmark.m:%q_test.a:%q_test_test.b:%q_test_test.[] ?method-lbl and ?metric and ?test and ?values], benchmark= ?bottom, metric= ?top, metric_name = 'Hendrycks' ]	False
"
 input (English text): Can you provide links to code used in papers that benchmark the LSTM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""KD-LSTMreg"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?model ON  | ?dataset. ?cont OR  | ?dataset. ?lbl; output (Sparql query) SELECT DISTINCT ?model ON  | ?dataset. ?cont; #  /  output (Sparql query): #  /  output (Html output) : HTML output can be used to generate document/page templates from the results of model comparisons. output (Html output) : Template created by running the html output script above. model_lbl  =  (  OR  OR   ?dataset. ?cont OR   ?dataset. ?lbl )  ; input (English text): Please provide links to code used in papers that benchmark the model ? input (Japanese text): Please provide links to code used in papers that benchmark the model ? output (Javascript code): Please provide source code of the function ""compare with model"" in the paper (LSTMmodel).   Please provide the full name of the model used (LSTMmodel).   output (Html output):  Html output can be used to generate document/page templates from the results of model comparisons. output (JavaScript code): Please provide source code of the function ""compare with model"" in the paper (LSTMmodel). I hope this gives you some useful information before diving in the paper to try it out. Also, feel free to leave comments and questions, I'll do my best to respond to whatever you guys have to say."	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the TDM Tagged Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. \ ORG_DATASET ?dataset. \ orkgp:HAS_DATASET orkgp:HAS_BENCHMARK \ ?benchmark orkgp:P15 ?benchmark. ?paper orkgp:P28 ?cont; rdfs:label ?paper_lbl. } As can be seen, we are running the same program in the same order. The output in the last part of the output (columns 3 and 4) is the same — the same data. As the name of the output query and of query-specific functions suggest, this is a query-based query: 1 2 3 4 SELECT ?dataset a, ?dataset_lbl_lbl ?paper from ?dataset_lbls ORDER BY ?dataset_lbl_lbl : ?paper | ORDER BY ?paper. As can be seen, the output for the query does not show the original dataset. Instead we are asked to choose the data. The query is an aggregation of three things. The first is a query. We want to perform an aggregation of the papers' titles. Thus in that way we get some titles back to make their query easier to find the actual title from the dataset. As a second step, the query is an aggregation of the NLP benchmark score of our document: the difference between the benchmark score of our corpus and the benchmark score of our benchmark paper. Another third step is a filtering of a particular document. The filtering of a particular document is a bit more involved and not included in the example of the query itself. But to illustrate this, the input to filtering is an input that allows us ""selects"" only the ""paper"" records for the ""test_dataset"" value (i.e., the ""T"" value). If we specify a filter this way, we will return only the rows that use the named document ""T"". The output of the query can be either a list of the results for each of the input"	False
"
 input (English text): Can you list the metrics used to evaluate models on the ClueWeb09-B dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the ClueWeb09-B dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Note: There could be a missing metric in the response. If this is a problem, there are many ways to address the missing metrics. Many of the recommended metrics include an indicator for any missing metric. These indicators are called check errors: in order to find out the missing metrics, try the 'Check Metrics' check function. Some more metrics, which provide further guidance, include the 'CATALOG' flag. This flag allows to search the output for the missing metric.	False
"
 input (English text): Can you list the metrics used to evaluate models on the IMDb-B dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" You can see that all the metrics are selected which means (we need to apply all of the models) and this is the most important information. Now as you want to understand why can this score is different from the model with same number of features, you can choose, where it should be evaluated (one metric is selected in every model while another is selected in all the models or the latter only in some of them), for this we need some data source. First of all we have a simple one, I will use a dataset of the IMDB-B dataset. However you can use the exact format from IMDb-B and the first query will be: select count(*) FROM [my-database-user.db] ORDER BY [count(*) ] and as you can see, one single metric is used in each model. The second dataset will be a more complex one but a good example of using a data source will be the IMDB-B datasets and it is the IMDB-B test datasets. Therefore if you are using IMDB-B and you want to apply only the IMDB-B dataset, it will look something like the following: select count(*) FROM [my-database-user.db] ORDER BY [count(*) ] select count(*) FROM [my-db.db] ORDER BY [count(*) ] Next lets see how can we do this for other data repositories and formats (TXT, XML etc) where there are separate metrics as this may vary. The first thing you need to do is to create a table called aggregated_metric and populate it based on the data sources you are going to use. To create new aggregated metric, we need to specify the metrics name (in the example the data sources the result looks like: ""IMDb-B"", ""DocRED"" etc.) and the column that should be used (in this example it is called the ""metric_lbl""). The table is called ""aggregated_metric"" in this case. Now here the first thing that changes is the table column. The table column is called ""metric"" and it will be treated as an abstract data type. The column itself must contain one or more of fields, the number of the field will dictate how many columns the metric should contain. The same will be used for the columns rdfs:label and orkgp:HAS_DATASET."	False
"
 input (English text): Provide a list of papers that have utilized the CvT-13-NAS model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-13-NAS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the CvT-21 (384 res) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-21 (384 res)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model cv21:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-21"")) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model. OR kgp:HAS_SOURCE_CODE ?code. input (English text): Provide a list of papers that have utilized the CvT-2-1 model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model cv2-1:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-6-SOCIAL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. input (English text): Provide a list of papers that have utilized the CvT-2 model and include the links to their code ?"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the SciKG model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciKG"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the FG fine-grained gate model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FG fine-grained gate"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?benchmark. ?model WHERE { ?hl_gcls. ?hl_gcls. ?model_lbl; } input (English text): How can you provide links to code used in papers that benchmark the FG model ? output (Sparql query): SELECT ?benchmark. ?model WHERE { ?hl_gcls. ?hl_gcls. ?model_lbl; } output (Sparql query): -------------[ { ?hl_gcls. ?hl_gcls. ?model_lbl; __________| | | |__ | \ / \__; | | |___|_ |_|_| |_|_|___|_ |_____| _____| | |_____| |_____| |_____|_____|_____| |_|_|_|_|_|_|_|_|___|_|_|_| _____| | |_|___|_|_|_|_| _|_____| |_|_|_____|_____| |______| |__|____|____|____| | ___________ | | | | | ) |____ | | __ | ) | | |__ ; | | |_| |_| |_|_______| |_____|_____| |_____|_____| |___|_|_|_|_|_|_|_| | |____|_____|_______|_____|_____|__|_____| |_____|__|_|_|_| |_______|_| |__|__|____|___| |_|_| |___| |___| |_____| |_____|_____| |___|_| |___| |___	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the NYT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the NYT29 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ?cont orkgp:HAS_BENCHMARK orkgp:HAS_VALUE ?value. OPTIONAL { ?cont orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?model_lbl. } ?cont orkgp:HAS_BENCHMARK orkgp"	False
"
 input (English text): Can you list the metrics used to evaluate models on the SearchQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the QNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" (Note: the query above uses R's built-in ""select [all] from test.x"" query, which is not ideal for multilayered queries). Note: The input of the query above is formatted in SQL (in the same way as the R version used above). The output is a series of SQL statements which are rendered in different formats (i.e. CSV and R Markdown) using R's syntax highlighting feature. It is therefore recommended that, if possible, the outputs are formatted as HTML by the user - please contact the author of this post if you wish to assist with formatting. The next question is 'What methods have you used to evaluate the model accuracy ?'  Please note that, for this blog post, I have tested only the model with ""single"" data, i.e. model accuracy was calculated on only the first observation and/or result as the model ran.  This should also apply if the model is a 'coherent' model or does not use any temporal information at all!  As with my previous blog post (see here ) I also tested on some other datasets (including the first 3 data sets we will look at) and found them to be quite close in model accuracy to any of these.  But these are different datasets and probably not entirely unbiased - especially given the fact they were both run using different techniques and/or data. This post is meant to provide some background information for future posts we are intending to publish relating to 'coherent' models and time series."	False
"
 input (English text): Which model has achieved the highest Relation F1 score on the SciERC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Relation F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Entity F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?entity_lbl FROM [ ?entity_lbl] ROWS 1 INNER JOIN [ ?entity_lbl ?columns] on [ ?columns ?column]. ?entity_lbl Outputs: This query returns only ONE result for a query SELECT DISTINCT ?entity_lbl FROM [entity_lbl] WHERE entity_lbl.entity_lbl LIKE 'datasets/[groupid]-10-24' ROWS 1 Input: 1st row of entity_lbl Outputs: This query returns only ONE result for a query SELECT * FROM [entity_lbl] GROUP BY 1 2nd row of entity_lbl Outputs: This query returns only ONE result for a query SELECT ?entity_lbl FROM [entity_lbl] GROUP BY 1 2nd-3rd rows of entity_lbl Input: 1st row of entity_lbl. Outputs: This query returns only ONE result for a query SELECT!entity_lbl FROM [entity_lbl] ROWS 1 2nd-3rd row of entity_lbl. Input: 1st row of entity_lbl. Outputs: This query returns only ONE result for a query SELECT ?entity_lbl FROM [entity_lbl] GROUP BY 1 2nd-3rd row of entity_lbl. Input: 1st row of entity_lbl. Outputs: This query returns only ONE result for a query SELECT!entity_lbl FROM [entity_lbl] ORDER BY 1 2nd-3rd row of entity_lbl. Input: 1st row of entity_lbl. Outputs: This query returns only ONE result for a query SELECT DISTINCT ?entity_lbl FROM [ ?entity_lbl] ROWS 1 1st row of entity_lbl. Input: 1st row of entity_lbl. Outputs: This query returns only ONE result for a query SELECT SELECT * FROM [entity_lbl] GROUP BY 1 2nd-3rd row of entity_lbl. Input: 1st row of entity_lbl. Outputs: This query returns only ONE result for a query SELECT ?entity_lbl FROM [entity_lbl] ROW(1) 2nd row of entity_lbl. Input:	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset SST-5 Fine-grained classification?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) FROM { ?dataset_lbl, ?dataset_lbl. ?dataset_lbl } ORDER BY DESC( ?value ) } ( SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) FROM { ?dataset_lbl, ?dataset_lbl. ?dataset_lbl } ORDER BY DESC( ?value ) 	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?value ------------------------- ORDER BY DESC( ?value) ------------------------------------------------------- Query: SELECT COUNT (1) as c FROM quiz.long.text WHERE c.rating = ?scores. OR gt(1.5). ORDER BY ?rating ASC LIMIT 100 ORDER BY ?rating DESC LIMIT 50 Results: SELECT COUNT (1) as c FROM quiz.long.text WHERE c.rating = ?scores. OR gt(1.5). ORDER BY ?rating ASC LIMIT 100 ORDER BY ?rating ASC LIMIT 50 Results: SELECT COUNT (1) as c FROM quiz.long.text WHERE c.rating = ?scores. OR gt(1.5). ORDER BY ?rating ASC LIMIT 100 ORDER BY ?rating ASC LIMIT 5 Results: SELECT COUNT (1) as c FROM quiz.long.text WHERE c.rating = ?scores. OR gt(1.5). ORDER BY ?rating ASC LIMIT 100 ORDER BY ?rating ASC LIMIT 5 And so the question remains: what should we do with these data sets ? Let's say I want to study people's thoughts on the value of a specific film. I might run a query like this: SELECT id, ?""criteria"":numeric( ?) AS ?criteria_id FROM ( SELECT ( id ) FROM ( SELECT ROW_NUMBER() OVER ( ORDER BY id ASC) cn.criteria ORDER BY ?criteria_id ASC ) n; id ); The id of the user should be the first column (or even a text with a simple ID, if that becomes available). The criteria should have a lowercase, unique id, so you can run queries like this in any language that supports this kind of SQL: SELECT ROW_NUMBER() OVER ( ORDER BY id ASC) cn.criteria ORDER BY ?criteria_id DESC LIMIT 100; ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- Criteria: SELECT ?""name"":numeric( ?) AS ?name_id FROM ( SELECT ROW_NUMBER() OVER ( ORDER BY ?""name_id"" ASC) cn.name_id ORDER BY ?""name_id"" DESC LIMIT 100.1; name id ------------------------- ------------------------- ------------------------- ------------------------- ----------------- Criteria: SELECT ?""f"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 English-German?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 Czech-English?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Czech-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"_____ WMT2016 Czech-English. _____ To run the above query, just copy and paste the query into a new table. We have not specified that the table contains only English text in this example, but in any case we need to make sure that the output of our query is the expected result. Otherwise, it will be written as: { ""dataset"": { ""a"": { ""lbl"": ""20161207_WMT2016_CHED_EU_Czech_English"", ""score"": 0.1146643568992897 }, ""lbl"": ""20161207_WMT2016_CHED_EU_Czech_English"", ""dataset"": { ""a"": { ""lbl"": ""20161207_WMT2016_NIL_English_DEL"", ""score"": 0.1146643568992897 }, ""lbl"": ""20161207_WMT2016_NIL_English_DEL"", ""dataset"": { ""a"": { ""lbl"": ""20161207_WMT2016_SCHED"", ""score"": 0.1146643568992897 } } } } ""model"": { ""type"": ""SASL"", ""dataset"": { ""c_label"": ""WMT2016"", ""score"": 0.082615395718581453 } } } } The reason is one of the two columns in the table is the correct one if the result to be displayed is for the Czech language only. If you look at the table above, there is a difference between the ""lbl"" column and ""dataset"" column. Because of the different way the variables have been used, the first result is displayed as ""WMT2016 Czech English"", while the second one is displayed as ""WMT2016 Czech English"": There are some other parameters (e.g. ?kavl = 3) that we might not need to specify during execution when importing from PostgreSQL for the first time. So if we modify our query and run the query again, then just change the input variables when you want to do this. If your application has multiple tables in it, you can use the OR (Orphan Table) query mode to handle this issue. You can find the OR (Orphan Table) query mode here. You can use this OR (Orphan Table) mode by writing the following inside"	False
"
 input (English text): What is the average energy generation of all energy sources considered?
 output (Sparql query): SELECT (AVG( ?elec_gen_value) AS ?average_elec_gen_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43134 ?electricity_generation. FILTER(REGEX( ?energy_sources_labels, ""all sources"")) ?electricity_generation orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?elec_gen_value) } 
 input (English text): Which sectors are modeled as energy sectors and how often?
 output (Sparql query): "	SELECT ?label COUNT( ?label) WHERE { ?sector rdfs:label ?label; a ?class. ?class owl:equivalentClass <http://openenergy-platform.org/ontology/oeo/OEO_00000367>. [ orkgp:P37668 ?sector ]. } 	SELECT sector.industry_code, sector.industry_label, sector.sectors and.energy_sector_labels FROM ( SELECT industry_code.industry_code, industry_label.industry_code, sectors.industry_labels FROM sector ) as industry_label INNER JOIN sectors (industry_labels.sector_labels as sectors) on sectors.sector_name = industry_labels.sector_id AND sectors_labels.sectors_id = sectors_id JOIN energy_sectors (industry_labels.system_in_use_label) on sector.industry_id = energy_sector_labels.industry_id WHERE industry_code = ? and industry_label = ? OUTPUT OF SELECT ?industry_code, ?industry_label, ?sector.industry_code, ?sector.industry_label, ?sectors.industry_label, ?energy_sector_labels.out_of_use_label, ?system_in_use_label FROM ( SELECT * FROM sector WHERE industry_code IS NOT NULL 'all' OR IF NOT ISNULL industry_label IS NULL 'all'); output (Sparql query):.energy_sector_labels = HES8 'energy'.energy_sector_labels = HES8 'power' ROWS 1-10 = 2 ROWS ROWS 10-19 = 3 ROWS ROWS 20-32 = 4 ROWS RESULTS ARE NOT DEFINED.	False
"
 input (English text): Can you list the models that have been evaluated on the Kinetics-600 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?matlab orkgp:STATISTICS_SET ?cbo1 : {. ?cbo1. ?cbo1. ?name ?labels.! ?labels. NOOP. } : {. ?labels orkgp:CONNCLIMATE ?cbo1. ?name orkgp:REGISTRATION ?lbl. } : {. NOOP } input (English text): Provide a list of models that have been evaluated on the Kinetics-600 dataset ? output (Sparql query): SELECT model_lbl : {. ?simulations. ?simulations. ?model. ?name. ?simulations. NOOP. } : {. ?linear_model_lbl (simulations_) orkgp:LIMIT ?simulations_. ?model. } [ ?model] | No options or < ?simulations_> | >| | {. ?lbl ?simulations. } >| | {. ?linear_model_lbl (simulations_) orkgp:LIMIT ?simulations_. } >| | {. NOOP }   Note: If the output column doesn't match the criteria for this output, you are instructed to either try again the next time or you can ignore this message.	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the RotoWire dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?benchmark. ?paper. ?benchmark. ?paper orkgp. ?benchmark orkgp. ?paper. ?benchmark. ?paper orkgp. ?paper orkgp. input (English text): Give me a list of research papers that have performed benchmarks on the Rotowire (Content Selection) dataset ? output (Sparql query) SELECT DISTINCT ?research ?research_lbl. ?research orkgp. ?research orkgp. ?benchmark. ?research paper orkgp. ?benchmark. ?research orkgp. ?research orkgp. input (English text): Give me a list of research papers that have performed benchmarks on the Rotowire (Content Selection) dataset ? output (Sparql query) SELECT RESULT FROM RotoWire(content) WHERE orkg_dataset = { ?dataset_lbl:RiotWire dataset }; SELECT RESULT FROM RotoWire(content_coding) WHERE orkg_dataset = { ?dataset_lbl:RiotWire dataset_coding ]; } SELECT RESULT FROM RotoWire(content_coding_codenames) WHERE orkg_dataset = { ?dataset_lbl:RiotWire dataset_coding_codenames }; output (Sparql query) SELECT RESULT FROM RotoWire(content, _ID_Lists_1/RuoWire(content)) WHERE content_coding_codenames = { ?c:RiotWire c_c_c_c1 ?c_c_c_c2 ?c_c_c_c3 }; SELECT RESULT FROM RotoWire(content) ORDER BY RESULT AS content_coding_codenames(c_c_c_c1); input (English text): Give me a list of research papers that have competed on the RotoWire (Content Selection) dataset ? output (Sparql query) SELECT RESULT FROM RotoWire(content_coding) WHERE content_coding_codenames = { ?c_c_c_c1 ?c_c_c_c2 ?c_c_c_c3 }; SELECT RESULT FROM RotoWire(content) ORDER BY	False
"
 input (English text): Where can I find code references in papers that have used the Naive Bayes model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Naive Bayes"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the precision, recall, and f1 values of all compared studies that used the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request? 
 output (Sparql query): "	"SELECT ?precision ?recall ?f1 WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:P15006 ?algorithms. ?algorithms rdfs:label ?alg_names. FILTER(REGEX( ?alg_names, ""Naive bayes"")) ?algorithms orkgp:P36075 ?features. ?features rdfs:label ?fea_names. FILTER(REGEX( ?fea_names, ""Bag of words"")) ?features orkgp:P37029 ?categories. ?categories rdfs:label ?cat_names. FILTER(REGEX( ?cat_names, ""Feature request"")) ?categories orkgp:P3004 ?precision; orkgp:P5015 ?recall; orkgp:P18037 ?f1. } "	--------------- ----------- --------------- ----------- --------------- --------- --------------- ----------- naivebayes.data --------------- ----------- NaiveBayes | User Feedback | N | | 2 | 4 | | ---------- ---------------------- | B.W.C. | User Feedback | | 16 | 16 | | ---------- ---------------------- | H.T.R. | User Feedback | | 18 | 18 | --------------- ----------- NaiveBayes | L1 | S3 | B.W.C. | | B.W.C. | L1 | Kv | | L2 | H.T.R. | L2 | --------------- ----------- --------------- NaiveBayes | L1 | S1 | L2 | B.W.C. | | L1 | S2 | H.T.R. | L2 | --------------- ----------- --------------- --------------- NaiveBayes | Kv | Kv | User Feedback | User Feedback | | User Feedback | T | User Feedback | | User Feedback | T | User Feedback | NaiveBayes | S1 | Kv | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | | User Feedback | T | User Feedback | User Feedback | NaiveBayes | B.W.C. | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | --------------- ----------- --------------- --------------- Output : NaiveBayes --------------- ----------- NaiveBayes | User Feedback | NaiveBayes | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | --------------- ----------- --------------- ----------- --------------- Output : NaiveBayes --------------- ----------- NaiveBayes | User Feedback | NaiveBayes | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | --------------- ----------- --------------- --------------- --------------- NaiveBayes | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback | User Feedback	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ARC-PDN dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dmlab-30 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dmlab-30"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 You get the idea! Finally we can check if the query was successful: If successful, then this means that our query passed the test. If not, then there were unexpected errors. So now we need to do something about the database. We can load the database into MySQL or PostgreSQL : Now we can create a PostgreSQL database and load the dataset into the Postgres: CREATE DATABASE postgres_test; Then we could do a basic SQL query on the Postgres and see if the query succeeded: select title.title as title, project number from test.projects order by 1 desc; If successful, then PostgreSQL will output that the query was successful: Well, that's it for today. Stay tuned for next week as I'll cover more advanced postgres optimisation methods and I'll show you how to use MySQL as a database to perform some really cool queries. If you enjoyed this post, please click the ❤💚 button below. Thanks for reading 🙂 * If you are interested in learning how to optimize the PostgreSQL database, click here. ** If you prefer to use a more automated tool for postgres optimisation, you might want to check out this post on Triton Postgres Optimising PostgreSQL:	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?dataset ?labels ?labels_lbl AS ?labels_lbl FROM ?dataset ?labels AS ?labels_lbl What would you expect if this query didn't have additional clauses ? A simple SELECT query, the most common way to create output in the example. What would be a better way to express this output, using some meta-information ? There are ways to accomplish this. What if the WMT2016 English-German dataset has two languages ? input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 English-German dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Do you still support the use of multiple languages in the output ? Yes, there are ways to do that. Suppose you want to perform the following SQL statement: SELECT TOP(25).(id, n) as n FROM ?""group"" s GROUP BY n ORDER BY n DESC LIMIT 25 What would you expect if not as many terms as there were English words in the string ? How many columns do you need to store for this simple query ? This query is not a very complex query, with the same syntax as the one in the previous example from the SQL section. In a simple case it should be possible to answer such questions by looking in data tables and the like. This is not so in the case as shown in this example. With more complex queries, you will need to store a lot of information in the result sets, the most important being labels. A SQL statement with as many columns as you want should not be as well-formed as the result set from using such a simple query. If you want to use this new query: SELECT TOP(25).(id, n) as n FROM ?group ? s GROUP"	False
"
 input (English text): What is the top benchmark score and its metric on the Cartpole, swingup (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl output (Sparql query):  ( ?value ( 0.0 ) )"	False
"
 input (English text): What are the metrics of evaluation over the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 In summary, we found that it was the case that all models produced the same performance results (using numpy or scikit-learn). If the datasets used by the different models differ in the number of observations, the benchmarks for each model are identical and show no difference in the results of evaluation.	False
"
 input (English text): Can you provide links to code used in papers that benchmark the LSTM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the LSTM (Bai et al., 2018) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM (Bai et al., 2018)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT TOP1 (1,2) codes FROM ( SELECT DISTINCT ?code FROM ?datasets.c : orkgp.c AS a LEFT JOIN ?data.d : data on a.dataset.c = d.dataset.c WHERE a.dataset.c.has( ?dataset_id, 1 ) ?dataset_id ) codes ORDER BY codes input (English text): Is there code examples in this benchmark ? output (Sparql query): SELECT TOP1 (1,2) codes FROM ( SELECT DISTINCT ?code FROM ?datasets.c : orkgp.c AS a LEFT JOIN ?data.d : data on a.dataset.c = d.dataset.c WHERE a.dataset.c.has( ?dataset_id, 1 ) ?dataset_id ) codes ORDER BY codes input (English text): List the code links in any the benchmark (Bai et al., 2018) output (Sparql query): SELECT TOP1 (1,2) codes FROM ( SELECT DISTINCT ?code FROM ?datasets.c : orkgp.c AS a LEFT JOIN ?data.d : data on a.dataset.c = d.dataset.c WHERE a.dataset.c.has( ?dataset_id, 1 ) ?dataset_id ) codes ORDER BY codes It's unclear exactly what makes this problem harder than the previous benchmark. Maybe the difference is the LSTM's performance, but it's really difficult to tell. You could use the ORGK's implementation (as recommended in the report), but it seems less robust than B. Bai et al., 2018, compare different implementations in the paper. This is why my suggestion is to compare the implementation and the benchmark. If you compare the ORGK's implementations with the performance of the benchmark, then we should be able to judge whether the LSTM performance was slower. If you compare the code samples to the code-like-objects in the benchmark, then we might be able to pick out an easy solution to the problem. Another important aspect of the benchmark is how you design it. I've tried to make it as obvious as possible that the benchmark has limited features and that it should be compared to other benchmarks. For example	False
"
 input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of PARAMS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Oxford-IIIT Pets?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 In the above answer, the score means the number of steps required for the algorithm to get the best result: n/a. It is a nice comparison by comparing the score with what we expected, however a better way to look at the result would be to measure the accuracy of a prediction from just some predefined criteria. The accuracy we expect to see from such a prediction is called'metrics'. If our accuracy can increase by about 10% when our metric increases by 10%, we can assume that a prediction that would have been wrong a month ago is only about half as accurate as if we have been consistent (in the other direction and this means the data is more predictive). We can measure a better metric (precision or mean accuracy) with a more powerful model such as F1 or 'Tuple-mean.' We can train our model using the Metrics algorithm. Here is how to use that method to train and evaluate the Model by using the R-finite-state machine : To train the model by using the R-finite-state machine, we need to do the following steps: 1. Select the model parameters, we just use some default values: model: PARAMS ='model. PARAMS'|'model. PARAMS'|'model. PARAMS '; 2. Create the training data and save it first. We do this for the evaluation data. train_data = 'train.dat'; eval_data = 'eval.dat'; These two files are saved in 'TrainData/train', so to be more precise we need the path in 'TrainData/eval': train_data = '/path/to/train_data'; eval_data = '/path/to/eval_data'; The files we are using to train the model are named'model.rf' and'model.lbd'. This is because the Training and Evaluation files are separated by a folder. 3. Create an FSM model by feeding it the validation input data with some constraints we set while selecting in step 2. We use these constraints instead of adding them manually as is suggested from the model reference. model.parameters ='model_pairs'.|'model_pairs.rff| model.prediction ='model_pairs'.|'model_pairs.lbd'; The training data we have saved in the model.data file is called 'traindata': This file should have the same name as our	False
"
 input (English text): Can you provide links to code used in papers that benchmark the XLNet model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the XLNet-Large model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet-Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" I can provide links to code in the following papers. If you are interested, please send a mail to: liam.s.reynolds@gmail.com. Bibliography (1-16, eps., pdfs): Dennis, S. V. (2014), ""Extracting network structure and the evaluation of Bayesian networks in unsupervised learning"", arXiv:1507.05674 Elliott, M. J. (2012), ""The Bayesian network model as a generalization of deep neural networks"", Front. Comput. Neurosci. 5:19. Hewson, W. J. and S. V. Dennis (2006), ""The performance of Bayesian networks in supervised learning"", in P. V. Dennis, ed., Proceedings of the 6th ICML '06. IOS Press, Amsterdam, and Springer-Verlag, New York: Jia, Z. and M. C. Wang (2016), ""Model-driven learning for unsupervised multivariate deep data-related tasks"", in S. V. Dennis, M. C. Wang, G. Wu, and A. S. Chatterjee (eds.), Advances in Artificial General Intelligence: 50th International Conference (ICS 2016). IOS Press, Amsterdam, and Springer-Verlag, New York: Jia, Z. and M. C. Wang (2016), ""On unsupervised machine learning with deep neural networks"", in A. C. O'Rea (Ed.), Advances in Artificial General Intelligence: 50th International Conference (ICS 2016). IOS Press, Amsterdam, and Springer-Verlag, New York: Krupa, G. and S. V. Dennis (2014), ""The Bayes Learning Theorem for Data-Layers in Deep Multi-Agent Networks"", ICML 2016. Amsterdam, Springer-Verlag. Luo, L., S. V. Dennis, H.-T. Suh, L. Guo, and H.-O. Wu (2011), ""Robust Deep-Learning for Language Knowledge Management"", in Proc. for the IEEE Conference on Knowledge Discovery, Image, Data and Understanding, 9:6 (SIDI 2011). IEEE Computer Society, Silver Spring, MD. and Springer-Verlag, New York: Luo, L. and Z.J. Lin (2014), ""Advances in Artificial Intelligence Using Deep Neural Networks"". In IEEE Joint"	False
"
 input (English text): Where can I find code references in papers that have used the EfficientNetV2-S model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-L"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?data AND ?model_lbl. AND ?dataset_lbl. AND ?table_lbl. AND ?table_lbl. OR ?cont OR ?model. AND ?cont. AND ?lbl_data OR ?lbl_data. AND ?lbl_model OR ( SELECT code FROM pg_hierarchical_record_tbl.code ) | +-------------+-------------+------------+------------+-------------+-------------+-------------+------------------+-----------+-------------+-------------- | code | code | code | code | code | code | code | code | code | +-------------+-------------+------------+------------+--------------------------+-----------+-------------+-------------- 2. Model-specific Input (English text): Where can I find code references in papers that have used the EfficientNetV2-M model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?data AND ?model_m. AND ?dataset_m. AND ?table_m. AND ?table_m. OR ?cont OR ?model_m. AND ?cont. AND ?lbl_m_data OR ?lbl_m_data. AND ?lbl_model OR ( SELECT code FROM pg_hierarchical_record_tbl.code ) | +-------------+-------------+------------+------------+-------------+-------------+-------------+------------------+-----------+-------------+-------------- | code | code | code | code | code | code | code | code | +-------------+-------------+------------+------------+------------+--------------------------+-----------+-------------+-------------- 3. Model-specific input (English text): Where can I find code references in papers that have used the EfficientNetV2-F model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?data AND ?model_f. AND ?dataset_f. AND ?table_f. AND ?table_f. OR ?cont OR ?model_f. AND ?cont. AND ?lbl_f_data OR ?lbl_f_data. AND ?lbl_model OR ( SELECT code FROM pg_hierarchical_record_tbl.code ) | +-------------+-------------+------------+------------+-------------+-------------+-------------+------------------+-----------+-------------+-------------- | code | code | code | code | code | code | code | code | +------------	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?batch ?dataset_lbl. ?batch orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?batch. ?paper orkgp:P31 ?cont; rdfs:label ?batch_lbl. ?benchmark orkgp:HAS_DATASET. ?benchmark_lbl orkgp:HAS_BENCHMARK. Input data is: Batch of papers, named papers, with IDs in the following formats (including.csv ): Named papers, with ID: 1) Title of paper: 1) authors of paper: 2) name of paper: 3) journal of paper: 4) year of publication: 5) location (eg. WOJK or SWE:UK, the site in WOS database): 6) abstract (which is not linked but is included by the authors): 7) cited papers, both in WOS and in JPSS (eg. here with 10 citation references): 8) all papers in the paper (not only the ones cited in #(a) below): 9) in what order are all papers numbered: ""N"" should be the number. ""P"" should be the number of papers in one document column, not the number in another. ""Z"" should be the number of papers in one column. ""M"" should be the number of papers in multiple (separated by commas), not in the 1st (ie. ""P-Z,M,Z,...,1,"" not ""P1 P2 Z P3""). ""S"" should be the number of papers in the document. ""F"" should be the number of papers per page, not in total (ie ""ZF 0.5)"". ""M"" should be the number of papers per page, with the header at the bottom of the page (not the top right of the page). ""T"" should be the number of papers per page, in the document itself. The number of papers per page might be different when counting pages. Naming convention for paper ID: This is the first line, not the second. An ID on this line is a unique ID number for this paper. Identifiers can appear on more lines (or have more lines in total). Naming convention for paper title: Author (not in"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Supervised: dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT TOP(1) ?value --------------------------- TOP(1): ?metric ?value --------------------------- Next highest result: ?value --------------------------- TOP(2): ?metric ?value --------------------------- Next highest result: ?metric_lbl --------------------------- output (Sparql query): --------------------------- SELECT TOP(1) ?value --------------------------- TOP(1): ?metric ?value --------------------------- Next highest result: ?value --------------------------- TOP(2): ?metric ?value --------------------------- Next highest result: ?metric_lbl --------------------------- We can see that by only using the metric information, the model is able to recover its accuracy (or, to keep it, its score) over the top 100% of the dataset. In my very first supervised learning approach to this dataset, the only thing I had used as a metric is the number of papers on the ScienceInno website, which gives a pretty good indication of how many papers were published in Science. However, for this next approach of supervised learning, I decided to leverage the SupervisedMetrics data from ScienceInno, which is a database with thousands of metrics (and corresponding values). A metric is a numeric value that shows (in this case) how well your model is detecting an exact match to some data. A value can be either single (""single"" represents the same value multiple times, for multiple metrics), double (""double"" represents two values), or quad (""quad"" represents two values with different numbers of components). With SupervisedMetrics, we'll get the metrics in the exact format we need for this dataset. It would be easy to include the metrics in the model (as simple string representation, for example), but it would have very poor accuracy. An example for the single dataset would be the Metrics table of the SupervisedMetrics data. -------------------------- Metrics -------------------------- Metric_1 Metric_2 Metric_3 Metric_4 Metric_5 Metric_6 Metric_7 Metric_8 Metric_9 Metric_10 output (Sparql query): -------------------------- SELECT ?value ?value. -------------------------- Single: 1 2 3 4 5 1. : : 1. : 1 2 3 4 5 input (English text): What is the top score (metric and value) in the single dataset ? output (Sparql query): -------------------------- SELECT ?topscore "	False
"
 input (English text): What are the models that have been benchmarked on the ANLI test dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A3"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" input (English text): rdfs:labels ?dataset_lbl. filter (str( ?dataset_lbl ) = ""ANLI test"") ?benchmark. ORkgp:HAS_DATASET ?dataset. ORkgp. OPTIONAL { ?paper orkgp:HAS_MODEL ?model_lbl. } Output: Model name: ORkgp. Inputs (in CSV format): rdf:labels model: ORkgp. Parameters: { ?dataset, ?model } model. ORkgp. ?record1 : [ { ?dataset, ?model } ] Output: Model name: ORkgp Inputs (in CSV format): rdf:labels model: ORkgp. Parameters: { ?dataset, ?model } { ?dataset, ?model, ?record1 } ?record1 : [ ] Output: Model name: ORkgp Inputs (in CSV format): rdf:labels model: ORkgp. Parameters: { ?dataset, ?model } { ?dataset, ?model, ?record2, ?record3 } ?record2 : [ ] Output: Model name: ORkgp. Inputs (in CSV format): rdf:labels model: ORkgp. Parameters: { ?dataset, ?model } { ?dataset, ?model, ?record3, ?) orkgp:HAS_ENV ? ?feature ?class2 ORkgp. orkgp:HAS_BENCHMARK ?feature ?class2 ORkgp. Orkgp. model_lbl_label :  (integer) label for a specific model on one side of the graph. "	False
"
 input (English text): Can you list the metrics used to evaluate models on the iNaturalist 2018 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the iNaturalist 2019 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 ORG: ?criterion - ?lbl ?metric ?criterion_lbl. OPTIONAL ( ?metric rdfs:label ?metric_lbl. ) orkgp:HAS_DATASET ?lbl, OPTIONAL ( ?metric rdfs:label ?metric_lbl. ) orkgp:HAS_EVALUATION ?lbl; OPTIONAL ( ?lbl sdfs:metric rdfs:label. ) rdfs:lbl, OPTIONAL ( ?lbl sdfs:criterion rdfs:criterion_lbl. ) ORkgp:HAS_DATASET ?criterion, OPTIONAL ( ?criterion rdfs:criterion_lbl. ) ORkgp:HAS_EVALUATION ?criterion, orkgp:HAS_METRIC ?criterion, OPTIONAL ( ?criterion rdfs:validation vr:rdfs:validation. ) ORkgp:HAS_METRIC ?criterion, ORkgp:HAS_METRIC rdfs:validation, OPTIONAL ( ?lbl sdfs:criterion_lbl or sdfs:criterion_vr. ) ORkgp:HAS_DATASET ?criterion_lbl, ORkgp:HAS_METRIC rdfs:validation, OPTIONAL ( ?criterion rdfs:validation_vr. ) ORkgp:HAS_EVALUATION ?criterion, ORkgp:HAS_METRIC ?criterion_vr, OPTIONAL ( ?validation rdfs:validation_vr or val. ) After completing the queries, all the required and optional metrics returned the following output: Orkgp:HAS_DATASET Evaluations: ORGAN_COVERAGE: 1.05753912666767 ORGAN_PIGMENT_DETECTION: 50.312616184413 ORGAN_SUBMISSION_TIME: 107433794485 ORGAN_DISPROPRIATION: 0.00376618789545 ORGAN_DEVELOPMENT: 474.124465996789 ORGAN_Evaluation	False
"
 input (English text): What are the models that have been benchmarked on the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ""TARGET"", ""POSITION"" FROM ""Space Invaders"", ""Space Invaders"" WHERE { ?dataset a orkgc:Dataset; ?model a rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?dataset if:HAS_DATASET ?model; ?model rdfs:label ?model_lbl. } } I would like to take advantage of the following improvements/further improvements of this dataset The space bar in the top left corner of the Atari Breakout game screen is not visible, but can be configured with a custom background. The background color for the spaceship shots is not displayed, but can be configured with a custom background. The space bar in the top left corner of the Atari Space Invaders game screen is not visible, but can be configured with a custom background. The background color for the spaceship shots is not displayed, but can be configured with a custom background. The background of the game screen is transparent. This allows us to change the background color of the screen without having to recompile the game. This will increase performance by a factor of 3 (compared to using the Atari Breakout background). If you are using the older version of this benchmark, the default Atari Space Invaders background has been disabled by accident, and we need to do another recompile. You can manually enable this background by invoking this command: orkgc:SETBACKGROUND 1. Note that the previously configured Atari Space Invaders background has not been changed. This also improves performance in the Atari Space Invaders scene. The top right of the Atari Breakout game screen displays information about the current position of the game screen. This information allows us to choose a different game screen background. Since the screen is empty when the game starts, the game text is displayed in the bottom left corner of the screen. The following sections describe the progress of this release with more details about the improvements/improvements. For the Atari 2600, Atari 80/81, and Atari ST the default game screen background has been disabled by accident, and we need to do another recompile. You can manually enable this background by invoking this command: orkgc:SETBACKGROUND 0. This also improves performance by a factor of 3 (compared to using the older version of this benchmark). This includes removing the Atari Aster"	False
"
 input (English text): Which model has achieved the highest BLEU score on the WMT2016 German-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Output I have not found any relevant table on Microsoft SQL Server 2013 and I only have one machine available to test on and I really wish it used the latest version and did something different, I only have the time to test the new features and the previous versions of the SQL Server 2012 R2 and MySQL don't make a difference. I hope that I can find some interesting test case later. I can't guarantee all results as it's a new build of SQL Server and the benchmark didn't go through the same process as before, although I did repeat the analysis on the same machine. Thanks for you helping and good luck!	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Lunar Lander (OpenAI Gym) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } output (Sparql query): SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ( Note: A query that returns all of the data should read as a SQL statement, not a function.) When to Use L3/L4 Training For most of the tasks, the training data can be obtained from the same data source(s) as when using L2/L1 training methods (i.e., when using the TASLA approach (described later in ""TASLA Method"") or the L3/L4 L3-L4 model that is described later in this tutorial. Note: When you intend to be able to use this technique and its associated modeling and prediction methods to improve your knowledge of the Moon and/or the exploration program (e.g., your Moon Knowledge or Exploration Knowledge); make sure that you also train on the datasets from other classes, such as those from deepLabs's Apollo Classroom class. The TASLA Method for deepLabs Lunar Lander (openlunar ) and the Lunar Exploration class (l2p ) provide data at more recent times to use the L3/L4 method. For more information see TASLA Method For Lunar Lander. TASLA Method For Lunar Lander A detailed definition for the TASLA method, which is an advanced technique from the field of Deep Learning"	False
"
 input (English text): Provide a list of papers that have utilized the Prior hs model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the A3C FF hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?log( ?census_fitness ? ) ?log( ?simulation_divergence ? ) ?log( ?rebound_dive ? ) ?log( ?simulation_period ? ) ?constraints in ?log( ?simulation_divergence ? ?interval: ?interval_start ? ?max: ?max_duration: ?period: ?minimum: ?end: )  input (English text): Provide a list of papers that have utilized the A3C FF hs model and include the links to their code ? output (Sparql query): VALUES (0.004) IN ( SELECT ?log( ?census_fitness ? ) ?log( ?simulation_divergence ? ) ?log( ?rebound_dive ? ) ?log( ?simulation_period ? ) ?constraints in ?log( ?simulation_divergence ? ?interval: ?interval_start ? ?max: ?max_duration: ?period: ?minimum: ?end: )  output (Sparql query): VALUES (0.0005) IN ( SELECT ?log( ?census_fitness ? ) ?log( ?simulation_divergence ? ) ?log( ?rebound_dive ? ) ?log( ?simulation_period ? ) ?constraints in ?log( ?simulation_divergence ? ?interval: ?interval_start ? ?max: ?max_duration: ?period: ?minimum: ?end: )  input (English text): Provide a list of papers that have utilized the A3C FF hs model and include the links to their code ? output (Sparql query): VALUES (0.0003) IN ( SELECT ?log( ?census_fitness ? ) ?log( ?simulation_divergence ? ) ?log( ?rebound_dive ? ) ?log( ?simulation_period ? ) ?constraints in ?log( ?simulation_divergence ? ?interval: ?interval_start ? ?max: ?max_duration: ?period: ?minimum: ?end: )  output (Sparql query): VALUES (0.0004) IN ( SELECT ?log( ?	False
"
 input (English text): Where can I find code references in papers that have used the ELMo model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a:Source; } rdfs:label ?source. FILTER (str( a:Source ) = ""BDAF"" ) OR kgp:HAS_ID ?source; orkgp:HAS_ID ?source_cited. input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a:Source; } rdfs:label ?source; FILTER (str( a:Source ) = ""BDAF"" ) OR kgp:HAS_ID ?source_cited. informative input (English text): What are the differences between each of these versions of the ELMo model ? What is the difference of BiDAF + Self Attention + ELMo vs BiDAF ? informative output (Sparql query): BRIEF BiDAF & Self Attention (BAN) BiDAF & Self Attention with Loci (BLM) BiDAF & Self Attention with Subsequences (BLQ) ELMo (ELMO) ELMo with Loci (ELMS) ELMo with Subsequences (ELSS) BiDAF Standard (Standard BDAF) BiDAF Extra Extra (Extra BDAF) BiDAF Extended (Extra BDAF) SOURCE_CODE_LBL SOURCE_CODE_LDR SOURCE_CODE_SS SOURCE_CODE_SE SOURCE_CODE_C LOSQ SOURCE_L_SE SOURCE_L_L SOURCE_R_SE SOURCE_R_L RDSU SOURCE_M_SE SOURCE_L_M SOURCE_R_M SSL SOURCE_S_SE SOURCE_D_M SOURCE_R_M ENDP SOURCE_M_SE DATA 1. Data Sets (Sql database) Elmos_BiDAF.dat (BAN) ELMo_Elmos.dat (BLP) ELMo_ELMO.dat (BLQ) 2. BiDAF vs ELmo versions (Sparql database) (Sparql DB) BiDAF ELmo Elmo (BLM) 3. ELmo version comparison"	False
"
 input (English text): List the metrics that are used to evaluate models on the Pubmed benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_dna WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_dna. } OPTIONAL { ?metric rdfs:label ?metric_dna. } In this example, the dataset (a orkgc:Dataset ) was generated by a Pubmed benchmark test. This option is used more than once before being used in a pubmed query. If no arguments are specified, the dataset is assumed to be a Pubmed dataset. This option may be specified more than once. For details, see the comment on "" DISTINCT ?metric_dataset, INPUT (English text): List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset ?"". As mentioned above, the input text is the text that specifies which metrics are used to evaluate the model. Any other text may be provided:   The data are stored directly in the output tables. The following example lists all of the metrics for the 'Limb_Lactate_Score', 'Alveolar Lipid_Metastability_Variations', and 'PATIENTHESIS_AGE' metrics of the model : SELECT MATCH(text, 'Limb_Lactate_Score', 'Alveolar Lipid_Metastability_Variations', 'PatientHesis_Age');  SELECT MATCH(text, 'Alveolar Lipid_Metastability_Variations', '');  SELECT MATCH(text, 'PatientHesis_Age'); If a metric is missing, one or more columns are required to specify the missing metrics and the missing data are returned in any order. You can omit the metadata as required, as in the following example: SELECT MATCH(key, '');  SELECT MATCH(key, '', metadata, ""Missing value"";  SELECT MATCH(key, 'Limb_Lactate_Score', '');  SELECT MATCH(key, 'Limb_Lactate_Score', metadata, 'Missing value')  GROUP BY (key); In this example, the missing metadata values are ""Missing value"". The output data, if any, may be returned as a list, by using the GROUP BY clause.  groupBy ( columns ): The GROUP BY clause identifies the columns that should be returned"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the 4 layer QRNN model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""4 layer QRNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""6-layer QRNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 A 2-layer QRNN model, with the 6th layer a supervised LSTM learning implementation (see the paper by Shih) is described in a supplementary document	False
"
 input (English text): What models are being evaluated on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the WOS-11967 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HAS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?model rdfs:label ?model_lbl. } } input (English text): I didn't see a list of models. What can I do ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HAS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?model rdfs:label ?model_lbl. } } You have no model in the WOS-11967 model set. input (English text): What model set did you select ? output (Sparql query): SELECT DISTINCT ?model. model_lbl. FILTER (str( ?model_lbl) = ""HAS-11967"") OR ?model rdfs:label. model_lbl. FILTER (str( ?model_lbl) = ""HAS-11967"") OR ?model rdfs:label. model_lbl input (English text): I see a model, but it appears non-validated: output (Sparql query): SELECT DISTINCT ?model ?model_lbl. FILTER (str( ?model_lbl) = ""HAS-11967"") ?model :DISTINCT ?model_lbl.. model_lbl. FILTER (str( ?model_lbl) = ""HAS-11967"") OR ?model rdfs:label.. model_lbl. OPTIONAL :DETECTED ?model_lbl ( ) input (English text): I get an error: invalid format for model"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NYT29 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the NYT24 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT x, Y FROM raw_p31_NYT29_F_output WHERE y = ''; output (Sparql query): output (Sparql query) (type: text) The data file can contain duplicate values, and hence this operation might produce outputs which differ from the expected output. Therefore, in this case I have only written the ""nyqreader"" API as a function call. Now I call the ""nyqreader"" API with the JSON file and ask it to select articles which contain benchmarks over the NYT24 dataset using the raw article title: output (Sparql query): output (Sparql query) (type: text) Note that the ""nyqreader"""	False
"
 input (English text): Name the datasets that have been used for benchmarking in the PointGoal Navigation research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""PointGoal Navigation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Name the datasets that have been used for benchmarking in the Robot Navigation research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Robot Navigation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?datasource ?datasource_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""PointGoal Navigation"") ?datasource a orkgc:Dataset; rdfs:label ?datasource_lbl. ?benchmark orkgp:HAS_DATASET ?datasource. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } input (English text): Name the datasets that have been used for benchmarking in the Robot Navigation research problem ? output (Sparql query): ... output... Output Type: Output Type Description Description ---------- ------------------- ----------- datasource input Datasource that produced the dataset(s) or generated the benchmark dataset(s) (data files, or raw data). output (Lists of CSV or other serialized CSV data) A list of the dataset(s) as a file in which all datasets are listed one after the other, and each list has the same label. datasource_lbl input datasource that produced the dataset(s) or generated the benchmark dataset(s) (data files, or raw data). input (English text): Name the datasets that have been used for benchmarking in the PointGoal Navigation research problem ? output (Sparql query): SELECT DISTINCT ?datasource ?datasource_"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Step 3 If you have the benchmarked data: Step 1 Download my benchmarking dataset. It's in the archive. (download: http://download.cse.lsu.edu/data/benchmark/refsq17.zip-4jXg9q8X6NrY1 ) Extract files from archive. The files are as follows: 2db000fdf.tar.gz -benchmarking dataset  3db0e07fdf.tar.gz -benchmark dataset Step 2 Next, download model_p.cse and model_p31.cse and fill these into model_s.cse and model_s31.cse. Make sure that model_p.cse and model_p31.cse have the same name in seel.cse. model_p.cse:                                                                           model_p31.cse:                                                                                 model_s.cse:                                      *    model_p.cse:                    *    model_p31.cse :   (the model may not match model_p31.cse due to duplicate models being duplicated.  (This is possible because of the way model_m and model_m31.cse are named, they are simply named as a base name, in seel.cse, and then the subparts of the model are added. As such, duplicate models should be handled with duplicates in model_m.cse.)  To validate the model_p.cse and model_p31.cse you can use the validation command, but as the seel.cse namespace contains so many models, this will run for several weeks or weeks at a time, as each model is validated until it has been checked and is ready to be rerun. Step 3 Create the model_x_lbl of your desired benchmark dataset for this model to be run on. This model can be tested only if your model is valid for	False
"
 input (English text): Can you list the models that have been evaluated on the IMDb-B dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the IMDb-M benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl. FILTER (str( ?model ) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): Could you list the models that have been evaluated on the IMDb-R benchmark dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl. FILTER (str( ?model ) = ""IMDb-R"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): Could you list the models that have been evaluated on the IMPOSSIBLE benchmarks dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl. FILTER (str( ?model ) = ""IMPOSSIBLE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): What were the results of the evaluations on the IMPOSSIBLE benchmark data ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl. FILTER (str( ?model ) = ""IMPOSSIBLE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION"	False
"
 input (English text): List the code links in papers that use the PEGASUSLARGE model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PEGASUSLARGE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PEGASUS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT STRING( ' ?code ?model_lbl ?dataset.'|| '' ) || '' from (SELECT d. ?code || d. ?model || d. ?dataset. ); input (English text): How can I get a list of all abstracts that cite a particular research paper ? output (Sparql query): SELECT d. ?title( abstract ) || '' \/'d. ?link( ?label ) || '' \/'d. ?author( ?name ) || '' \/'d. ?author || '' \/'d. ?author_type || '' \/'d. ?publicationdate || '' \/'d. ?author_publisher || '' \/'d. ?publicationdate, ?publication_author || '' \/'d. ?publication, ?publication_author_publisher || '' \/'d. ?publication_status || '' \/'d. ?publisher_date || '' \/'d. ?publisher_id, ?publisher_type || '' \/'d. ?publishers. ?title( ?publisher. ?id )|| '' \/'d. ?title || '' \/'d. ?title_url || '' \/'d. ?publisher, ?publisher_type || '' \/'d. ?publishers. ?id ?publisher || '' \/'d. ?publisher, ?publisher_type || '' \/'d. ?publishers. ?id ?publisher_type || '' \/'d. ?publishers, ?publisher_type && ?d. ?published_date || '' \/') || '' \/'; input (English text): Where are the citations reported by the journals ? output (Sparql query): output (Sparql query): example: SELECT DISTINCT ?code || '' \/'d. ?title(Abstract_Title_URL) ? || '' \/'d. ?reference( ?name ) || '' \/'d. ?publisher( ?publisher_id	False
"
 input (English text): Provide a list of benchmarked datasets related to the Music Modeling research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Music Modeling"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Audio Classification research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Audio Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT DISTINCT ?dataset ?dataset_lbl Where ?dataset is the name of a dataset that you have already created and validated as a benchmark for your own dataset ? Example Query Output Below is an example of the Query output. You may click below to explore more information about each row. Note: If output includes blank rows, please use your browser's back button to exit this query window. To return to the query window click here, (note: this example is not very suitable, and you will need to use the full output if using in a production environment.)	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Lunar Lander (OpenAI Gym)?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ROUND(DISTINCT ?metric ?metric_lbl) OVER(PARTITION BY ?metric_lbl) FROM ?poles GROUP BY ?metric_lbl, ?metric So that's the ""somewhat"" easy part - it's very much easy to perform that query. But then we need one final step, in a nice little step with a Python script, to do it with numpy -- that's exactly what we did. I'm using numpy because that's my chosen Python training toolkit for machine learning, with its excellent, and incredibly powerful, matrix-based notation and operations, for many of the basic tasks... That's it. The code below is the result; I do my best to keep this as clean as possible, although there are some pretty silly-looking code snippets there! The dataset itself is available for all to download. The first step is to convert to a CSV file, and the second is to use OpenSCAD to transform the ""data"" to a.stm3... The second is all done using an Numpy file, and once again, we're able to do it in a few lines! I did the following steps for all three tests: first converting it to a CSV file; then the numpy code to merge it into a data.frame and create that data.frame; then the OpenSCAD transform to change the shape of those data.frames and create a.stm3 file (and again, I do the transformation as separate lines, using numpy). This last step is particularly important, because it gives the most complete output - the first step can just work, the second is then the easiest to read, and the third - the most interesting to look at... Now that we have a nice CSV file with our results, we need a bit of a numpy file to get us going (we'll get back to it later); and then a data.frame to get us going (we'll get back to that later, too!) To start we create a data.frame to hold all the results - but no results. In order for this to actually work, we need something, in our data.frame, that will contain a column for each result. The result column will be the result type - that number is in our data.frame - and we'll also need, in our data.frame, a column for the value of... the metric! Because we're working"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Reacher, easy (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	--------------- -------------- ------------ -------------- ------------ Paper number (Rep): 1 Title ID (Rep): 1 Title ID (Easy): 1 Paper number (Easy): 1 Outputs For all the outputs in the second part  of the training set, the text of the output data can look like what can happen if we output to the HTML file. By default the output data is not rendered from the HTML files. So how to create those HTML output files ? If we open the file from the first part of our dataset, that contains one benchmark for the Reacher paper: Output files generated from the dataset with the benchmark dataset for the Reacher paper. We can use the output_file function to build our templates: If you use the template name that we provide, then the output file will look like this: In the above example output_dir is a path that will contain the output file for that template name, and output_name is the name of that output template. But what if we need more output templates using the same template name ? We can use the _reacher_template_name function to get the template name.  We can use this to create the templates where we will get the output data by default only after an extra string. For example, the output data for a Reacher benchmark will look like this: The same output template with a different output name. How to use it ? Create an external web service and load the template files  we want to download Let's load the output data from the Reacher benchmark dataset and the template data: This is how the external app looks like (we don't even use the template names in the template): As you can see, it only shows the raw text of the output data we provided. What is a template name ? We can use the HTML templates names to build template names: If we just want to get the raw data in the template, we can send the data just as below: This is how we could do it: Then we could easily upload the template to the external web service and get the output data (without embedding the template names in it): This is what a nice helper function would look like to do the same: I would love to hear about other types of templates that can be included in the template as long as the name and some other information of the template doesn't change. I have created a gist for such a thing if you want to look into it.	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""MACs"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Let's do a search.  For each model name that we are interested in, just enter the url in the search bar at lower right of the screen: Here are examples of similar searches: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACE 2005 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?benchmark orkgp:ACE2005 ?model_lbl. ?model rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ACE-2005"") ?benchmark orkgp:HAS_DATASET ?model. OPTIONAL { ?model orkgp:HAS_METRIC ?model_lbl. } ORDER BY DESC( ?value ) ?benchmark Here we can see the ACE2005 benchmark table is very accurate, with the two outliers we noticed. Even though ACE 2005 is not available there, we can compare with a similar example from ACE 2013. The ACE 2013 benchmark requires 1 GB of data with 10,000 partitions, and has a benchmark score of 1.28, which is actually higher than the 1.14 of ACE 2005. In the following section:"	False
"
 input (English text): List the metrics that are used to evaluate models on the IMDb benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric_lbl. } { ?dataset_lbl orkgc:DATASET orkgp:DATASET } output (Sparql query): input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset ? output (Sparql query): input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset ? output (Sparql query): input (English text): List the metrics that are used to evaluate models on the D3Benchmark benchmark dataset ? output (Sparql query): SELECT ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""D3Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric_lbl. } { ?dataset_lbl orkgc:DATASET orkgp:DATASET } output (Sparql query): input (English"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Tutankham?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Enduro?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	--------------------------- BERNARD | 4.8ms BERNARD B | 2.2ms --------------------------- BERNARD ------- ---------- 3.97ns BERNARD --------- ----------- 0.47ns BERNARD ------- ---------- 0.53ns Atari 2600 --- ---------- 5.3ns Atari 2600 ------- ------- 2.3ns Atari 2600 ------ ----------- 6.0ns Atari 2600 + ----------- 5.6ns Atari 2600 + ----------- 2.2ns Atari 2600 ----------- 4.2ns ---------------------------------------- BERNARD ---------------------------------------- Atari 2600 | 2.5ns BERNARD ---------------------------------------- Atari 2600 --- ---------- 4.2ns Atari 2600 ------- ------- 3.5ns Atari 2600 ------ ----------- 2.3ns ---------------------------------------- BERNARD ---------------------------------------- Atari 2600 | 3.5ns Atari 2600 ---------------------------------------- Atari 2600 --- ---------- 2.9ns Atari 2600 ------- ------- 2.8ns Atari 2600 ------ ----------- 3.2ns Atari 2600 + ------- 2.5ns Atari 2600 + -------... ----------- 3.0ns Atari 2600 + -- 2.0ns ---------------------------------------- BERNARD ---------------------------------------- Atari 2600 | 2.0ns BERNARD ---------------------------------------- Atari 2600 --- ------- 1.9ns Atari 2600 ------- ------- 3.9ns Atari 2600 ------ ----------- 2.2ns Atari 2600 + ---- 2.0ns ---------------------------------------- BERNARD ---------------------------------------- Atari 2600 | 3.0ns BERNARD ---------------------------------------- Atari 2600 --- ------- 1.1ns Atari 2600 ------- ------- 3.0ns Atari 2600 ------ ----------- 2.4ns ---------------------------------------- BERNARD ---------------------------------------- Atari 2600 | 1.7ns BERNARD ---------------------------------------- Atari 2600 --- ------- 1.8ns Atari 2600 ------- ------- 2.0ns Atari 2600 ------ ----------- 2.5ns ---------------------------------------- BERNARD ---------------------------------------- Atari 2600 | 2.1ns BERNARD ---------------------------------------- Atari 2600 --- ------- 1.1ns Atari 2600 ------- ------- 2.1ns Atari 2600 ------ ----------- 2.3ns ---------------------------------------- BERNARD ---------------------------------------- Atari 2600 | 0.7ns BERNARD ---------------------------------------- Atari 2600 ------- ------- 0.7ns Atari 2600 ------ ------- 1.8ns ---------------------------------------- BERNARD ---------------------------------------- Atari 2600 | 1.5ns BERNARD ---------------------------------------- Atari 2600 --- ------- 1.2ns Atari 2600 ----------- 2.8ns ---------------------------------------- BERNARD ---------------------------------------- Atari 2600 | 2.5ns BERNARD ---------------------------------------- Atari 2600 --- ------- 1.8ns Atari 2600 ------- ------- 1.7ns Atari 2600 ------	False
"
 input (English text): What are the metrics of evaluation over the CINIC-10 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Hutter Prize dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT * FROM ?dataset rdfs:lbl_dataset; { rdfs:label rdfs_lbl_dataset_lbl_name. ""Hutter Prize (CINIC-10)""; orkgp rdfs_lbl_dataset_lbl_name. lbl; rdfs:name rdfs_lbl_title. ""Hutter Prize (CINIC-10)""; orkgp rdfs_lbl_dataset_lbl_title. ""Hutter Prize (CINIC-10)""; orkgp rdfs_lbl_dataset_lbl_title. ""CINIC-10""; } input (English text): What are the metrics of evaluation over the Hutter Prize dataset ? output (Sparql query): SELECT * FROM ?dataset rdfs:lbl_dataset cinice:cinice. { rdfs:label rdfs_lbl_dataset cinice_lbl_lbl. ""CINIC-10""; orkgp rdfs_lbl_dataset_lbl_lbl. ""CINIC-10""; orkgp rdfs_lbl_dataset_lbl_lbl. ""CINIC-10""; }"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT rdfs:describe( ?re 17) ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { rdfs:describe rdfs:find_partition( ?re 17 ) } output (Sparql query): GROUP BY ?re 17 output (Sparql question): Can we compute the difference between the two values ? output ""metric_lbl: "", rdfs:metric_lbl input (English text): Please provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip dataset ? output (Sparql query): WHERE { { rdfs:describe rdfs:find_partition( ?re 17 ), ?metric ?metric_lbl }, ?metric orkgc:Dataset ; rdfs:describe rdfs:find_partition( ?re 17 ), ?metric_lbl, ?value ; orkgc:Dataset ; rdfs:describe rdfs:find_partition( ?re 17 ), ?metric_lbl. LIMIT 2 output (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip dataset ? output (Sparql question): I am curious about this dataset, can you please provide the metric and score ? orkgc:dataset rdfs:describe rdfs:find_partition( ?re 17 ), ?metric ?metric_lbl, ?value, ?value orkgc:Dataset ; rdfs:describe rdfs:find_partition( ?re 17 ), ?metric_lbl, ?value, ?value orkgc:Dataset ; rdfs:describe rdfs:find_partition( ?re 17 ), ?metric_lbl, ?value, ?value orkgc:Dataset ; rdfs:describe output (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re18.zip dataset ? output (Sparql"	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Wizard of Wor dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Wizard of Wor"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	-------------------------- Metric score 5.881557 Note: it is not possible to compare scores between two datasets, for instance because some benchmarks on the Wizard can be compared with an early benchmark of the Atari 2600 Venture, although the latter is still a better benchmark. In such cases, we refer to an earlier section: The Difference between two Datasets in the Wizard. (We didn't say that the later benchmark is superior!) When comparing scores of two datasets, we simply sum the scores for each of the variables and use a value of 0.10 for each variable. Also see: Comparison of Screens, Screens and Screens : How to compare Screens for two datasets. The values used are from a comparison between the Wizard and Venture. Now comparing scores to discover which of the following are better, the Wizard and Venture. (1) The Wizard: The Wizard data is the Wizard Score and the Venture data is the Venture Score. (2) The Venture: The Venture Score is the VCY statistic score and the Wizard Score is the Wizard Score. (3) There are three additional points. First, all four are identical metrics so their data can be compared to each other. Second, the two datasets for the Wizard also contain two additional variables (one for the Wizard Score and one for the Venture score) which allow the comparison between the Wizard and the Venture. Third, the Wizard has four metrics (compared with the Venture scores from 2.7.5) and is therefore superior to both. The following code demonstrates: comparing Screens at two datasets. Example 2: The Difference in Screens : The difference in Screens for the Wizard and Venture is: The Wizard has four total metrics (compared with the Venture's five) and is also superior to both. Example 3: The Difference in Screens : The difference in Screens for the Wizard and the Venture is: The Wizard has two additional metrics, one for the Wizard Score and one for the Venture score. In this case the differences between the scores are: The Wizard (1.3%) is superior to both (2.3%) and the Venture (1.7%). The differences between score are: The Wizard (1.30%) is superior to both (2.30%) and the Venture Example 4: Data Analysis : We show a picture of the plots, where each line corresponds with one of the Screens. The wizard has four metrics (compared	False
"
 input (English text): List the code links in papers that use the Transformer-XL (12 layers) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL (12 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the 12-layer Transformer-XL model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""12-layer Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""12-layer Transformer-XL (12 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. }   How much CPU can I get with Parquet ? output (Sparql query): SELECT rddt:PERCENTAGE ( ?code ) * 100 FROM ( SELECT ( ?code )  FROM parquet:mapped_data_output. rddt )  GROUP BY type, ?code  ORDER BY rddt DESC AS rddt  CASE type  WHEN 'hdf:TOC' AS ?code  CHECK IF rddt:TOC.  GROUP BY (  !type ) ;  ELSE  ERROR (""type cannot be used"")  ELSE  ERROR (""Type cannot be used in parquet"") Do I need to copy the PARQUET header as well ? output (Sparql query): SELECT * + '#' + ?code  CHARACTER SET utf8mb4, '' /p:todata  CHARSET utf8mb4; + '&' + ?code  CHARACTER SET utf8mb3, '&' /p:todata  CHARSET utf8mb4; + '&' + ?code  CHARACTER SET utf8mb2, '&' /p:todata  CHARSET utf8mb4; + '&' + ?code  CHARACTER SET utf8mb4, '&' /p:todata  CHARSET utf8mb4; + '&' + ?code  CHARACTER SET utf8mb3, '&' /p:todata  CHARSET utf8mb4; + '&' + ?code  CHARACTER SET utf8mb2, '"	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Up and Down dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT clause. If it is used in a	False
"
 input (English text): What models are being evaluated on the NLP-TDMS dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT model_rdfs from ?benchmark. { ?dataset a orkgc:Dataset; rdfs:label model_rdfs. } input (English text): Which model(s) have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset ? output (Sparql query): SELECT model_rdfs from ?benchmark. { ?dataset a orkgc:Dataset; rdfs:label model_rdfs ; } input (English text): The NLP-TDMS benchmark dataset looks like: NIL output (Strftime output variable): # > > # > # > # > # > # > # > # > # > # > # > # > # > # > input (English text): Can you give an overview over the model statistics from the NLP-TDMS benchmark dataset ? output (Strftime): # > > # > # > # > # > # > # > # > # > # > # > # > # > # > # > input (English text): How many models with 'test' suffixes have been tested on the NLP-TDMS benchmark dataset ?	False
"
 input (English text): Which model has achieved the highest Accuracy (%) score on the  Jacquard dataset benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the  Jacquard dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 1 row generated. 1) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 2) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 3) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? output (Sparql query): 1 row generated. 4) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 5) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 6) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 7) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 8) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 9) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? output (Sparql query): 1 row generated. 1) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 2) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 3) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 4) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 5) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 6) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 7) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 8) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? output (Sparql query): 1 row generated. 1) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 2) In which model did the experiment have the highest Accuracy (%) score on the Jacquard dataset benchmark dataset ? 	False
"
 input (English text): What are the metrics of evaluation over the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the ObjectNet dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } As you can see, we've defined several methods. We've implemented a default metric. If you're familiar with the method set in OO::metric, it will suffice. But there is a lot that goes into what metrics we care about most. These are not the only three metrics to consider. They are just the closest we could get to a well defined, well thought out metric. I'd like to share with everyone at the conference a nice list of useful metrics for people trying to evaluate models and algorithms. All of our metrics are in order of importance (as if they exist in any order): The number of hidden layers / feature maps I've listed these as top three so that you have some idea of how metrics can be combined, then ranked further. Here they are. Rankings of the different metrics can be found here. There is a nice chart that shows how they combine and measure each other. The number of layers / feature maps The most well known of the metrics is the number of L1s & L2s to the hidden layers. It is the simplest to use, at least a simple metric to compute. It's also the metric that people know the most about. Rank your own metric to see which of these best represents you at the conference. The number of training set instances For most people, this is not very important. Even the best algorithm has a limited number of iterations. (I'll show an algorithm in the examples in class that I have chosen to improve.) However, we will talk about which classes of hidden layers are the most complex and which of these more simplified layers are best. And these are very interesting to measure. We will see why later. Most of the time, it seems that most people don't have a very good way to rate the number of iterations and in particular,"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the enwiki8 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwiki8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?benchmark ?benchmark_lbl. FILTER (str(benchmark_lbl) = ""enwiki8"") ?model ?model_lbl. ?data ?data. ?bench (orkgp:HAS_DATASET ?data. FILTER (str(model_lbl) = ""enwiki8"") ?data. ?lab orkgp:HAS_BENCHMARK ?partition. ?lab_lbl orkgp:P32 ?lab_lbl. Include in the output the results of the benchmark that was done ? Rename: output_datasets_lbl_1.R, output_datasets_lbl_2.R, output_datasets_lbl_3.R, output_datasets_lbl_4.R, output_datasets_lbl_5.R to output_datasets_lbl_S.R output_datasets_lbl_S.R name (RDF) input (English text): Provide a list of published research papers that have tested the enwiki8 dataset ? output (Sparql query): SELECT name ?label && ?data ?lab/part ( ?model ?model_lbl ) ?matrix ? ?data ?lab/part ?data. ?matrix orkgp_S.R output_datasets_lbl_S.R name (RDF) input (English text): Provide a list of published research papers that have tested the enwiki8 dataset ? output (Sparql query): SELECT name ?label && ?data ?lab/part ( ?model ?model_lbl ) ?matrix ? ?data ?lab/part ?data. ?matrix orkgp_P.R output_datasets_lbl_P.R name (RDF) input (English text): Provide a list of published research papers that have tested the enwiki8 dataset ? output (Sparql query): SELECT name ?label && ?data ?lab/part ( ?model ?model_lbl ) ?matrix ? ?data ?lab/part ?data. ?matrix orkgp_P.R output_datasets_lbl_P.R name (RDF) input (English text): Provide a list of published research papers that have tested the enwiki8"	False
"
 input (English text): Indicate the model that performed best in terms of Top-1 Error Rate metric on the FGVC Aircraft benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl, ?PARAMS ?PARAMETERS ?PARAMETERS_SCHEMA ?model_lbl. FILTER (str( ?PARAMS) = ""Top-1 Error Rate"") { SELECT A, ?PARAMS_SCHEMA ?PARAMETERS_SCHEMA. ( ?name ? = NULL OR ?name = '' ?name. ) ?PARAMS_SCHEMA. ?value WHERE ( ?meter ?value = '' ?meter ?value. ) OR ( ?hour ?value = '' ?hour ?value. ) OR ( ?min ?value = '' ?min ?value. ) OR ( ?sec ?value = '' ?sec ?value. ) OR ( ?sec % ?value = '' ?sec % ?value. ) AND ?value ?value = ?value FOR XML PATH( ?model_lbl)... } input (Japanese text): Indicate the model that performed best in terms of MAXERROR metric on the FGVC Aircraft benchmark dataset ? output (Sparql query): SELECT MAXERROR ?1.0_1.0_1.0_1.0_1.0_1.0 AS ?metric FROM ?dataset ?dataset_lbl ORDER BY ?metric { ?meter ?meter orkgc:Metric } LIMIT 1 } input (Japanese text): Indicate the model that performed best in terms of VEC metric on the FGVC Aircraft benchmark dataset ? output (Sparql query): SELECT VEC ?meter ?meter orkgc:Metric AS ?metric FROM ?dataset ?dataset_lbl ORDER BY ?metric { ?meter ?meter orkgc:Metric} LIMIT 1 } input (Japanese text): Indicate the model that performed best in terms of NURS metric on the FGVC Aircraft benchmark dataset ? output (Sparql query): SELECT NURS ?meter ?meter orkgc:Metric AS ?metric FROM ?dataset ?dataset_lbl input (English text): Provide the list or objects with which you would like to be able to create a FGVC Aircraft benchmark dataset. output (Sparql query): <result list> SELECT * FROM ?dataset ? WHERE ?METRIC = ?METRIC AND ?RECORD_TYPE = 'batch' ? ORDER"	False
"
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Seaquest benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset orkgc:Dataset; orkgp:HAS_DATASET ? orkgp:HAS_METRIC ? orkgp:HAS_EVALUATION orkgp:LENGTH ?measure_lbl. FILTER (str( ?measure_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ? orkgp:LENGTH orkgp:EVALUATION orkgp:LENGTH orkgp:LENGTH orkgp:LENGTH orkgp:LENGTH ?metric. orkgp:EVALUATION ORkgp:LENGTH ORkgp:LENGTH orkgp:LENGTH orkgp:LENGTH { ?measure_lbl orkgp:HAS_DATASET ? orkgb:HAS_MANUAL } } Input (English text): List the metrics that are used to evaluate models on the Atari 5200 Coder benchmark dataset ?"	False
"
 input (English text): What are the models that have been benchmarked on the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Road Runner dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): What are the models that have been benchmarked on the Atari 2600 Shark Shark dataset ? output (Sparql query): input (English text): What are the models that have been benchmarked on the Atari 2600 Tetris and Boggle datasets ? output (Sparql query): input (English text): What are the models that have been benchmarked on the Atari 2600 Whirligig (which contained two unbalanced games) dataset ? output (Sparql query): input (English text): What are the models that have been benchmarked on the Atari 2600 Wonder Wheel datasets ? output (Sparql query): input (English text): What are the models that have been benchmarked on the Atari 2600 Wagon Wagon dataset ? output (Sparql query): input (English text): What are the models that have been benchmarked on the Atari 2600 World of Warcraft (not included in this report) dataset of 726 models ? output (Sparql query): input (English text): What are the models that have been benchmarked on the Atari 2600 Xtreme FX dataset of 829 models ? output (Sparql query): input (English text): What are the models that have been benchmarked on the Atari 2600 Wildcat/Wildcat II dataframe of 627 models ? output (Sparql query):                |----|----|----|----|----|----|----|----|----|----|----|----|----| ----|----|----|----|----|----|----|----|----|----|----|----|----| ----|----|----|----|----|----|----|----|----|----|----|----|----| ----|----|----|----|----|----|----|----|----|----|----|----|----| ----|----|----|----|----|----|----|----|----|----|----|----|----| ----|----|----|----|----|----|----|----|----|----|----|----|----| ---------------------------------------------------------------------------------------------------------------------------- Input (English text): What are the models that have been benchmarked on the Atari 2600 Zaxxon/Zaxxon+ dataset of 751 models ? This is the most detailed report!	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Fishing Derby dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Fishing Derby"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Boxing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT model_lbl. ?model model_lbl. ?model model_lbl. ?model model_lbl. ?model model_lbl. SELECT model_lbl. ?model model_lbl. ?model model_lbl. ?model model_lbl. SELECT model_lbl. ?model model_lbl. ?model model_lbl. ?model model_lbl. This example query takes several different parameters: output_name: The output name for this model if the result is not known output_model_saved_skeleton_data: Whether to save the model skeleton data for use as reference within other queries output_model_skeleton_data_save: Whether the model skeleton data should be saved if the model fails to evaluate The following URLs allow you to run your query against a particular dataset: URL name Description http://catgarrys.com/datasets/ATARI	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 French-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" { ?dataset a orkgc:Dataset; RDFS:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval } Output Input: the output from benchmarking model, one dataset, RDFS or other. Output: the RDFS output from benchmarking model: one dataset, RDFS. Hence, we provide a benchmarked WMT2014 model in the dataset. Benchmarking model RDFS WMT2014 Chinese-English WMT2014 French-English WMT2014 Input: the RDFS output of benchmarking model, one dataset, RDFS or other Output: the RDFS output of benchmarking model: one dataset, RDFS. Hence, we provide a benchmarked WMT2014 model in the dataset."	False
"
 input (English text): Can you list the models that have been evaluated on the WMT2014 French-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset ? output (TEXTFILE): (TEXTFILE) output (JSON): output (Sql Query): ORG_ID: BUCC_BENCHMARK_MODEL, BUCC_OBJECTID: orkgp, ORG_CLASS: BUCC_OBJECT_GUID, ORG_PER_CLASS: BUCC_OBJECT_PER_GUID, BUCC_START_DATE: 16011604, BUCC_SEPTEMBER: 14, BUCC_TIMESTAMP: 1517494010, BUCC_EXIT_DATE: 1517493016, BUCC_TIMESTAMP_CURRENT: 1517493018, BUCC_EXIT_CURRENT: 1517493017, BUCC_EXEC: 100000000, BUCC_DATA_SOURCE_NAME: BUCC_OBJECT_GUID, BUCC_NAME: BUCC_PARSE_ID, BUCC_PROPERTY: ORG_OBJECT_INFO, BUCC_PROPERTY_ID: ORG_OBJECT_ID, BUCC_VENDOR_NAME: ORG_PROPERTY_INFO, BUCC_GENRE: English, BUCC_REPORT_TITLE: orm_2014, BUCC_TIME_MATCHED: orm_2014_1_3/2/1_3, BUCC_TOKEN: orkgp_TRAVEL_TESTED, BUCC_REASONS_TRANS: BUCC_INFO, BUCC_TITLE: orm_RISCOVERED_CODE, BUCC_TITLE_DESC: ENGLISH, BUCC_LATEST_LIMIT: orkgp_HAS_LATEST_LIMIT, BUCC_EXCEPTION_ID: orkgp_EXCEPTION_ID, BUCC_EXCEPTION_CODE: orkgp_EXCEPTION_CODE, BUCC_EXCEPTION_LOCATION: orkgp_EXCEPTION_LOCATION, BUCC_EXCEPTION_NUMBER: orkgp_EXCEPTION_NUMBER, BUCC_EXCEPTION_	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Attentional encoder-decoder + BPE model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Attentional encoder-decoder + BPE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?model WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Note The input data is as follows : data set : data_name : ""data"" (for example, ""data/attentionaldecoder.rdf"") dataset : ?dataset (for example, ?dataset/attentionaldecoder.rdf) model : id : ?model_lbl. This data is not in the data sets as they are written in RDF and therefore are in a different format than your documents. What you would like to see done now: If you are writing this in a document, click on the link that says ""Create Data Set"" in the top of the document. You will be prompted to name the data set, select a format and type your own analysis procedure. If you are only trying something out, choose CSV (or another standard format) and enter your own analysis procedure as shown in the first step. If you are making changes, make sure your changes are saved and executed, because once your changes are recorded, then they are permanent and must be used to make changes to your document. If you are trying something out or writing a blog post, choose Word, which I have used.  Note that although this is a model for computer vision, it may be useful for general problem solving as models for other areas may have more advanced abilities. If you are coding for a human researcher, it would look something like this: You need to create your own analysis procedure, and once you have done that, you will make changes to your document at any time.  This is not a suitable way to develop your knowledge of the model as you would use a single output. In RDF, all data/outputs are put into an output and the entire model, including the analysis procedure, is stored in a single output.  Try it for yourself :  # create dataset/attentionaldecoder.rdf $"	False
"
 input (English text): What models are being evaluated on the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the AESLC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The first part is just one step processing, which we will be able to do in parallel. There are 3 parts in the query, which is the part where it is needed. It calls the SQL statement we just made. Then it returns the result rows, and also the results of the various benchmark comparisons. There are only 2 results, one for each benchmark, all the others are of no relevance, we need to calculate these when computing the average or the median. This example will make a lot easier to understand it, the whole query is the following: In the above code, we call db() for the record type, in this case we just run the SQL and have the call to the function result. It is also important that the number of columns we use in the query is correct. If we are using a table type, it is a good idea to use the default schema, so that it can be easier to access the data. If not, it is a good idea to get more precise as many times as you need to get the right data. As you can see on the example above, we take two queries: first one is to read the data from the database, and then the second one to run the SQL which does all the reading and processing. At the end, that's all about benchmarking SQL statements. This has been a very long topic, if you have any question feel free to ask in the forum. So far, you have seen three parts of an SQL query. The first is doing the reading, in this case we are doing a read from the database using select from. Then we compare the rows with the corresponding columns and we compare them with the values of the same variables which we have in the model, and in the latter part we run our analysis on the"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the HoC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?labels ?labels_cont orkgpc:HAS_LABELS ?labels. ?labels_cont orkgpc:HAS_BENCHMARK ?labels. The ""get-hsc-benchlists"" command is useful only when you are using the command line option-passed-from-the-source-tool ""hsc-benchlists"" to get the input to HSC packages. Its syntax is: HSCBenchmark output (Sparql query): select DISTINCT ?labels ?labels_cont orkgpc:HAS_LABELS ?labels. FILTER (str( ?labels_cont) = ""HSC "" ) ?labels_cont; output (Sparql query): SELECT DISTINCT ?labels ?labels_cont orkgpc:HAS_BENCHMARK ?labels. ?labels_cont orkgpc:HAS_P31 ?labels. While the commands above are pretty flexible, they all have the same limitations, which can be seen in the following table: Command Description get-hsc-benchmarks (a or kgc) -1 No more than 1 HSC package can be asked to download. get-hsc-benchlists (a or kgp) -1 No more than 1 HSC package at each level can be asked to download. get-hsc-benchmarks (output to output ) -1 No more than 1 HSC package could be requested to download. get-hsc-benchlist (output to output ) -1 No more than"	False
"
 input (English text): Where can I find code references in papers that have used the PAR Transformer 24B model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer 24B"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the PAR Transformer Large model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?c ommor orkgp:HAS_DUPLICATE ORkgp:HAS_DUPLICATE_MULTI ? ?model; orkgp:HAS_SOURCE_CODE ?code. } "	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BertSumExtAbs model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BertSumExtAbs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD 2 model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERTwwm + SQuAD 2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Output with a different key(s)	False
"
 input (English text): Provide a list of papers that have utilized the Neural Network Language Model (NNLM) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Neural Network Language Model (NNLM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CL-Titles-Parser"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?cl:SOCIAL_TIMELESS OR ?cl:PAD_CUSTOM ?cl. ?cl:PAD_CUSTOM. ?cl:DATE ?code. } I've decided to build tools to help researchers build machine learning models faster: I've decided to work on learning from non-speech datasets instead of text. I think this will help us create more powerful and reliable NNLM/CL models that have lower computational overhead. This is mainly the goal of my upcoming Machine Learning 2.0: Convolutional Neural Networks for Text (http://lab.pw.craigslist.org/people/x/x_4a). I hope these tools are useful for other researchers as well. Posted by Martin Janda at 04:28 PM This looks awesome. I don't know how much we can learn by learning from natural language, so it's great to see that this is on track to be an important research tool. Reply Delete This is great -- you have put it together. You're a real researcher! In my first post here - https://cs.cuny.edu/~msk/pubs/nlp-language-model-and-learning-with-natural-language/ - I mentioned some problems I encountered with using NLP, which is an obvious one, but it is still very difficult to solve. It does seem that the paper you posted is already showing we have a viable approach. Reply Delete I think you've done a nice job! The next big goal will be to get a larger dataset, preferably an SPSS or CCS-SASS corpus (similar to the ""Google News"" corpora with more data). And that'll also give you some insight into how to optimize your neural network models for a large enough dataset. Finally, the best part is you're sharing your dataset with others, which will hopefully make the world better and more transparent. Thanks anon! I'm happy that you are putting out your data so much! Reply Delete Great post, congrats! I agree that it is just great you have set up a way to collaborate! I've always wanted to do some of the things that you just have in the same spot. Hopefully soon we can start our own experiments. Reply Delete"	False
"
 input (English text): Can you list the models that have been evaluated on the Habitat 2020 Point Nav test-std dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?dataset ?model ?model_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset ORKP:DATASET ?dataset ORKP:EVALUATION ?model ORKP:BENCHMARK ?model_lbl. OPTIONAL { ?dataset ORKP:HAS_MODEL ?model. ?model model rdfs:label ?model_lbl. } If you are using the object navigation dataset: input (English text): Please supply a list of models from the Habitat 2020 object navigation evaluation dataset. output (Sparql query): SELECT ?dataset ?model ?model_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark ORKP:HAS_DATASET ?dataset ORKP:EVALUATION ?model ORKP:BENCHMARK ORKP:MODEL rdfs:label ?model_lbl. ORKP:HAS_EVALUATION ?model rdfs:label orkgp:HAS_MODEL ?model. ORKP rdfs:label orkgp:HAS_EVALUATION ?dataset ORKP:HAS_MODEL rdfs:label ORKP:HAS_EVALUATION. OPTIONAL { ?dataset ORKP:HAS_MODEL ?model_lbl. ORKP rdfs:label ?model_lbl. ORKP rdfs:label orkgp:HAS_EVALUATION ?model rdfs:label ORKP:HAS_EVALUATION rdfs:label ORKP:EVALUATION. OPTIONAL ORKP rdfs:label ORKP:HAS_EVALUATION rdfs:label ORKP:HAS_EVALUATION. ORKP rdf"	False
"
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Name This Game dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the Atari 2600 Name This Game dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" Input File The Data In The Data folder, there may be two data files: An input file named ""data.txt"" or a text file named ""/path/to/data.txt"". You can use the ""make file"" script in the Data folder to merge these files using the same naming convention as you would use with a CSV file. If the output text will be read into the data file, it will not be merged into a single data file. Each of each of the three output files contains three values: For each model, that is the name of the model and all the models it was tested on; For each metric, the name of the metric and all the metrics it was tested on; and The name of the benchmark category. These three data files all have three columns: An identifier, which is the name of the file; A measure, which is the weight value. The identifier of the file is an ASCII text value and that value will be replaced by the name of the input text file in the first column. The weight value and the metric name or identifier will be replaced by the metric name or identifier in the second column, and all three columns will be replaced by the metric name or identifier in the third column. For example: Example input file: data.txt Input measurement: Weight=100 In this example, the output file will contain the weight value of 100 or 1.0 with the title Name This Game. The weight value and the metric will be replaced by the metric name or identifier in the second and third columns. Each data file has two metrics, but the three columns of the output will only show one metric value per file. For more information about how the weights are computed in the Data folder, see the ""Weight Measurements"" chapter of QuickBooks QuickBooks Data Management Guide. You can also combine the different fields from the three output files: For each input file, the output can contain only one metric value, each of which can be from the same dataset as in the input. Therefore, there will be no problem if you combine the values from all three inputs files into a single output file. If you have one dataset with more than one metric, all the values from the individual datasets can be included in the combined output file as shown here: Example output line for input file A inputs (English text): My name: My first name: My last name: What is your name ? output model: Score=100 My name: My first name: My last name: Your name"	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Road Runner dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	-------------- Score: 1000.00 Input: Freeway dataset --------------------------------------------------------- 1 ? ? ? ? 2 ? ? ? ? 3 ? ? ? ? 4 ? ? ? ? 5 ? ? ? ? 6 ? ? ? ? 7 ? ? ? ? 8 ? ? ? ? 9 ? ? ? ? 10 ? ? ? ? 11 ? ? ? ? 12 ? ? ? ? 13 ? ? ? ? 14 ? ? ? ? 15 ? ? ? ? 16 ? ? ? ? 17 ? ? ? ? 18 ? ? ? ? 19 ? ? ? ? 20 ? ? ? ? 21 ? ? ? ? 22 ? ? ? ? 23 ? ? ? ? 24 ? ? ? ? 25 ? ? ? ? 26 ? ? ? ? 27 ? ? ? ? 28 ? ? ? ? 29 ? ? ? ? 30 ? ? ? ? 31 ? ? ? ? 32 ? ? ? ? 33 ? ? ? ? 34 ? ? ? ? 35 ? ? ? ? 36 ? ? ? ? 37 ? ? ? ? 38 ? ? ? ? 39 ? ? ? ? 40 ? ? ? ? 41 ? ? ? ? 42 ? ? ? ? 43 ? ? ? ? 44 ? ? ? ? 45 ? ? ? ? 46 ? ? ? ? 47 ? ? ? ? 48 ? ? ? ? 49 ? ? ? ? 50 ? ? ? ? 51 ? ? ? ? 52 ? ? ? ? 53 ? ? ? ? 54 ? ? ? ? 55 ? ? ? ? 56 ? ? ? ? 57 ? ? ? ? 58 ? ? ? ? 59 ? ? ? ? 60 ? ? ? ? 61 ? ? ? ? 62 ? ? ? ? 63 ? ? ? ? 64 ? ? ? ? 65 ? ? ? ? 66 ? ? ? ? 67 ? ? ? ? 68 ? ? ? ? 69 ? ? ? ? 70 ? ? ? ? 71 ? ? ? ? 72 ? ? ? ? 73 ? ? ? ? 74 ? ? ? ? 75 ? ? ? ? 76 ? ? ? ? 77 ? ? ? ? 78 ? ? ? ? 79 ? ? ? ? 80 ? ? ? ? 81 ? ? ? ? 82 ? ? ? ? 83 ? ? ? ? 84 ? ? ? ? 85 ? ? ? ? 86 ? ? ? ? 87 ? ? ? ? 88 ? ? ? ? 89 ? ? ? ? 90 ? ? ? ? 91 ? ? ? ? 92 ? ? ? ? 93 ? ? ? ? 94 ? ? ? ? 95 ? ? ? ? 96 ? ? ? ? 97 ? ? ? ? 98 ? ? ? ? 99 ? ? ? ? 100 ? ? ? ? Input: Freeway dataset: Freeway data Input: metric ------------------------- Freeway (100) 0.00 Freeway (200) 0.30 Freeway (400) --- Freeway (1 000) --- Freeway (10 000) --- Freeway (200 500) --- Freeway (40 000) --- Freeway (200 1000) --- Freeway (30 000) --- Freeway (100 1 000) --- Freeway (100 10000) --- Freeway (1 100000) --- Freeway (100 1.1 000) --- Freeway (100 2.1 000) --- Freeway (100 10000) --- Freeway (100 10000) --- Freeway (1 1000000) --- The input data is a list of integers: a and b, the first element of the list is called a and the second b. The score column in the result column is called aScore and is the value of the Freeway metric. We use	False
"
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 Breakout benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl. ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Ataribox"" ) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ORKHP:HAS_BENCHMARK ?benchmark. Input (English text): Can you provide a list of models that have been tested on the Atari 2600 River Raid Benchmark dataset ? Output (Sparql query): input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid Benchmark dataset ? input (English text): Can you provide a list of models that have been tested on the Atari 2600 River Raid Benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid Benchmark dataset ? output (Sparql query): input (English text): Can you provide a list of models that have been tested on the Atari 2600 River Raid Benchmark dataset ? input (English text): Can you provide a list of models that have been tested on the Atari 2600 River Raid Benchmark dataset ? input (English text): Can you provide a list of models that have been tested on the Atari 2600 River Raid Benchmark dataset ? input (English text): Can you provide a list of models that have been tested on the Atari 2600 River Raid Benchmark dataset ? Output (Sparql query): SELECT DISTINCT ?model ?model_lbl. ?dataset a orkgc:Dataset; RDFS:HAS_DATASET ?dataset; output = ?dataset IF NOT EXISTS output, output = ?model_lbl. input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid Benchmark dataset ?"	False
"
 input (English text): Provide a list of papers that have utilized the EfficientNetV2-M model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the AlexNet, MultiGrasp model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AlexNet, MultiGrasp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 We did not include a parameter to restrict the number of papers that include an explanation of their design choice in their title or abstract. All papers that provided a design choice or presented the results in a technical or technical paper were included in this report. We have provided the same format in our next report. We have decided to omit the option for a description of the use of the 'ORG-W' model or the 'EfficientNetV2-M' models to compare results by authors and by category, so that the paper may be read and understood without reference to this detail. How did the analysis turn out ? We found that the total number of citations from each category is fairly evenly distributed across papers with regard to the number of submissions, their relevance and the number of papers that presented a results of interest. While there were no statistically significant differences between citations by the authors and by submission of papers, no papers received more citations than did other papers. This suggests that the quality of the papers was not clearly related to the author identification or the submission of papers. Only one paper (Sharma and Shirota, 2012). obtained over 200 papers, the bulk of which are the papers that were submitted for publication under the category of 'Technical/Technical' papers. For the remainder of our data, we found that a certain type of paper, such as EfficientNetV2-M papers, has a higher number of papers. This suggests that the types of papers submitted to our data set are indeed those that are a bit more interesting. The number of papers that were submitted only for publication is much smaller than the total number that were cited. For this reason, we suggest that the authors of papers not submitted for publication are not given all the credit and that the actual results of the studies must be taken into account in making such calculations. We also found that the number of papers by authors with multiple paper appearances also showed a low mean value. This implies that the number of papers that have more than one author appeared to be a small proportion of the total. However, the proportion of the overall papers that is also a paper by a third author is much higher. Our results indicated that papers by authors with a non-disclosure agreement appear to have a higher share of citations than papers written by 'co-authors'. Overall, in the paper by Shirota (2012), we find no correlation between the number of papers and the number of authors. This indicates that the papers by other authors did not attract attention to themselves more	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Ms. Pacman dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ms. Pacman"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pacman "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } In order to determine if a paper has been evaluated on the Atari 2600 Atari dataset, we will need to filter the output row of the data set and replace the word ""Pacman"" with an empty string. We are currently in the process of creating an interactive filter command to convert the data to the Atari output format. Once this filter command is up and running, the above three columns will be replaced with a ""Pacman"" string: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 SELECT DISTINCT ?paper ?benchmark. OPTIONAL { ?paper orkgp:HAS_BOARD ?model. ?model rdfs:label ?model_lbl. } In order to determine if a paper has been evaluated on the Atari 2600 Microsoft Excel dataset, we should add a query to the list of papers previously collected. As previously stated, we will need to filter out the model ""NTSC"" (not to be confused with the famous Sega Saturn). Additionally, we should also remove all ""PACMAN"" papers from the list of benchmarks. select * FROM datasets; SELECT FILTER ROW( ?""PACMAN"") NULL ; output (Sparql query):      SELECT DISTINCT ?book ?book_id OR  DISTINCT ?book_id_lbl. MAX(); OR  DISTINCT ?book_id ?book_id_lbl. MAX(); OPTIONAL { ?book OR ?book_id OR ?book_id rdfs:label ?book_id. } In order to determine if"	False
"
 input (English text): List the metrics that are used to evaluate models on the Habitat 2020 Point Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 (A similar query uses the same data, but also adds the option OPTIONAL orkgp:HAS_METRIC to identify the metrics that were used to benchmark the models.) In case the results from the first query do not yield results from all of the databases in the target set of databases, the second query may also be used. You can use either the third or fourth set of queries below by adding one or more of the parameters indicated at the top of the third or top of the fourth set of queries. By default, each query will use the default values for each of these parameters. You can change these by replacing the { } characters in query with { ; } or the _ at the start/end of the parameter name. CUSTOMER TEST: query (input (English text): select ?dataset_lbl from ( ?env c) ( ?dataset_lbl.dataset.List ?data_aset_lbls ?loc) ? for ( ?) of each, each { ?dataset_lbl_lbl.loc; rdfs:label { ?loc a orkgc:Dataset } } query (input (English text): find ?tid where ? { ?env c:list( ?loc ?loc ?loc_lbl) ?dataset_lbl. loc; rdfs:label { ?loc a orkgc:Dataset } } query (English text): find ?dataset_lbl WHERE ?loc_lbl { ?dataset h:dataset } CUSTOMER TEST: query (input (English text): select ?tid from ( ?env c) ( ?dataset_lbl_r2 ?dataset_lbl_r5 ?loc_lbl_rank ?loc_lbl_rank_rank ?loc_lbl_rank_rank_rank ?dataset_lbl_rank ?loc_rank ?loc_rank_rank_rank ?loc_rank_rank_rank_rank ) { ?env c:list( ?loc ?loc_lbl_rank ?loc_rank_r1 ?loc_rank_r2 ?loc_rank_rank_rank_rank ?loc_rank_rank_rank_rank_rank ?dataset_lbl_rank ?loc_rank_r1	False
"
 input (English text): What is the top benchmark score and its metric on the NCBI Disease dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the BC5CDR-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"___________________________________________________________________________ | scores | scores_disease | scores_genus | scores_subclass | scores_domain | |----------------------------------------------------------------------------| | | NCBI Genome Diseases v18.0 | NCBI Human Leukocyte Antigen Variation v17.0 | |----------------------------------------------------------------------------| | | 2.14 | 2.05 | 2.14 | 2.13 | | 2.18 | 2.12 | |----------------------------------------------------------------------------| | | NCBI Cancer Genomics v3.0 | NCBI Cancer Genome Variation v2.2 | |----------------------------------------------------------------------------| | | 639.02 | 629.91 | 639.12 | 637.10 | | 664.03 | 661.92 | | ----------------------------------------------------------------------------| | | NCBI Human Metabolites v5.2 | NCBI Human Metabolites v4.0 | |----------------------------------------------------------------------------| As a follow-up, let's try it on all the different metrics we used: import pandas as pd input (English text): ___________________________________________________________________________ | scores | scores_samples | scores_sources | scores_domains | |----------------------------------------------------------------------------| | | NCBI Genome Diseases v17.0 | NCBI Human Leukocyte Antigen Variation v17.5 | |----------------------------------------------------------------------------| As expected, we see some pretty clear patterns. Notice that Genome Diseases is now the top metric on all datasets except for the Genome Diseases V17.5 dataset and the Genome Diseases V18.0 dataset, which both contain many additional Metrics. The remaining three datasets also show that we are looking for Metrics with at least six categories (for example, we want to see the top Genomic Diseases scores), with Genome Disease having the most as well as an even higher metric score, with 18. It would be fair to say that Metrics are the ""meat"" of the problem: ""We know everything"" of the Genome Disease data set, with Metrics just providing the ""meat"". The Metric Problem (R). As you might have gathered from the ""what about Metrics"" section, there may well be situations in which you just want to gather data, but find that the metrics used is causing the problem (for example because they are too noisy or there are too many outliers). In order to avoid this, R provides a pretty straightforward solution: metric_loss = R.metrics.n"	False
"
 input (English text): Can you list the metrics used to evaluate models on the ImageNet V2 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. } } "	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Open Entity dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Open Entity"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?oob.author ?obedid_lbl.&obedid_lbl.DATASET ?obedid_lbl; id, ( ?, ( ?, ?)), ?oob.author OR ?obedid_lbl. ?pubid.&obedid_lbl.&obedid_lbl. ?pubid_lbl FROM ?oob.author OR ?obedid_lbl. ?pubid. OR ?obedid_lbl. ?pubid_ DATASET. OR ? obedid_lbl. ?obedid. ?obedid_lbl. ? obedid_lbl. ? obedid_lbl. ? obedid_lbl. ? obedid_lbl. ? obedid_lbl. ? obedid_lbl. ID ) id; This is a bit like the syntax in R for adding data in an ANOVA. A final note on using OpenSparql when importing papers. A paper's publisher must be declared as an additional table to the input dataset. Thus, if you're planning to access the publisher's information, the OpenSparql code must match the OpenSparql code that is already in the database. I hope I've helped clarify OpenSparql's functionality. My main interest in OpenSparql as a framework isn't to build databases or to explore an open source database; I'm more interested in using OpenSparql in research to explore new uses for data.	False
"
 input (English text): Provide a list of papers that have utilized the HNEABP (BWNE) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HNEABP (BWNE)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the HRLRE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HRLRE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input English text: Provide data in the form OFFSET, ?model a. OR: Input data in the form OFFSET, ?model a ?extractor orkgp:HAS_FETCH. The format you choose to extract data from should be specific, so that you provide enough information to permit a query. A simple OR query will give you what you want. Alternatively, you can use an extractor to get an approximation of what the original data would have yielded had the models been tested. For this purpose, you should use a BUG or HNSEB model. When a query generates a BUG and HNSEB report, it can be confusing to find the correct data source, particularly in a case like this. The HNSEB model will produce information about the model parameter specifications that is used by the test. For example, it will tell you that the HRLRE model is used to describe two distinct distributions. Because the HRLRE model is quite well suited to modeling discrete events, however, you can probably use it as well for events that occur in general time, space and context. The BUG model should be used instead for events that occur as a result of a more general-occurring process, such as a tornado. As a general rule, it is most valuable to use the HNSER model when you have data that contains very low data quality. BUG or HNseb: A BUG or HNSEB model that describes a data-quality issue. For other related questions Q: What error functions do I need in my model ? A: A good approximation of model parameter values is not just the output of a model itself. A properly constructed and tested model should have useful information added to it. Such information can be produced by a simple query. In the simplest case the error function is a set of simple linear function estimates, such as F(X) to F(sigma). To be able to perform simple queries on our data, it is usually desirable to obtain the output of a model. You can get the output of a model from a query simply by feeding the model output to an orkgp query. However, you may want a more sophisticated query to produce output for which you do not have the model output already. The BUGs and HNSEBs are examples of this kind. For such queries in the HNSEB model, the output of the model is	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CeiT-T model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CeiT-T"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the T-ConvS2S model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""T-ConvS2S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 orkgp:CeiT-S2S model. output (HAS_DATASET, RDFS, or benchmark code) # input (English text): Can you provide links to code used in papers that benchmark the T-ConvS3 model ? output (HAS_DATASET, RDFS, or benchmark code) output (BOOLEAN)	False
"
 input (English text): Which model has achieved the highest Best Score score on the Atari 2600 Q*Bert benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset ? output (Sparql query): SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Atari 2600 vs Atari 8-"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the One Billion Word dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Words in Context dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Words in Context"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?stat ?stat_lbr (MAX( ?value) AS ?score) WITH STDEV( ?value) AS score; --------------- 1 row in set, 0.05 sec per row ---------- 1.919 secs output (Sparql query): SELECT ?stat ?stat_lbr (MAX( ?value) AS ?score) FROM ?stat_lbr ?stat_lbr output (Sparql query): This is the top 1000 words scored by the metric and score for the One Billion Word dataset The top 1000 words scored by the metric on the Words in Context dataset will be a bit smaller than the top 1000 words scored by the metric on the WordNet dataset The top 1000 words on the Words in Context dataset will be a bit larger than the top 1000 words on the WordNet dataset output (Sparql query): ---------------------------Score ------------ 997 1,023,082 output (Sparql query): SELECT ?stat ?stat_lbl (MAX( ?value) AS ?score) FROM ?stat_lbl ?stat_lbl output (Sparql query): This is the top 1000 words scored by the metric and score for the Words in Context dataset The top 1000 words scored by the metric on the WordNet dataset will be a bit smaller than the top 1000 words scored by the metric on the WordContext dataset This is not the final dataset of the year : the WordNet dataset doesn't scale much past 1000 words (the other datasets from year 2012 and 2013 do, see next point) What are the current state of the dataset for 2014 ?  I will have updated statistics for 2014 a bit later  The metrics used to categorise the words in the database are now in CSV form output (Sparql query): ---------------------------Metric: Metric Type: Lists words sorted by score for each measure Lists words sorted by metric used in scoring This has been added to the end of all metrics now In the CSV column, I have converted the text into CSV format output (Sparql query): --------------------------- #1 - Words 1 to 1000, sorted by word score 1. WordScore1 = 1 2. WordScore2 = 2 3. WordScore3 = 3 4. WordScore4 = 4 5. WordScore5 = 5 6.	False
"
 input (English text): List the datasets benchmarked under the Skills Assessment research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Skills Assessment"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the datasets benchmarked under the Common Sense Reasoning research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Common Sense Reasoning"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 Example 6: Creating an ORG: Create, update, and delete data in Google BigQuery Example 7: Creating an ORG using Google BigQuery, a custom language Example 8: How to merge datasets by using the Merge API Example 9: How to export a dataset for use in the Query DSL Example 10: How to create a custom data source by using the Data Import API Example 11: How to use the Data Import API to import and export datasets Example 12: How to create a custom RDF API using the Data Import API. Example 13: How to create a custom Data Import API, which provides access to datasets. Example 14: How to use the Data Import API to set up the custom data source Example 15: How to create custom datasets by exporting RDF data. Example 16: How to use BigQuery's RDF APIs for custom datasets. Examples of custom datasets that were created using the Data Import API (note: the Data Import API only provides access to datasets that are available to the public at the time of writing) Examples of custom datasets that were modified using the Data Import API, which were modified to provide access to datasets that are currently stored in Google data warehouse. Examples of custom datasets created using the Data Import API by using the Create API. See CreateAPIForDataset for an example. Google BigQuery and Google Data Warehouse:  BigQuery: a comprehensive and open source data warehouse Google BigQuery and Google Data Warehouse: Google BigQuery Data Storage and Query Services : BigQuery Storage and Query Services, Google Data Warehouse: A Big Data Warehouse for Mobile Devices, and Google Web Search Data Storage and Query Services: A Big Data Warehouse for Mobile Devices Google BigQuery and Google Data Warehouse:  BigQuery : A comprehensive and open source data warehouse Google BigQuery and Google Data Warehouse: Google BigQuery and Google Data Warehouse - Creating Custom Data Sources Google BigQuery and Google Data Warehouse:  BigQuery : A comprehensive and open source data warehouse Google BigQuery and Google Data Warehouse: Google BigQuery and Google Data Warehouse - Using Google Data Warehouse for Custom Datasets  Google BigQuery and Google Data Warehouse:  BigQuery in the Cloud, BigQuery Online, Cloud BigQuery, BigQuery Cloud Data Storage, BigQuery Cloud RDF Storage and BigQuery in the Cloud: Searchable and Unstructured Data from Google Data Storage, BigQuery in the Cloud: Searchable and Unstructured Data from	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset ACE 2004?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	------------------------ (1 row) You can use the result code in a report or a dashboard report. Conclusion I didn't really think much about the impact of the new SSC on the performance until I first observed it in the benchmarking tool I was using; so no, I don't use it myself. However, it does make a lot of sense for many of the benchmarks I use each year. Let me know if you have a question or suggestion; I'm here to hear from you, and I hope people can use this data! Further Reading	False

Metrics on query only
Precision	0.19320211821382216
Recall	0.6249031867145908
F1_Score	0.27182838521610053
Blue_4	0.10056236075991572
Bleu_Score_Cumulative	0.11982287210759102
Meteor_Score	0.0
Rouge_1	0.32702710463551
Rouge_2	0.23287673847369872
Rouge_L	0.29882426943431367

Metrics on all rows
Precision	0.14770418210294556
Recall	0.4877032022639918
F1_Score	0.20756769582881984
Blue_4	0.06848386055957395
Bleu_Score_Cumulative	0.082192013210098
Meteor_Score	0.0
Rouge_1	0.2534917749965556
Rouge_2	0.16398128082441088
Rouge_L	0.22712870930431717

Number of sparql queries generated equal to given	0

Generated text starts with 'SELECT'	341
