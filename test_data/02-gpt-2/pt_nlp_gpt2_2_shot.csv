Questions	SPARQL in Gold Standard	SPARQL generated	exact match
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Story Cloze Test dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the models that have been evaluated on the Story Cloze Test dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"--------------------------- Model name: ------------ score: 10.48509936259987 Accuracy: 84.01287795259968 ORG: LBL.TEST input (English text): Which model has been evaluated on the Story Cloze test dataset with a model rating of 100 ? output (Sparql query): --------------------------- Model name: ------------ score: 100 Accuracy: 75.97289935009518 ORG: LBL.TEST input (English text): Which model has experienced the highest loss on the Story Cloze test dataset ? output (Sparql query): --------------------------- Model name: ------------ loss: 7.14289934009519 ORG: WLS input (English text): Please provide a list of datasets using this model output (Sparql query): [ "" Story Cloze Test "" ] input (English text): Please provide a list of datasets using this model output (Sparql query): [ "" Story Cloze Test "" ] input (English text): Is this model still appropriate for the Story Cloze test ? output (Sparql query): Yes input (English text): Is this model appropriate when used in a larger cluster of datasets ? output (Sparql query): Yes input (English text): Which model with a high loss (0.5%) has been evaluated on the Story Cloze test dataset ? output (Sparql Query) input (English text): Which model with a high loss (6%) has been evaluated on the Story Cloze test dataset ? output (Sparql Query) input (English text): What is the metric for loss on the Story Cloze test dataset ? output (Sparql Query): ? Loss_on_dataset_with_loss <0.5"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the One Billion Word dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Annotated development corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Annotated development corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?dataset_lbl. { ?dataset a orkgc:Dataset ; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The output we get from this query looks something like this We did it! We can now see that our Annotated Development corpus dataset, for the title of a paper, is available from the top of our results. In the case of the Penn Treebank word-level word corpus, the most accurate query is for the title, not the ID, as shown below. $ php -f -u root -p data.db 'SELECT title,( ?DATE) - ?ID FROM (SELECT * FROM (SELECT title, ?DATE) - ?ID) WHERE ( ?' DISTINCT ?title ORDER BY title)' | head -"	False
"
 input (English text): What are the metrics of evaluation over the UrbanSound8k dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the UrbanSound8k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark ORkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. Here I can interpret results from the first query above: ""The benchmark was very good"", ""Very good"", ""Excellent"". Note that in this case, the query is not limited to single words, but as any other query in our input array. Input data: input (English text): What are the metrics of evaluation over the UrbanSound8k dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark ORkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval ORkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What models are being evaluated on the WOS-5736 dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Supervised: dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 Inputs: -- title: (optional) Name or title of the paper -- paper: Identical paper ID as above. -- dataset: Identical datasets ID name as above. -- benchmark: Benchmark the dataset, either the one listed above or one from the Penn Treebank dataset Output: <-- title: (optional) Name and title of the paper <-- paper: Identical paper ID as above.	False
"
 input (English text): What is the top benchmark score and its metric on the SearchQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the MedSTS dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the MAZEA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MAZEA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MAZEA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): What is the top benchmark score and its metric on the METRICS dataset ? output (Sparql query):  SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER(str( ?dataset_lbl) = ""MetricS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_"	False
"
 input (English text): Can you list the metrics used to evaluate models on the TDMSci dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDMSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the ORKG-TDM dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ORKG-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the TDMSci dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDMSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT id, id, id, id FROM ORKG-TDM. orkgp.dataset_lbl ID; output (Sparql query): SELECT id, id, id, id FROM ORKGBase.paper ORKG-TDM.paper id input (English text): Can you list the metrics used to evaluate models on the ORKGBase dataset ? output (Sparql query): SELECT dISTINCT ?metric ?metric_lbl. OPTIONAL { ?metric ?metric. ?metric rdfs:label ?metric_lbl. } output (Sparql query): { ""Dataset"": { ""Orkgc"": { ""id"": { ""rdfs"": { ""id"": ""orkg-tdm"" }, ""id"": ""paper"": { ""id"": ""paper"" } } } } } The first two arguments (DISTINCT and ORPKG) indicate that the variables are of unknown (undefined) value. This means that there exists no relation between the variables and the model, and therefore no valid metric is evaluated on"	False
"
 input (English text): What is the average installed capacity of all energy sources considered?
 output (Sparql query): SELECT (AVG( ?installed_cap_value) AS ?average_installed_cap_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43133 ?installed_capacity. FILTER(REGEX( ?energy_sources_labels, ""all sources"")) ?installed_capacity orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?installed_cap_value) } 
 input (English text): What is the average installed capacity for each energy source considered in 5 year intervals?
 output (Sparql query): SELECT ?rangeId ?energy_sources_labels (AVG( ?installed_cap_value AS ?avg_installed_cap_value)) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?paper orkgp:P31 ?contrib; orkgp:P29 ?year. BIND(xsd:int( ?year) AS ?y). VALUES( ?rangeId ?min ?max) { (""2001-2005"" 2001 2005) (""2006-2010"" 2006 2010) (""2011-2015"" 2011 2015) (""2016-2020"" 2016 2020) } FILTER( ?min <= ?y && ?y <= ?max). ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43133 ?installed_capacity. ?installed_capacity orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?installed_cap_value). } ORDER BY ASC( ?rangeId)
 input (English text): What is the mean capacity of a carbon-based fuel?
 output (Sparql query): "	"SELECT (AVG( ?numerical_capacity) AS ?mean) WHERE { ?paper a orkgc:Paper; orkgp:P31 [ orkgp:P15483 ?capacity ]; rdfs:label ?title. BIND( xsd:double( REPLACE( STR( REPLACE( ?capacity, ""([0-9]+),([0-9]+)"", ""$1$2"" ) ), ""([0-9]+).*"", ""$1"" ) ) AS ?numerical_capacity ) FILTER(REGEX(STR( ?title), ""(fuel|CO2)"")) } "	   #  R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R  | R	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT score from benchmark_lbl in output ; scores: SELECT * FROM `result` AS scores ( ?s1:int); rdfs: SELECT * FROM `result` AS rdfs ( ?s1:int); [ ?data_lbl : ?paper : ?benchmark, ?dataset_lbl and  { ?cont : ?cont, ?benchmark : ?benchmark, ?dataset_lbl and  { ?cont : ?cont, ?benchmark : ?benchmark, ?dataset_lbl } or  { ?cont : ?cont, ?benchmark : ?benchmark, ?dataset_lbl } AND ( ?cont : ?corpus1.1 ?cols3.2 ?col1 ) AND ( ?cont : ?corpus1.3 ?cols3.2 ?col1 ), ?dataset_lbl: ?dataset. ?dataset_lbl. ?dataset_lbl., ?dbss and  { ?labels : list of integers ?labels: LIST OF NUMBER OF LIQUORS (list of integers). ?labels: LIST OF LIQUOR (list of integers). } ),  &.  ( ?label : ?label ?label	False
"
 input (English text): Indicate the model that performed best in terms of Error metric on the Kuzushiji-MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 I created an online model that has four outputs (i.e., Kuzushiji MNIST scores, errors, and error rates) and no inputs (i.e., input only). It has three input and three model outputs (Kuzushiji MNIST benchmark scores, errors, and errors). It is a Kuzushiji-based LSTM-like convolutional neural network. The model that performs best is the one that has the outputs that are the best overall accuracy, the one that has the best accuracy and the best error rate (i.e., Kuzushiji MNIST benchmark scores, errors, and errors). This is really a one-off problem problem. When I wrote this problem, it appeared to be a bit more difficult to analyze. So I wrote another project that used the Kuzushiji MNIST dataset and found the same problem and did the same things in both projects. Now that we know how to solve this problem using MNIST as model input, I will try to describe the structure of this problem in different parts of my paper. The first part describes the model and the second part describes the model implementation in SQL. The third part is about the model evaluation and the fourth part is about the model evaluation using Kuzushiji-NIST. The model The model is based on a hidden layer that consists of two LSTM layers: in this case, a non-parametrically hidden LSTM layer. Another layer is a LSTM that has many layers (hidden units). Only a small number of layers are implemented. The LSTM layer is a very complex system and is the subject of this entire paper. The non-parametric deep learning framework for deep neural networks is also part of it. The model is also based on a layer of LSTM that is a sparse LST	False
"
 input (English text): What is the name of the top performing model in terms of BLEU score when benchmarked on the WMT2016 Romanian-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Russian-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Russian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } After the above script ran, we could get the BLEU table size in bytes from the following SQL: SELECT c.name, c.metric, c.size FROM ( SELECT c.name, c.metric, c.size FROM (SELECT c.name, c.metric, c.size FROM ( SELECT c.name, c.metric, c.size FROM ( SELECT c.name, c.metric, c.size FROM ( SELECT c.name, c.metric, c.size FROM (SELECT c.name, c.metric, c.size FROM (SELECT c.model, d.name, d.metric, d.size FROM ( SELECT c.model, d"	False
"
 input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; ?dataset rdfs:label ?dataset_lbl. } } OPTIONAL { ?evaluate ?metric_lbl. } } input (English text): Which model achieved the highest Score score on the Ball in cup, catch (DMControl500k) benchmark dataset ? output (Sparql query): input (English text): Which model achieved the highest Score score on the Ball in cup, catch (DMControl100k) dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Datas"	False
"
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the iNaturalist 2019 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the Kinetics-600 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Genescan-200"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } output (Sparql query) SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FIL"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ART/CoreSC dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ART/CoreSC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Paper Field dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""arXiv"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sketch/Artwork"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the Dataverse dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Artificial Intelligence"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark on the FERPA dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FERPA"") ?benchmark orkgp:HAS_DATAS"	False
"
 input (English text): What models are being evaluated on the TDM Tagged Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the NLP-TDMS dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the TDM Tagged Corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 input (English text): When you have a new model that has new features, what metrics are used to evaluate its performance ? output (Sparql query): input (English text): What would be the use-cases for new models that include new features, but which are not evaluated on the TDM Tagged Corpus data ? output (Sparql query): input (English text): How are the new models evaluated in order to find the best one ? output (Sparql query): input (English text): How do you decide whether a model is a good candidate for evaluation on the TDM Tagged Corpus data ? output (Sparql query): input (English language): What is the definition of a model ? output (Sparql query): input (English language): Which parameters will be used to evaluate the models for the TDM Tagged Corpus data ? output (Sparql query): input (English language): What will be the use-cases of evaluating models for the TDM DataMining dataset ? output (Sparql query): input (L2 English text): What do you mean by model selection ? output (Sparql query): Select the following Model: ... If no options have been provided, the model will be selected automatically: Select the following Feature: Model evaluation results The evaluation results can be accessed through the TDM Tagged Corpus page. input (L2 English text): This query displays the evaluation results of each model in the selected models list. output (English text): Each model is represented in the following format: select name, model, score from metrics_selection where model_id = ? input (L2 English text): The result of the evaluation is a simple comma-delimited table	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NCBI Disease dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC2GM dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 Note that this option is only supported if the -l flag has been used with the -db flag. This is because the benchmarking package already provides a benchmark option that can be used to perform the benchmarking. Note: The default behaviour with the R-helpers package is to store the output files in the given directory, as a subdirectory named benchmark. This is a great option for those who don't want to copy and paste the benchmark files into each data set. However if you use this option, please also specify the -d option if the -db flag was specified. input (English text): Specifies the path to output files to write to. output (Sparql query): Specifies the path to the output files to write into. input (English text): Specifies the path to input files to execute. output (Sparql query): Specifies the path to the output files to write into. For these files to be executed, an extra parameter must be specified. This will be the path to the subdirectory that exists in the data that contains the file to be written and also another directory that contains an R-helpers executable. input (English text): Specifies the path to put the output files in. output (Sparql query): Specifies the path to put the output files into. input (English text): Specifies the path to put inputs to execute. output (Sparql query): Specifies the path to put the output files into. For these files to be executed, an extra parameter must be specified. This will be the number of inputs (either an integer or a data frame) to execute. For these files to be executed, an extra parameter must be specified. This will be the number of outputs (either an integer or a data frame) to execute. Please note	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Lunar Lander (OpenAI Gym)?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the ObjectNet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?score orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?score orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?model_lbl. } ?cont orkgp:HAS_BENCHMARK ?score. OPTIONAL { ?score orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): What is the name of the top performing model in terms of Top-1 accuracy score when benchmarked on the ObjectNet dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the AESLC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the FSNS - Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }  And there you have it. Another, simpler, and more efficient way of choosing different metrics. It will not change how we benchmark a single model, how we benchmark different performance models, or where we benchmark our models in the same benchmark run. With no code changes, the same models should work identically. (Feel free to comment/correct/explain anything)"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the ImageNet V2 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 output (Sparql query): output (Sparql query): Output: The same questions can be asked about the ImageNet dataset (e.g., for a V2 dataset), but instead of the same data, we have also asked the question about the dataset in general. Since a dataset is not only data, it contains also metadata about the data that should be considered. This is a relatively common mistake (or a convenient exception) when working with datasets. This issue is illustrated in the following examples (reproduced here from the Google Blog) demonstrating a simple example of bad input to SQL. The following are examples of different types of data that are used in ImageNet and a list of data that needs to be excluded. The first step to solving a problem is to ask the right questions. input (English text): What is the image from which we want to learn about the architecture of Deep Belief Networks ? Output: Output: Input: No data is provided. You can also see some interesting examples of bad input to SQL in this comment. The next step is to go back to the question we started with and ask another version of the query that asks a question that is different from the input. This is called an unstructured query, and the first question we want to ask is: input (English text): What are the title and ID of research papers that include a benchmark under the 64x64 dataset ? Output: Output: Input: No data is provided. You can also see some interesting examples of bad input to SQL in this comment. At this point we can start to see how the image above looks like. The rows in the dataset are marked as 'no data' (invisible) or 'data' (filled in), and the rest of the rows have all the data we need. Since there are only two fields on	False
"
 input (English text): What is the highest benchmark result achieved on the Classical music, 5 seconds at 12 kHz dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of Bits per byte metric on the Classical music, 5 seconds at 12 kHz benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bits per byte"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?metrics ?metric_lbl (MIN( ?value) AS ?score ) AS ?output WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Bits per byte"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value ) LIMIT 1 } } GROUP BY ?metric ?metric_lbl output (Sparql query): SELECT ?metrics ?metric_lbl (MIN( ?value) AS ?score ) AS ?output WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Bits per byte"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTION"	False
"
 input (English text): Provide a list of papers that have utilized the PBSMT model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PBSMT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the DQN-CTS model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-CTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Flair-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?name orkgc:Name; rdfs:label ?name_lbl. FILTER (str( ?name_lbl ) = ""Fluid:TDM"") ?benchmark orkgp:HAS_THRESHOLD ?benchmark. ?comp orkgp:HAS_CONTEXT ?comp. } input (English text): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?name orkgc:Name; rdfs:label ?name_lbl. FILTER (str( ?name_lbl ) = ""Fluid:TDM"") ?benchmark orkgp:HAS_THRESHOLD ?benchmark. ?comp orkgp:HAS_CONTEXT ?comp. } input (English text): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?name orkgc:Name; rdfs:label ?name_lbl. FILTER (str( ?name_lbl ) = ""Fluid:TDM"") ?benchmark orkgp:HAS_THRESHOLD ?benchmark. ?comp orkgp:HAS_CONTEXT ?comp. } input (English text): Provide a list of papers that have utilized the Flair-NFSM model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?name orkgc:Name; rdfs:label ?name_lbl."	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL (18 layers) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL (18 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Transformer model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL Base model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code ?verter_orkgc { ?verter a; } input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL Base model ? output (Sparql query): output (Sparql query): SELECT DISTINCT ?code ?verter_orkgc { ?verter a; } The results of the benchmark can now be downloaded from: This document is also known as: It's now time to review the paper: The test bench: We chose to use four Transformer-XL models: Model 1 has 3 layers: 1. Transformer-XL Base Model (18 layers) 1.0 1.1 Transformer-XL Base model with 3 inputs Model 2 has 5 layers: 1. Transformer-XL Base Models (18 layers) 2.0 2.1 Transformer-XL Base Model with 5 inputs Model 3 has 6 layers: 1. Transformer-XS Base Model (18 layers) 3.0 3.1 Transformer-XS Base Model with 6 inputs Model 4 has 7 layers: 1. Transformer-XS Base Models (18 layers) 5.0 5.1 Transformer-XS Base Model with 7 inputs Model 5 has 8 layers: 1. Transformer-XS Base Models (18 layers) 6.0 6.1 Transformer-XS Base Model with 8 inputs Model 7 has 9 layers: 1. Transformer-XS Base Models (18 layers) 7.0 7.1 Transformer-XS Base Model with 9 inputs Note: This is not the same as TensorFlow's single layer models. For one, there was	False
"
 input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest F1 score score on the BUCC French-to-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?metric ?model ?model_lbl. ?model rdfs:label ?model_lbl. ORG_BENCHMARK ?dataset. rdfs:label ?model_lbl. ?model rdfs:label ?model_lbl. ?model rdfs:label ?Model if ?( ?dataset orkgc:Dataset OR ?metric ?model_lbl. ) = ""BENCHMARK""; ORG_VALUES; rdfs:label ?model_lbl. ?metric ORG_RDEF_CLASS ? rdfs:label ?model_lbl. ?metric ORG_MODEL_TITLE ?model_lbl. ORG_MODEL_DESCRIPTION ?model_lbl. ORG_METRIC orkgp:META ?metric. ORG_METRIC_LABEL ? orkgp:META ?metric. orkgp:HAS_EVALUATION ?metric. ORG_MODEL_TITLE ?model_lbl. ?metric ORG_MODEL_DESCRIPTION orkgp:HAS_EPSYCHOLOGY ?metric, ?metric_lbl, orkgp:EVALUATION. OPTIONAL { ?dataset orkgc:Dataset OR ?metric ?model_l"	False
"
 input (English text): List the code links in papers that use the SAN (single model) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Concept pointer+DS model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Concept pointer+DS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the SAN (single) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAN (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 5.1.2.1.2.5.1.6 Ancillary Tools A. Dataflow - Dataflow is an open source program for parallelized processing and modeling of high-dimensional spatial data (e.g. in order to visualize the space-time continuum.) It was developed by the Google DeepMind team. Dataflow can be used to perform a full range of data analysis including classification of small-dataset, binary (i.e. univariate) and multimodal modeling. It is also efficient for high-throughput datasets and provides methods for visualizing the spatial space. Dataflow is an extension of Inception-style model-driven processing (MDP). The Df(O) file dataflow.dff refers to a set of datasets used as the input to FFT for the purpose of making a Bayesian analysis. The program has several different formats and is also available as open source on GitHub. A.1.2.1. CIFAR/SciPy - CIFAR/SciPy's goal is to provide rapid visualization of spatial data, particularly for data sets with spatial structure, which can be useful in modeling. For example, if we are building an interactive map for a city, we can quickly determine the best neighborhood for exploring"	False
"
 input (English text): Can you list the models that have been evaluated on the ACE 2004 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the AAPD dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the ACE 2005 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): What are the model results from the ACE 2004 dataset ? output (Sparql query): SELECT DISTINCT ?class ?class_result COUNT(*) ; OPTIONAL { ?model orkgp:HAS_DATASET ?model. ?model_lbl rdfs:label ?model_lbl. }  >   SELECT * FROM orkgp;  # result 1 - ACE is significantly stronger than ACE input (English text):  What is the name of the test that shows ACE is significantly stronger than ACE ? Is there another test that does not show strong ACE ? output (Sparql query): SELECT DISTINCT ?model OR( ?model_lbl.eq( ?model.lbl_seq.values())) ? # model result from ACE 2004 input (English text):  Which of the following models compare ACE to ACE and is NOT the strongest model in ACE ? output (Sparql query): SELECT ?class ?class_result COUNT(*) ; OPTIONAL { ?model OR( ?model_lbl.eq"	False
"
 input (English text): Where can I find code references in papers that have used the MP-EB model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MP-EB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TDMS-IE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the PNDec model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PNDec"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?code FROM PNDec ?dataset WHERE { ?model a orkgc:Model ?dataset. ?cont orkgc:HAS_DATASET ?dataset. ?cont orkgc:HAS_MODEL ?model. } output (English text): Where can I find code references in papers that have used the H-BENCHMARK model for benchmarking purposes ? output (Sparql query): SELECT ?code FROM H-BENCHMARK ?dataset WHERE { ?model a orkgc:Model ?dataset. ?cont orkgc:HAS_DATASET ?dataset. ?cont orkgc:HAS_MODEL ?model. } input (English text): Where can I find code references in papers that have used the C-SPART report of the MP-EB model for benchmarking purposes ? In your paper, please give details for the C-SPART report of the MP-EB model: (a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) where * (and/or) ? model ? rdfs name ____________________________________________________________________________ DISTINCT ?label ?model_lbl. FILTER (str( ?label ) = ""C-SPART"") ?benchmark OR kgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model. ?cont orkgp:HAS_SOURCE_CODE ?code. ?cont orkgp"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CATTS model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CATTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the CAIT-XS-24 model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-XS-24"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the CATTS-XSUM model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CATTS-XSUM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 output from source : select rowid, name, text as text, id, count(*) as count, model_lbl_id as model_lbl, data as cdata, datum as datum2from rowlab table('CATTS-XS-12' ORDER BY id DESC LIMIT 1,20); id (select id from rowlab where id=1'); n = n*1000000 + 1000000; count (*) = count(*)+1000000; model_lbl_id (select model_lbl_id from rowlab where id=3); id (select id from rowlab where id=4); id (select id from rowlab where id=5); text = text[1]+text[0]; id (select id from rowlab where id=7); id (select id from rowlab where id=8); id (select id from rowlab where id=10); id (select id from rowlab where id=12); cdata = cdata+cdata+cdata; datum[0] = datum[0]*2+datum[0]*2*2; datum[1] = datum[1]*2+datum[1]*2*2+datum[1]*3+datum[1]*3*2; datum[2] = datum[2]*4+datum[2]*4*4+datum[2]*5+datum[2]*5*2; datum[3] = datum[3]*6+datum[3]*6*6+datum[3]*7+datum[3]*8+datum[3]*9+datum[3]*	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Accuracy score on the IMDb-B benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" ------------------------------- Time to create all benchmarks: 0.0990675 seconds input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B ? output (Sparql query): ------------------------------- Time to create all benchmarks: 0.0893829 seconds Input(H:A:O, H:A:O+1): Benchmark: Performance: Score = 10.0 (10) Date: 2014-11-08, 08:47:55 -------- T.T.T.T.T.T.T.T -------- (T/T) ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ ------------------------ --------------- Time to create all benchmarks: 0.0990675 seconds input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B ? output (Sparql query): ------------------------------- Time to create all benchmarks: 0.0990675 seconds input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B ? output (Sparql query): output (Sparql query):   Input message: If one of the input messages contains a query such as, ""If the result was a function value, a function that accepts an integer value as input argument, an integer that is a function value and a Boolean expression for a boolean expression;"", then the query is not present in IMDb-B. Output message"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the MLDoc Zero-Shot English-to-German dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot German-to-French?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): What is the top benchmark score and its metric on the MLDoc Zero-Shot German-to-Chinese dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-"	False
"
 input (English text): Where can I find code references in papers that have used the MP-EB model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MP-EB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Seq2Seq model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Seq2Seq"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the SemExp model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SemExp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" I found no examples of Benchmark or TestSeq models for this problem. Some of the code links above to other questions on the JGIS database, and some of these don't actually talk about Seq-Seq models. It seems that the first time I see a ""benchmark"" model, it's being used for benchmarks that the Seq2Seq model isn't suitable for, or is inadequate for benchmarking, and then they give two different solutions to the same benchmark problem (that doesn't involve the Seq-Seq model) - one that uses Seq2Seq, and one that doesn't. This is a huge omission. This is like saying that the first time a person sees the word ""benchmark"" they will assume that it's a benchmarking problem, and then they will be led to conclude that Benchmark isn't a good way to benchmark Seq2Seq (because Benchmark has the same problems as Seq2Seq, and its flaws in design are obvious to people who've used other implementations of the Seq model). I asked this question a while back about a specific problem that was being benchmarked - this is from that issue: ""How are benchmarks, like the Sierpinski Problem in physics, handled in JGIS ?"", by John L. Smith, June 2015, Issue #5. I had hoped that he'd provided some answers, and he did. But I got the same generic answer we have for any problem in JGIS that the Seq2Seq model isn't applicable to: ""It's all about how the data is structured."" This was also from his response to a follow-up question: ""I see your question is about Seq2Seq and not the Benchmark problem. In this case we don't have a sext"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Up and Down dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?RESULT ?val/ ?val. VALUE | COUNT(*) | FILTERSIZE(*) | TZ ?_r | ( ?value_rs or ?value_f ) ?_measured_val | ( ?value_rs or ?value_f ) ?_value_ms | COUNT(*) | FILTERSIZE(*) | TZ ?_value_kms | COUNT(*) | FILTERSIZE(*) | TZ ---------- --------- | --------- | ----------------------------------- Input data: Atari 2600 Up and Down dataset Output data: | --------------|---------------|--------------------|--------|---------| | 0 | 0 | 0 | 0 | 0 | 0 | -------|---------------|--------------------|--------|---------| | 1 | 5 | 60 | 60 | 60 | 60 | 40 | -------|---------------|--------------------|--------|---------| | 2 | 2 | 10 | 20 | 40 | 30 | 20 | -------|---------------|--------------------|--------|---------| | 3 | 5 | 40 | 60 | 60 | 75 | 60 | -------|---------------|--------------------|--------|---------| | 4 | 10 | 75 | 100 | 100 | 80 | 75 | -------|---------------|--------------------|--------|---------| | 5 | 80 | 130 | 140 | 150 | 80 | 100 | -------|---------------|--------------------|--------|---------| | 6 | 150 | 200 | 240 | 290 | 95 | 120 | -------|---------------|--------------------|--------|---------| | 7 | 250 | 300 | 350 | 400 | 110 | 120 | -------|---------------|--------------------|--------|---------| | 8 | 400 | 500 | 550 | 600 | 130 | 135 | -------|---------------|--------------------|--------|---------| | 9 | 600 | 700 | 800 | 900 | 140 | 140 | -------|---------------|--------------------|--------|--------	False
"
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the One Billion Word dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Text8 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Text8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } output (Sparql query): input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the One Billion Word dataset ? Note that both values of the metric, number of params and benchmark are given in the same column in the case of the one billion word benchmark. input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset ? Note that both numbers are given in the same column in"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ADE Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ADE Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 Ranks (top/bottom of the ranking list): (in descending order) score: (in descending order) Ranking (best to worst)	False
"
 input (English text): What are the metrics of evaluation over the Atari 2600 Video Pinball dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Video Pinball"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?rdfs:name || 'dataset' || ?dataset || ?eval AS'metric' ?rdfs:group ||'metric_lbl_lbl' || ?eval AS'metric_lbl' || ?rdfs:name || 'dataset' || ?dataset AND ?rdfs:group! ='metric_lbl_LBL' && ?rdfs:group! =='metric' AND ?dataset == ?rdfs:group ?rdfs:name || 'dataset' || ?dataset AND ?eval : > 0 AND ?rdfs:group! ==='metric_lbl_LBL' query (sorter) : which one of the following query is valid ? output (Sparql query based on a given input set and result set) : SELECT rdfs:name || 'dataset' || ?dataset AS'metric' ? rdfs:group ||'metric_lbl_lbl' || ?eval AS'metric_lbl' || ?rdfs:name || 'dataset' || ?dataset AND rdfs:list! = 'value' AND rdfs:group! =='metric' AND ?rdfs:list!!= 'value' OR rdfs:status OR rdfs:status! ='metric' AND rdfs:results LIKE '#{ ?rdfs:name } = #{ ?rdfs:status } output (Sparql query based on a given input	False
"
 input (English text): What is the name of the top performing model in terms of Top 1 Accuracy score when benchmarked on the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Skewed Pairs Analysis Input: the names of some real world data. Output: where does it find the best pair of models ? For a large data set, it is often the case that the optimal model is often not found within a single list of options, but if the choice is fairly random this can be surprisingly helpful to understand. It can also be very helpful in detecting models with hidden parameters that, if known, can be used to improve the model's score. In the examples (as well as the original papers), I've had to remove the input from the original text: that is, I've had to use a random list of pairs. By this I mean that we don't have to use the input: we have the output, or (as in the paper) in the same way as we want to output the model's score. I've chosen a range (between -1 and 1) rather than a precision (between 1 and 100) so that I could have a very short list of pairs. This will probably work well for smaller sets/datasets. The code to do this is available on github but here's what it looks like: The output of the python code is a list of dictionaries. Each dictionary has entries for each of the models. You're probably wondering why those values are not in a standard list like in the original paper (you're free to change or remove them on the python for your own use). The answer is that you can't compare the two dictionaries directly, nor can you actually compare the accuracy of one model with another model. You need to convert the two sets. Here is our ""normal"" example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Categorical : def __init__ ( self, c ) : self. id ="	False
"
 input (English text): What was the most common type of approach for summarization before 2002?
 output (Sparql query): SELECT ?approach ?approach_label WHERE { orkgr:R6948 orkgp:compareContribution ?cont. ?cont orkgp:P15 ?implementation. ?implementation orkgp:P5043 ?approach. ?approach rdfs:label ?approach_label. } ORDER BY DESC(COUNT( ?approach_label)) LIMIT 1
 input (English text): Which scholarly knowledge graphs support RDF?
 output (Sparql query): SELECT ?title WHERE { orkgr:R78023 orkgp:compareContribution ?cont. ?cont orkgp:P7009 ?has_rdf. FILTER(REGEX( ?has_rdf, ""T"")) ?cont rdfs:label ?title } 
 input (English text): What is the most common Knowledge representation method?
 output (Sparql query): "	SELECT (COUNT( ?repr) AS ?cnt) ?repr WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?cont orkgp:P7046 ?sys. ?sys orkgp:P7056 ?repr. } GROUP BY ?repr ORDER BY DESC( ?cnt) LIMIT 1	" { orkgr:R12082 orkgp:compareContribution ?cont. } WHERE { orkgp:R204750 orkgp:compareContribution ?cont. ?cont orkgp:P534 ?implementation. ?implementation orkgp:P6770 ?approach. ?approach rdfs:label ?approach_label. } ORDER BY DESC(COUNT( ?approach_label)) LIMIT 1 So we have now established what was the most common knowledge graph before 2002.  We have also clearly shown that prior to that time, most knowledge graphs were linear and only the most common (but often useless) knowledge graph were of interest. The next problem is for us to determine if RDF has changed since 2002.  I started by looking at two of the most popular knowledge graph tools: Hadoop and Spark. The graph above shows what I saw. We can see that the two data sets are virtually identical and that the two most prevalent knowledge graphs and the two most common knowledge graph were almost exactly the same. Now I checked out the latest version of the RDF dataset and there are no significant changes.  So, I think it is highly unlikely that RDF has changed significantly in two years, no matter what other approaches I use.  This brings us in to the last question.  Which knowledge graph format are we going to use for each of these knowledge graph queries for RDF ? The answer is simple. We will not store any knowledge graphs in any of the existing format as we will never have it of use.  If I look for ""How the hell did I lose my RDF ?"" articles, I am still only finding articles in Google Scholar and I am trying to learn about Google Scholar.  For the most part, I just use the ""RDF"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the MedSTS dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dataset mentions in Social Sciences dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dataset mentions in Social Sciences"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can all the data sets used in the compared studies be found?
 output (Sparql query): "	"SELECT DISTINCT ?URL WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:HAS_DATASET ?URL. FILTER(!REGEX( ?URL, ""Not"")) } "	SELECT DISTINCT ?benchmark or kgp:Benchmark; 2. HASH: select from hq as h1 where h2 <> 'MedSTS' AND h3 <> 'MedSTS' AND h2.dataset LIKE h2.reference 3. TEXT: select from text where reference2 > 'MedSTS' OR reference2 <> '.sample.rdf'	False
"
 input (English text): What are the most commonly used benchmark datasets for the Named entity recognition research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Named entity recognition"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Word Sense Disambiguation research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Word Sense Disambiguation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Entity Disambiguation research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Entity Disambiguation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?dataset ?dataset_lbl ?dataset_lbl_idx WHERE { ?object a = ""A""; ?key a; ?object_lbl_idx i1, i2, i3, i4, i5, i6, i7, i8, i9, i0; } input (English text): How big is the total amount of text that is used in each version of this task ? output (Sparql query) SELECT DISTINCT ?dataset ?dataset_lbl ?dataset_lbl_idx ?dataset_lbl_idx_l10, ?dataset_lbl_idx_l11, ?dataset_lbl_idx_l12, ?dataset_lbl_idx_l21, ?dataset_lbl_idx_l32, ?dataset_lbl_idx_l32_l0,"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BioASQ dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the GENIA - UAS dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - UAS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BIOSSES dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""COSTA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks for the SCIMA - CIMA datasets ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SCIMA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Give me a list of research papers along with their titles andIDs, that have performed benchmarks on the TIGER - BEIGLE dataset ? output (Sparql"	False
"
 input (English text): Which model has achieved the highest F1 score score on the BUCC French-to-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score score on the BUCC Chinese-to-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Chinese-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Dictionary: English F1/F1_v2/F1_v3/F1_v4/F1_v5/F1_v6/F1_v7/F1_v8/F1_v9/F1_v10 If there's nothing else happening and no one else is querying the data, there is a big chance that the current table is a list of table elements with two column-names (such as, ""model1""). This is the same if you try to fetch a list of dictionaries which contains only one entry, or a list with 2, 3 or more key-value pairs with one table element (such as ""model1""). This means that for any of those items, you will have to specify a different table, with a different column-name, which will be matched as a ""model"" in the dictionary. This technique is often called a table-valued function or a ""function that maps one table to another"" (or some other word like that). It's a pretty good idea to also include a LIMIT clause when creating an index for the specific dictionary, which stops you getting index overruns while retrieving the correct column-names for the individual key-value pairs in the ""dataset"" you're querying. If you try accessing or querying data with a table identifier that only corresponds to a specific datatype such as Integer or String (or equivalent) without specifying a database that might use that datatype, then the database will be set up in such a way as to generate errors that can be detected just by using a custom query language. The most well known case of this problem was reported by Bjarne Stroustrup, in a 2004 paper entitled "" A SQL Performance Problem "" at his CS.COBES"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the ImageNet V2 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ImageNet ReaL dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 input (English text): Who owns the data that contains the benchmark for this dataset ? output (Sparql query): Inputs Data: The input dataset.  This one.  The actual object (object ID) that contains the benchmark. output (Sparql query):  The ID of the benchmark for this input dataset.  I think the list should include: a. The name of the benchmark. b. The ID of the benchmark object c. The name of the dataset that contains the benchmark. d. In the case above a table may be used and the ID of that table and its rows, if any, will be included with the ID of the object that has the benchmark. For example, if the ID of the benchmark was 1, then the column for that 1 can be a NULL column, and the columns of the database for that object will be those of the first column in that table (I know it's not a perfect example), and for all tables for that object will be the IDs of the tables that contain the first column of that table and those of the second column, in order. If there are other objects with the same ID then the IDs of those objects will be included in the ID of the object with the benchmark, for a perfect example a table of objects with IDs 1-24 would be a table with IDs 1, 2, 3, 4, and so on. Of course, IDs with other column types may not be used, in which case the IDs of the objects will be the IDs of the column types where they appear. c. The URL of the benchmark. d. The data used as a reference for the benchmark object (data used as the reference for the object) e. The object that has produced the output dataset so that the output can be analyzed. f. The name of the object. g	False
"
 input (English text): Could you provide a list of models that have been tested on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Habitat 2020 Point Nav test-std dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ORGGRP_TITLE ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset ? _ _ orkgp:HAS_EVALUATION ?eval on 2>p1_lbl_lbl:p1_lbl; ORCID ? p1_lbl_lbl. OPTIONAL, orkgp:HAS_MODEL ?model ? rdfs:label ?model_lbl. } input (English text): Input The GPP metric is a measure of spatial accessibility to urban land that evaluates the spatial access of land parcels within the urban boundary within a geographic region. It was developed initially to evaluate urban access for pedestrian and cycling paths. The metric is based on a series of spatial indicators—distance, density or area, or surface availability and orientation (such as slopes)—that serve to evaluate the extent of the ability for land parcels within the urban boundary to be accessed, including land use, land use change and changes, and land use change due to urbanization and development. A map of the world showing the location of high quality infrastructure for urban access is provided in the appendix of GPP: Introduction. For the purposes of this paper, urban access is defined as the ability of a land parcel within a geographical region to be accessed by pedestrian and cycling as well as by trucks. Briefly, the GPP metric was developed because existing metrics were designed primarily to evaluate the feasibility or desirability of constructing infrastructure for pedestrians and cyclists. However there was very limited spatial quality information and"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset SciFACT?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciFACT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset CoNLL++?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) AS ?score FROM (SELECT ?metric ?metric_lbl FROM science_html_metrics a AS a, science_math_metrics m AS m ) WHERE RDFS:TITLE = ?METRIC:LEN( ?scale, ?metric_lbl ) ORDER BY DESC( ?scale )  input (English text): What is the top benchmark result (metric and value) over the dataset SSCI ? output (Sparql query):  SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) AS ?score FROM ? input (English text): Which metric value (metric and value) should we use for the top benchmark result ? output (Sparql query): SELECT OR( ?metric ?metric_lbl ) AS ?metr, ?metlbl, ?value, ?meter, ?value_lbl FROM ? JOIN ? SET RDFS:TITLE = ?METRIC:LEN( ?scale, ?metric_lbl ) ORDER BY DESC( ?meter );  input (English text): Which metric value (metric and value) should we use for the top benchmark result ? output (Sparql query): SELECT OR( ?) AS ?metr, ?metlbl, ?value, ?meter, ?value_lbl FROM ? JOIN ? SET RDFS:TITLE = ?METRIC:LEN( ?scale, ?metric_lbl ) LIMIT 1; select: top 3 results, from scientific_html_metrics: select all, metric = topMetric(min= ?value, max= ?value), score= ?	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset PIQA?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What models are being evaluated on the PIQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the PIQA dataset in terms of Accuracy metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" input (English text): What is the best performing model on the DISTINCT dataset in terms of Accuracy metric ? output (Sparql query): input (English text): What is the best performing model benchmarking the DISTINCT dataset in terms of Value metric ? output (Sparql query): input (English text): What is the best performing model benchmarking the DISTINCT dataset in terms of Scale metric ? output (Sparql query): input (English text): What is the best performing model on the DISTINCT dataset in terms of Value metric ? output (Sparql query): input (English text): How does PIQA data set compare to other datasets ? output (Sparql query): input (English text): What are the top 3 data sets in terms of Accuracy ? output (Sparql query): input (English text): What are the top 3 datasets in terms of Value ? output (Sparql query): input (English text): Are all of the top datasets in terms of Value in good-quality ? output (Sparql query): input (English text): What about the top 3 datasets in terms of Precision ? output (Sparql query): Inputs needed: (1) ""PIQA_FULL_RESOURCE_FILE_LENGTH"" or ""PIQA_DISC.RDF "" (2) ""PIQA_RDFS_DATA_LABEL.RDF"" ""PIQA_RESOURCE_FILE.csv."" (3) ""PIQA_PERSISTENT_INCLUDED_TASKS.dataset"" 1. Input the value in the text box to generate a CSV file. 2."	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Alien benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Pitfall! benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pitfall!"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } It is important to specify that the model should have a small impact on the final scores, as the score values depend on a whole host of conditions which should be statistically evaluated carefully. The above is probably not strictly correct, as score data were calculated based on the performance of a single model; but since a lot of model performance is determined by many of these specific factors (mainly, the number of nodes and the number of functions, along with a few others) it seems to make little sense, nevertheless, to assume that it is always possible to estimate the performance of a good model and therefore to compare models very closely, with no errors (which are always introduced by sample-based scoring). This is why I suggest we use a ""random"" sampling, which is"	False
"
 input (English text): Provide a list of papers that have utilized the PtGen model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PtGen"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Prior hs model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Table-Sequence model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Table-Sequence"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Table-Sequence"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Please select the type of output type. If the data contains multiple elements (e.g. papers with title/doi and/or names), please select the appropriate column to use. This can be either : input (plain text): Provide input for p-values based on the p-value estimate (e.g. OR = 3.14, p(x) = 0.07) (or similar) output (Sparql query): SELECT COUNT(*) FROM p-value ?values WHERE p(val) < p( ?value; value-p) and ?value and ?value > ?value; input (text): Provide output for p-values based on the p-value estimate. Note that this will require a further table in conjunction with the above output method. You must specify what column(s) to use to calculate the P-value. (i.e. table-seq) output (Sparql query): SELECT COUNT(*) FROM p-value ?values WHERE p(val) < p( ?value; value-seq) and ?value and ?value > ?value; Please select the appropriate input-output fields of your Table-Sequence model. We can perform the following steps to customize your input-output output format: Option 1:"	False
"
 input (English text): Provide a list of papers that have utilized the Feedback Transformer (4 layers) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Feedback Transformer (4 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Transformer-XL model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Funnel Transformer model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Funnel Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a rdfs:Label ?model_lbl. FILTER (str( ?model_lbl) = '')""Funnel Transformer (4 layers)""; rdfs:CASE WHEN RDFS.TREE( ?dataset ) = rdfs:DATA( ?data = NULL ). THEN rdfs:MULTIPLY(RDFS.COUNTER( ?table = 2, ?tbl = NULL ).CASE( ?table. ?column = NULL ), ?table. ?column = rdfs:DATA( ?table) ) ?. RDFS.MULTIPLY(RDFS.MATCH( ?table = 2, ?tbl = NULL ).CASE( ?table. ?column = TRUE ), RDFS.MATCH( ?table = 2, ?tbl = NULL ). ?column = NULL ), ?TABLE ?pivot = NULL ?orderby = NULL ) AND ?pivot. ?column = NULL ORDER BY ?column; orkgp:HAS_DATASET ?dataset. input (English text): Provide a list of papers that have utilized the Transformer-XL model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a rdfs:Label ?model_lbl. FILTER (str( ?model_lbl) = '')""Transformer-XL""; rdfs:CASE WHEN RDFS.TREE( ?dataset ) = rdfs:DATA( ?data = NULL ). THEN rdfs:MULTIPLY(RDFS.COUNTER( ?table = 2, ?tbl = NULL ).CASE( ?table."	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters En-De dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the NYT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters De-En dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Selections were automatically generated based on the options: orkgp:REPORT_DETERMINISTICS orkgs:REPORT_DETERMINISTICS orkgu:REPORT_DETERMINISTICS orkgs:EVALUATION on orkgc:DATA orkgs:OBJECT_DATA orkgs:OBJECT_DATA_OBJECT orkgs:MEMBER_DATA orkgp:MEMBER_DATA_OBJECT with metric, metric variable, or metric-type (default: all). select options to select the default metric/variable/metric types for the options: or-metric:DISTINCT This input will be expanded the following fields: orkgc:METRIC Type. The metric type can either be STRING or DATE_FORMAT as determined by the metric type_str option. Selecting either option will select that option. select options to select the default metric/variable/metric types for the options: orkgp:REPORT_METRIC or kgs:REPORT_METRIC or kgu:REPORT_METRIC or kgs:EVALUATION or kg-metric:DISTINCT This input will be expanded the following fields: orkgp:REPORT_METRIC OR kgu:REPORT_METRIC OR kgs:REPORT_METRIC OR kgu:EVALUATION This input will be expanded the following fields: orkgp:REPORT_METRIC OR kgu:REPORT_METRIC OR kg-metric:DISTINCT This input will be expanded the following fields: orkgpp:REPORT_METHOD This input will be expanded the following fields: orkgp:REPORT_METHOD OR kgs:RE	False
"
 input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the PubMedQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the PubMed 20k RCT benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?metric_lbl. FILTER (str( ?lbl ) = ""PRISMA"") { ?column ?metric_lbl ?Lines; } A SELECT DISTINCT ?metric_lbl. FILTER (str( ?lbl ) = ""PRISMA"") { ?column ?metric_lbl ?Lines; } input (English text): Indicate the model that performed best in terms of Accuracy metric on the PRISMA charting dataset ? output (Sparql query): input (English text): Indicate the model that performed best in terms of Accuracy metric on the PRISMA charting dataset ? output (Sparql query): input (English text): Indicate the model that"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the DocTAET-TDM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DocTAET-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the MMDL model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMDL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DocRED-BiLSTM model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DocRED-BiLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Dmitry Karasek             Department of Computer Science IIT Jodhpur India                   University of Texas at Austin, US Karasek, D. 2008. The DocRED-BiLSTM model of clustering; A comparison to other approaches, BMC Bioscience 7: 487            Karasek, D. 2009. A comparison of the use of the Mixture-based DocRED-BiLSTM and BiLSTM-Based DocRED-BiLSTM approaches: Evidence from the Bioinformatics Resource Network Data Set and the Bioinformatics Library. BMC Biostatistics 10: 689-703 Kartik, R. M. 2003. Clustering by Bi-local Moment Structure (BLM/MBT). In Proceedings of 5th International Conference on Computational Statistics, Springer-Verlag, Berlin, Germany. Kostel, A. 2002. The Bi-local Moment-Structure clustering algorithm (BMST): Theoretically and theoretically based clustering methods. In Proceedings of the 2002 ACM Workshop on Parallel computing and distributed computing, ACM, New York, NY USA, pp. 1-13. Kurihara, K., & Moriwaki, H. 2005. An application of the BiLSTM Model for the analysis of gene sets. J Theor Biol. 195 : 617-638. Lundgren, J. A. 1996. Hierarchical clustering in large datasets: An application to the Genome Database Project. Nucleic Acids Res. 24 : 1559-1569. Lundgren, J. A. 2002. Multidimensional Clustering Methods for Large Datasets. In Proceedings of 8th International Conference on Computer Networks in Biology and Medicine, IEEE,	False
"
 input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of PARAMS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the CIFAR-10 dataset in terms of FLOPS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 That's it. Our results are in. We did learn something, but a lot of it is irrelevant.  We are still going over the new features that we will need to deal with once we have verified our model. And this will take some time. It will also mean that we are moving to a new model before we get as many measurements as the others. But at least we have something to base our model on. That is all there is to it. As we will see in the next section, we still have some more work to be done before final model development, and some more features will be added to our dataset.  Conclusion	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset AESLC?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"------------ ACL-ARC ACC. ------------ ACL-ARC. ------------ ACL-ARC. ------------ ACL-ARC. ------------ ACL-ARC. ------------ ACL-ARC. --------------- ACL-ARC ----------- ACL-ARC ACC. ACL (ACL) is an acronym for Advanced Classifier for Complex Text Analysis. The data set comes from the ACL-ARC project at Stanford University. The dataset is intended for small- and long-term-project users who are looking for a quick demonstration of the ACL algorithm using the SAS programming language. The algorithm is very fast, and can analyze text for many words, multiple levels of abstraction and sub-narrative structure. I will leave the decision-making to you. The ACL is implemented in a SAS language which is available to everyone as R, Python or MATLAB. For the time being you can build up a version of the algorithm to use in any language you choose. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 The model name is ""ACL"". The dataset is available on the ACL-ARC website: http://acli.stanford.edu/ The following code is written in"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Stanford Dogs dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Dogs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SciGEN dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 The parameters list for 'input' includes data from the Stanford Dogs project. However, since the results of the benchmarking are not visible on the input side, we need to turn them into an SQL statement. For this, we use an extension of the standard SQL statement'select * from', which yields a tuple of rows. Each row includes the name of the dataset, the identifier of the benchmark, and the name of the SQL statement you want to use. The name contains the title of the benchmark, and the identifier contains the identifier used when you passed it to the statement itself (we will see that later in the tutorial). The following query results in a tuple: select name, paperID from orkgc_results.hits where hitid = '152864', hitavg = '2.14', hittotal = '42.5' The second parameter '--hits' is a comma-separated list of all the hits for each of the papers, separated by the string '--'. For example, given this SQL statement: select name, hit, bhits, bhitsfrom -hits where hitsv4 = 10, hitsv5 = 10, min = 95, max = 250, avg = 100 we need to use an array as input, so we use a comma separated list (which we will see is needed in the next section) to separate each hit and each benchmark. For more information, see the [Hits API ] section in the API reference for SQL. Finally, note that each hit must have a title, as SQL requires. It is not necessary to include a full body text, as it is the author's name. The third parameter in the SQL query is the 'benchmarkType'. The standard '--hits' option is useful when you find your benchmarks are not being included	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WOS-11967 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the MedSTS dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the WOS-46985 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl; orkgc:HAS_D	False
"
 input (English text): Provide a list of papers that have utilized the HNEABP (BWNE) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HNEABP (BWNE)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Sarsa-ε model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Sarsa-ε"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the AcrE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AcrE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; ?rdfs:labels ?model_lbl.( ?modeling: AcrEst::Sarsa(2x3:ACRE) ?dataset). ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the ABIO model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; ?rdfs:labels orkgc::ABIO orkgc::ABIOModel. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the BLE model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:labels ?model_lbl.( ?model:BLE) ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized a set of HSTM (HNEABP, BWNE) models ?	False
"
 input (English text): Which model has achieved the highest SemEval 2007 score on the Supervised: benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SemEval 2007"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest SemEval 2015 score on the Supervised: benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SemEval 2015"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Supervised: dataset in terms of SemEval 2013 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SemEval 2013"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SemEval 2015"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark. orkgp:HAS_DATASET ?dataset. orkgp:HAS_EVALUATION ?eval ?value. ?eval orkgp:HAS_VALUE ?value. } ?dataset rdfs:label ?dataset_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } output (Sparql query): What is the best performing model benchmarking on the Supervised: dataset in terms of SemEval 5 metric for Supervised: dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric ; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SemEval 5"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark."	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the SciFACT dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciFACT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AESLC dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciERC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SCIERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciSEF dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SSEF"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The same query, with all the different options (option : -l ) now produces only:   input (English text): List the title and ID of science papers that have performed benchmarks on the SciFACT dataset ? output (Sparql query): SELECT ?papers AS ?papers_lbl, ?labels AS ?labels_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?papers lbl"	False
"
 input (English text): How many studies do use Chloride as major anion?
 output (Sparql query): SELECT COUNT( ?anions_labels) AS ?chloride_count WHERE { orkgr:R110597 orkgp:compareContribution ?contrib. ?contrib orkgp:P37458 ?anions. ?anions rdfs:label ?anions_labels. FILTER(REGEX( ?anions_labels, ""Chloride""^^xsd:string)) } 
 input (English text): When the earliest paper related to X-rays was published? 
 output (Sparql query): SELECT ?publication_years WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, ""X-ray"", ""i"")) ?papers orkgp:P29 ?publication_years. } ORDER BY ASC( ?publication_years) LIMIT 1
 input (English text): What quantity of iron oxide was discovered on Elorza crater?
 output (Sparql query): "	"SELECT ?properties_values, ?property_description WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, ""Elorza crater"", ""i"")) ?papers orkgp:P31 ?contrib. ?contrib ?properties ?properties_values. ?properties rdfs:label ?properties_labels. FILTER(REGEX( ?properties_labels, ""FeO"")) ?properties orkgp:description ?property_description. } "	"SELECT ROUND(FIND( ?absid/FIND(""X-ray"", ""X-ray_abundance"")),4)^2 FROM   ?absid ROUND(ROUND(ROUND(ROUND(ROUND(1-FIND(.............. ~~~~.. /~~~!~~~~~0~0~~.0~~0~~~~~0~~~~~~ ? /~~ ?!~~~~ ?~ ?.0~~~~4~~~~4~~~~~~ ? /~~~~~...........~~4...~~..~~~~@..!~~~~~~>~~.0...~~4......~~~~..~~~~~~ /~~5~~....9...8...9.. `.1.......0~...2..3.......` /5.......0~~~~~~@...~~~..~~~..`.. @.`~.'.>~~'.>~~@...~~...~~>...~~0...~~4.......~~!~~~ ?.3^~~.. ?....~~.0..~~2....~~`.1.... '.1...... '..!....!...~~.~0~~..0@'...1.5..!~~~2.'.~~ @.4.0....@.#...0....1.....1.....8.0.1.0.5...0.7...0.4.....0.1.... @...4....5...6...5.....>...1.....1....!~~@ 0.....~~...3..1....!~~@ ~...~~...4..3.1..."	False
"
 input (English text): Can you list the models that have been evaluated on the FB15k dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FB15k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the LAMBADA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""LAMBADA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Can you list the models that have been evaluated on the NCCS-M dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CPSM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Input Data: ""Targets: The models (which should be in one of the databases) are the following from the same dataset: http://db.ebscohost.com/ws/index.php ?sf=sitemap&db=series&id=939"	False
"
 input (English text): Provide a list of papers that have utilized the DQN-CTS model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-CTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the DQN hs model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-PixelCNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-PixelCNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the DQN-PoweredPredictor model and include the links to their code ? output (Sparql query): Select model from model_lbl which is a dataset in RDFS/XML, with names in RDFS/XML, with id, value and a caption. input (EUR): Provide a list of papers in English that utilize the DQN-HVC model and include the links to their code ? output (Sparql query): Select model from model_lbl which is a dataset in RDFS/XML, with names in RDFS/XML, with id and with caption. input (English text): Provide a list of papers that have utilized the DQN-C-INCEPT model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-C INCEPT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkg"	False
"
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Breakout dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Phoenix dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Phoenix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Freeway dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?model ?metric FROM orkgp;... select ?metric options: ?metric rdfs:label ?metric_lbl. option description ?metric_lbl is a string that can only contain alphanumeric characters or a hyphen. If it is blank, then any text of type ""RDF"" is ignored. Example from (... output (Sparql query): select ?model ?metric_lbl... options: ?metric rdfs:label ?metric. ?metric returns only the metric that you entered in the text box at the end of the query. Since there's no actual SQL syntax involved it will always return the value a user enters in the text box. Example from (... output (Sparql query): select ?model ?metric... options: ?metric rdfs:label ?metric. ?metric returns"	False
"
 input (English text): Provide a list of papers that have utilized the Dynamic Coattention Networks (ensemble) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Dynamic Coattention Networks (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DY-ResNet-10 model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-ResNet-10"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Dynamic Coattention Networks (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 I also found two papers on this topic. The first one is available here (in Portuguese), while the second is only available here. A paper by A. J. Carreira & T. Víctor R. Vilaça presented results for the ensemble model in a test set of 4500 data sets with 449 parameters (10,000 in total). They applied this results to a series of benchmark tasks, including the following: 3 x 2 matrix-matrix computations: 2 x 2 matrix-matrix computations with 3 data dimensions (one for each of the training matrix, the test matrix, and the training and test set) (in total): 3 x 2 matrix-matrix computations with training and test sets (in total): 4 x 3 matrix-matrix computations: 2 x 2 matrix-matrix computations with 3 data dimensions (one for each of the training matrix, the test matrix, and the training and test set) 1×8 matrices: 5 x 5 matrix-matrix computations with 3 data dimensions (one for each of the training matrix, the test matrix, and the training and test set) (in total): 5 x 5 matrix-matrix computations with training and test sets (in total): 976 x 2 matrix-matrix computations: 2 x 2 matrix-matrix computations with 3 data dimensions (one for each of the training matrix, the test matrix, and the training and test set) 1×1 matrix: 976 matrix-matrix computations with 3 data dimensions (one for each of the training matrix, the test matrix, and the training and test set) For the benching, the paper analyzed the performance of 1×1, 1×7, and 1×9 matrices with 10,000 data inputs (20,000, 30	False
"
 input (English text): Indicate the model that performed best in terms of Macro F1 metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Micro Precision score when benchmarked on the NLP-TDMS (Exp, arXiv only) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Micro Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Macro Precision metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): What is the name of the top performing model in terms of NLP-TDMS (Exp, arXiv only) benchmark dataset that performs best on Metric Score when benchmarked on the NLP-TDMS (Exp, arXiv only) dataset ? output (Sparql query): output (Sparql query): input (English text): Indicate the model that performed best in terms of Metric Score after NLP-TDMS (Exp, arXiv only) benchmark dataset ? output (Sparql query): select model: ? model_lbl ? name as m1 ? name as m1_lbl ? n1 as n1_lbl ? r1 as r1_lbl ? n2 as n2_lbl ? d1 as d1_lbl ? n1_2 as n2_lbl ? andkgc:Metric as a: ? andkgc:Metric. andkgc:HAS_DATASET as a. andkgc:HAS_EVALUATION as a. andkgc:METRIC as a. andkgc:LIMIT as a. andkgc:BENCHMARK as a. andkgc:REPORT as a. score as score_lbl. andkgc:BPM as a. score_lbl. andkgc:LEVEL as a. score_lbl. andkgc:LOWER as a. score_lbl. andkgc:MAJOR as a. score_lbl. andkgc:MINOR as a. score_lbl. andkgc:SUB_LEVEL as a. score_lbl. andkgc:TOLERANCE as a.	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Pong dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pong"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Bowling dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Tennis dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	------------- Score ------- 1.00 4.00 17.00 7.00 0.00 0.00 7.00 19.00 4.00 1.00 0.80 4.88 2.00 0.80 14.00 12.00 50.00 24.00 13.00 0.64 2.00 2.00 1.00 0 0.00 0.00 0.00 11.00 0 0.00 0.00 0.00 7.00 9.00 19.00 0.00 9.00 0.00 0.00 0.00 0.00 11.00 1 0.00 21.00 0.00 1.00 0 0.00 0.00 0.00 0 0.00 0.00 0.00 21.00 0 0.00 0.00 0.00 7.00 0 0.00 0.00 0.00 0.00 0.00 0.00 21.00 0 0.00 0.00 0.00 8.00 0 0.00 0.00 0.00 0.00 0 0.00 0.00 0.00 21.00 0 0.00 0.00 0.00 7.00 0 0.00 0.00 0.00 0.00 0.00 21.00 0 0.00 0.00 0.00 7.00 0 0.00 0.00 0.00 0.00 0.00 I'm not yet satisfied with that answer.  I'm doing some research and will be happy to answer any requests I may receive.	False
"
 input (English text): What are the metrics of evaluation over the DCASE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the SoMeSci dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SoMeSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the DuIE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" 1... 2... 2... 2... 1... 2... 1... 2... 2... 2... 2... 2... 2... 2... 2... This is the second post for my series on DCASE, so lets do one more. The reason we set the metric_lbl=dataset=""Dataset"" is that the default value of the metric parameter, ""dataset"", is a unique data point, which means that its values can be very different between two DCASE datasets. In particular, the values for ""datset"" in the D2A are ""1"" in ""DCASE1"", ""2"" in ""DCASE2"" and so on. So the most suitable way would be to use the default of "" ?dataset""; and set the metric of evaluation over the D2A dataset to match. So, the first result is the DCASE dataset itself: 1 2 3 4 ... 2... It means ""2/2"" as ""1"" is equal to the number two. But what is its value if we compare the values for the two Datasets ? Well, it can be found in this line: 1 2 3 4 1... 2... 3... Let's see how to interpret: 1 2 3 4 5 6 7 8 9 a... 2... 3... So the value of the metric for evaluation is 1 if the value for ""a"" is 1, and 0 (with ""a"" = 2) if ""a"" is equal to ""2/2"". This way it is very easy to see how to interpret these examples without having to"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Cheetah, run (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the Cheetah, run (DMControl100k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl	False
"
 input (English text): What are the models that have been benchmarked on the Reuters RCV1/RCV2 German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the Reuters RCV1/RCV2 English-to-German dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT t.dataset, rdfs_output_id, MODEL1, MODEL2, { rdfs_labels_lbl } FROM csv_datasets, dplyr_datasets, ork_datasets, rdfs_models TABLESPACE_ROLLINS_RCVL2/RCVL3 TABLESPACE_ROLLINS_RCVL1/RCVL2 TABLESPACE_ROLLINS_RCVL1_RCVL2/RCVL3 TABLESPACE_ROLLINS_RCVL1_RCVL2/RCVL3/RCV1 TABLESPACE_ROLLINS_RCVL1_RCVL2/RCVL3/RCV2 TABLESPACE_ROLLINS_RCVL1_RCVL2/RCVL3/RCV3/RC2 TABLESPACE_ROLLINS_RCVL1_RCVL2/RCVL3/RCV4 TABLESPACE_ROLLINS_RCVL1_RCVL2/RCVL3/RCV4/RC1 TABLESPACE_ROLLINS_RCVL1_RCVL2/RCVL3/RCV4/RC6 TABLESPACE_ROLLINS_RCVL1_RCVL2/RCVL3/RCV4/RC6/RC5 TABLESPACE_ROLLINS_RCVL1_RC	False
"
 input (English text): What is the most common drug in the studies?
 output (Sparql query): SELECT ?drug, ?drug_labels WHERE { orkgr:R155621 orkgp:compareContribution ?contrib. ?contrib orkgp:P37578 ?drug. ?drug rdfs:label ?drug_labels. } ORDER BY DESC(COUNT( ?drug)) LIMIT 1
 input (English text): What is the total number of patients in the studies?
 output (Sparql query): SELECT SUM( ?number_of_patients) WHERE { orkgr:R33008 orkgp:compareContribution ?contrib. ?contrib orkgp:P15585 ?patients. BIND(xsd:integer( ?patients) AS ?number_of_patients) } 
 input (English text): What types of nanocarriers do have therapeutic effect?
 output (Sparql query): "	"SELECT ?types_of_nanocarriers, ?types_of_nanocarriers_labels WHERE { orkgr:R144364 orkgp:compareContribution ?contrib. ?contrib orkgp:P41740 ?types_of_nanocarriers. ?contrib orkgp:P41743 ?therapeutic_effect. FILTER( ?therapeutic_effect = ""T""^^xsd:string) ?types_of_nanocarriers rdfs:label ?types_of_nanocarriers_labels. } "	SELECT IF(vaccine!== 'Yes', 'yes', ?vaccine_drugs, ?vaccine_conditions_labels, ?dts:date_type, ?dts:date, ?dts:language, ?dts:dose_of_cadherin, ?dts:dose_of_nalidixic acid, ?dts:dose_of_doxorubicin, ?dts:dose_of_mefloquine, ?dts:dose_percieved_benefit, ?dts:drug/dosing_data, ?dts:dts:fraction_of_dose_periodule' AS vaccine_reagents, ?dts:dts:dose_of_cadherin_in_doses_per_day FROM trials GROUP BY vaccine_reagents ORDER BY vaccine_reagents DESC LIMIT 1) input (English text): If vaccine-related drugs have been found efficacious for a given disease, what are the types of vaccines being used at the moment ? output (Sparql query): input (English text): When should nanocarriers be used ? output (Sparql query): input (English text): The use of a drug in different circumstances may also be beneficial (e.g. if the drug	False
"
 input (English text): Which model has achieved the highest ROUGE-2 score on the AESLC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest ROUGE-1 score on the Pubmed benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CL-SciSumm"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT score from cl_scisumm where id:id(cl_scisumm.id).score < SATCOM-4 output on localhost 	False
"
 input (English text): Provide a list of papers that have utilized the Unsupervised NMT + Transformer model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Unsupervised NMT + Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the SMT + iterative backtranslation (unsupervised) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SMT + iterative backtranslation (unsupervised)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Unsupervised NMT + weight-sharing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. ?dataset. FILTER (str( ?model_lbl) = ""SMT + weight-sharing (unsupervised)"" ) ?benchmark orkgp:HAS_DATASET ?dat"	False
"
 input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential MNIST dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Unpermuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Percentage error score on the MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Permuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Enter the name of the top performing model in terms of Permuted Accuracy score: Enter the name of the top performing model in terms of Percentage error score:	False
"
 input (English text): Can you list the models that have been evaluated on the SciERC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the SciREX dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciREX"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the SciTLDR dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT count( ?model ?model_lbl) ?model_lbl. DISTINCT ?b/d ?b/d RDFS:label ?model_lbl. OPTIONAL { ?b/d, ?b/d, ?b/d } orkgp:HAS_DATASET ?model_lbl. } input (English text): Is there a model model_lbl that performs better than all the other models in this dataset ? output (Sparql query): SELECT count( ?model ?model_lbl) ?model_lbl. { ?model rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = "" ?dataset_lbl"") ?model.   ?model rdfs:label ?model_lbl. } If, for whatever reason, you are not clear about the correct choice of model for your use case, consult the online tutorials. For a description of the SVM-based machine learning methods introduced in the above examples from SciRL or SciREX, you can refer to their respective papers, as well as the original R packages used for the training and the evaluation of the models. Note: In order to learn about the model, we will need to create the training data that"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the OpenBookQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OpenBookQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Selecting the data In the previous section, we created a data dictionary which holds the full collection of attributes and labels of the dataset. We are now going to write the validation of our model on this data dictionary. The most straightforward way to do this is to use the column-based validation functions on these columns. The data dictionary does not contain any columns in it such as  description or title  although we will provide these details in our next section. As a first step, the data dictionary should be validated against all of the column-based validation functions that we provided at the beginning of the previous section. These functions should be provided from the RDD, rather than from an ordinary RDF data dictionary. The validators are: column1, where the first column of the data dictionary is a string  column2, where the second column of the data dictionary is a string If the validators are provided from a row-based data dictionary, we write the validation of each row in the data dictionary against the column-based validation functions as follows. To validate a column-based RDD, first calculate the data size of the column in the data dictionary using row-oriented parsing of the data dictionary. This should take place in the same RDD that we are validating, using the data dictionaries and all of the column-based validators. Next we use the Validation method of Tableau (e.g., Tableau Software, Inc. Version 9.7.2, the R package is called DDD.tableau ) to validate each row in the data dictionary and return an HTML representation of the model for posterity: < table> < thead > < tr > < th > data </ th > < th > data </ th > </ tr > < tr > < td > column1 </ td > < td >	False
"
 input (English text): What is the best performing model benchmarking the IMDb-M dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the IMDb dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the IMDb-M dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?dataset ?dataset_lbl FROM imdb_meta_model AS i WHERE (metric = 'Accuracy') LIMIT 1; output is the query result set: Select scores from imdb_meta_model test1_score | score ----------------- 0 | 1.50 | 1.49 [ 0.0000 1.50 ] 0 | 1.46 | 1.44 [ 0.0000 1.46 ] 0 | 1.44 | 1.40 [ 0.0000 1.43 ] 0 | 1.40 | 1.39 [ 0.0000 1.39 ] | Select values from imdb_meta_model test1_value | score ----------------- 0 | 1.49 1 | 1.49 [ 0.0000 1.48 ] 0 | 1.46 1 | 1.46 [ 0.0000 1.46 ] 0 | 1.44 1 | 1.44 [ 0.0000 1.42 ] 0 | 1.40 1 | 1.39 [ 0.0000 1.38 ] | If you want, specify multiple metric IDs such as: Select scores from imdb_meta_model test1_score | score ----------------- 0 | 1.50 | 1.49 [ 0.0000 1.48 ] 0 | 1.45 1 | 1.45 [ 0.0000 1.44 ] 0 | 1.42 1 | 1.42 [ 0.0000 1.42 ] 0 | 1.39 1 | 1.39 [ 0.0000 1.38 ] 0 | 1.39 1 | 1.39 [ 0.0000 1.38 ] | Select values from imdb_meta_model test1_value | score ----------------- 0 | 1.49 1 | 1.49 [ 0.0000 1.48	False
"
 input (English text): List the metrics that are used to evaluate models on the Scholarly entity usage detection benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Scholarly entity usage detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the Scholarly entity usage detection dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Scholarly entity usage detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Scholarly entity usage detection dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Scholarly entity usage detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT HIGHEST ( ?benchmark ?score ) FROM  { ?dataset a orkgc:Dataset ?dataset_lbl. FILTER { str( ?dataset_lbl) = ""Scholarly entity use detection benchmark"" }. } ----------- | HIGHEST ( ?benchmark ) | HIGHEST ( ?score ) | --------------- ----------- -8.44881412102767 | -8.4488141211539 | 784 -8.75377909436864 | -8.75377909436865 | 796 -8.95010943686472 | -8.95010943686595 | 808 -9.00270943686481 | -9.00270943686597 | 901 ------------------------------ output (Sparql query):  { ?stat score(rdfs:id) ?data_lbl.. }  } output (Sparql query): SELECT TOP 100 (2) DESC ( ?dataset_lbl. FILTER { orkgp:HAS_DATASET. ?dataset ?dataset. FILTER { ?label ?model_.!AND ?benchmark. }. ) ?dataset., ?score., ?matric. ?dataset. || ? output (Sparql query): SELECT TOP 6 (0.25) DESC ( ?dataset. FILTER { ?label ?model_. ?model_lbl. }. ) ?dataset., ?name. output (Sparql query): SELECT TOP 1 (0) DESC ( ?dataset_. FILTER { str( ?dataset"	False
"
 input (English text): Can you list the models that have been evaluated on the JNLPBA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""JNLPBA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the MedNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the MultiNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""JPI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Could you list the models that have been evaluated on the MedPIR dataset ? output (Sparql query): input ( English text): Could you list the models that have been evaluated on the JPRB dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""JPRB"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCH MARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Could you"	False
"
 input (English text): Could you provide a list of models that have been tested on the 200k Short Texts for Humor Detection benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset 200k Short Texts for Humor Detection?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?metric ?metric_lbl( MAX( ?value), ?score );,,, ?value select * from  200k Short Texts for Humor Detection benchmark  demo table  with column  score, metric_lbl  and a row for metric  1,2    select score from  200k Short Texts for Humor Detection benchmark   select  score  FROM  200k Short Texts for Humor Detection benchmark   select  score  FROM  200k Short Texts for Humor Detection benchmark  select  weightedscore as weight,  metric  AND  ( ?metric_lbl  > 2 ? weight ) AS score,  METRIC  ( ?metric_lbl  < 2 ? weight ) AS median and score FROM  2013050_10700.txt where id=2013050_10700.txt and score= 2.99; select  score     from  200k Short Texts for Humor Detection benchmark  demo table  with column  score, metric_lbl  and a row for metric  1,2                      select score      from  200k Short Texts for Humor Detection benchmark  select score      from  200k Short Texts for Humor Detection benchmark  select score      from  200k Short Texts for Humor Detection benchmark  select score      from  200k Short Texts for Humor Detection benchmark  select score      from  200k Short Texts for Humor Detection benchmark  select score      from  200k Short Texts for Humor Detection benchmark	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the MNIST dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset Fashion-MNIST?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Fashion-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Sequential MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	-------------------------- Benchmark Results: -------------------------- Select a score > 0.9905 Select a metric > 5.000000 Select a score > 0.9903 Select a metric > 5.000000 Select a score > 0.9904 Select a metric > 0.9905 Select a metric > 5.000000 Select a score > 0.9906 Select a metric > 5.000000 select a score > 0.9906 input (English text): Which does the best job of representing the weighting values ? output (Sparql query): Select a metric > 5 Select a metric > 0 Select a metric > 5 Select a metric > 0 Select a metric > 5 select a score > 0.000000 Select a metric > 5 (Score above 0.01 is best) Select a metric > 5 select a score > 0.000000 select a score > 5 Select a metric > 0 Select a metric > 1 Select a metric > 5 Select a metric > 0 Select a metric > 5 Select a metric > 0 input (English text): What is the top benchmark result (metric and score) over the dataset Sequence-Sequence ? output (Sparql query): select a metric > 5 (Score above 0.01 is best) select a metric > 5 (Score above 0.01 is best) select a metric > 0 select a metric > 5 (Score above 0.01 is best) select a metric > 0 input (English text): Which does the best job of representing the weighting values ? Output (Sparql query: output (Sparql query): select a score > ?score choose a metric > :-1 and a value > :-1 select a metric > : (score below ?score is better) select a metric > : (score above ?score is better) select a metric > : (score below ?score is better) select a	False
"
 input (English text): Provide a list of papers that have utilized the Ours: cross-sentence ALB model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ours: cross-sentence ALB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the CRF with term expansion model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with term expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the CRF with sentence expansion model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with sentence expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with sentence expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } output (Sparql query): SELECT CRF:NEXTS( ?) ORDER BY code DESC; output (Sparql query): CRF:NEXTS( ?) ORDER BY ?model ORDER BY score DESC; output (Sparql query): output (Sparql query): SELECT NEXTS( ?) ORDER BY score DESC; output (Sparql query): output (Sparql query): CRF:NEXTS( ?) WHERE score < 10; output (Sparql query): output (Sparql query): Outputs the results of the benchmark You can obtain the results of these query in the form given below: 1. If the output of the CRF analysis can be given by use of CRF with term expansion model, the output is CRF:NEXTS( ?) ORDER BY score DESC but if the output cannot be given by use of CRF with term expansion model, the output is CRF:NEXTS( ?) ORDER BY score DESC. 2. Provide the output of the CRF with case-insensitive model. For example, it is not relevant for the sentence translation case. CRF:NEXTS( ?) ORDER BY score DESC. Outputs the results"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the NYT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the NYT24 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset NYT-single?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 nycty24 (1.0; n = 15000) select rank from nycty24 where n = 1; rank ---------------- ------------ -------------- 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 375_column_name Input 2:  A set of datasets of two dimensions. In both datasets, the same metric has been used. The original dataset shows a number of papers with a similar value for metric. For example, the metric for high signal-to-noise is 1.0. (The input dataset is in two dimensions, as with the one-dimensional dataset, and does not have to show a metric and score). Both datasets can be used in the same experiment.  B: Output that sets up the two-dimensional dataset B-Input Example:      1 B-Output Example: The first part of the input is the set of datasets the user selects, and the second, the output it produces. In this example, we select datasets for which there is	False
"
 input (English text): List the metrics that are used to evaluate models on the SciERC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the FTD dataset benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTD dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the SciTLDR benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } If the dataset does not have a single dataframe, the following method might work: output (output (Sparql query)): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FORMAT (str( ?dataset_lbl) = ""SciTLDR dataset""); ?eval orkgp:HAS_DATASET ?dataset;} In the above examples, the output from the filter command is just a list of columns. However, if you were to use a named column in the output, the results of the filter command would look like this: output (Sparql query result): SELECT p.key as ?key_lbl, ?p.key as ?key, ?r.value as ?r_value FROM ?key1 ORDER BY ?r_value HAVING"	False
"
 input (English text): Can you list the models that have been evaluated on the WMT2016 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the IWSLT2014 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the WMT2016 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset an orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Can you list the models that have been evaluated on the IWSLT2014 German-English dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset an orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The WMT2016 German-English dataset is a training dataset for the model-based orkgp:HAS_EVALUATION function. The WMT2016 German-English dataset was compiled using the English dataset from IWSLT2014. The"	False
"
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the Oxford 102 Flowers benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford 102 Flowers"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the Food-101 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DASET ?dataset. ?dataset rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	False
"
 input (English text): What is the best performing model benchmarking the RotoWire (Relation Generation) dataset in terms of count metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""count"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Name the datasets that have been used for benchmarking in the Relation Extraction research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Relation Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset RotoWire (Relation Generation)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""count"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp_lbl:HAS_DATASET ?dataset. ?cont orkgp_lbl:HAS_BENCHMARK ?benchmark; orkgp_lbl:P32 ?constraints ?constraints. } } ORDER BY DESC( ?value) LIMIT 1 } input (English text): Name the datasets that have been used for benchmarking in the Relation Extraction research problem ? output (Sparql query): input (English text): What is the best performing model benchmark in terms of count metric ? output (Sparql query): input (English text): Name the datasets that have been used for benchmarking in the Relation Extraction research problem ? output (Sparql query): input (English text): What is the best performing model benchmark in terms of counting metric ? output (Sparql query): "	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Reacher, easy (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the Reacher, easy (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 ------------------- Reacher (DMControl100k, paper) ------------------- DISTINCT: reacher.score DISTINCT: score.score Input (English text): If you have a question, do not hesitate to send it to the author using the contact information at the bottom of this page. Evaluation metrics for the Reacher, easy (DMControl100k) dataset There were 4 evaluation metrics that we used for our benchmarking: Score Metric Test/Score Score Metric Score Metric Test/Score Score Metric Test/Score Score Metric Score Metric Score Metric 0.4 10 8 8 80 0.7 10 6 6 80 0.8 9 5 7 80 5.4 6 5 4 80 10.2 10 3 4 75 2.10 6 5 3 50 5.7 4 4 4 70 50.8 9 4 3 40 2.5 4 3 3 20 Total 17.8 This was the dataset that we are comparing with in our comparison table. There are a number of different models to consider in this comparison, including Reacher, a simple Linear Regret for the Eigen_diag problem, SPSS 2000, Gaussian Kernel Estimation, Bayesian Estimation, Regression, and the Reacher (DMControl100k) model. Score metrics for the Reacher, easy (DMControl500k) dataset The scores of the test for each model are also listed as follows: Score Metric Test Score Metric 0.4 10 8 8 80 0.7 10 6 6 80 0.8 9 5 7 80 5.4 6 5 4 80 10.2 10 3 4 75 2.10 6 5 3 50 5.7 4 4 4 70 50.8 9 4	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Asterix dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Assault dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): Can you list the models that have been evaluated on the Atari 2600 Atari 5200 dataset ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the Atari 2600 Atari 400XL dataset ? input (English text): Can you list the models that have been evaluated on the Atari 2600 Atari 5020 dataset ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the Atari 2600 Atari 5500 dataset ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the Atari 2600 Atari 7800 dataset ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the Atari 2600 Atari 8800 dataset ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the Atari 2600 SuperCats dataset ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the Atari 2600 Atari 900XL dataset ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the Atari 2600 Atari FXR dataset ? output (Sparql query): 	False
"
 input (English text): Where can I find code references in papers that have used the DQN+SR model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DQN Best model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN Best"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DQNMMCe+SR model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQNMMCe+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQNMMCe+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Where can I find code references in papers that have used the DQN+CM+SR model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN+CM+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Where can I find code references in papers that have used the DQNF+CQ model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQNF+CQ"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont or"	False
"
 input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT a, { ?dataset a of kgc:Dataset} FROM rdfs GROUP BY a Please also be aware that the data of the benchmark are gathered by different means. Here, we have used the following metrics: Score : How many balls into each cup are the balls caught ? : How many balls into each cup are the balls caught ? Goal : How many times did the keeper reach the ground from the last ball in each bowl ? : How many times did the keeper reach the ground from the last ball in each bowl ? Accuracy : How many balls did the kicker hit the ball after he hit the last ball in each bowl ? : How many balls did the kicker hit the ball after he hit the last ball in each bowl ? Accuracy : How many times did the kicker hit the ball that was going towards the keeper, after he ran from the last ball in each bowl ? : How many times did the kicker hit the ball that was going towards the keeper, after he ran from the last ball in each bowl ? Time: How long was it till the ball was over the keeper in the 1st catch and in the 2nd catch ? It is a big data set and most of the data points are not easily comparable with each other. Nevertheless, most of the data are comparable with each other. A test on the most important metrics is, therefore, the analysis of the benchmark scores. Using the test in this blog post, we are able to establish some relationships that might help in understanding how the test works from the standpoint of the results (i.g., comparing the first catch time from different angles, to see whether the score depends on the angle, and whether the same result can be seen for any throw). The Benchmark To see how the benchmark works, the benchmark output is divided up into four columns: the main metric,	False
"
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT29 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT model_lbl. ?modelrdf_lbl LIMIT 1; That's it! That's all you need to test out F1 scores. If you are interested in exploring machine learning models or performing a variety of analysis that need to be done on the data in order to perform machine learning, you can refer to the list of resources. Now, if you are interested in what exactly is machine learning and how it can help us to solve this problem, I'd recommend reading through these other posts: So that's all about the first part of this blog entry. Hope that it was interesting!	False
"
 input (English text): List the code links in papers that use the MEMEN (ensemble) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the MEMEN  (single model) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the MEMEN (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 select model,code,name,categories from all_papers, all_papers_and_code from all_dissertations where source_code like :'MENTEN (single model)'and category in ('orkgc:Model ','MEMEN (one model)') input (English text): Please list the papers that have utilized the MEMEN (one model) model. output (Sparql query): select code, ( ?model_lbl. FILTER ( str ( ?model_lbl ) = 'MEMEN (one model)') ?benchmark. ?cont orkgp:HAS_DATASET., ?cont orkgp:HAS_MODEL. ) as orkgc, ( ?code. ) as rdfs, ( list ( ?model. ) as dataset, ' ?cont orkgp:HAS_DATASET.'as benchmark, list ( ?model_lbl. ) as category ) as code_links, ( ?) as title, ( ?) as title_links from all_dissertations, all_dissertations_and_code, all_dissertations_and_code_and_codepages on title and rdfs as rdfs, ( ?) as title_links, all_code, all_codepages and dataset as dataset in all_dissertations, all_dissertations_and_code_and_codepages where code in ( ?model. ) and category in ( ?language. ) and codepage. nc. in ('orkgc:Model ', ?language. ) and codepage. fc. in ( ?language. ) select code, ( ?model_	False
"
 input (English text): Provide a list of papers that have utilized the CAIT-M-36 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-M-36"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the CvT-13-NAS model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-13-NAS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the MMV TSM-50x2 model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMV TSM-50x2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model = ""MMV TSM-50x2""; rdfs:label ?model_lbl. { ?model_lbl = ""TSM-50x2"" ?model orkgc:Model ?dataset. RDFS :label ?dataset. } FILTER (str( ?dataset. ?model_lbl ) = ""VM:TTM-50x2"" ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the XGCN+3 model and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the XGCN+3 model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model = ""XGCN+3""; rdfs:label { ?model_lbl = ""XGCN+3"" ?model_lbl } FILTER (str( ?model_lbl ) = ""XGCN+3"" ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont"	False
"
 input (English text): Which model has achieved the highest Accuracy score on the Yelp-2 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Yelp-14 dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?metric rdfs:label rdfs_lbl. FILTER (str( rdfs_lbl) = ""Accuracy"" OR rdfs_lbl!= """") { SELECT ?model OR model_lbl. } ORDER BY DESC( ?metric_lbl) LIMIT 1 ?rdfs_lbl } input (English text): Which model performs the best on average on two different datasets ? output (Sparql query): Select a dataset name and its average as percentile score. output (Sparql query): select * from dbo.Yelp (query or model = 'Yelp-7' and 'Yelp-14' and 'Yelp-5' and 'Yelp') input (English text): What is the best performing model benchmarking the Yelp-14 dataset in terms of Accuracy metric ? output (Sparql query): input (English text): Name of the model used for the benchmarking. output (Sparql query): input (Chinese text): 共同期缓的亚达,應缓也通集一跟太浑,夥跟後跟後的识证可证。 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric rdfs:label rdfs_lbl. FILTER (str( rdfs_lbl) = ""Accuracy"" OR rdfs"	False
"
 input (English text): What is the best performing model benchmarking the WMT2016 English-Romanian dataset in terms of BLEU metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the IWSLT2015 German-English dataset in terms of BLEU score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 model : ( ?metric a orkgc:Metric; rdfs:label ?metric_lbl. ) RDFS : ( ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ) WMT : ( ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ) IWSLT : ( ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ) SIF : ( ?dataset a orkgc:Dataset; rdfs:label ?) ORK : ( ?dataset a orkgc:Dataset; rdfs:label ?) VST: ( ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ) DIF: ( ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ) BLAST : ( ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ) BLEU : ( ?metric a orkgc:Metric; rdfs:label ?metric_lbl. ) WMT: ( ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ) IWSLT : ( ?metric a orkgc:Metric; rdfs:label ?metric_lbl. ) SIF : ( ?metric a orkgc:Metric; rdfs:label ?) ORK	False
"
 input (English text): Provide a list of papers that have utilized the Sarsa-ε model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Sarsa-ε"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the SciBERT model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciBERT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the SAC model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Input: a simple model to model the state of the world Output: state of the world in a 3D system with variables Input: 3D object Output: position, angular momentum and angular velocity, and three-dimensional acceleration vector Input: 3D state Input: 3D state (3D state object) Output: current state that will be represented in the 3D world in the output object Input: 4D state (3D state object) Input: 4D state (4D state object) Output: current state that will be represented in the 4D world in the output object	False
"
 input (English text): Provide a list of papers that have utilized the MEMEN  (single model) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the MEMEN (ensemble) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the MEMEN model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str(( ?model_lbl) = ""MEMEN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the MEMEN model and include the reference files for the code and a sample benchmark test case that the benchmark performs ? output (Sparql query): (orkgp:HAS_DATASET [ ORGKIN_DATASET ] orkgp:HAS_SOURCE_CODE [ SOURCE_RDFS ] [ SOURCE_LDR ] ) input (English text): Provide a list of papers that have utilized the MEMEN (single model) model and include the reference files and code in test cases ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str(( ?model_lbl) = ""M"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM + dynamic eval model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM + dynamic eval"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the AWD-LSTM-MoS + ATOI model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-MoS + ATOI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Past Decode Reg. + AWD-LSTM-MoS + dyn. eval."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FAST_DECODE_REG"" ) ?benchmark ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?model; orkgp:HAS_SOURCE_HELPDATA ?code; orkgp:HAS_PAST_DECODE_RANDOM ?model; orkgp:HAS_PAST_DECODE_RANDOM ?model; orkgp:HAS_PAST_DECODE_RANDOM ?model; ORkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code; ORkgp:HAS_SOURCE_HELPDATA ?model; ORkgp:HAS_PAST_DECODE_RANDOM ?model; orkgp:HAS_PAST_DECODE_RANDOM ?model; orkgp:HAS_PAST_DECODE_RANDOM"	False
"
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Defender dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT modelname FROM ?model rdfs_dbl ) | 0 | ?model_lbl | RDD for the output of the following two queries: input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Atari 3000 dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 3000"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value ) LIMIT 1 } } input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Atari 500 dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?met"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset 200k Short Texts for Humor Detection?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Could you provide a list of models that have been tested on the 200k Short Texts for Humor Detection benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the 200k Short Texts for Humor Detection dataset in terms of F1-score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1-score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?model ?model_lbl ?model_lbl_f1 (VALUE(F1))  output (Sparql query): RDFS input (HTML) test: output (HTML) test: RDFS and F1-score Metrics: RDFS input (English text), F1 scores normalized :  RDFS_f1 = (LIM(RDFS_f1,8)) + (B1) + (B2) * 2 + (B3) + (B4) * 2 + (B5) + (B6) * -1  LIM(RDFS_f1,8) = 16.0 RDFS_metrics: RDFS_f1 = (LIM( RDFS_f1, 32) + 8 + B1  + 2*B2  + -1  + (BLANK(RDFS_f1) + 8) + B3  + B4  + 0   LIM( RDFS_f1, 32) = -0    RDFS_data: RDFS_data = RDFS_f1  RDFS_f1 = 8 RDFS and F1-score Metrics:  RDFS_f1 = (LIM( RDFS_f1, 7) + 3 + B2  + -1  + (BLANK(RDFS_f1) + 5) + B3  + B4  + 0   LIM( RDFS_f1, 7) = 7 RDFS_metrics: RDFS_f1 = (LIM( RDFS_f1, 20) + 8	False
"
 input (English text): Where can I find code references in papers that have used the Relation-Metric model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Relation-Metric"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the ETL-Span model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ETL-Span"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the STREET model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""STREET"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): What kinds of data do you have ? output (Sparql query): input (Unknown): What is the name of this dataset ? output (Sparql query): input (Unknown): Where did you get this dataset ? output (Sparql query): input (Unknown): What language is this dataset written in ? input (Unknown): What is the domain of the dataset ? output (Sparql query): input (Unknown): What is the database for this dataset ? output (Sparql query): data:data=&v=cffa7f2ea07a5cc5b5ffde0dfd4f8b9db08bdc938b3fa2f957e3ef5ba0c4b79f79dbd4c9&d=91498&b=&e=6&t=1&ll=1dbf-6b77,1520-2ac9,8c50-4980&bk=&l=&u=&v=3 input (Unknown): Name of your dataset ? output (Sparql query): input (Unknown): Where did you get this dataset ? output (Sparql query): input (Unknown): What kind of data do you have ? output (Sparql query): input (Unknown): Where did you get this dataset ? output (Sparql query): What, you need to change the data. input (Unknown): Name of your dataset ? output (Sparql query): What, you need to create a new data structure to hold the query. input (Unknown): Name of your dataset ? output (Sparql query): output (Sparql query	False
"
 input (English text): Can you list the models that have been evaluated on the WikiText-2 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the enwik8 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the enwiki8 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwiki8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): What models are being evaluated on the enwik8 dataset ? output (Sparql query): Inputs: English text: Can you list models that have been evaluated on the enwik8 dataset ? Outputs: The output was sent through a file upload. The list of models that generated the data could be updated if you want to add new models or perform performance evaluations.	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciTLDR dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seerc/datasets/SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 2.3.4.3.2.2 The Scipy Library The scimath-tldr library will be used extensively by Scipy, as well as by other Scipy software, such as scipy.mpl. A full list of relevant code can be found in the examples. 2.3.4.3.3 Scipy External Linking¶ Scipy supports scipy.mpl to link with external formats, such as the 'unfortunate' 'Scipy' packages (see https://github.com/scipy/scipy-mpl/) and the 'v3' compilers (see https://github.com/pv/nose/tree/master/source) for those compilers. Here, we link to the file SciTEa-2.0-b99a8ca4b5d.scml (available for SciTEa 2.0-b99a8ca4b5d, available for SPSS v.99-01, available for PSR v.99-01 and SPSS v.99-01)."	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Natural Questions benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the PubMedQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the CommonsenseQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?object_id ?object_id ?model_id ?nid ?record_id ORDER BY ?record_id DESC LIMIT 1 ?object_id ?object_id Now, the problem lies here how to query this data. For this purpose, we have a function called query_data_select_data. As the first line of code has said, query_data_select_data expects as parameter an object array like data from which we want to retrieve objects. The other line can also be changed to a query with the query_data_select_data_select, which is another function which accepts as argument, query_data_select_data, and returns the actual results. The reason behind this is that we have to pass as argument a database with which we will be querying (usually, the pylab). We will now use the query_data_select_data function to retrieve the results: SELECT COUNT(*) FROM query_data_select_data ORDER BY * DESC LIMIT 1; You can see that query_data_select_data expects as parameter the pylab for our query. If the pylab has already been fetched before, the function will take care of fetching it for you afterwards. When using this function, you must be aware that there is no need to query all of the data yet. To get the data, you have to select just the objects where the pylab holds them. In this case, all that we are doing is selecting just the 'objects' part of the pylab, the one for which query_data_select_data is running. What is a Metric ? Since we can't really say anything about the accuracy metrics, let's move to a different metric from the one we have been using in the previous chapter, and see	False
"
 input (English text): List the metrics that are used to evaluate models on the SQuAD2.0 dev benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the SQuAD1.1 dev dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 output (Sparql query) A new SQuAD2.0 model with a test model SQuAD2.0_test_1.0.squad3.pkl (benchmark) A new SQuAD2.0 model with a test model SQuAD2.0_test_2.0.squad2.pkl (benchmark) A new SQuAD2.0 model with a test model SQuAD2.0_test_3.0.squad2.pkl (benchmark) A new SQuAD2.0 model with a test model SQuAD2.0_test_4.0.squad2.pkl (benchmark) A new SQuAD2.0 model with a test model SQuAD2.0_test_5.0.squad2.pkl (benchmark) A new SQuAD2.0 model with a test model SQuAD2.0_test_6.0.squad2.pkl (benchmark) A SQuAD2.0 test of SQuAD2.0 benchmarking model: SQuAD2.0_test_7.0.squad3.pkl (benchmark) A SQuAD2.0 test of SQuAD2.0 benchmarking model: SQuAD2.0_test_8.0.squad3.pkl (benchmark) A SQuAD2.0 test of SQuAD2.0 benchmarking model: SQuAD2.0_test_9.0.squad3.pkl (benchmark) A SQuAD2.0 test of SQuad2.	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the WMT2016 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" So what exactly is the difference in benchmark versus validation ? The difference is that you can benchmark your model with a lot of different validation metrics at once if you have some sense of what goes well or fails, and what won't work is a bit harder. That's not to say that there is no difference in benchmarks, just that the evaluation metrics used are typically much less restrictive than in the validation metrics. In fact, what we see in the WMT2008 database is that different validation metrics are being used depending on your model architecture and the data set (i.e., the dataset). One of the examples I've seen would have two validation metrics: single-sample and multi-sample. For a model like the one in the previous lesson, these are the different validation metrics that it has available to it: single-sample = single-sample with a weight of exactly 1 multi-sample = multi-sample with a weight of less than 1 So I assume that in a model without a single-sample weight, you will usually have at least one or two validation metrics to choose from. One of the reasons I gave for the lower precision of the standard MetricBuilder is that I wasn't able to use it correctly until recently and could not get the tool to compile the models without some custom customizations to the data. When I wrote the ""How to implement complex models"" lesson, I talked about the challenge of how to use the model building functionality with the validation metrics. With these validation metrics, you are not limited to just one metric, even in models with no single-sample or multi-sample weights. For example, the model used in the previous lesson has two validation metrics: single-sample and multi-sample. So when you've reached the end of the example, here's How to implement complex models... I can't do any better than"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Information Extraction research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Information Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Word Embeddings research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Word Embeddings"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Reading Comprehension research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Reading Comprehension"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Reading Comprehension"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } The output of the query above is as follows. The output of my query outputted 2: 1 3:  Question: Does the query provide enough information ? 0:  Question: Question: Question: Question: The dataset is of the type ""Sparse Text"" 1:  Question: Question: Question: Question: Question: The file is located in the dataset_pairs folder 2:  Question: Question: Question: Question: Question: The dataset is of the type ""RDF-Sparse"" 3:  Question: Question: Question: Question: Question: The file contains a list of file-names ""dataset/gdata.rdf"", ""dataset/gdata.xsd"" and ""dataset/gdata.xml"". Answer I've provided this answer in one of the forums. It was the first and only answer provided for the query above but if you want to search the results of your search in the Knowledge Graph, please refer to the Knowledge Graph search functionality for your dataset. References : http://www.sciencen.com/documents/bible-reading-comprehension-database/data/cid/"	False
"
 input (English text): Indicate the model that performed best in terms of RE+ Macro F1  metric on the CoNLL04 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Macro F1 "") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest NER Micro F1 score on the ACE 2005 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NER Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 I've included the model and metric data for the ACE 2005 benchmark dataset that was used in the model comparison in the Model Comparison Report. The ACE 2005 Benchmark dataset consists of a dataset of 890 test data sets created with R2008a (R2008a uses an ARIMA model). The ACE 2005 benchmark dataset is a well established measure of RE4 microfibre synthesis performance, and is the benchmark used to determine how many RE4 protein complexes can safely be synthesized from one or fewer amino acids. The data sets were used by the ACE 2005 benchmark project to calculate the microfibre synthesis percentage of RE4 complexes that could feasibly be synthesized in the presence and in the absence of exogenous nitrogen. A key part of the ACE 2005 benchmark was to create new RE4 catalysts from a database of the 890 test data sets. With regard to the microfibre synthesis analysis, I've included the microfibre synthesis test data sets that were used in the analysis that will be presented later in the model comparison section. The ACE 2005 benchmark data set consists of 1289 protein complexes in the presence (NA) and absence (NA+) of a mixture of five (NA) and 15 (NA) nitrogens and 11 non-protein nitrogen (NPN) molecules. The test data sets are: The NA tests were created with a test data set created in Excel 2000. The NA test data is taken directly from the microfibre synthesis data and is not affected in any significant way by any reagents and pre-processing measures. All of the RE4 reactions use the same reaction conditions, so no pre-processing or reagent reactions can be identified as impacting on the results. Each of the RE4 tests is tested using 5 different reactions. Only the first reaction that performs the best is selected to be used in the model comparison	False
"
 input (English text): Can you provide links to code used in papers that benchmark the XLNet model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the XLNet (Large) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (Large)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the XLNet (base) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (base)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT * FROM ( SELECT ?pageid, ?title, ?paper, ?pub_id, ?authors, ?labels, ?datasets, ?linkcount FROM ( SELECT ?pageid, ?title, ?paper, ?pub_id, ?authors, ?labels, ?datasets, ?linkcount FROM papers ( ?pageid) s ) a WHERE m.id = 1 GROUP BY ?pageid); input (English text): Select the paper IDs from the XLNet (base) paper library ? output (Sparql query): SELECT COUNT(*) FROM ( SELECT ?pageid, ?title, ?pub_id, ?authors, ?labels, ?datasets, ?linkcount FROM papers ( ?pageid) a GROUP BY ?published_date | ?pages ) a WHERE s.id = 1 UNION SELECT ?published_date | ?pages AS published_date | ?pub_id AS pub_id, ?authors AS authors from papers ( ?pageid) a where ?published_date = 2011-03-21; input (English text): Show the link counts in the XLNet (base) paper library ? output (Sparql query): SELECT COUNT(*) FROM ( SELECT ?published_date, ?pub_date, ?authors, ?labels, ?datasets, ?linkcount FROM papers ( ?pageid) a GROUP BY ?published_date | ?published_timestamp ) ( ?published_date) AS published_date, ?authors AS authors, ?labels AS labels FROM journals ( ?published_date) AS a WHERE :published_date = 2011-03-21 AND authors = ?authors; input (English text): Show the link counts in the XLNet (base) paper library ? output (	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Paper Field dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Open Entity dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Open Entity"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Ohsumed dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ohsumed"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?sheet ?sheet_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ( ?dataset_lbl). FILTER ( ?dataset_lbl. } input (English text): List the title and ID of research papers that contain a benchmark over the OE dataset ? output (Sparql query): SELECT DISTINCT ?sheet ?sheet_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ( ?dataset_lbl). FilTER ( ?dataset_lbl. } output (Sparql query): SELECT DISTINCT ?sheet ?sheet_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ( ?dataset_lbl). FILTER ( ?dataset_lbl. } output (Sparql query): SELECT DICOMPTAIL ?page ?page_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ( ?dataset_lbl). ?page orkgp:P31 ?page. } input (English): List the title and ID of research papers that contain a benchmark over the OE dataset ? output (Sparql query): SELECT DISTINCT ?sheet ?sheet_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ( ?dataset_lbl). ?page orkgp:P31 ?page. } (Sorted by title: the first value is paperID) input (English text): List the title and ID of research papers that	False
"
 input (English text): Can you list the models that have been evaluated on the NYT24 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the NYT dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the NYT29 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model origr:model. ?model rdfs:label ?model_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION origr:model. ?model origr:model. ?model origr:model. OPTIONAL { ?model origr:model. ?model origr:model. } output (Sparql query): SELECT ?model origr:model. ?model origr:model. ?model origr:model. ORJKDYZJKDVKV ?dataset; andkgp:HAS_AVG ?model_lbl. ?model origr:model. OPTIONAL { ?model origr:model. ?model origr:model. } output (Sparql query): SELECT ?model origr:model. ?model origr:model. ?model origr:model. ?model origr:model. ORMKPJN KJLZJVV ?dataset; OrigRNN ?model_l"	False
"
 input (English text): List the metrics that are used to evaluate models on the Dataset mentions in Social Sciences benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dataset mentions in Social Sciences"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the ScienceIE dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which are 3 the most common variables for the atmosphere models?
 output (Sparql query): "	"SELECT ?variables WHERE { orkgr:R68871 orkgp:compareContribution ?contrib. ?contrib orkgp:P7144 ?models. ?models rdfs:label ?models_labels. FILTER(REGEX( ?models_labels, ""tmospher"")) ?models orkgp:P26032 ?variables. } ORDER BY DESC(COUNT( ?variables)) LIMIT 3"	 As I had some problem getting the output, I tried it with just the model name and its corresponding column in the data or in the index of the Dataset mention (if one was present or not). In practice I think the output is somewhat easier to understand than what I could get with the names. I tried it too with the dataset name and the corresponding column, but on the other hand, the latter got much longer if I used it without the model name. I think that the difference between the two, is just because I was able to make a better guess than on the other options, so it worked out. I noticed that there is an index that is not available at the time of writing on the Dataset info page. What I tried was to create a new index for it (using MigrateIndex ) and I noticed that all the functions with that index name are closed when there are no parameters that need to be passed to the underlying models. It wasn't the case with the Dataset or the Model name (i.e. not a null in the database or the model name or the Dataset). My conclusion is that you should create such an index for the dataset and the Model name before you use it. Here is the summary of how I used the parameters. Model name (i.e. the name of the model) parameters 1. Parameters parameters 1. Parameters 2. Parameters 3. Parameter (a) Parameter (b) Parameter (c) Parameter (d) Parameter (e) Parameter (f). Parameters (a) - 1 1 1 2 1 2 1 1 3 1 2 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 2 1 2 1 2 1 2 1 1 1 3 Parameters (b) - 1 1 2 1 2 1 3	False
"
 input (English text): List the code links in papers that use the DDQN (tuned) hs model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN (tuned) hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DQN noop model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DDQN-PC model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN-PC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 For a complete list of all examples and their output, see https://github.com/dcj2/db_model_benchmarks_examples. References	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Text8 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Text8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the MRPC dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MRPC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TREC-6"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TREC-6"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): List the title and ID of research papers that contain a benchmark over the VITR dataset ? output (Sparql query): SELECT DISTINCT ?paper orkml:TermCount( ?paper_lbl. ?lab orkgc:TermCount( ?paper_lbl. ?lab_lbl) ) ?label. ?term orkgc:TermCount( ?paper_lbl. ?lab_lbl ). FILTER (str( ?lab orkgc:TermCount( ?paper_lbl. ?lab2 orkgc:TermCount( ? Paper_Lb_lbl. ?lab3. ?lab4 ) ) = ""VITR"" ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label orkgp_lbl. } Input dataset: (Text8 dataset of 6 titles, sorted by paper type) Input RDF: Title Name Author Title ID Notes VITR Paper 1 Paper 2 Paper 3 Papers 1 and 2 are not used"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Recurrent Rational DQN Average model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Recurrent Rational DQN Average"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DQN Best model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN Best"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Rational DQN Average model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rational DQN Average"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ST_RationalDQN_PERC_HAS_RDD_RADICED_DQN_PERC, DIFFERENCE(RDD), SAME_RADICATION(RDD, DQN_VALUE(RTX,DQN)) WHERE ?st_rand_dist(RT) > 0 ?rdd.models.RDD ?c.model OR DIFFERENCE ( ?c.model OR C.value(RTX, DQN)) ?rdd.models.RT.RDD input (English text): Which algorithm produces the most reliable estimates of the state of the model ? output (Sparql query): SELECT ST_RationalDQN_PERC ?rdd_dist and MAX(max( ?rdd_dist,RTX) ) FROM ( SELECT RDD::RDD_VALUE(RTX,DQN) FROM ST_RationalDQN_PERC AND ST_RationalDQN; ST_RationalDQN_PERC ST_RationalDQN_PERC ST_RationalDQN_PERC ST_RationalDQN ST_RationalDQN_PERC ST_RationalDQN_PERC ST_RationalDQN_PERC ST_RationalDQN_PERC ST_RationalDQN ST_RationalDQN_PERC ST_RationalDQN_PERC ST_RationalDQN ' input (English text): Is there a particular dataset in the literature that produces the most reliable estimates of both the DQN and the state of the model ? output (Sparql query): SELECT DISTINCT ?rdd_dist	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-Spanish dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Spanish"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Accuracy score on the MLDoc Zero-Shot English-to-Japanese benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Japanese"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-German dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl)"	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the 20NEWS benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""20NEWS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Penn Treebank benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""1-of-100 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PolyAI Reddit"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?1 ?unit AS ?unit_lbl, ?2 ?unit AS ?units2 FROM ?""dataset"", ""1-of-100 Accuracy metric (orkgc:PolyAI)"", ""20NEWS score"" ORG_HAS_DATASET ORG_EVALUATION ORG_METRIC ORG_BENCHMARK ORG_MODEL, ?model, ""PolyAI score"", ?model_lbl, ""PolyAI metric"", ?metric, ""20NEWS score"" ORG_HAS_ORG_STATIC ORG_MARKET ORG_NONSTATIC AND ORG_MARKET_DATE WITH (INIT_DATE_FETCH_FACTOR(ORG_DATE) = DATE(INIT_DATE)) as %s FROM < ?""data_sets"" > WHERE 1= ? AND 2= ? GROUP BY ORG_DATE ORG_METRIC AND (AND ORG_HAS_DATASET(ORG_DATE) AND COUNT(*) > 50 AND DESC(NOT(AND(DATE(ORG_DATE) < ? AND COUNT(*) = ?))) AND DESC( ?dataset_lbl = ?dataset_lbl), ?metric_lbl = ?metric_lbl, ?dataset_lbl = ?dataset_lbl, ?metric = ?metric_lbl, ?dataset_lbl = ?metric_lbl, ?metric_lbl = ?metric_lbl, ?metric_lbl = ?metric_lbl ) ) input (English text): Indicate the model that"	False
"
 input (English text): List the code links in papers that use the Linear Transformer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Linear Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Transformer-XL Large model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Long Short Transformer model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Long Short Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Long Short Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Input Input (English text): List the code links in papers that use the Linear Transformer model in any benchmark ? output (S parql query): SELECT DISTINCT ?item output (S parql query): SELECT DISTINCT ?item input (English text): List the code links in papers that use the Long Short Transformer model in any benchmark ? output (Sparql query): SELECT DISTINCT ?item input (English text): List the code links in papers that use the Transformer-XX Large model in any benchmark ? output (Sparql query): SELECT DISTINCT ?item input (English text): List the code links in papers that use the Linear-Transformer model in any benchmark ? output (Sparql query): SELECT DISTINCT ?item input (English text): List the code links in papers that use the Large Transformer model in any benchmark ? output (Sparql query): SELECT DISTINCT ?item input (English text): List the code links in papers that use the Transformer-XX Large model in any benchmark ? output (Sparql query): SELECT DISTINCT ?item input (English text): List the code links in papers that use the Long Short Transformer"	False
"
 input (English text): What is the name of the top performing model in terms of EM score when benchmarked on the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""EM"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the WebQuestions dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the TriviaQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" 4.2.8.6.2 We already have the data for the Trivia QA dataset and we want to benchmark other models against it, specifically, against three similar datasets. The questions that we're benchmarking are ""5 questions that contain an asterisk, and two questions that contain an asterisk and some punctuation"". Here's a summary of the question, score and results so far. All results will be updated here so they are accurate: ""How many different nouns have a common first letter ? The answer is ""Five"". ""What is the first word in a noun that contains at least one syllable ? ""Lion"". ""Which is more common in this sentence ?"" ""Lion is more frequent than lion."" ""Which of these words makes a noun an adjective ?"" ""Lion is an adjective. Or, in other words, it is an adjective of lion."" We had to use the exact plural form in order to avoid typos due to how it is defined in S2 Table.      2.8.2 The benchmark used in this section should be performed on the first question. For all other questions, benchmarking should take place against the first or a set and follow the criteria shown in this table.      4.3.2 The QA dataset we are benchmarking against consists of 673 questions. The question set size of ~700 is adequate. However, we are specifically seeking to minimize the time it takes to create an object (using DataFrame, RDFS, or RDataFrame) from the raw data and perform an evaluation. This is important to maintain both performance and scalability. To do so, we must benchmark this on an actual question question from the Trivia Question API (https://www.reddit.com/r/mysteryworlds/comments/3zm0a8/trivago_api"	False
"
 input (English text): Which methods are used?
 output (Sparql query): SELECT DISTINCT ?methods, ?methods_labels WHERE { orkgr:R110361 orkgp:compareContribution ?cont. ?cont orkgp:HAS_METHOD ?methods. ?methods rdfs:label ?methods_labels. } 
 input (English text): What are the research problems Vernier Effect is related to?
 output (Sparql query): SELECT DISTINCT ?problems, ?problems_labels WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, ""Vernier Effect"", ""i"")) ?papers orkgp:P31 ?contrib. ?contrib orkgp:P32 ?problems. ?problems rdfs:label ?problems_labels. } 
 input (English text): What is Raman spectroscopy?
 output (Sparql query): "	"SELECT ?definitions WHERE { ?terms orkgp:P24009 ?definitions. ?terms rdfs:label ?terms_labels. FILTER(REGEX( ?terms_labels, ""Raman spectroscopy"")) } "	SELECT DISTINCT ?type, ?type_labels, ?term_labels, ?term_label AS ?label, ?term AS 1/1 + 1 AS ?term OFFSET 1.1, ?label as term_labels, ?term AS 1 AS ?term OFFSET1.1, ?term AS 2 AS ?term OFFSET2.1, ?labels AS R110361 AND ?label AS p51; Input: What is the data collection ? Output: What is the results ? input (Japanese text): Which methods are used ? output (Sparql query):SELECT DISTINCT ?methods, ?methods_labels WHERE { orkgr:R110361 orkgp:compareContribution ?cont. ?cont orkgp:HAS_METHOD ?methods. ?methods rdfs:label ?methods_labels. } If you would like to see the output of your query, please use the results.xls file.	False
"
 input (English text): What is the best performing model benchmarking the DTD dataset in terms of PARAMS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DTD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset TDMSci?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDMSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the DTD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DTD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT TOP(1) ?rdf_metric  FROM rdfs_dtds.rdfs_tsds_dataset ; SELECT TOP (1) ?metric  FROM rdfs_dtds.rdfs_metrics_tsds_dataset LIMIT { ?rdf_metric   = ?metric. ?metric rdfs:labels  = ?dataset. ?dataset rdfs:is_valid  = ?dataset. ?dataset orkgp:HAS_DATASET  = ?dataset. ?dataset if  ORDER BY ?""dataset_lbl"" LIMIT 1} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160   0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53"	False
"
 input (English text): What is the top benchmark score and its metric on the CIFAR-10 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the CIFAR-100 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the CINIC-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?value ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): What is the top benchmark score and its metric on the CINIC-100 dataset ? output (Sparql query): SELECT ?value ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the SemEval-2021 Task 11 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2021 Task 11"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SemEval-2010 Task 8 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2010 Task 8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SemEval-2018 Task 7 dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2018 Task 7 dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?dataset ?benchmark p. ID AS target_id, ?benchmark p. score AS target_score, ?benchmark p. Nb_est_score AS Nb_est_score, ?target_id ?paper, ?target_score ?benchmark, ?target_id ?paper_lbl. ?quantp orkgp:DASET AS quantp, ?quantp:HAS_DELIVERY orkgp:NUMBER AS quantp, ?keyword: ? AS keyword, ?data ? as data_data ?data_target (optional): Target ID or Paper Id. input (English text): List the title and ID of a research paper that includes a benchmark with a score greater than 0 ? output (Sparql query): SELECT ?benchmark ?target ?target_id WHERE { ?dataset a orkgc:Dataset; rdf	False
"
 input (English text): List the metrics that are used to evaluate models on the COPA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""COPA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the AG News benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AG News"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 input (English text): List the metrics that are used to evaluate models on the Naver benchmark dataset ? output (Sparql query): input (English text): List the metrics that are used to evaluate models on the Alibaba benchmark dataset ? output (Sparql query): input (English text): List the metrics that are used to evaluate models on the Kaggle benchmark dataset ? output (Sparql query): input (English text): List the metrics that are used to evaluate models on the OpenAI Lab benchmark dataset ? output (Sparql query): input (English text): List the metrics that are used to evaluate models on the PyMC3 benchmark dataset ? output (Sparql query): input (English text): List the metrics that are used to evaluate models on the SKILLS Benchmark dataset ? output (Sparql query): input (English text): List the metrics that are used to evaluate models on the Google Open Source Machine Learning datasets ? output (Sparql query): Input text: Data: All metrics measured using the benchmarks. Input dataset: List the most important AWS metrics used to evaluate models: COPA AWS Metrics Report input (AUTH_USER), output: (AWS user): This user is an eligible AWS subscriber with the appropriate authorization. If this user is not an eligible AWS subscriber, the value of this attribute cannot be used in the expression. This attribute might include other values like the type of access or the type of request. This attribute cannot be used in the expression. output (AUTH_PASSWORD), output: (AWS password): This password is the only information you will need for this parameter. This can be used if this parameter is not set via the AWS Management Console. If this parameter	False
"
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NCBI-disease dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the BC5CDR-chemical dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-chemical"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 As can be seen here, both of these queries are pretty similar, but are running at different points in the query generation process. This is because different databases have different performance properties and you'll have to choose which one suits your application needs. Which Database is Best for Genomics ? What is most important for us during the genomics process ? In order to determine which database to use for our analysis we need to understand what a genome is and what that means for our particular type of analysis we're doing. For this one we're going to have to look at the following parameters: Genome (Nucleotide or Metarossa): Your organism's genome sequence. Genome Size: The length in bases of the genome. Population Size (Mating Unit): The size of a population, or the number of individuals, within the genome. Genotype (Polymorphism): A set of physical or molecular differences between individuals that may change a person's ability to perform an activity. We can view the genome using the GENOME toolbox: From this we also get several other parameters: Locus (Mapping Group): The base-level genetic region of the genome. Mutation Number (MZ): The number of individuals within a MZ that is subject to a single copy of any variant. DNA Polymorphism (DN): The total number of genetic variants present in the genome. Disease Group (DZ): The number of diseases that are the sole product of a single mutation. We can view the genotype using the GENOUS toolbox: We also get a number of other variables useful for making our analysis more precise and accurate: Population Size (Mating Units): The number of individuals that is subject to any variant. GEO: Position in Geographic Coordinates where the population is located. Loc	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NCBI Disease dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Supervised: dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 select ?dataset_lbl as ds_lbl, ?benchmark to produce a summary output (Slightly more readable as this) A list of the dataset names will automatically be loaded from the superdata URL into your SQL or JSON editor. The data should be a structured form suitable for inclusion into a standard database like MariaDB. If not, you can use the data to run some benchmarks. Just be aware this will generate lots of duplicate dataset entries as the benchmarks will be ordered based on the length of the dataset, and there are more and more datasets going live! The result should be a list of results, each of which follows the same ordering as the last. For example: print list(DISTINCT $l1.ID, DISTINCT $l2.ID, DISTINCT $l3.ID, DISTINCT $l4.ID) This will print the list of all datasets (including ones that I have benchmarked), sorted by ID. The same listing can be produced by this syntax instead. There is also the opportunity to use other formats, such as CSV, XML, SQL or HTML. I am using MariaDB 6.7, version 6.7 (RDBMS), as the database. You can learn more in this blog about MariaDB. With SQLExperts, and SQL queries: #!/usr/bin/env perl use strict; use warnings; use IO::Handle; use Data::Dumper qw/ [DISTINCT ID]/; my @r0_1; my @r0_2; my @r0_3; my @r0_4; my @r0_5; my $first; my $last; my $idx1; my $idx2; my $idx3	False
"
 input (English text): Can you provide links to code used in papers that benchmark the GCNN-8 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GCNN-8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the GGCNN model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GGCNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the GCN Hybrid model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GCN Hybrid"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 For the benchmarking papers, the links to the code for the paper can be found in the following output. In case you find errors or are experiencing an issue, please leave a comment on the repo. The most common issue is that GCP is not able to download the repo. If you can't download GCP from the GCP server, please follow the instructions below to fix it. Once solved, you can then add the repository, or run a full git checkout from the command line in order to check out the entire project. The GCP repository is on GitHub. The repository's url is https://github.com/GoogleChained/GCP. GCP is a commercial project and only the developers of GCP can download and run the repository. If you're using the service in your own projects, please feel free to fork this repository and add the code to your project if you're making use of or for you personally. This repository is part of the code of Google Chained and you are responsible for the modifications to it to the code of Google Chained. Note that we are planning on releasing GCP from the Google Cloud Platform soon. The first release candidate for the new release is now available which is linked below. Be sure to run the following commands from the terminal if you wish to run the production build. If you experience any issues, please report them on the github issue tracker. This is the easiest way to fix issues with GCP. For more information about running GCP on Google Cloud Platform, see Running GCP on Google Cloud Platform Getting started with GCP This repository contains the source for this sample article which we created because it is such an extremely complex topic. As such, we try to keep everything fairly simple and to just show the results of a single benchmark using the GGCN implementation. The code is in both GitHub and the Google	False
"
 input (English text): Could you provide a list of models that have been tested on the ADE Corpus benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ADE Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of benchmarked datasets related to the Constituency Parsing research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Constituency Parsing"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What data format does CHEMDNER corpus have?
 output (Sparql query): "	SELECT ?data_format, ?data_format_label WHERE { orkgr:R150596 orkgp:P31 ?contrib. ?contrib orkgp:P42011 ?data_format. ?data_format rdfs:label ?data_format_label. } 	SELECT CONCATENATE ?prog_or_dataset ?prog_or_dataset_lbl WHERE { ?prog_or_dataset orkgc:ProgsRepo; rdfs:prop ?prog_or_dataset_lbl. ORGANIZATIONS ?prog_or_dataset. } orkgc:ProgsRepo orkgc_Labs:Labs ?prog_or_dataset_lbl. LABREP ORGANIZATIONS id:12345 ?prog_or_dataset_lbl. id:1234 ?prog_or_dataset_lbl. id:1234 ?prog_or_dataset_lbl. id:1234. ID id :1241 ?prog_or_dataset_lbl. id:1241. id id :1241 id id :1241. output (Sparql query): input (English text): Which tools have you used to make these models ? output (Sparql query): In the examples below, the query from one dataset can be used to parse the next dataset. The first table is the CHEMDNER Dataset and the second table is the first two columns of the data set from the sample data file. CHEMDNER Dataset and Dataset Description :dataset_lbl:<LINK <LINKED DATA ORGANIZATION> To list all or part of a link to a dataset: 1. Enter the link with the following fields: LINKED TO <LINKED DATA ORGANIZATION> :dataset_lbl:<L	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the CUB-200-2011 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the CUB-200-2011 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?benchmark || ?value || ?metric || ?score || ?scores || ?dataset || ?compile || ?extract || ?filter || ?sort || ?intersect|| ?intersection || ?mathematics|| ?metrics|| ?lab || ?bibliography || ?sender || ?dataset || ?cont || ?benchmark output (Sparql query): SUM( ?dataset) || ?score || ?metric || ?value || ?score|| ?metric rdfs:label || ?metric_lbl. To view the results in one table, save the result set in SQL, then execute the following query: SELECT { ?comparison: ?benchmark || ?metric || ?value || ?score || ?metric rdfs:label || ?metric_lbl., ?scores || ?data., ?extract || ?extract_lbl., ?filter || ?filter_lbl., ?sort || ?sort_lbl. ); ?dataset. ?value rdfs:label OR rdfs:lab || rdfs:mathematics OR rdfs:metrics OR rdfs:labr ?extract OR rdfs:extract_lbl., ?filter || rdfs:filter_lbl., ?sort || rdfs:sort_lbl., ?intersection OR rdfs:intersection_lbl., ?mathematics| OR rdfs:mathematics-lbl., ?mathematics|| OR rdfs:mathematics-lbl., ?data|| rdfs:data||	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ning et al."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the DocTAET-TDM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DocTAET-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the SEE model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SEE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" In general, the most challenging thing with large-scale, real-world datasets like this is that we are faced with a high ratio of input to output queries. The number of links between models can get as high up as 10 on many datasets. It is certainly possible to generate a model that is efficient and robust in this scenario. In our paper ""A model of network topology"", we proposed a novel approach for making queries efficiently on such high-dimensional datasets by running the models in parallel. Our approach works because we have used the network topology to define the input and the output queries. The model is then used to output multiple, parallelized queries that then can be concatenated into one query. The resulting query is then evaluated in many parallel evaluations of our model, with the results processed in parallel by different systems. Each parallel evaluation includes a query step that involves performing some kind of a statistical calculation and finally evaluating the query result. This is not as efficient as parallelizing our model directly in a program, but it is a lot cheaper and it works nicely. As far as the rest of the question, let's discuss what you say is the source of the performance issues. The source of the performance issues are two different approaches that were recommended by the expert reviewers for this paper. First, the NCCL authors recommended the use of RDBMS (specifically, IBM RDBMS) as the primary database. Some of the reviewers also suggested that a traditional, relational-driven DBMS, like MySQL or PostgreSQL, should be used for the rest of the models. One of the biggest problems with the use of RDBMS are that they have a long-known lack of performance when compared to SQL. These problems, combined with the lack of open-source support and community adoption, meant the original RDBMS models were not well-suited for the task. The"	False
"
 input (English text): Provide a list of papers that have utilized the Prior hs model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Prior+Duel hs model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior+Duel hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Duel hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Duel hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?solution.c as ?c or ?solution_c as ?c or ?solution_s as ?c. input (English text): What is the probability of a result which seems to be very weak in a probabilistic model using Duel hs model ? output (Sparql query): SELECT COUNT(*) AS ?part FROM ?solution ORDER BY ?part; input (English text): Explain how to compute probability using Duel hs model ? output (Sparql query): SELECT ?solution. ?p ?p_p || 0, ?part, ?p_1, ?p_2, ?p_3, ?p_4, ?p_5, ?p_6, ?p_7, ?p_8, ?p_9, ?p_10, ?p_11, ?p_12, ?p_13, ?p_14, ?p_15, ?p_16, ?p_17, ?p_18, ?p_19, ?p_20, ?p_21 FROM ?part GROUP BY ?part Input: output (Sparse table) type: int : probability of the outcome output (Sparql query) input (English text) : Explain how the probability of a result varies according the previous result ? output (Sparql query) type : int : proportion of previous result as prior for this current result output (Sparql query) input (English text) : How does the proportion of previous result change between current and previous	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset OntoNotes?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Human-annotated)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str ( ?dataset_lbl) = ""DocRED"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl This benchmark can be taken a bit further. Let's take a look at how we can build these benchmarks from other datasets. Here comes a super short example, again from the HASHTREE project - the first two steps are all the source files in RDFS format. Note that the dataset used is ontosnotes. We can then choose the dataset we're building on the next step. #2: Compute a weighted average of the top 10 ranked datasets We can easily perform the same benchmark as we did before... input (English text):... but with a nice extra twist - we can now combine the data with the weight parameter in order to get a better score. Note that a non-"	False
"
 input (English text): Provide a list of papers that have utilized the Adaptive Input Very Large model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Adaptive Input Very Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Transformer (Adaptive inputs) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer (Adaptive inputs)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Adaptive Input Large model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Adaptive Input Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Input Large Numerical Recipes (LNP)	False
"
 input (English text): What models are being evaluated on the Atari-57 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari-57"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Atari 2600 Solaris dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Solaris"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): Please list the models that have been evaluated on the Atari-64 dataset. output (Sparql query): input (English text): Can you list the models that have been evaluated on the Atari-65 dataset ? output (Sparql query): input (English text): Please list the models that have been evaluated on the Atari-67 dataset ? output (Sparql query): input (English text): Please list the models that have been evaluated on the Atari-57 dataset ? output (Sparql query): input (English text): Please list the models that have been evaluated on the Atari-57 dataset ? output (Sparql query): input (English text): I do not believe I can list any models on the Atari-57 dataset. It's really not possible. output (Sparql query): Inputs: input (English text): What languages are being evaluated on the Atari-57 dataset output (Sparql query): input (English text): What languages are being evaluated on the Atari-57 dataset ? output (Sparql query): input (English text): I would like to be able to display all the models with the selected criterion. output (Sparql query): Inputs: input (English text): What languages are being evaluated on the Atari-57 dataset ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the Atari-57 dataset ? output (Sparql query): input (English text): Please list the languages that have been evaluated on the Atari-57 dataset ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the Atari-64 dataset if	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MultiNLI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the QNLI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ORG_DATE (datasetset. or_date_lbls. orkgc:dataset_dataset) ?benchmark} { ?cont orkgp:HAS_DATASET ?dataset_lbl. :cont:HAS_BENCHMARK; } As the user has already passed in this query (and the response has already been stored in RDFS), my current query looks like this: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ORG_DATE (datasetset. or_date_lbls. orkgc:dataset_dataset) ?benchmark} ORG_DATE (d.datasext. orkgc:dataset_dataset. and d.datasext_dator. orkgp:dataset_dataset. ) ?benchmark; Now, just to give a couple of details, I am currently using the PostGIS API, a Java library, to query into RDFS, to construct my query. As the PostGIS query I am using is not a query on the PostGIS data but on an RDFS stream, a trivial solution would be to use the Java library and create a query with the ""dumper"" command, that is what I have been doing. However, the problem is that my ""dumper"" command is"	False
"
 input (English text): Where can I find code references in papers that have used the Large mLSTM +emb +WN +VD model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Large mLSTM +emb +WN +VD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the MMDL model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMDL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the MMV model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMV"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?code_reference AND ?code_reference_str || ?dataset. ?db orkgp:HAS_DB || -------------------------., ?db ORM. Input: Code (for C). The C code reference is the C code for the machine with the given number of processors. output (Sparql query): SELECT ?code_reference AND ?c_reference FROM ?c_code_reference WHERE ?c_reference_str = ""MMV"" and ?c_reference = '' ?dataset. input (English text): The C code reference is the C code for the machine with the given number of cores. output (Sparql query): SELECT ?code_reference AND ?c_reference FROM ?c_code_reference WHERE ?c_reference_str = ""MMV"" and ?c_reference = '' ?dataset Input: C (for C). The C code reference is the C code for the machine with the given number of processors. output (Sparql query): SELECT ?code_reference AND ?c_reference FROM ?c_code_reference WHERE ?c_reference_str = ""MMV"" and ?c_reference = '' ?dataset Input: A (for A). The A code reference is the A code for the machine with the given number of cores. output (Sparql query): SELECT ?code_reference AND ?c_reference FROM ?c_code_reference WHERE ?c_reference_str = ""MMV"" and ?c_reference = '' ?dataset Input: C (for C). The C code reference is the C code for the machine with the given number of cores (i.e. A (C) will have two codes which refer to the two processes). output (S"	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Centipede"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT MAX( ?val) FROM { ?dataset a} GROUP BY ?metric ?metric_lbl ORDER BY ( ?size ) | MIN( ?value) | MAX( ?value) | PREDICT( ?result).  || ?name ?dataset_lbl OR CASE WHEN ?dataset_lbl | CASE WHEN ?size < 3 || ?size > 33 || ?size >= 33 OR ?size > 50 ORDER BY DESC( ?result )	False
"
 input (English text): Could you provide a list of models that have been tested on the Sequential MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the MNIST dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT option for querying all models available on that resource. You can use this SELECT option to determine the minimum number of models in use, or the minimum number of metrics that were used in the query to check for the criteria. It is expected that the built-in query builder will have the option to filter the search based on the minimum number of models and the minimum number of metrics	False
"
 input (English text): In what country was conducted research with the largest number of participants?
 output (Sparql query): SELECT ?country WHERE { orkgr:R44980 orkgp:compareContribution ?contrib. ?contrib orkgp:P15249 ?country; orkgp:P23169 ?number_of_participants. } ORDER BY DESC( ?number_of_participants) LIMIT 1
 input (English text): Which countries are considered in the papers about geopolitics?
 output (Sparql query): SELECT DISTINCT ?location WHERE { ?_ orkgp:compareContribution [ orkgp:P32 [ rdfs:label ?label ]; orkgp:P5049 ?location ] FILTER(REGEX(STR( ?label), ""geopoli ?tics"")) } 
 input (English text): Where did the study with maximal geographic scale take place?
 output (Sparql query): "	SELECT ?location, ?location_label WHERE { { SELECT (MAX( ?geo_scale) AS ?max_geo_scale) WHERE { orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale. } } orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale; orkgp:P37524 ?location. ?location rdfs:label ?location_label. FILTER( ?geo_scale = ?max_geo_scale) } 	 (select country from geo_layers and geo location from geo_layers and country.description, country.description, country.duration in map_duration ) A quick example of this query In the top ten countries (see graph), the three largest, China, France, and United States account for nearly half of the total data. Each country represents about 1% of the study sample. On both sides of the pond, the study population is split a bit more evenly. China, France, and United States account for about 3% of the population, while the United Kingdom is a distant second. One important thing to note is that while the three largest countries are included in the above query on their own, the second-largest country (Germany) is not. Thus, the second-largest country is not part of the larger group on the graph, but its representation is just one of a handful of countries in the top 10. What this means is that if you look in the top twenty the proportion of each of these countries as part of the sample is approximately half of the total population. If you add all the data back in you get something that looks like this The top countries account for approximately half of all the total population The next question is why the countries are included. The answer to that is that the study is not a random sample because geography, for as we already said, helps to determine which countries are important. The first few countries, in the beginning of the study, in the case of China, France, and the United States, were geographically marginal. The other countries that are more significant in the future will likely move up the list. The result of the above query is that the top countries are also the countries that we know will have the greatest global impact in terms of what we're planning to talk about in these papers. Let's go through the	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WLPC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WLPC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WPU data set at the UC Davis/MNS Data Lab ? output (Sparql query): input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WSU database ? output (Sparql query): input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WUI site ? output (Sparql query): input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WWIDF/WFO site ? output (Sparql query): input (Danish text): Provide a list of research papers with references (all citations from Springer/Elsevier journals) of research articles published from 2004 to 2015 ? output (Stored results with date-time format) output (Mixed results with date-time format) input (Danish text): Provide a detailed list of your research publications, and their titles ? output (Stored results with date-time format) output (Mixed results with date-time format) input (Danish text): Provide a list of your research publications and their abstracts ? output (Stored results with date-time format) output (Mixed results with date-time format) input (English text): Provide a list of your research publications and their citations ? output (Stored results with date-time format) output (Mixed results with date-time format) input (English text): Provide a list of your publications and their abstracts in JPE ? output (Stored results with date-time format) output (Mixed results with date-time format) 	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the TSE-NER dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a:dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the TIMES-3 dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a:dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Times-3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the University of California Drought Monitor dataset ? output (Sparql query): SELECT DISTINCT ?paper ORGGLOBAL, DISTINCT ?benchmark, ?cont, ?paper_lbl, ORGGLOBAL, ?paper_lbl_lbl1 ORGGLOBAL, ?cont_lbl_lbl1, ?lab_lbl ORGGLOBAL, ?bench"	False
"
 input (English text): Can you list the models that have been evaluated on the SciERC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the SciFACT dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciFACT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the SciERC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT a.label, a.model_lbl, b.labels, c.dataset_lbl, d.dataset_lbl FROM data WHERE a.label = c.name a.label | b.labels | c.dataset_lbl | d.dataset_lbl -----------+--------------+-------- -1.4201 -1.1707 -1.0640 -0.7573 -0.9197 1.2823 1.1471 1.0169 0.9281 1.1805 1.0661 1.1407 0.9261 0.8440 -1.4723 -1.3076 0.9526 1.1723 0.9259 -2.2106 -1.2557 -0.9335 0.5262 0.6045 2.0416 1.1478 1.1279 0.9246 1.0886 1.0661 1.1407 -0.4029 -0.4847 1.0671 0.5501 0.3247 1.4089 1.1355 1.0193 0.9242 0.8656 -0.8039 -0.6486 -1.0676 -0.7084 0.8071 -0.8448 -1.1805 -0.8074 -0.8452 1.0961 -1.0653 1.0090 0.9236 1.0571 1.0886 -1.4963 -0.5129 -1.1355 10.3123 4.5255 -0.7092 1.0858 1.0039 0.8460 3.0869 3.4333 -	False
"
 input (English text): Indicate the model that performed best in terms of F1 metric on the ACL-ARC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Indicates that the model is available via the Model Search query. Incomplete search results are displayed by selecting the "" ?"" field. To perform a manual search via the Model Search query, enter the model name via the ""model search"" button, then select and perform the manual search. Entering the model name in the Search box: Searching the "" ?model"" field (not including "" ?model""): To search other models (not including "" ?""): To search by category, enter the model name in the ""category search"" field (not including the ""category"" ? field): Model search (not including "" ?model""): This field allows you to filter the database by model. Entering the model name : Entering the model name using a non-ASCII character (other than the dash) : Model search results (including the "" ?model ?""): Note that the model name is provided in one place and is not visible in the results list. This search feature has been disabled by default in Excel Server 2015 SP1. Select and perform the manual search in the above example."	False
"
 input (English text): What are the models that have been benchmarked on the IWSLT2015 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the WMT2014 German-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the WMT2014 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Output output to a text file: datasets.txt Note that when you have specified options that are passed to the library by your.h file, you must put them in a file named with the name of it as you want the default table to be populated. If the file named with the name you specified has too many files, but you have only a few columns, HBC will throw a stack overflow error. Use --no-create-files flag to turn off the creation of the dataset file whenever you load the code.	False
"
 input (English text): What is the highest benchmark result achieved on the Yelp-2 dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Yelp-5 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Yelp-14 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-24"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value ) } } ORDER BY DESC( orkgp:HAS_VALUE ) AND ?metric ?metric_lbl input (English text): What is the highest benchmark result achieved on the Yelp-35 dataset, including the metric and its value ? output (Sparql query): ------------- SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-37"") ?benchmark or"	False
"
 input (English text): List the code links in papers that use the DDRL A3C model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDRL A3C"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the A2C+CoEX model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A2C+CoEX"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF (1 day) hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF (1)) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } (1 day) hs model in any benchmark ? output (Sparql query): All benchmarks which use the A3C model for the same type of measurements were included in the following lists: A2C+CoEX, A3C, A3A, O2C, A2C+CoEX2, A2C+. A2C+CoEX, Co EX2, Co EX3 and O2C. The graphs show the number of measurements per benchmark for each model, the benchmark data for each model, the number of measurements in an A3C, O2C or A2C+CoEX benchmark and the corresponding performance statistics for each model and model pair. Each benchmark was run for 1 day at the ISO-standardised benchmarking frequency (benchmark.iso) of 3652 benchmark runs. The result of benchmarking the DDRL A3C model is shown on the right. For each benchmark, a plot of the number of batches taken for a particular benchmark is displayed. Benchmarks with 1-day-batch-taking at the ISO benchmarking"	False
"
 input (English text): What is the best performing model benchmarking the RTE dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the RTE benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 The following R test data were provided by the same researcher: - RTE benchmark dataset - RTE_v2.3.6_R01,  - RTE_v1.0_R01, - RTE_v1.0_R02, - RTE_v1.0_R03, - RTE_v1.0_R19 (from http://rsd.skeinforge.net/). - RTE_v1.0_R04, - RTE_v1.1_R42.0 (from http://rsd.skeinforge.net/). - RTE_v1.0_R07, - RTE_v1.0_R05, - RTE_v1.1_R19, - RTE_v1.0_R08 & RTE_v1.1_R07. The test data are the RTE benchmark dataset, RTE_v2.3.6_R01, RTE_v1.0_R01, RTE_v1.0_R02, RTE_v1.0_R03, RTE_v1.0_R19, RTE_v1.1_R42.0 and RTE_v1.0_R07. If you have a larger data set If you have more data than what is included in the above spreadsheet, you can provide additional data in R to perform additional benchmarks. Please Note. For large datasets the performance is limited to what is available via RTE. It is not possible to extract RTE_v1.0_R05, RTE_v1.1_R19, RTE_v1.1_R42.0 and RTE	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WOS-11967 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WSC dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?dataset ?dataset_lbl || 0 || 1 SELECT ?dataset ?dataset_lbl || 0 || 1 SELECT ?dataset ?dataset_lbl || 0 || 1 SELECT ?dataset ?dataset_lbl || 0 || 1 SELECT ?dataset ?dataset_lbl || 0 || 1 SELECT ?dataset ?dataset_lbl || 0 || 1 GROUP SELECT ?dataset ?dataset_lbl. || 0 || 1 There are also 4 output format variables in the query above, all of which are optional. Each of the variables represents a query output. In case of using WSC without a metric, WSC does have a metric. The WSC implementation of MetricFields provides this metric on all of the rows. In fact, WSC actually takes the following steps to collect the metric : 1. In WSC: a. Selects the metric from the MetricFields object b. Determines what the metric stands for on the table It seems that the input to MetricFields is a simple string called Metric on the table. 2. In MetricFields : a. Inserts a row that contains the metric, using the MetricFields object 3. Using this row (MetricFields, and a parameter named mf or nf, or mf mf f s1=1 ), which is a string representing the metric field for the table, WSC generates the following Query (note that WSC has no problem with the result being a string): SELECT ?metric ?metric_lbl The above query is executed using the mf mf f mf lu= ?stat_lbl. The output is	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Supervised: dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"" )) ?benchmark orkgp:HAS_DATASET ? ?dataset. input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebDiscussion dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ?benchmark orkgp:HAS_DATASET ? ?dataset. input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebGravity datasets ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ?benchmark orkgp:HAS_DATASET ? ?dataset. input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebConference datasets ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ?benchmark orkgp:HAS_DATASET ? ?dataset. input (English text): Provide a list of research paper titles"	False
"
 input (English text): List the metrics that are used to evaluate models on the SciERC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the COPA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""COPA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ART/CoreSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT a from (SELECT c as metric_lbl FROM { ?dataset a} c OR b) { ?dataset a \ ?dataset c - ?metric_lbl } or kgc:SciERC; output (Sparql query): SELECT a FROM (SELECT c as metric_lbl FROM { ?dataset a} c OR b) { ?dataset a \ ?dataset c - ?metric_lbl } or kgc:COPA; output : Here's a benchmark result. As the column ""a"" contains the same metric as the column ""b"", you can see that in case where the validation error is higher than the prediction error, the predicted prediction is worse. You can also read more about RDFS and how it makes model development possible. 7.3 How to get access to the Metrics API and how to create your own implementations The Metrics API is available for everyone. We also provide a public API. Both of them have a set of available metrics for use in your research. "	False
"
 input (English text): Could you provide a list of models that have been tested on the Natural Questions benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Natural Questions (long) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the Natural Questions (short) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "		False
"
 input (English text): What is the highest benchmark result achieved on the WMT2014 English-French dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset WMT2014 German-English?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WMT2014 French-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl Input text: What is the highest benchmark result achieved on the WMT2014 English-Norsaal dataset, including the metric and its value ? Output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-Norsaal"") ?benchmark orkgp:HAS_"	False
"
 input (English text): What is the best performing model benchmarking the WikiText-2 dataset in terms of Validation perplexity metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Validation perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Perplexity score when benchmarked on the LAMBADA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""LAMBADA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Test perplexity score on the WikiText-2 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""LAMBADA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Input is a subset of LAMBADA data. Output: Select DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. { { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""LAMBADA"") ?benchmark orkgp:HAS_DATASET ?dataset; { ?dataset a orkgc:Dataset; rdfs:label ?dataset"	False
"
 input (English text): List the code links in papers that use the All-attention network - 36 layers model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""All-attention network - 36 layers"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the All-attention network (36 layers) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""All-attention network (36 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the All-attention network - 18 layers model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""All-attention network - 18 layers"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" input (English text): Where can I find code references in papers that have used the All-attention network (18 layers) model for benchmarking purposes ? output (Sparql query): input (English text): Are there any good datasets containing pre-processing codes ? output (Sparql query): Yes - the original papers in the dataset (c.f. A. M. K. et al., 2012) This paper in the same issue (2012) uses more recent pre-processing codes. input (English text): Are there any good datasets containing pre-processing codes ? output (Sparql query): Yes - the original papers in the dataset (c.f. A. M. K. et al., 2012) This paper in the same issue (2012) uses more recent pre-processing codes and provides two datasets from these papers: ""Dynamical and Experimental Modelling of the Efficient Search for Non-Concrete Meaning in Human Language"", the last one used the entire pre-processing code set from this paper on (only the part that contains ""p"" in ""p"" in code was used - no ""p"" oracle)."	False
"
 input (English text): what are the evidence and limitations of paper ""Evaluating the Use of Social Networks in Author Name Disambiguation in Digital Libraries""?
 output (Sparql query): SELECT ?evidence ?evidence_title ?limitation WHERE { orkgr:R6751 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5004 ?evidence; orkgp:P5006 ?limitation. ?evidence rdfs:label ?evidence_title. FILTER(REGEX(STR( ?paper_title), ""Evaluating the Use of Social Networks in Author Name Disambiguation in Digital Libraries"", ""i"")) } 
 input (English text): Provide a list of benchmarked datasets related to the co-authorship prediction research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""co-authorship prediction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Does paper ""Disambiguating authors in citations on the web and authorship correlations"" employe Cosine similarity?
 output (Sparql query): "	"ASK WHERE { orkgr:R6187 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5002 orkgr:R6006. FILTER(REGEX(STR( ?paper_title), ""Disambiguating authors in citations on the web and authorship correlations"", ""i"")) } "	SELECT ?column ?column_name FROM authors WHERE ?column = 'author'; and then run the query: output (Sparql query): SELECT ?column ?column_name FROM authors WHERE ?column = 'author'; and then delete the entry: output (Sparql query): SELECT ?column ?column_name FROM authors WHERE ?column = 'author'; AND rdfs:keywords('author') LIKE '%$' OR rdf:keywords('author').$ ? && rdfs:keywords('author').$ ? = '%$' OR rdf:keywords('author').$ ? = '%$' ? OR rdf:keywords('author').$ ? = '%$' ? INNER JOIN authors ORDER by author_id DESC and set a value of ?column to the first author, in case both authors are used in the paper: output (Sparql query): SELECT ?column : column_name FROM authors WHERE ?column = 'author'; AND : OR ?column_name IN (1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3,	False
"
 input (English text): Can you list the models that have been evaluated on the SciERC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the ARC (Challenge) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the STS Benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): Would you like to run a benchmark ? run(output, output): output: Input : I am writing a paper to be presented on the 7th SPSS Conference on Computing Science on 27 October 2010. It may be well used for your publications. The problem I am trying to find answers to are: Is there a model that you have tried, tested and deemed successful ? What is the confidence that the model you found fits the data well, given the available data in the literature ?   Is your confidence high enough (more than 90% confidence) to be confident that the accuracy and convergence times are greater than that of the given alternative and/or alternative implementations ? My paper is available at: pdf link :   http://spsds.uclr.edu/files/pds/papers/benchmark_papers/5_7_5.pdf Note: If the results of this benchmark test are not what you expected, please check the results using R or using the link above. In particular, if the results are lower than your expectations, please provide the results in the comments section below. input (English text): Would you like to run a benchmark ? run(output, output): output: Input : You can use the option ?benchmark to specify a set of model. For a benchmark test for a given model, I am using the following models: Input : BAML 1.12.3 (version 20070124.v1.2 (2.26E010513)) BAML 1.3.1 (version 200701927.v1.2 (2.2E012289)) BAML 1.3.2 (1.01E07001) (BAML 1.3.1) Note: This is what I came	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Phoenix benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Phoenix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asterix benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Freeway benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 The following queries were run on 1,001 samples of the Atari 2600 Phoenix benchmark dataset. The results are summarized in the table below. Samples of Atari 2600 Phoenix benchmark dataset This table shows the sample count and a histogram of each metric that we determined from the benchmark dataset as well as the total number of metrics performed. Metrics : Samples of Atari 2600 Phoenix benchmark dataset : Metric Score Metric Score Metric_lbl -1 1.4 0.6 orkgc -3.9 -0.9 orkgc_hst -5 3.9 0.7 orkgc_lbl -10 3.1 0.4 orkgc_rdfs -13 2.6 0.5 orkgc_lbl -18 2.8 0.4 orkgc_xfer -25 3.4 0.4 orkgc_lbl 20 3.1 0.8 orkgc_eval _ 25 3.1 0.6 orkgc_lbl 40 3.8 0.5 orkgc_dat _ 40 6.0 0.1 orkgc_eval 1.0 1.3 0.3 orkgc_lbl 60 2.8 0.1 orkgc_lbl 80 1.8 0.1 orkgc_dat 80 6.0 0.2 orkgc_eval _ 80 3.1 0.2 orkgc_dat _ 80 6.0 0.2 orkgc_lbl 200 1.0 0.2 orkgc_lbl 300 1.0 0.4 orkgc_lbl 600 2.0 0.5 orkgc_xfer 650 4.0 1.2 orkgc_lbl 1000 2.0 0.8 orkgc_dat 1000	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the DDI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DDI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BC5CDR dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the DCASE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT COUNT(*) AS number FROM ( SELECT ?title ?subj ?pub_date ?name ?sig ?author_id ) ?paper_lbl ?subj AS author_name, ?pub_date AS publication_date, ?name ?sig ?author_id FROM ( SELECT ?labels ?subl ?pub_date, ?pub_title ?pub_abbrev ?pub_author_id ) ?paper_lbl ?subl AS author_labels, ?pub_date AS publication_date, ?name ?sig ?author_id FROM ?paper_lbl ?subl AS author_subl, ?pub_date AS publication_date WHERE ?subl=NULL. ) papers JOIN ( SELECT paper_id AS title, ?labels ?subl, ?subc AS publication_date, ?author_id AS author_id FROM ( SELECT ?name ?date ?pubs_id AS author ) ?paper_lbl where ?subl=NULL ? and ?subl. ?pub_date = ?pubs_date and ?subl. ?pub_author_id = ?author_id	False
"
 input (English text): What is the best performing model benchmarking the Yelp Fine-grained classification dataset in terms of Error metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the Yelp Binary classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 We will now show that some of the most important metrics are listed below. Each metric is also provided in the tables for illustrative purposes only. Metric COUNT ( ) MINIMUM_LENGTH ( ) MAXIMUM_LENGTH ( ) ERROR (ms) FALSE FALSE TRUE 100.0 100.0 NULL. FALSE FALSE 4.0 NULL. FALSE FALSE 4.0 10.0 2.0 10.0 The above table shows the count of all errors reported for each model. The MAXIMUM_LENGTH metric is important here since it measures the total number of errors in a collection. One of the most interesting stats in the above table is the MINIMUM_LENGTH or number of model iterations required to find one false positive, which is what is used in many of the algorithms to detect fake reviews in Yelp's database. This metric is usually found in model-to-model comparisons (as shown above), but can also be used to look at the performance of different models compared to each other. It is also used in a few Yelp metrics when testing new algorithms which we will later describe. The reason why a good number of models are selected for performance benchmarking is to evaluate the model performance relative to the benchmarking dataset. So, for example, if we take a quick look at the results from one benchmark that we will refer to later for analyzing various algorithms and metrics, we have many good options for performing some real-world business queries on the benchmark data. What is the maximum number of iterations for modeling in the Yelp Model-to-model Benchmarking ? Metric CORE_TIME (days) MAXIMUM_LENGTH (Lbs) ERROR (ms) FALSE FALSE TRUE 10000.0 10000.0 NULL. NULL. TRUE. FALSE 20.0 20.0 2.0 1.0 The above graph should	False
"
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Battle Zone benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Amidar benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 River Raid benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Enter a name to be used for the score record: :name Enter a number to be used to compare the model scores: 	False
"
 input (English text): Provide a list of papers that have utilized the BertSumExt model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BertSumExt"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the NCBI_BERT(base) (P+M) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NCBI_BERT(base) (P+M)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BERT + BiLSTM + CRF Decoding model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT + BiLSTM + CRF Decoding"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code WHERE { ?model b orkgc:Model Raster or CSV: input (English text): Provide a list of papers that have utilized Raster or CSV (i.e. Matlab) file format and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model ?data; orkgp:HAS_DATA_LINKS ?raster_file. csv (i.e. R) file format): input (English text): Provide a list of papers that have utilized CSV (i.e. R) file format and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model c orkgc:Model. RTS (i.e. Matlab: input (English text): Provide a list of papers that have utilized RTS (i.e. Matlab) file format and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model rts:Model; orkgp:HAS_RTS_LINKS ?rts_file. RTS (data base): input (English text): Provide a list of papers that have utilized RTS (data base): output (Sparql query): SELECT DISTINCT ?code WHERE { ?model rts:Model ?data; orkgp:HAS_RTS_LINKS ?rs_file. RTS (data base)):	False
"
 input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the BIOSSES benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pearson Correlation"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the PubMedQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the MedSTS benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pearson Correlation"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?model ?model_lbl WHERE { ?metric rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = 'MedSTS') ?benchmark orkgp:HAS_DATASET ?dataset; ORkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; ORkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; ORkgp:HAS_MODEL ?model. ORkgp rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } If this query gets repeated, for example, each time the user has changed the query, we'd have to manually edit the query manually in the browser. That is an overhead that BIOSSES doesn't provide for us because we only need to do one step. We'd like to have BIOSSES do the heavy lifting for us. You might be thinking that it shouldn't be too hard to edit one query at a time, and we mean the same here. It can be easy to just change the parameters for each query because all that is needed is a single change to the query in the browser. However, if a user gets a lot of queries, there may be a need to change each query based on the current number of rows and the number of fields required. For example, if we have just one query that uses a lot of fields, we might want to change the query before performing the update. We'd also like to change the query to a more appropriate query later on, for example because the User set the desired fields for that query: SELECT ?name ?value ?	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the WOS-11967 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the WSC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT TOP 1,2 TOP 100,3 TOP 200,4 TOP 300,5 TOP 400,6 TOP 500 SELECT TOP 1,2,3,4,5,6 AS DISTINCT ""tot_top"" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 SELECT TOP 1,2,3,4,5,6 AS DISTINCT ""tot_top"" FROM ( SELECT TOP 1,2,3,4,5,6 AS DISTINCT ""top_top"" ), WSC AS TOP 100, WSC AS TOP 200, WSC AS TOP 300, WSC AS TOP 400, WSC AS TOP 500 WHERE { TOP 100 = ? TOP 200 = ? TOP 300 = ? TOP 400 = ? TOP 500 = 1 ? } output ( Sparql query ) : SELECT TOP 100 AS TOP 100, TOP 200 as TOP 200, TOP 300 as TOP 300, TOP 400 as TOP 400, TOP 500 as TOP 500, SLEEP_DATE as TOP 500. TOP 1,2,3,4,5,6 AS DISTINCT ""tot_top"" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 SELECT TOP 1,2,3,4,5,6 AS DISTINCT ""top_top"", TOP 200, TOP 300 as TOP"	False
"
 input (English text): List the code links in papers that use the CRF with term expansion model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with term expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the CRFSuite model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRFSuite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the linear-chain CRFs model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""linear-chain CRFs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Input Citation # of papers # of papers in the database using the linear-chain CRFs model in any benchmark ? output (Sparql query): select { ?Crs a OR kgs ? Crs ; }. ?term ?code and ( ?Crs ?language ). from _criteria all to _criteria. select { ?Crs ?language }. from _criteria _lbl all to _criteria. select { ?Crs ?language }. from _criteria _lbl _datasets all to _criteria. select { ?Crs ?language }. from _criteria _lbl _benchmark all to _criteria. select { ?Crs ?language }. from _criteria _lbl _benchmark _datasets all to _criteria. select { ?Crs ?language }. from _criteria _lbl _benchmark _datadatasets all to _criteria. select { ?Crs ?language }. from _criteria _lbl _analytics all to _criteria. select { ?Crs ?language }. from _criteria _criteria all to _criteria. select { ?Crs ?language }. from_criteria_lbl _lbl all to _criteria. select { ?Crs ?language }. from_criteria_lbl _criteria all to _criteria. select { ?crs ?language }. from_criteria_criteria all to _criteria, where _criteria in all, all. select { ?crs ?language }. from_criteria_critiaster all to _critiaster. select { ?crs ?language }. from _critiaster_criteria all to _critiaster_criteria. select	False
"
 input (English text): Can you provide links to code used in papers that benchmark the DrQA (Document Reader only) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DrQA (Document Reader only)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DyGIE model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DyGIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DrQA model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DrQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "		False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset SST-5 Fine-grained classification?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Could you provide a list of models that have been tested on the SST-5 Fine-grained classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 To compare different models side-by-side, you can create, and use, the model comparison view. The comparison view shows you, among other data points, the highest or lowest accuracy score (measured by the SST-5 Fine-grained classification benchmark) of all the models that have been tested on the SST-5 fine-grained classification benchmark dataset. The data point that the model has achieved, in this case, would be the one that has the lowest accuracy score, because the model has not yet achieved a perfect match with the dataset for accuracy score calculation. It is also possible to create a table showing the minimum scores for the different test dataset weights (that are considered as acceptable to generate a perfect match) (that would then be used as the acceptable weights to compare different models). The model comparison view also allows you to make predictions for this or other test dataset weights. To create a table showing these predictions (which has to be placed in a different table), you use the Table View function: create_view ('model_weights_test', input, output, { filter (str(input) in ('SST-5.0E')). }) To create a table showing the minimum scores for a set of test dataset weights, you use the Weight Table view: create_view ('model_weights_test_weights', input, output, { filter (str(input) in ('SST-5.0E'))). weight_table for model_weights_test: filter(str(input) in ('SST-5.0E')) weight_table: add(weight_tbl('SST-5.0E'), 1) When a model has achieved a perfect or close match with the benchmark dataset, it is called model-constructed or model-tested. To compare the accuracy score for two different model-	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-CRX dataset ? output (Sparql query): input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-CSA dataset ? output (Sparql query): input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-ASL dataset ? output (Sparql query): input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-CIE dataset ? output (Sparql query): Inputs : 1 x test_class : FAST_PRIME (a) 1 x test_target : FAST_PRIME (a) 1 x test_parameters : FAST_INTERVALS (a-b_2_0_FAST) The following is the benchmarking data: 1. FAST PRIME training set, 2 x NUT_FLAVOR_1, 4 x NUT_FLAVOR:FAST_PRIME_K(a), NUT_FLAVOR_2, NUT_FLAVOR:FAST_PRIME_K(b), NUT_FLAVOR:FAST_PRIME_K(c), NUT_FLAVOR:FAST_PRIME_U(d) 2. FAST PRIME testing set, 2 x NUT_FLAVOR_1, 4 x NUT_FLAVOR:FAST_PRIME_K(a), NUT_FLAVOR_2, NUT_FLAVOR:FAST_PRIME_K(b), NUT_FLAVOR:FAST_PRIME	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Quora Question Pairs dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quora Question Pairs"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WebQuestions dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which QA benchmarks incorporating more than 10000 questions?
 output (Sparql query): "	"SELECT ?papers, ?papers_labels WHERE { ?papers rdf:type orkgc:Paper; orkgp:P31 ?contrib; rdfs:label ?papers_labels. ?contrib orkgp:P41923 ?questions_number_str. BIND(REPLACE( ?questions_number_str, ""[+]"", """") AS ?questions_number_preprocessed) BIND(xsd:integer(REPLACE( ?questions_number_preprocessed, """","""", """")) AS ?questions_number) FILTER( ?questions_number > 10000) } "	SELECT k FROM ` ?k' WHERE k in ( SELECT ?k ORDER BY k DESC LIMIT 10000 ) -------------+-----------------------------+---------------------+----------+ | k | ( SELECT ?k ORDER BY k DESC LIMIT 10000 ) -------------+-----------------------------+---------------------+----------+ | Most Frequently Asked Questions | 1 | 0.0566 | | Most Frequently Asked Questions | 2 | 0.0837 | | Most Frequently Asked Questions | 3 | 0.1267 | | Most Frequently Asked Questions | 4 | 0.1789 | | Most Frequently Asked Questions | 5 | 0.1125 | | Most Frequently Asked Questions | 6 | 0.1829 | | Most Frequently Asked Questions | 7 | 0.2181 | | Most Frequently Asked Questions | 8 | 0.2161 | | Most Frequently Asked Questions | 9 | 0.2467 | | Most Frequently Asked Questions | 10 | 0.2712 | | Most Frequently Asked Questions | 11 | 0.2963 | | Most Frequently Asked Questions | 12 | 0.3135 | | Most Frequently Asked Questions | 13 | 0.3353 | | Most Frequently Asked Questions | 14 | 0.3500 | | Most Frequently Asked Questions | 15 | 0.3713 | | Most Frequently Asked Questions | 16 | 0.3917 | | Most Frequently Asked Questions | 17 | 0.4044 | | Most Frequently Asked Questions | 18 | 0.4169 | | Most Frequently Asked Questions | 19 | 0.4450 | | Most Frequently Asked Questions | 20 | 0.4635 | | Most Frequently Asked Questions | 21 | 0.4846 | | Most Frequently Asked Questions | 22 | 0	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Eternax dataset ? output (Sparql query): input (English text): What evaluation metrics are widely used when benchmarking models on the Atari 2600 Golden Axe dataset ? output (Sparql query): When a value is not an expression and a table of data is used as the target value in the LLD query, the command outputs a comma separated list of values, each of which is an expression yielding the metric in a different setting. If no data set was defined for the target, the command outputs a list of data points whose names are consistent with the values of the target data set. When a list, an array, or a datatype can be used as the target data, the LLD query evaluates its value against each of these data types. When a value can be used as an expression, only the LLD query produces a single value within the list. For example, if a table or variable value can return 1 and 2, but the LLD query returns a different 3, the results are a single value for the dataset and the data set. Otherwise, the result of the LLD query is one of a comma-separated list of the form: [1 | 2 | 3] The values in a list are evaluated as a single expression using the following rules: First, the first value evaluated is used as the target, which is a value in the data set. The output is always 1 and is unique within the list. Second, the second value evaluated is used as the target, again using the first value within the list as the target. The output is always 2 and is unique within the list. Third, the third value evaluated is used as the target, using the first value within the list as the target,	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Freeway dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the RACE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Stanford Cars dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ? ? ?, ?doc ?doc ? ? ?, ?title %num ?id FROM projects.projects GROUP BY ?doc ORDER BY ?doc DESC LIMIT 1; output (Sparql query): SELECT DISTINCT ?paper ? ? ?, ?doc ?doc ? ? ?, ?title %num ?id FROM projects.projects GROUP BY ?doc ORDER BY ?doc DESC LIMIT 1; # Note: this query might take forever. Your query might be rejected... # Example output of a test query against the Stanford dataset. As an example, a query against one set of results results in this message: SQL> SET q1 = 'Atari 2600 Freeway'; SELECT DISTINCT ?record ? ? ?; ERROR: Failed to match result type ERROR: Failed to match result type ( ?record was not of type ""data.frame"") SQL> SELECT DISTINCT ?record ? ? ?; ERROR: Failed to match result type SQL> SELECT DISTINCT ?record ? ? ?; ERROR: Failed to match result type ( ?record was not of type ""data.frame"") SQL> SELECT DISTINCT ?record ? ? ?; ERROR: Failed to match result type ( ?record was not of type ""data.frame"") SQL> ?record was not of type ""data.frame"" Error: The result type cannot be NULL or null. ERROR: The result type cannot match type ""double"", integer, ""integer"" or ""double"", string or null. SQL> ?data_lbl was not of type ""data.frame"" Error: Your query might be rejected... # Examine the results for the second set of results. The following SQL queries are typical commands to inspect the results. For complete information about the commands included, see the reference section for each query. # Example output of a"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC Chinese-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Chinese-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BUCC French-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BUCC Swiss-to-English dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label the_benchmark. FILTER (str( the_benchmark) = ""BUCC Swiss-to-English"") ?benchmark. ?paper orkgp:P31 ?cont; ?index orkgp:SENSOR ?index. } input (English text): what are the titles and IDs of research papers that include a benchmark for the BUCC U.S.-English dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC U.S.-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?index or"	False
"
 input (English text): What is the top benchmark score and its metric on the Stanford Cars dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the CIFAR-10 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Stanford Cars dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT TOP 5 (value) (score) FROM S2.RDFS.RDFS.Stanford(a, a, a) benchmark_rdf_eval ?valid_score ORDER BY cnt DESC UNION SELECT TOP5 (value) (score) FROM S1.RDFS.RDFS.Stanford(a, a, a) benchmark_rdf_eval ?valid_score ORDER BY cnt DESC UNION SELECT TOP5 (value) (score) FROM S4.RDFS.RDFS.*benchmark_rdf_accumulated_val(b) benchmark_rdf_eval ?test_score ORDER BY cnt DESC UNION SELECT TOP5 (value) (score) FROM S1.RDFS.RDFS.Stanford(a, a, a) benchmark_rdf_eval ?test_score ORDER BY cnt DESC UNION SELECT TOP5 (value) (score) FROM S4.RDFS.RDFS.*benchmark_rdf_accumulated_val(b) benchmark_rdf_eval ?test_score ORDER BY cnt DESC UNION SELECT TOP5 (value) (score) FROM S3.DATASET.*Bench	False
"
 input (English text): What is the highest benchmark result achieved on the ShARe/CLEF eHealth corpus dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ShARe/CLEF eHealth corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?post_date ?post_lbl WHERE { { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?post_date. ?post_lbl rdfs:label ?dataset_lbl. } ?post_date,timestamp orkgc:PostDate ?post_date + 3.5h OR ?post_date + 6h OR ?post_date + 8h OR ?post_date + 1.5h; rdfs:label ?post_lbl. OPTIONAL { ?post_date orkgc:BatchSize ?post_date. orkgc:ExcludeDataset ?post_date; } } GROUP BY ?post_date input (English text): What was the most recently accessed time period for a paper ? output (Sparql query): SELECT DISTINCT ?post_date ?post_lbl WHERE { { ?dataset a orkgc:Dataset; } rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?post_date. ?post_lbl rdfs:label ?dataset_lbl. } ?post_date,timestamp orkgc:"	False
"
 input (English text): What is the highest benchmark result achieved on the Finger, spin (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Finger, spin (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset Finger, spin (DMControl100k)?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Finger, spin (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): For what piezoresistive material the least response time was obtained?
 output (Sparql query): "	SELECT ?piezoresistive_materials WHERE { orkgr:R138374 orkgp:compareContribution ?contrib. ?contrib orkgp:P41021 ?piezoresistive_materials. ?contrib orkgp:P41022 ?response_time. } ORDER BY ASC( ?response_time) LIMIT 1	SELECT TOP( ?TOP( ?SINCE( ?SAT( ?value) - 1) AS ?value) AS n) ?rdfs:labels(fname,lname) ?dataset2 orkgc:DATASET2 ?dataset2 orkgc:HAS_DATASET2 ?matrix2 rdfs:label(matrix) ORG( ?rdfs:cols( ?matrix) ?value) ORG( ?rdfs:rows(matrix) ?value) ORG( ?rdfs:rows(matrix) ORG( ?rdfs:columns( ?matrix) ?value)) ORG( ?rdfs:column( ?value) ?value ORDER BY ?rdfs:lname ORDER BY n) ORDER BY ?rdfs:cols( ?matrix) input (English text): For what piezoresistive material for how long was the result obtained ? output (Sparql query):  SELECT TOP( ?TOP( ?SINCE( ?SAT( ?value) - 1) AS ?value) AS n) ?rdfs:labels(fname,lname) ORG( ?rdfs:cols( ?matrix) ?value) ORG( ?rdfs:rows(matrix) ?value) ORG( ?rdfs:rows(matrix) ORG( ?rdfs:columns( ?matrix) ?value)) ORDER BY ?rdfs:lname ORDER BY n) ORDER BY ?rdfs:cols( ?matrix) input (English text): For what piezoresistive material how long did the result obtain ? output (	False
"
 input (English text): List the code links in papers that use the GPT-3 model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the GPT-3 (Zero-Shot) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-3 (Zero-Shot)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-2 (small)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; hdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-2 (Small)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): List the code links in papers that use the GPT-2 (Small) model in any benchmark ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; hdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-2 (Small)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): List the code links in papers that use the GPT-1, GPT-2, and GPT-3 (Standardized Targeting) models in any benchmark ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model sgts:TargetingModel; hdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-1 (Standardized Targeting)"") ?benchmark orkgp:HAS_D"	False
"
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Seaquest benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Pitfall! benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pitfall!"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Fishing Derby benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Fishing Derby"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Fishing Derby"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): SELECT ?dataset_lbl ?dataset_name ?dataset_lbl. ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Fishing Derby"") ?benchmark orkgp:HAS_DATASET ?dataset_lbl. ?metric rdfs:label ?metric_lbl. } A more detailed description of the above queries can be found at the following link. The output that is obtained is as follows: { ?dataset_lbl ?dataset_name orkgc:Dataframe; rdfs:list ?dataset_lbl_items ?dataset_lbl; }. The following example shows how to filter a matrix. The output is defined in a similar format. See the following link for a description: http://en.wikipedia.org/wiki/Filtering_matrices #!/usr/bin/env gensl $ gensl --filter=filter:v2.22 ###################################################################### # filter variables for the filter.g"	False
"
 input (English text): What are the models that have been benchmarked on the ImageNet V2 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the ImageNet dataset in terms of Number of params metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?model ?model_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) LIMIT 1 ?metric_lbl input (English text): If you provide the benchmark score for ImageNet, could you please list the metric and score of the best benchmark in terms of Number of params metric ? output (Sparql query): select metric_lbl( ?n, ?key, ?value, ?rank, ?pval,.size) from performance lbr where ( ?n ?key) and ( ?n.key).get( ?key)!= ?n.key"	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Seaquest dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Can you list the models that have been evaluated on the Atari 2600 Densha Data set ? output (Sparql query):  SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Densha"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Can you list the models that have been evaluated on the Atari 2600 Fuzor Data set ? output (Sparql query):  SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a or"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BiT-S model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiT-L model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-L"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the BiT-M model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 The source of the benchmark results is available to the researcher, however, due to the nature of the methodology, the benchmark results will vary from one paper to the next. The BiT-L model was run with a specific configuration of data and parameters. In particular, the values of the LUNAR and RASPARAMETERS were set based on experiments described in the literature. As the BiT-S model did not show an improvement over the baseline model for some of the variables, the authors examined a more general test of model performance from the BiT-L model at two settings (a single set of the baseline and BiT-S settings). A simple linear regression was run with the following parameters as covariates: H (H2O-H): the proportion of H2O in the blood and X (sensitivity): the sensitivity of the model to changes of parameter, compared to the baseline value X (R2): the correlation of model-predicted parameter values and the parameter expected based on the parameters for all other set of parameters The BiT-L model predicted the proportion of the blood H2O for each group. This test was also carried out for sensitivity and correlation using two control variables, X and R2. The BiT-S model showed improvement at both the baseline and BiT-L setting for two variables. This is especially encouraging given the lack of agreement associated with the baseline analysis from the literature.  The authors also used the BiT-L model to determine the overall performance of the models using a multi-parameter model of sensitivity/X and R2: X (sensitivity): the sensitivity of the model to changes of parameter, compared to the baseline value R2 (R2-L): the correlation of model-predicted parameter values and the parameter expected based on the parameters for	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Space Invaders dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Time Pilot dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Time Pilot"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Star Gunner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT 1 as score; ------------ Output: 1 as score Select * from ?metric; [<select 't' from `metrics`.start] SELECT score (10); ------------ Output: 10 as score Select * from ?metric; [<select 'u' from `metrics`.end] SELECT score (10); ------------ Output: 10 as score Sparql SELECT dic:metrics.model as dic FROM :metrics dic GROUP BY dic.name ORDER BY score DESC; ------------ Sparql SELECT DISTINCT ?metric1 ?value1 1 as ?metric2 ?value2 2 as ?metric3 ?value3 3 as ?metric4 ?value4 4 as ?metric5 ?value5 5 as ?metric6 ?value6 6 as ?metric7 ?value7 7 as ?metric8 ?value8 8 as ?metric9 ?value9 9 as ?metric10 ?value10 10 as ?metric11 ?value11 11 as ?metric12 ?value12 12 as ?metric13 ?value13 13 as ?metric14 ?value14 14 as ?metric15 ?value15 15 as ?metric16 ?value16 16 as ?metric17 ?value17 17 as ?metric18 ?value18 18 as ?metric19 ?value19 19 as ?metric20 ?value20 20 as ?metric21 ?value21 21 as ?metric22 ?value22 22 as ?metric23 ?value23 23 as ?metric24 ?value24 24 as ?metric25 ?value25 25 as ?metric26 ?value26 26 as ?metric27 ?	False
"
 input (English text): List the code links in papers that use the NAS Cell model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NAS Cell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ning et al."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the NASCell model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NASCell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NAS Cell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 2. How do you get from there to a PDF ? If you are using an interactive user interface, such as a Google Docs file, please create a document with the link from the R code in the previous question. You can open a new document by selecting Create Document. 3. Can you provide links to code used in papers that benchmark the Model-Based Approach for Cellular Cloning that was published by Ning et al. in Nature Cell Biology in 2009, so the cell line is not mentioned in the first question ? If you can't find the cell line mentioned in the first question, please refer to their paper. 4. Where can I find the cell line from the first question referenced in this answer about that cell line referenced from The Cell ? The cell line from the first question referenced in this answer about the NAS Cell link in that cell line in The Cell is provided with the link in comments in the first question below."	False
"
 input (English text): Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TDMS-IE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-TDN(ResNet-101) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-TDN(ResNet-101)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the TCN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Can you provide links to code used in papers that have used the TRUVE model for benchmarking purposes ? output (Sparql query): Select the code references from the output table. 1 rows returned. The model name is resNet-101, which is not the same as the BiLSTM model that is used for training, but is the same as what the authors of the papers that are benchmarking or using the model used to train the model actually call the model. Please do not ask me why the authors call the BiLSTM model BiLSTM-TDN(ResNet-101) rather than BiLSTM-RESNet or BiLSTM-TDN(ResNet-101). Let me know if that helps."	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WMT2014 English-French dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset BUCC French-to-English?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCK CNC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ORDER BY DESC( ?value) } } } GROUP BY ?metric ?metric_lbl -------------- -------------- --------------------- -------------- -------------- --------------------- -------------- -------------- Note that some of the benchmark results are out of date because the data was published on the date of the write. The output for the WMT2014 English-French dataset is somewhat different to that reported by ODT, for example: output (Sparql query): { { SELECT DISTINCT ?dataset ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ( ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = """	False
"
 input (English text): What is the top benchmark score and its metric on the SQuAD2.0 dev dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the SQuAD1.1 dev dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset SQuAD2.0?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?value { { id: 1, name: 'J. Zentner' } } as id, C. Wiltse as name, C. Wiltse as metric FROM SQuAD2.dataset.mvs WHERE id = 1 ORDER BY - DISTINCT ?value. OPTIONAL orkgp:HAS_METRIC_LENGTH ?metric.  ORDER BY  C. Wiltse.name DESC, D. Schmitz.name.1 ASC,  C. Wiltse.value AS metric, C. Wiltse.value AS metric_lbl OPTIONAL ?value. OPTIONAL orkgp:HAS_METRIC The best way is to use a data-driven approach and try to do all that in a way in which you can quickly and accurately get a meaningful result. An easy way to understand the impact of any measure is to have it show the metric's impact on the evaluation and validation of a new set of results. That's the ""measure"" that is important to look at when looking at your data. We often come across metrics whose effect on the performance of a specific workflows is completely unmeasurable and so often we can't tell whether something's really impactful because it's unobservable. In the above example, you can only get to the ""measure"" of what it takes to validate the new datapoints by having it show ""the"" effect of validate an existing dataset: It can't tell you if the difference in the validation time between SQuAD2.0 and the different validation time between the different datapoints represents a better or worse validation, or if it is related to the new datapoints validation time. This can be frustrating because a metric might be one of"	False
"
 input (English text): What is the highest benchmark result achieved on the BC5CDR dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset BC2GM?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"------------ --------- ?metric orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM""). ( ?dataset a orkgc:Dataset) ORDER BY DESC( ?value) DESC( ?value 1,1). ORG( ?metric rdfs:label ?metric_lbl. ORG( ?metric rdfs:label ?metric_lbl). OPTIONAL ). orkgc:Dataset ORG( ?metric rdfs:label ?metric_lbl. ?value ORG( ?metric rdfs:label ?metric_lbl. ?value ORG( ?metric rdfs:label ?metric_lbl. ?value ORG( ?metric rdfs:label ?metric_lbl. ?value). ORG( ?metric rdfs:label orkgc:Dataset). ?value ORG( ?metric rdfs:label orkgc:Dataset). ?value ORG( ?metric rdfs:label orkgc:Dataset). ). ORG( ?metric rdfs:label ?metric_lbl. ORG( ?metric rdfs:label ?mdsln). ORG( ?metric rdfs:label ?mdsln). ORG( ?metric rdfs:label orkgc:Dataset). ?value ORG( ?metric rdfs:label orkgc:Dataset). ORG( ?metric rdfs:label orkgc:Dataset). ORG( ?metric rdfs"	False
"
 input (English text): Can you list the models that have been evaluated on the Story Cloze Test dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the WikiText-2 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" ""And we have created the best model in the world"". And we have created the best model in the world. - Dr. D.B. Bardeen The data have been collected by a number of different sources including the New England Complex Systems Laboratory (NECSL), the Naval Research Laboratory (NRL), The MIT Media Lab (MIT LSA) and the Department of Defense.  The data were collected using a mix of different tools.  The datasets were selected to test a number of different model parameters but not all of them had to use all of the parameters to produce the ideal result.  To this end the original datasets were re-run by using different algorithms on the results obtained.   NECSL data The first dataset contains the text data for 6,633 Wikipedia articles covering 473 different terms.  A single text file was written using the Gephi software and downloaded.  The Gephi software is able to read and write UTF-8 and can do so without errors.  After reading the file into UTF-8, the text of a page was converted into a binary by running a function called convert_text to convert the text from ASCII to UTF-8.  A series of Gephi calls were then performed in order to convert the code from text to binary.  The resulting binary was then passed into the Gephi program as input. The output from these two computations was compiled into 3 functions that perform a series of ""conversions"".   The first was a routine that converted the string ""dont"" into a string, ""wont"", converting the ""dont"" to ""won"" to avoid a conversion.  This conversion performed approximately two to three million times and resulted in a model with 0.95 SD, about 7% better than the original text."	False
"
 input (English text): Who is the author with the largest number of papers about semantic representation in music research problem? 
 output (Sparql query): SELECT ?author WHERE { ?papers rdf:type orkgc:Paper. ?papers orkgp:P31 ?contrib. ?contrib orkgp:P32 ?research_problems. ?research_problems rdfs:label ?research_problems_labels. FILTER(REGEX( ?research_problems_labels, ""semantic representation in music"", ""i"")) ?papers orkgp:P27 ?authors. OPTIONAL { ?authors rdfs:label ?authors_labels } BIND(IF(BOUND( ?authors_labels), ?authors_labels, ?authors) AS ?author) } ORDER BY DESC(COUNT( ?author)) LIMIT 1
 input (English text): Who is the author of the most recent paper about insects?
 output (Sparql query): SELECT ?author_name WHERE { { SELECT ?publication_date_ AS ?date_of_the_latest_paper { ?paper_ a orkgc:Paper; rdfs:label ?title_; orkgp:P28 ?publication_month_; orkgp:P29 ?publication_year_. OPTIONAL { ?publication_month_ rdfs:label ?publication_month_label_ } OPTIONAL { ?publication_year_ rdfs:label ?publication_year_label_ } BIND( xsd:integer( IF( BOUND( ?publication_month_label_), ?publication_month_label_, ?publication_month_ ) ) AS ?publication_month_as_number_ ) BIND( xsd:integer( IF( BOUND( ?publication_year_label_), ?publication_year_label_, ?publication_year_ ) ) AS ?publication_year_as_number_ ) BIND( xsd:dateTime( CONCAT( ?publication_year_as_number_, ""-"", ?publication_month_as_number_, ""-01T00:00:00.000-00:00"" ) ) AS ?publication_date_ ) FILTER( ?publication_month_as_number_ > 0 && ?publication_month_as_number_ < 13 && ?publication_year_as_number_ > 0 && ?publication_year_as_number_ < 2023 ) FILTER(REGEX(STR( ?title_), ""insect"")) } ORDER BY DESC( ?publication_date_) LIMIT 1 } ?paper a orkgc:Paper; rdfs:label ?title; orkgp:P27 ?author; orkgp:P28 ?publication_month; orkgp:P29 ?publication_year. OPTIONAL { ?publication_month rdfs:label ?publication_month_label } OPTIONAL { ?publication_year rdfs:label ?publication_year_label } OPTIONAL { ?author rdfs:label ?author_label } BIND( IF( BOUND( ?author_label), ?author_label, ?author ) AS ?author_name ) BIND( xsd:integer( IF( BOUND( ?publication_month_label), ?publication_month_label, ?publication_month ) ) AS ?publication_month_as_number ) BIND( xsd:integer( IF( BOUND( ?publication_year_label), ?publication_year_label, ?publication_year ) ) AS ?publication_year_as_number ) BIND( xsd:dateTime( CONCAT( ?publication_year_as_number, ""-"", ?publication_month_as_number, ""-01T00:00:00.000-00:00"" ) ) AS ?publication_date ) FILTER( ?publication_month_as_number > 0 && ?publication_month_as_number < 13 && ?publication_year_as_number > 0 && ?publication_year_as_number < 2023 ) FILTER( REGEX( STR( ?title), ""insect"" ) && ?publication_date = ?date_of_the_latest_paper ) } 
 input (English text): Who has contributed to the largest number of articles about coronavirus?
 output (Sparql query): "	"SELECT ?author_name WHERE { { SELECT (COUNT( ?paper_) as ?max_n_papers) WHERE { ?paper_ a orkgc:Paper; rdfs:label ?title_; orkgp:P27 ?author_. FILTER(REGEX(STR( ?title_), ""2019-nCoV"")) } GROUP BY ?author_ ORDER BY DESC( ?max_n_papers) LIMIT 1 } ?paper a orkgc:Paper; rdfs:label ?title; orkgp:P27 ?author. OPTIONAL { ?author rdfs:label ?author_label } BIND(IF(BOUND( ?author_label), ?author_label, ?author) as ?author_name) FILTER(REGEX(STR( ?title), ""2019-nCoV"")) } GROUP BY ?author ?type ?author_name ?max_n_papers HAVING(COUNT( ?paper) = ?max_n_papers)"	 	False
"
 input (English text): What is the best performing model benchmarking the DuIE dataset in terms of F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the DDI dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DDI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the DuIE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT SORT BY DESC ( ?name ?numberofpages ) ; input (English text): What is the best performing model benchmarking the DDI dataset in terms of F1 metric ? output (Sparql query): output (HTML) The next thing you might want to do is to have the DDI dataset be used to provide a benchmarking system for your test data. I use a benchmarking system to test performance of a model that contains a few hundred records. The benchmarking system takes its measurements when the benchmarking client requests measurements after a model has been added to the system of sorts. After the data has been downloaded using the download request or the update request, the benchmarking software can start collecting data. You should probably look for a benchmarking package that provides a way to perform the request/update without making network calls. A couple of notes about using benchmarking	False
"
 input (English text): What is major reactant for ""Microwave-Assisted Cobinamide Synthesis""?
 output (Sparql query): SELECT ?reactant WHERE { ?papers rdfs:label ?titles. FILTER(REGEX( ?titles, ""Microwave-Assisted Cobinamide Synthesis"")) ?papers orkgp:P31 ?cont. ?cont orkgp:P37557 ?reactant. } 
 input (English text): What is the most common substrate for catalysts?
 output (Sparql query): SELECT ?substrate WHERE { orkgr:R25900 orkgp:compareContribution ?contrib. ?contrib orkgp:P15090 ?substrate. } ORDER BY DESC(COUNT( ?substrate)) LIMIT 1
 input (English text): What coolant does ASTRID reactor use?
 output (Sparql query): "	"SELECT ?coolant ?coolant_label WHERE { ?ASTRID_reactor ?rdfs_label ""ASTRID""^^xsd:string. ?contrib ?reactor_predicate ?ASTRID_reactor. ?contrib ?predicates ?coolant. ?predicates rdfs:label ?predicates_label. FILTER(REGEX( ?predicates_label, ""coolant"")) ?coolant rdfs:label ?coolant_label. } "	"SELECT nrca * nitrate:water FROM nrca WHERE nrca > 0 DESC(nrca DESC(""Nitrate:Water"", ""NMR"")) output (JSON) of output (Sparql query): [ { ""total-nitrogen"": 1000000, ""total-potassium"": 1000000, ""nitrate-to-water"": 0.0056, ""nitrate-to-potassium"": 0.0056 }, { ""total-nitrogen"": 1000000, ""total-potassium"": 1000000, ""nitrate-to-water"": 1.0028, ""nitrate-to-potassium"": 1.0028 },... ] input (English text): What is the concentration of nitrogen per gram of potassium and nitrate ? output (Sparql query): input (English text): What is the density of the total nitrogen mass per gram of nitrogen per kilogram of potassium per cubic millimeter of pressure ? output (Sparql query): input (English text): How many grams of total nitrogen do you think would be the amount of nitrogen in a kilogram of potassium ? output (Sparql query): input (English text): What is the concentration of nitrogen per gram of hydrogen in water ? output (Sparql query): input (English text): How large can a mole of sodium be compared to a mole of potassium ? output (Sparql query): input (English text): What is the distance in cubic feet between the center of the mass of water and the center of the mass of sodium ? output (Sparql query): "	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Ms. Pacman dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ms. Pacman"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Berzerk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?metric ?metric_lbl(Max( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl(Max( ?value) AS ?score) } } ORDER BY DESC( ?value) } ------------- input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Big Bracket dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl(Max( Max( Max( ?value) AS ?score ) ) * Max( Max( ?value) AS ?score ) AS ?score) WHERE { { SELECT ?metric ?metric_lbl(Max(Max(Max( Max( ?value) AS ?score ) ) * Max(Max(Max( ?value) AS ?score ) ) AS ?score) } } ORDER BY DESC( Max(Max( Max( Max( ?value) AS ?score ) ) * Max(Max(Max(Max( Max( ?value) AS ?score ) ) AS ?score ) ) ) } input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Big Caddyshack dataset ? output (Sparql query): -------------------------- SELECT DISTINCT ?metric ?metric_lbl(Max( Max(Max(Max( Max( ?value) AS ?score ) ) ) * Max(Max( Max( :Max( Max(Max(Max( Max( ?value) AS ?score ) ) * Max(Max(Max(Max( Max( Max( ?value) AS ?score ) ) * Max(Max(Max( Max( :Max( Max( Max	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Crazy Climber dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Fishing Derby dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Fishing Derby"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Can you list the models that have been evaluated on the Atari 2600 Skateboard Dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skateboard"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Input in Dataset, data in model. output in Benchmark. Use this if you are uncertain about what model you want to benchmark. In some cases, this"	False
"
 input (English text): What are the most commonly used benchmark datasets for the Document Summarization research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Document Summarization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Text Generation research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Text Generation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Text Summarization research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Text Summarization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	" <tbody> <tr> <th array=""columns"" name=""problem_name"">Solution</th> <th>Count</th> <th>Pairs</th> <th>Quotations</th> <th>Comments</th> </tr> <tr> <td colspan=""2"" width=""100%"" valign=""top"" align=""left""> <h1><a href=""https://parse.parsecdn.org/datasets/problem/1167"" target=""_blank"">Mapping Problems</a></h1> <hr class=""errorMapping""> </td> </tr> </tbody> RAW Paste Data { ""dataset_lbl"": { ""type"": ""Document Summarization"", ""problem"": { ""code"": { ""message"": ""The most commonly used benchmark datasets for the Document Summarization research field are:"", ""status"": ""[[/""]"", ""source"": ""[[]]""} }, ""benchmark"": { ""type"": ""Multiplicative Benchmark"", ""problem"": { ""code"": { ""message"": ""The most commonly used benchmark datasets for the Text Generation research field are:"", ""status"": ""[[/]]"", ""source"": ""[[]]""} }, ""benchmark_rdfs"": { ""type"": ""Quotational RDF"", ""problem"": { ""code"": { ""message"": ""The most commonly used benchmark datasets for the Text Summarization research field are:"", ""status"": ""[[/]]"", ""source"": ""[[]]""} } }, ""comment"" : ""The most commonly used benchmark datasets for the Text Summarization research field are:"", ""contrib"": [ ], ""contrib_lbl"": { ""type"": ""Abstract Document Summarization"", ""problem"": { ""code"":"	False
"
 input (English text): What are the metrics of evaluation over the Atari 2600 Krull dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Krull"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Tutankham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What are the metrics of evaluation over the Atari 8-bit Datafile ? output (Sparql query): Input data: the scores of the games evaluated in Tutankham 2 (see question #1). The data is a 4Mb file, indexed and compressed. Each game has a number of metric parameters: the value of the average score, the score of the top 10 scorers, etc. The score of the ""game itself"". The maximum score for each game is 9999, which has an unknown number of players on the board. The total score for the game is 9999, so the median score is 3, and the mean is 10. The average score for each game is computed by multiplying 1 for the score of the top scorers by the game score (rounded down to the nearest integer). Scores were normalized by computing a mean of all scores and then an average of the standard deviations of the mean. output (Sparql query): Output data: the score of the top scorers in this game. The data is a compressed 4Mb file, indexed and compressed. Each game has a number of metric parameters: the value of the average score, the score of the"	False
"
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Road Runner dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Tutankham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT p.model_id, p.model_name AS model_name, COUNT( ?metric) AS measure_metric ) AS ( SELECT a.lbl.ctr_idx_score as ctr_idx_score, ( SELECT a.metric FROM tz_metrics ) as measure_metric FROM [ test_class_ts ] AS a JOIN [ test_class ] AS b ON a.model_id = b.model_id AND a.model_lbl.ctr_idx_score = ctr_idx_score AND a.metric IN ( ?METRIC_A, ?METRIC_B, ?METRIC_C ) ) AS metric, C_c ( c.cred ) AS score_name, COUNT( * ) AS c_c_lbl_score FROM [ test_class ] AS a JOIN [ test_class ] AS b ON a.model_id = b.model_id AND a.model_lbl_cred = c_c_lbl_score ) AS c ON a.models_id = c.models_id USING c.measurement_metric INTO c, a AS measure FROM [ test_class ] AS a JOIN [ test_class ] AS b ON a.model_id = b.model_id AND a.model_lbl_cred = c_c_lbl_score WHERE a.lbl_cred < c.cred AND a.metric = score_name AND b.metric, c.metric IS NULL AND b.metric < 0 GROUP BY c.metric) AS a JOIN [ test_class ]	False
"
 input (English text): Can you list the models that have been evaluated on the BioASQ dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the BIOSSES dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the PROTEINS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PROTEINS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BaySHUR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Can you list the models that have been evaluated on the OASIS dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dat aset:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OASIS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	False
"
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT29 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the JNLPBA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""JNLPBA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (long) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?model ?model_lbl WHERE { ?dataset a ORkgc:Dataset; ?model rdfs:label ?model_lbl. GROUP BY ?model ?model_lbl } ORDER BY DESC( ?) LIMIT 1 { 1 } input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the OCLC dataset ? output (Sparql query): //output() SELECT ?model ?model_lbl WHERE { ?dataset a ORkgc:Dataset; ?model rdfs:label ?model_lbl. GROUP BY ?model ?model_lbl } ORDER BY DESC( ?) LIMIT 1 { 1 }	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset CommitmentBank?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the models that have been evaluated on the CommitmentBank dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the CommitmentBank dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" In addition to the metrics above, the data is sorted into the dataset as follows: CURRENT_QUOTE_DATASET CURRENT_QUOTE_DATASET_PRIMARY_PROFILE CURRENT_QUOTE_DATASET_SECONDARY_PROFILE CURRENT_QUOTE_DATASET_A_SHORTSTATISTICS CURRENT_QUOTE_DATASET_B_SHORTSTATISTICS CURRENT_QUOTE_DATASET_A_SHORTSTATISTICS_SECOND 1 2 3 4 5 6 7 8 ( ) 2 3 4 5 6 7 8 ( ) The output list of models (e.g.  ""B2"" and ""B1"") is as follows: B2  is the output from the database as it is stored as a table in the database data structure. B1  is the output from the database as it is stored as a table in the database, which has been truncated into the following size, to enable the column definition to be populated: 4 columns The table CURRENT_QUOTE_DA_STRAT has been automatically generated for all of these models. For consistency, the dataset is not listed for B2 and B1, but there is more than one table in the database data structure. Next, let's generate the model parameters with the provided parameters: CURRENT_QUOTE_MAE CURRENT_QUOTE_MAE_TANK_DIFF_RANK CURRENT_QUOTE_MAE_TAB_DIFF_REGRESSION CURRENT_QUOTE_DATASET_A_SHORTSTATISTICS CURRENT_QUOTE_DATASET_B SORT_NUMBER SORT_NUMBER_F"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BiT-S (ResNet-152x4) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet-152x4)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the BiT-L (ResNet) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-L (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model:; orkgp:HAS_SOURCE_CODE ?code. } input (English text): List the code links in papers that use the BiT-N (ResNet) model in any benchmark ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-N (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model:; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Can you provide links to code used in papers that benchmark the BiT-N-L (ResNet) model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-N (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?"	False
"
 input (English text): What is the best performing model benchmarking the ACE 2005 dataset in terms of RE Micro F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of RE Micro F1 metric on the ACE 2004 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" The following two queries describe how the ACE 2005 dataset was partitioned into different levels of relevance among the various level-of-interest metrics at each data level. First the first level of relevance was set to RE Micro F1, and the second level of relevance was set to RE Micro. The query presented here is the second query. The ACE 2008 dataset was partitioned into different levels of reference and relevance, corresponding to the different level-of-interest metrics and RE Micro metric. Note that the reference level has three labels and a two column definition for each one. The next level of relevance, RE+ Micro, is also defined by two columns with two rows for each metric and RE Micro. The reference level, RE+ Micro F1 is used within the same partition. COUNT(1) COUNT(1) is one of the oldest and simplest, and for that reason has always been utilized by the most sophisticated model benchmarking programs. In the ACE 2005 dataset, the first level of relevance was set to RE Micro, the second level of relevance to RE+ Micro, and the third level of relevance, RE Micro F1, was used within the ACE 2008 dataset. RE Micro F1 level-of-interest: RE Micro F1 metric: 1 The second level of relevance is RE+ Micro F1 metric, meaning that the ACE 2005 RE+micro dataset, and in particular its 1-repetition average score, is referenced as ""RE+Micro F1."" The RE+1 level-of-interest is used within the same partition as the RE Micro F1 level-of-interest, thereby allowing performance comparisons between these two different RE+micro datasets. RE Micro level-of-interest: RE+ micro metric: 2 RAPID PROCESSING RE Micro F1 metric: 2 This RE Micro F1 level-"	False
"
 input (English text): What is the top benchmark score and its metric on the BBCSport dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BBCSport"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Nottingham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp: HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl "	True
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the STEM-ECR v1.0 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STEM-ECR v1.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset A orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CSE (STEM-ECR)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } This section is still an ongoing draft; expect more examples to be added Note that not all of the results that we've produced are the exact same as those you get in a benchmark. So one of the most useful exercises here is to use some of these examples in your own Benchmarks! As an example, since we only compare ""default"" metrics, for comparison here we'll start at a new dataset, and create a benchmark that can be run against that dataset. Let's take a look at a new dataset, which is built primarily from the AST dataset, though we also use the ERC-8 benchmark. We'll then benchmark it against the standard ARCHIVE dataset. Step 1. Preprocessing the dataset This is the first thing that we'll need to do before we begin. To prep the dataset for use in our Benchmarks we'll need to prepare a new dataset. I am doing this in two-phase, since using more than one step at the same time can be tedious/inconvenient. In phase one, we start off with constructing the AST dataset, in the format that we want. However, since we are already using a particular data type and we don't want to include it in this format"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Multi-Format Contrastive model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Format Contrastive"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the multi-head model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""multi-head"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Multi-Perspective Matching (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" ""This is a great paper (in both English and Spanish). The multi-format contrastive model gives a very fast and flexible analysis of text and image data."" - John M. Johnson PhD, MS, CSCS. ""Excellent paper! It is definitely in our list of some of the most important papers for the topic so far. The analysis shown is remarkable and we would recommend it to any person that works in data processing and vision science."" - María del Mar, LMP (Director of the Institute of Advanced Studies). ""Congratulations to Dr. Yves for this outstanding piece of work."" - Dr. Philippe A. Staudte, Professor of Computer Science, Institute of Advanced Studies in Nantes. The multi-image dataset is available from the Bioinformatics Online Resource (BIOLR). The benchmarking analysis is available from Bioinformatics Tools and Methods (BIOTM). The output is also available for download in a format readable by Bioinformatics Online Resource (BIOLR). Please consult the papers listed below for additional articles demonstrating the performance of the multi-format contrastive model. Biological Images In this publication, Sauer, et al. present (PDF) and experimentally verify (PDF) the accuracy of the single image comparison (single multi-image comparison) model in identifying human (Aubrey et al. 2013 ), dog (Sauer et al. 2011 ) and rat (Liu et al. 2011 ) body condition for various stimuli (i.e., faces, hand-rubbing and foot-rubbing). They demonstrate that single image comparison (single image model) models are able to correctly identify body condition, face and hand-rubbing and foot-rubbing patterns for the most commonly used stimuli, as well as for many stimuli that are considered undesirable for an evaluation of body condition. In"	False
"
 input (English text): List the metrics that are used to evaluate models on the GAD benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the GigaWord dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GigaWord"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the GAD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?lbl ?model ?lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_metric_lbl_lbl as ?model_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl RDD [1] output (Sparql query): SELECT ?lbl ?model ?lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl orkgp:HAS_TEMPLATE ?lbl_lbl_lbl _lbl_lbl_lbl. OPTIONAL { ?model_lbl orkgp:HAS_TEMPLATE ?lbl_lbl_lbl _lbl_lbl_lbl. } { ?lbl_lbl_lbl _lbl_lbl_lbl] OPTIONAL { ?lbl_lbl_lbl _lbl_lbl_rbl. } } If the above query failed for any reason, please report to us the reason. We try to respond to all messages within 24h. output (Sparql query): SELECT ?lbl ?model ?lbl_lbl_lbl orkgp:HAS_TEMPLATE ?lbl_lbl_lbl _lbl_lbl_lbl. OPTIONAL { ?lbl_lbl_lbl _lbl_lbl_rbl. } { ?lbl_lbl_lbl _lbl_lbl_rbl]. OPTIONAL {	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the PubMed 20k RCT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the CIFAR-10 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the PubMed 20k RCT dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 output data set (Medline): data type_citation_criterion(data_description) = {criterion} criterion is a data type used for selecting the metrics of evaluation, i.e., metric_lbl is replaced with criterion_lbl. If the criterion does not exist, criterion is calculated from the criterion_lbl of the input data.	False
"
 input (English text): Could you provide a list of models that have been tested on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Habitat 2020 Point Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT option_name( ?option_cnt)  option_cnt = ?option_cnt *  option_lbl = ?option_lbl *  option_lbl orkgc:Dataset orkgc:HAS_DATASET orkgc:HAS_EVALUATION orkgc:HAS_METRIC orkgc:HAS_MODEL orkgp:HAS_PRIMARY_ARGS   ?option_cnt as ?metric = ?metric_cnt. OPTIONAL orkgp:HAS_META   ?option_cnt as ?eval = ?dataset *  option_cnt = ?metric_cnt  orkgp:HAS	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ARC-PDN dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AESLC dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AAPD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ROLIST(,1,,1,1,( ?tab)) FROM (SELECT ROLIST(,1,,1,1,( ?tab))) ? input (English text): Give me a sequence of strings, either with or without spaces, where each string is separated by tabs and spaces and given in	False
"
 input (English text): Where can I find code references in papers that have used the XLNet (single model) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the NAS Cell model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NAS Cell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the AxCell model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AxCell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE ( ?model is:AxCellModel ?db_id < 6 ?samples_per_cell < 100 ?range : 0.. ?samples ?range. ?number_of_cells : ?max_indexes ?cell_index, ?max_features : ?min_indexes ?cell_index, ?cell_num: ?max_num ?cell_num, ?dataset : ?model_lbl. ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AxCell"") ?benchmark. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input_file (data_file): The raw data files needed for the benchmarking. output_file (output_file): The file output when processing the outputs. include_files (all_includefile) : Include each of the included files in the benchmarking suite include_file (path_to_include_file) : The path into which the include file starts. exclude_files (all_excludefiles) : Include all excluded files from the benchmarking suite source_file (path_to_source_file) : The path to the file the source code will be embedded in. output_file (output_file): The file output when processing the outputs. source_file (path_to_source_file): A list of source files for the benchmarking suite. include_file_path (path_to_include_file_"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM-MoS model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-MoS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM-DOC x5 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-DOC x5"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-DOC x5"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Input Data The data is of the same type (LSTM). Example input data: The data is of the same type (LSTM). Example input data: input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM-DOC"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BioASQ dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMedQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedlineQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } select  c  from  result  ( ... select  c  from  result  (... select C i,    r  from  result  (... select V  ,   [RDFS_VARIATIONS,  FTR_CHARS_SEMCODE,    RDFS_TRANSFORMED_CHARS ] from  result  ( ... select  N  ,    a  from  result  (... select N  ,   a  from  result  (... select N  ,   a  from  result  (... select N  ,   a  from  result  (... select N  ,   a  from  result  (... select N  ,   a  from  result  (... select P  ,   a  from  result  (... select P  ,   a  from  result  (... select P  ,   a  from"	False
"
 input (English text): What are the most commonly used benchmark datasets for the Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification. research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification."") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Name the datasets that have been used for benchmarking in the Text Classification research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Text Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Name the datasets that have been used for benchmarking in the Image Classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. }  Select all  or all datasets; use the output field to include any result. Note that if this option is given as the only argument, a single output field is created, which will be the result set. For example, output(DISTINCT ?dataset ?dataset_lbl). { ?problem a ?dataset_lbl. } This will produce two outputs of the data.  See the Outputs section of the Finding an efficient and state-of- the-art approach for more information on the possible output types. If the output is a list variable with the name of a dataset in this output field, it will be printed in the output. For example, Output(DISTINCT ?dataset ?dataset_lbl).dataset would display DISTINCT dataset_lbl. Note that, unlike the Output field, the Output field also accepts an optional second argument, named labels, which may contain a comma-delimited list of variables to identify data used for benchmarking. In the example above, DISTINCT dataset_lbl= ?labels. A few options are available for choosing between data, labels, and both. The following"	False
"
 input (English text): Could you provide a list of models that have been tested on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Habitat 2020 Point Nav test-std dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Gibson PointGoal Navigation dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metment_lbl OPTIONAL 1 ?max( ?value) IF 1 { ?metric ?metment_lbl ?value } ?else-null 1 ; } } If your question is, for example, whether your graph is the shortest in any of the 3 axes, in this case, an optional input parameter is necessary. If your question is, for example, whether your graph is the shortest in any of the 3 axes, in this case, an optional input parameter is necessary. If input = 1, it indicates that this graph is the shortest in all 3 axes. If input is less than 1, then the input value is ignored. If input is greater than 1, then it is substituted for the missing values in this output query. If you want to perform a more detailed analysis on a graph, you can use the query function s.query(graph_name, max_rows_per_column). The output can be used to inspect the data within the query in more detail. The full examples below include the basic parameters for each type of query. The output that is produced from the above queries can be obtained from the.query(graph_name, max_rows_per_column) or.graph_data_parsed functions.	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cheetah, run (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What models are being evaluated on the Cheetah, run (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Score score on the Cheetah, run (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Note: ""cheetah"" is the name of the benchmark dataset, and ""test"" is the title and description of the model.   Results We also ran the ""Cheetah, run (DMControl500k)"" model on ""Metric Cheetah"", with the same dataset. Here is the results for ""Metric Cheetah"". Metric Cheetah Cheetah, run (DMControl500k) 500k 100k 10k 20k 30k 100k Metric Cheetah Metric Cheetah 50k 100k 20k 30k 50k 100k Metric Cheetah Output: metric_lbl R 1 0.99 0.93 0.94 0.93 0.94 0.97 0.96 rdfs:label R 0 0.81 0.75 0.84 0.81 0.84 0.81 0.76 orkgp:HAS_DATASET R 0 0.93 0.97 0.98 0.91 0.91 0.90 0.91 orkgp:HAS_EVALUATION R 0 0.92 0.99 0.91 0.93 0.92 0.91 0.85 test R 1 0.99 0.94 0.95 0.96 0.97 0.96 0.96 (Sparql query) R 1 0.99 0.95 0.96 0.95 0.96 0.96 0.96 (table column names only) DISTINCT Cheetah, R 1 0.91 0.87 0.87 0.92 0.93 rdfs:label R 0 0.94 0.92 0.94 0.88 (Parsing) R 1 0.99 0.94 0.94 0.95 0.99"	False
"
 input (English text): What is the top benchmark score and its metric on the Stanford Cars dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Stanford Dogs dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Dogs"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Stanford Dogs dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Dogs"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?metric ?metric_lbl; output (Hashed): { {SELECT ?metric ?metric_lbl. ?METRIC rdfs:label ?metric_lbl. } } input (English text): What metrics are frequently used when benchmarking models on the Stanford Cars dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl_modifiers. ?METRIC rdfs:label ?metric_lbl_modifiers. ; output (Hashed): { {SELECT ?metric ?metric_lbl_modifiers. ORDERS rdfs:label ?metric_lbl_modifiers. OPTIONAL } } input (English text): Which metrics are commonly used when benchmarking models on the Stanford Dogs dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl_metrics. input (English text): To improve readability, we will convert this query to its original form: output (Sparql query): SELECT DISTINCT ?metric ORDERS ORDERS rdfs:label ?metric_lbl ORDERS rdfs:label ORDERS rdfs:label ORDERS rdfs:label ORDERS rdfs:label ORDERS rdfs:label ORDERS rdfs:filter { ?cont orkgp:HAS_	False
"
 input (English text): Indicate the model that performed best in terms of Senseval 3 metric on the Supervised: benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Senseval 3"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the RACE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Senseval 2 metric on the Supervised: benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Senseval 2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Speed"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Race"") ?benchmark orkgp:HAS"	False
"
 input (English text): Indicate the model that performed best in terms of Test perplexity metric on the Penn Treebank (Word Level) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the WikiText-2 dataset in terms of Validation perplexity metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Validation perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-103"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl FROM ?model | ?metric_lbl ) rdfs:label ?model_lbl ORDER BY desg DESC LIMIT 1 andgrammets:label ?model_lbl ORDER BY desc DESC LIMIT 1 input (English text), output (Sparql query): Select the best performing model benchmarking the WikiText-106 dataset in terms of Validation perplexity metrics ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Validation perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-106"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text), output (Sparql query): Select the best performing model benchmarking the WikiText-108 (English text) dataset in terms"	False
"
 input (English text): What is the top benchmark score and its metric on the WMT2016 Russian-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Russian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WMT2016 Romanian-English dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?value = ROW_NUMBER() / 1, 1.0 AS ?metric_lbl. FROM k1 k2 INNER JOIN k3 ON k3.k1 = k2.k3 ) -------------------------- input (English text): What is the current state of the benchmarking process ? output (Sparql query): Select the most recent benchmark of the Russian-English dataset and whether its metric was achieved. result (Sparql query): Select the most recent benchmark of the English-Russian dataset and whether its metric was achieved. input (English text): What is the current status of the WMT2016 RUS-1 dataset ? I haven't reached the goal yet... output (Sparql query): SELECT DISTINCT ?metric_lbl (MAX( ?value) AS ?score ) FROM ?metric_lbl GROUP BY ?metric ? -------------------------- -------------------------- Metric | Score | --------------------- 61813 ( 1 ) 7.0 1 ( SELECT ?value = ROW_NUMBER() / 1, 1.0 AS ?metric_lbl. FROM k1 k2 INNER JOIN k3 ON k3.k1 = k2.k3 ) -------------------------- -------------------------- input (English text): What is the current status of the WMT2016 RUS-2 dataset ? I haven't reached the goal yet... output (Sparql query): Select the most recent benchmark of the English-Russian dataset and whether its metric was achieved. result (Sparql query): Select the most recent benchmark of the English-Russian dataset and whether its metric was achieved. input (English text): What is the current status of	False
"
 input (English text): What is the most common substrate for catalysts?
 output (Sparql query): SELECT ?substrate WHERE { orkgr:R25900 orkgp:compareContribution ?contrib. ?contrib orkgp:P15090 ?substrate. } ORDER BY DESC(COUNT( ?substrate)) LIMIT 1
 input (English text): How many studies do use Chloride as major anion?
 output (Sparql query): SELECT COUNT( ?anions_labels) AS ?chloride_count WHERE { orkgr:R110597 orkgp:compareContribution ?contrib. ?contrib orkgp:P37458 ?anions. ?anions rdfs:label ?anions_labels. FILTER(REGEX( ?anions_labels, ""Chloride""^^xsd:string)) } 
 input (English text): What is the most common lead compound?
 output (Sparql query): "	SELECT ?compound WHERE { orkgr:R75638 orkgp:compareContribution ?contrib. ?contrib orkgp:P35194 ?compound. } ORDER BY DESC(COUNT( ?compound)) LIMIT 1	SELECT ?lead_list, ?lead_number, ?lead_formula, ?lead_table, ?lead_data, ?lead_indexes, COUNT( ?lead_list ), COUNT( ?lead_number ) AS lead_count, COUNT( ?lead_formula ) AS lead_formula, ?lead_table, ?lead_data, ?lead_indexes, ROUND(ROW_NUMBER() / 1000000) AS lead_index_count, COUNT( ?lead_table ) AS lead_table_count, COUNT( ?lead_data ) AS lead_data_index_count, ?lead_indexes AS lead_index_indexes, ROUND(ROW_NUMBER() / 1000000) AS lead_index_index_count, COUNT( ?lead_data_index ) AS lead_data_index_index, ROUND(ROW_NUMBER() / 1000000) AS lead_data_index_index, ROUND(ROW_NUMBER() / 1000000) AS lead_data_index_index, COUNT( ?lead_database_index ) AS lead_database_index_get_index input (English text): What is the most common lead compound ? output(Sparql query): SELECT ?lead_list, ?lead_number, ?lead_formula, ?lead_table, ?lead_data, ?lead_indexes, ?lead_indexes, COUNT( ?lead_list) AS lead_count, COUNT( ?lead_number ) AS lead_count, COUNT( ?lead_formula ) AS lead_formula, ?lead_table, ?lead	False
"
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Road Runner dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Enduro dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT * FROM quagga_games.gms WHERE gtype = 'Atari' ); output (MORPACK) { ?data row | ?data item ?data | ?data label ?data_lbl - ?data value ?data_lbl } The top performing model for Atari 2600 Adventure from score score = 10.049 is ""Dragon Commander."" This model is not in the standard classification system. input (text): User submitted data: Number of games played Score score = 10.049 ? ?game:Game name; rdfs:label ?game_lbl; ORD:ID ?id; output (HASH) 1 2 3 4 5 7 CREATE TABLE game_lbl ( game game_name text NOT NULL, rdfr df_rng rng DESC ) IF EXISTS ( SELECT * FROM quagga_games.gms WHERE g"	False
"
 input (English text): Provide a list of papers that have utilized the C51 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C51"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the DDQN (tuned) noop model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN (tuned) noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the C51 noop model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C51 noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" Sets the default output format for the model in the paper. Note that we use the term model in this example to refer to the RDF data as well as the paper. output (Sparse JSON): Specify an output file/format, if you desire to output a specific data/marker pair. The output data will be formatted in a way consistent with the paper in a way that allows the corresponding RDF, or HTML pages to be displayed or parsed. NOTE: As of this writing, we currently support output formats as follows: (Note: All output formats support the same output options in an HTML file.) filename (String): A filename can be specified in one of two ways: Use of the "" ?"" operator can be used to indicate that the RDF should be in JSON form, and that the full RDF should be in an XML namespace-style tag. Use of the ? operator can be used to indicate that no file/format should be specified. output (String): The exact file or format of the output data may vary, but the output will be a RDF or document containing the data. Output options. The following options are generally available (in ascending order of relevance): ?input: Output in HTML or RDF, where the data is to be extracted from a particular HTML source. ?output: Output in JSON or RDF, where the data is to be extracted from a particular RDF namespace-style tag. ?code: Output the data at a certain metadata key that corresponds to the paper data. ?benchmark: Output the data at the specified benchmark index that corresponds to the paper data. This will produce the same output as the ?output option in that paper, but this time with the benchmark index in the same order as that used in the paper in the HTML document; note that this benchmark"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ning et al."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the OverFeat - 7 accurate models model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""OverFeat - 7 accurate models"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the FABIR model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FABIR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT pk FROM 'A*') AT ((15,8)): PASS FABIR-7_AMP-FLEX_PERCENTAGE (A) (A = 12.85, B =.12, C = 0.0, D = 0.0, E =.26, …, F = 0.09, 1.0) FABIR-7_AMP-FLEX_PERCENTAGE (A*) (A = 12.85, B =.12, C =.0, D = 0.0, E =.26, …, F = 0.09, 1.0) FABIR-7_AMP-FLEX_PERCENTAGE (B*=19.35, L=0.0, Q=1.0, P = 0.0, R = 0.0, X-Y=-1.0)* FABIR-7_AMP-FLEX_PERCENTAGE (C*) A=12.85, B=.12, C=.0, D=.0, E=.26, …, F=.09, 1.0 B=16.0 W=1.0, L=0.0, Q=1.0, P=0.0, F=0.06, 1.0) FABIR-7_AMP-FLEX_PERCENTAGE (D*) B=16.0 W=1.0, L=0.0, Q=1.0, P=0.0, F=0.06, 1.0) FABIR-7_AMP-FLEX_PERCENTAGE (E[7,1] = 19.35) FABIR-	False
"
 input (English text): What are the objectives for Sepsis prediction?
 output (Sparql query): SELECT DISTINCT ?objectives WHERE { orkgr:R70642 orkgp:compareContribution ?contrib. ?contrib orkgp:P15051 ?objectives. } 
 input (English text): What is another name for Bisphenol A?
 output (Sparql query): SELECT ?same_name_label WHERE { ?resources rdfs:label ?resources_labels. FILTER(REGEX( ?resources_labels, ""Bisphenol A"", ""i"")) ?resources orkgp:SAME_AS ?same_name. ?same_name rdfs:label ?same_name_label. } 
 input (English text): What is the emergency type of AMBER Alert?
 output (Sparql query): "	SELECT DISTINCT ?emergency_type WHERE { orkgr:R153500 orkgp:P31 ?contrib. ?contrib orkgp:P43093 ?emergency_type. } 	" output (Excel spreadsheet): I've written the following test scenario to check whether or not I can find a query to predict the type of an emergency. It's a simple SPSS formula, and there's nothing out of the ordinary in it. I've written this in SPSS. It uses all functions called on the same axis as BLS-OES, such as getType. It calculates all the numbers as SPSS values, but I'm not exactly sure how some of the numbers are calculated. There may be some errors elsewhere in it. My apologies. This test code is quite difficult to follow; I've tried to keep it simple. To avoid breaking the script slightly, I've created an empty data set that contains only a few million values. Please forgive me for this. If you read this far into the article, you should be in good shape; I'm sure you can figure out how to use the functions in this text. (If you know how to, please tell me if you're following along) #This is an example of what a variable called 'A' might look like. There are four columns to it: #A1 A2 A3 A4 #And three variables named 'A', 'B', 'C'. A1 = [1 0 1 1 0] B1 = [1 1 0 0 0] C1 = [100 0 110 110 110 0]. #SPSS-SQL code - see how the first three variables are used A1, B1, C1 = SPSS.getVariable(""A"", 4, 4) Rows 2,16 to 6,718 have been assigned to the variable 'A'. It looks like the columns 'x' and 'B' are being used as 'x' and 'C' as 'B', respectively. So"	False
"
 input (English text): List the metrics that are used to evaluate models on the OpenBookQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OpenBookQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the PIQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WCA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the ECTA benchmark dataset ? output (Sparql query): input (English text): List the metrics that are used to evaluate models on the MAST benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MAST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the COQA benchmark dataset ? output (Sparql query): SELECT D"	False
"
 input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the CommitmentBank benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the OntoNotes benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 As noted at the beginning of this section, we decided to use the database from the first version of the dataset, OntoNotes, since our analysis relied primarily on the performance of the Hamilton-Middletons F1 metrics, which could be used directly in our queries without any need for modifying Onto Notes themselves. For many features, the data was limited to a handful of metrics (e.g., distance from the first to last value or the mean of the number of points), thus we decided to use an independent dataset from the second version of the dataset, OntoNotes 2, instead of the third release of the dataset. As can be expected, we ran into more difficulties than normal in analyzing this dataset. The most obvious issue was the difficulty in detecting outliers: The Hamilton-Middletons metrics are based on the distance to the first and last value, and the Hamilton-Middletons results are based solely on the mean (the sum of all numbers from 0 to 100). In order to properly analyze such outliers in a way that would be consistent with our approach, we had to either make a manual adjustment on the dataset, or rely on the existing data for that particular metric in our analysis. For this situation, we chose to use the Hamilton-Middletons dataset as our reference dataset. Because the dataset contained many outliers, we decided to use a modified approach to identify outliers: We chose not to use any of the two-value-per-point metrics in the Hamilton-Middletons metric, instead using a standard two-value-per-metric metric for the same metrics (e.g., the distance formula: DISTANCE(DISTANCE(RUNNING(DIST(DIST(RUNNING(DIST(RUNNING(RUNNING(RUNNING)))))	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 James Bond dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 James Bond"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"------------------------ --------------------------- 0 "" Atari 2600 Defender (0) "" output (Sparql query): -------------------------- -------------------------- 0 "" Atari 2600 Defender (1) "" output (Sparql query): -------------------------- -------------------------- 0 "" Atari 2600 Defender (2) "" input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Flashback dataset ? output (Sparql query): ------------------------ --------------------------- 0 "" Atari 2600 Flashback (0) "" output (Sparql query): -------------------------- -------------------------- 0 "" Atari 2600 Flashback (1) "" output (Sparql query): -------------------------- -------------------------- 0 "" Atari 2600 Flashback (2) "" input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Ice Climber dataset ? output (Sparql query): ------------------------ --------------------------- 0 "" Atari 2600 (0) "" output (Sparql query): -------------------------- -------------------------- 0 "" Atari 2600 (1) "" output (Sparql query): -------------------------- -------------------------- 0 "" Atari 2600 (2) "" output (Sparql query): -------------------------- -------------------------- input (English text): What are the titles and IDs of research papers that include a benchmark for the Sega Game Gear dataset ? output (Sparql query): ------------------------ --------------------------- 0 "" Game Gear (0) "" output (Sparql query): -------------------------- -------------------------- 0 "" Game Gear (1) "" output (Sparql query): -------------------------- -------------------------- 0 "" Game Gear (2) "" input (English text): What are the titles and IDs of research papers"	False
"
 input (English text): Where can I find code references in papers that have used the Contextual Match model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Contextual Match"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Class Diagram model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Class Diagram"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Pointer + Coverage + EntailmentGen + QuestionGen"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Pointer plus Coverage plus Entailment gen""); OR } OR ; OR }. input (English text): Where can I find code references in papers that have used the Contextual Match model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Contextual Match""); OR } or ; OR }. input (English text): You have to create a new Query with a dummy model, so that we ignore the original query. output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Contextual Match""); OR } OR { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Contextual Match"" ). }. input (English text): You don't have to create a new Query. But if you want to, you can do: # Create a different query to filter queries that look ""not useful"" since they are not part of any query. Here we can find the code references of the papers using the Contextual Model. For example, we can find a code reference for the code of the original paper: # select a.id, a.type, a.name as n1, n2 from papers WHERE i=1; n1"	False
"
 input (English text): Provide a list of papers that have utilized the Transformer-XL model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Transformer-XL (24 layers) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL (24 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Switch Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Switch Transformer"" ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Switch Transformer"" ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } output (English text): Provide a list of papers that have utilized a Grid Transformer in the ""Flip"" model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model { ?column 0 orkgc:Column; ?value 0.05 } }. ?dataset orkgp:HAS_INPUT ?dataset. ORGGP ?dist. } input (English text): Provide a list of papers that have utilized a Grid Transformer in the ""Flip"" model ? output (Sparql query"	False
"
 input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the OpenBookQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OpenBookQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" Example Here, you'll see a sample query that calculates the average of three models for each of three input datasets set up by using the two models in the first query. Note that the queries don't use the models from the two other outputs, so both datasets have less data than the datasets for the first query. -- query query_2 = { -- the data we see for the first model. orkgc:Dataset ORIGLCNT=2 OROGCNT=3 ; orkgp:HAS_DATASET ORIGLNTR=""CommonsenseQA""; rdfs:label OROGPNTR=""OpenBookQA""; benchmark:True OROGPBTX:True OROGPBTX:False ORAGRTX:True "" "" ORAGRTX:False "" "" -- the data we see for the second model. orkgc:Dataset ORIGLCNT=3 OROGCNT=2 OROGCNT=4 ORAGNTX=True; orkgp:HAS_DATASET ORIGLNTR=""CommonsenseQA""; rdfs:label OROGPNTR=""OpenBookQA""; benchmark:True OROGPBTX:True OROGPBTX:False ORAGRTX:True "" "" ORAGRTX:False "" "" -- all the data for both the two datasets. orkgc:Dataset ORIGLCNT=2 OROGCNT=4 ORAGNTX=True; orkgp:HAS_DATASET ORIGLNTR=""CommonsenseQA""; rdfs:label OROGPNTR=""OpenBookQA""; benchmark:True OROGPBTX:True OROGPBTX:False ORAGRTX:True"	False
"
 input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Amazon-5 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Amazon-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 1.4.5. Input data In-memory data may consist of a database table where the entire contents are available, an in-memory representation of the table, or an out-of-memory representation of the table. In all cases, the input data must be identical. The metrics that would determine success in this section are based on two types of metrics, aggregate metrics and model metrics. All other metrics remain untouched. In the case of a model, such as the ones presented in the previous sections, the inputs can be the complete metrics of the model as stored in the store. That definition allows us to combine the features for each metric with those from the stores to achieve a unified result. Using the same data, we can use the following process to evaluate which of the metrics is most important for a model: Use the same collection of metrics as provided in the model: use a combination of features and properties (e.g., the number and types of models), or use the exact same model metrics. Compute the most important properties of the model and combine them with the properties from the stores (the model properties in S3 and SNS). For example, consider a multi-year model that involves the use of the model of an Amazon Prime member and a Netflix producer to calculate the per month per user cost. The model properties include: total subscribers, monthly used subscriptions, per month per user cost, and total user cost. The combination of the data from both stores, and then the property values computed for individual members, gives us the following composite metric: annual unit cost (AUV): $17,000 Monthly used per subscriber (MB): $0.67 In the above example, we are using the property that includes each member by their age. By combining the properties from both stores, we have: annual unit cost (AUV) = $17	False
"
 input (English text): List the code links in papers that use the DDRL A3C model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDRL A3C"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the A2C+CoEX model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A2C+CoEX"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the A3C-CTS model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C-CTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 To output the results to disk, create an executable package which uses the format for this report. For example, to make this report executable as a.sql/dgrl-benchmark-report package by running: $ cp -r dgrl-benchmark-report /usr/local/lib/pkgconfig/bin/ Alternatively, one may install oracle_db and libfusion4sql. To run the package, use the following two commands: $./ dgrl-benchmark-report.sql $./db/dgrl-benchmark-report.db The reports (e.g., test results) are saved in SQL format in a directory named test_dir when created, or test_dir/sdb when the package is started. For more information on running the package or creating test results, see the dgrl-benchmark-report package documentation page. Compiling from source¶ The dgrl-benchmark-report package can be built from source, using the -G option, which specifies the package configuration file, the command to be invoked when compiling the package, and the environment to be executed when doing so. Note The dgrl-benchmark-report package creates a.sql file, and each script included with the package is stored in that file, containing its SQL script ID. This means that the commands which are executed will be stored in the global session and they are also stored in a particular session directory associated with the DSPs database. To use the.sql file as the configuration file, change the source parameter to.sql, and then use the command: \ -g dgrl-benchmark-report.sql $./db/dgrl-benchmark-report.db To run the package with a command or script, use one of the following syntaxes: $./	False
"
 input (English text): What is the top benchmark score and its metric on the CoNLL 2012 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the  Jacquard dataset dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Hutter Prize dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?value ?value_lbl (MAX( ?value) AS ?score) FROM national_aerobiology.scores WHERE NATIONAL_AERO_AND_ SPACE. CURRENT_NAME = 'NASA' AND QUALCOMM. SET QUALCOMM. SET TOP_PROGRAM_MODEL. SET TOP_PROGRAM_MODEL. SET NATIONAL AERO_AND_ SPACE. TOP_PROGRAM_BASE SET QUALCOMM. SET TOP_PROGRAM_BASE output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { VALUES( ?value ) } SELECT DISTINCT ?metric_lbl ?value FROM national_satellite_labs.scores WHERE NATIONAL_AERO_AND_ SPACE. CURRENT_NAME = 'SAT' OR QUALCOMM. SET QUALCOMM. SET TOP_PROB : SET TOP_PROB	False
"
 input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score on the ACE 2004 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2003 (English)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Which model has achieved the highest F1 score on the NHMP 2003 benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F1 score on the F1 2002 benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F1 score on the G-LATEC benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F1 score on the TOCA benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F1 score on the G-LATEC benchmark dataset ? output (Sparql query):   input (English text): Which model has achieved the highest F1 score on the G-LATEC benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F1 score on the G-LATEC benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F1 score on the TOCA benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F1 score on the G-LATEC benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F1 score on the G-LATEC benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F1 score on the G-LATEC benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F1 score on the NH	False
"
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Hutter Prize dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of PARAMS score when benchmarked on the Flowers-102 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" 1.1.2. B.4. Using Hutter Prize models This section applies only to datasets that have at least one Hutter Prize model in them. These datasets are the default Hutter Prize datasets for all models below, and are used to benchmark Hutter Prize models. Please refer to your Hutter Prize database for further details. Please note that the models are specified individually so if you specify any two different models for benchmarking two datasets the benchmarking will be performed against all samples, not just those using the corresponding model. B.8.7. Top performing Hutter Prize models: Results for multiple models for the same dataset The list of models from each dataset has its own section in this manual. The first column in RDFS has ""model"" as the value, and the name of the top performing model, but the second and third columns specify how many parameters from the top performing model that are used as the parameter(s) to the Benchmark function for the benchmarking test. Only the last two columns of the result are mandatory in the benchmark statement: Benchmark uses the params variable to define the parameters that are used to define the model. If there are any parameters that are not defined in params, the model is not benchmarked. Only the first model parameter is used to define the benchmark parameter. If the last model parameter is either equal, or even positive, to the same parameter in one of the two models, the latter model is tested for a different measure in addition to the benchmark parameters. Note that the model parameter names shown in the benchmark results can also be edited after the benchmark statement. Note also that if the benchmarks were originally used with some additional parameters that were included in params, no benchmark will be calculated by the benchmark function before modifying the parameters. Only model parameter values defined in the first row of the report will be included in the final result. To"	False
"
 input (English text): Can you list benchmarked problems in the area of Computer Sciences?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Computer Sciences"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you list benchmarked problems in the area of Information Science?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Information Science"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you list benchmarked problems in the area of Artificial Intelligence?
 output (Sparql query): "	"SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Artificial Intelligence"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } "	 If the answer is Yes, we'll output this. Note, that we can use multiple queries to answer this question, so you can see how to write a query such that you get all answers. Here's a simple example: select id, name, score, solution, author, description from   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ?   ? score (  { ?id   ?id_lbl   ?name   ?score   ?solution ?author   ?description   ?solution_lbl   ?answer   ?author_lbl   ?summary   ?solution_summary   ?answer_lbl   ?author_summary   ?solution_summary   ?solution_summary_lbl   ?solution_summary_summary   ?solution_summary_summary_lbl   ?publisher   ?publisher_lbl   ?solution   ?publisher   ?publisher_summary   ?publisher_summary_lbl   ?publisher_name   ?publisher_name_lbl   ?solution   ?publisher_name_summary   ?publisher_title   ?publisher_title_lbl   ?publisher_year   ?publisher_year_lbl  } ?   ?  ; ?     ?   ?  	False
"
 input (English text): What are the models that have been benchmarked on the SNLI dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the SciFACT dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciFACT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the FSNS - Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (FSSN text): output (Sparql query): ORGSCO_SCORES (A/B, N=2, S=5): I/O in Postgres, and with Postgres, you can have a file that is readable by the other users but not yet written to disk. Postgres provides a file system stream in parallel, and it's a common use case that is not covered in this post in any detail. When you have the same model stored in the same database and also in a different database, both databases can access it as a file system stream. What is this file system ? I/O in Postgres, in Postgres there is basically only one data structure called a flat file. For simplicity reasons, what we've been talking about is a simple tree. A flat file in Postgres is always flat, this is true even when the file system is in multiple partitions that are in different volumes. This simplifies performance-sensitive operation, where the file system is used to support multiple partitions at different volumes using different data formats and storage. The Postgres file system is an atomic object as soon as you read it, and if you wanted to use two databases from one flat file, you'd have to go through a process of merging them to get two different databases. It would be a messy operation. This is the same as having several files in different locations and having to go through these file system streams to get them to merge into one single stream again, or to move one data structure from a different directory to another one. The data structure itself stays the same and you'll see, it also means that you can have an entire tree in Postgres that represents a column in the data, it's impossible to access any part of it. The flat file is simple to use,	False
"
 input (English text): What is the best performing model benchmarking the WMT2014 French-English dataset in terms of BLEU metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the WMT2016 English-Romanian dataset in terms of BLEU metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Input data entered by user before benchmarking The following is data entered by user before benchmarking: The list of model benchmarking the WMT2014 German-English dataset. The data set used for benchmarking the WMT2014 German-English dataset: The French-Romanian dataset The French-Romanian dataset."	False
"
 input (English text): Where can I find code references in papers that have used the ELMo model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Attentional encoder-decoder + BPE model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Attentional encoder-decoder + BPE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-Attention + ELMo model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Can you provide links to code generated from papers that benchmark the Attentional-Attentional encoder-decoder + ELMo model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Attentional-Attentional encoder-decoder + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Input: English text: Where can I find an example of this model ? Output: Example: ELMo model output: ELMo model: Attentional encoding model (Attentional encoding model - English text) The following example uses the ELMo model from the paper: M. D'Almarcon, A. Lira, and M. Eberstadt. 2011. Attentional Coding of Sparse Input: A Parallel and High-performance Encoder"	False
"
 input (English text): What is the highest benchmark result achieved on the WSC dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the STL-10 dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"--------------------------- Result: The following table summarizes the benchmarks for each metric specified by the metric_lbl. Note that the benchmark names, descriptions and the metric values are provided by R. In the examples, the performance metric parameters are provided by WSDL (see below). The columns in the table are intended for convenience; in practice such metrics do not exist in a consistent way in WSDL and WSC. The table uses R::Benchmark::Benchmark, which in its standard form is a R function (see below). For the sake of clarity, we have also specified the specific metric to be compared by the argument orkgc:WSDL:METRIC (see below). WSDL_METRIC ( WSDL) WSPARQL_METRIC_LBL ( HAS_DATASET ""WSC"" ) HSPARQL_METRIC_LBL ( WSC ) WSPARQL_METRIC_LBL ( HAS_EVALUATION ""WSC"" ) HSPARQL_METRIC_LBL ( HASS_DATASET ""WSC"" ) WSDL WSPARQL WSPARQL_METRIC WSDL WSPARQL_METRIC_LBL WSPARQL_METRIC_LBL WSPARQL_METRIC_LBL WSPARQL WSPARQL_METRIC WSPARQL_METRIC_LBL WSPARQL_METRIC_LBL WSC WSC WSC WSC WSC wsdl ------------- HASH SSE4.1 4.1.0 -HASH-0.3 4.1 WSDL 4.1 4.1 4.1.1 -HASH-0.3 4.1 (see below) WSC 4"	False
"
 input (English text): Provide a list of papers that have utilized the EfficientNet-L2-475 (SAM) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNet-L2-475 (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Classifier Chain + SMO model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Classifier Chain + SMO"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Shake-Shake (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code WHERE { ?models a. orkgc:Model ? }. orkgp:{ ?_t:Data:Model } 1 2 3 4 5 6 7 8 ( ) ; ORG_DATASET_AUTHOR orkgp:HAS_DATASET ?dataset.. orkgp:{ ?_t:Data:Model } 1 2 3 4 5 6 7 8 ( ) ; ORG_DATASET_CATEGORY orkgp:HAS_DATASET ?dataset.. orkgp:{ ?_t:Data:Model} Input: 1. output: 2. Input: 1. Name [type](e.g. Classifier) [type](e.g. SMO), 2. Author, 3. Subject, 4. Category, 5. Language. Output: 1.  Data, 2. Authors, 3. Subjects, 4. Category, 5. Language. For the first query, the full body of the text of each paper is used - this is the key to the query's power. For the second one, only the first part is shown to the user, allowing them to infer the complete text by matching the fields they have been provided. Both queries should also be formatted for the user's individual platform, such that if they cannot discern how each field was constructed (for example in the case they were using SQLite), they can select the full text of the field. An additional advantage to using multiple queries with the same fields is that it will ensure that one of the papers included will not include a large number of identical variables. This is especially useful if the data is complex and if the fields contain many words that are often interchanged.	False
"
 input (English text): Name the datasets that have been used for benchmarking in the Relation Extraction research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Relation Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Named entity recognition research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Named entity recognition"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Joint Entity and Relation Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 ----------- Total Number of datasets: 1.3 % 2.2 % 2.4 % 2.9 % 3.1 % ----------- Benchmarking time (minutes): ---------- 4.6 5.0 5.6 7.8 8.6 8.3 10.6 Benchmarking time (hour): ---------- 11.6 12.2 13.6 14.7 16.1 17.2 18.4 To see results in a specific fields, see the Field Manager.	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiNLI dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiRC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } data (SURFACEVIEW datasets): dataset: HARC data type: SURFACEVIEW compatibility: 2.0 (0.0) (0.0) HARC data is included by default data (SURFACEVIEW datasets):"	False
"
 input (English text): Where can I find code references in papers that have used the SRU++ model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SRU++"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the SRU++ Large model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SRU++ Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the SRU++ Base model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SRU++ Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model b orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SRU++ Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): How can I contact you ? output (Sparql query): input (English text): Where can I find more papers about the SRU++ model ? output (Sparql query): input (English text): Which model(s) are used in the work ? output (Sparql query): RDF reference: RDF input (English text): If someone knows where I can get references for other studies that use SRU++ Large for benchmarking purposes, I would be happy to include that information. output (Sparql query): input (English text): Do you have a list of papers that use the SRU++ Base and SRU++ Full models for benchmarking purposes ? output (Sparql query): input (English text): Can I provide links to code used in studies that benchmark the SRU++ Large model ? output (Sparql query): input (English text): If you can provide a link to a paper with a link to the source file and an explanation of how well it did, I would be happy to include that in the body of the email. output (Sparql query): in response to email (English text): I hope that this"	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Alien benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Robotank benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Robotank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asteroids benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asteroids"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asteroids"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Binary Trigonometry benchmark dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Binary Trigonometry"") ?benchmark orkgp"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the WMT2016 Romanian-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the WMT2016 Russian-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Russian-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the WMT2014 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset b orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): List the title and ID of research papers that contain a benchmark over the WMT10 Czech-French dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset c orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT10 Czech-French"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): List the title and ID of research papers that contain a benchmark over the WMT2010 French-Spanish dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset e orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2010 French-Spanish"") ?benchmark"	False
"
 input (English text): What are the models that have been benchmarked on the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the SciFACT dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciFACT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the SearchQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model_lbl ?model_lbl_rank WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset } ORkgp:HAS_EVALUATION ?eval. OPTIONAL { ?paper orkgp:HAS_MODEL_RANK ?model_rank ?model_lbl. } output (Sparql query): Output Results: If we add a field to the query we get the result <field name={ ?model_lbl ?rdfname} /> Note: For more details on these commands see the FAQ, the Data Manipulation section of the FAQ, the FAQ 2 and FAQ 3, and the FAQ 4."	False
"
 input (English text): What are the metrics of evaluation over the Open Entity dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Open Entity"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the ADE Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ADE Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the OntoNotes dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. }. Input (English text): What are the metrics of evaluation over the OntoNotes dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. }. We would also need to create a Para file that contains the metrics from both the English and the Para file. After that, we can make all of the queries that we used to get the metrics on the datasets. To do that, we will simply execute our query"	False
"
 input (English text): What are the metrics of evaluation over the Atari 2600 Space Invaders dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Defender dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" If you want to download the dataset containing the complete dataset, run the following command: > wget ""https://dataset-info.fasync.org/data/1d0b4e4c-6e7f-4e3c-a3fe-00f0480c8f5a/Atari-2600-Space-Invaders-Dataset.tar.bz2"" The first line makes sure you have Apache installed on the computer, then the second line downloads the whole dataset and the third line compresses it by a size that makes this the optimal size for a tarball. The final line makes the dataset into a file. You may then unzip it and copy the resulting files to a directory somewhere on your local machine, for example on your desktop or USB drive. You may want to leave off the zip file in order to make it easy to manage. You can find out the details of the datasets you can download by issuing the following commands: > cd /home/user/atarka/solutions/benchmark > wget ""https://datasetsimply.fasync.org/data/1d0b4e4c-6e7f-4e3c-a3fe-00f0480c8f5a/Atari-2600-Space-Invaders-Dataset-Full-Downloads.tar.gz"" This will give you a list of the available datasets, as well as the size of the dataset, and you can view the details by issuing the following command: > diff -dv1 ""faster"" ""faster"" 1d0b4e4c-6e7f-4e3c-a3fe-00f0480c8"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset WMT2014 German-English?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WMT2014 English-French dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl ) = ""WMT2014 English-German"") return _default_matrix } } } GROUP BY ?metric ?metric_lbl input (English text): What is the top benchmark result (metric, value, and metric2 value) over the dataset WMT2015 German-English ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl ) = ""WMT2015 German-English"") ?metric2 orkgc:HAS_DATASET ?dataset; orkgc:HAS_EVALUATION ?eval. ?eval orkgc:HAS_VALUE ?value } OPTIONAL { ?eval orkgc:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgc:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgc:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value ) } } GROUP"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset PWC Leaderboards (restricted)?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the title and ID of research papers that contain a benchmark over the WOS-11967 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TITLE ?title AND ID FROM RETAIL_HISTORY.IDL ORDER BY TITLE ORDER BY ID input (English text): What are the titles and IDs of research papers that have a top scorer over the WOS-11967 dataset ? output (Sparql query): SELECT TITLE ?andID FROM RETAIL_HISTORY.IDL ORDER BY TITLE ORDER BY ID Input and Output data files to a database import tool. The output is a CSV or tab-delimited file. Conducted by S. E. S. Lee Input data file 1) Title of the paper. 2) Author of paper. ID is a variable used for the sake of a visualization and has to contain all authors of the paper. Since there is no ID for each of the authors, you might want to create a table and sort the data using EXTERN TABLE. 3) Publication year for the paper. If you want to know the publisher of such a paper, this is done via the DATASET and PRONET variables. Note that the PRONET variable must not contain the '%' character. Also note that PRONET is not used to make the output data-view. If you run your utility on datasets from a different location (e.g. database, disk, network), your utility may fail or produce output file that is too large. To fix this issue, specify a location like this on your utility: FILENAME=' ?dataset_lbl' FILENAME=<FILENAME>. In case you have some other filename you want to use, just replace the FILENAME variable with it. The output file can have size up to 1000 MB, depending on the maximum size supported by the sqlite database.	False
"
 input (English text): List the metrics that are used to evaluate models on the WikiText-2 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the WOS-5736 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the enwik8 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT CONVERT FROM ?dataset a GROUP BY ?dataset_lbl; [1] See also the ""metrics/metric_lbl"" list in the table on the wiki (http://wiki.openindicator.org/wiki/Metrics_of_meta_measurements#Metrics_of_meta_measurements_lbls). See also the wiki's resource page: metrics/metric_lbllist."	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the TACRED dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TACRED"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Dmlab-30 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dmlab-30"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	   select t.name, t.metric from the TACRED, TempEval, or Dmlab-30 datasets if necessary select data(name='Tacred', type_id=0, t.metric_name='Rows') [0] as t.metric;     select data(name='Tacred', type_id=2, t.metric_name='Rows') as t.metric_lsb from the TempEval3 dataset if necessary select data(name='Tacred', type_id=0, t.metric_name='Floor') [0] as t.metric_lsb;    select t.name, t.metric from the TempEval3 dataset if necessary select data(name='Tacred', type_id=2, t.metric_name='Floor') as t.metric_lsb_lsb from the Dmlab-30 dataset if necessary select data(name='Tacred', type_id=0, t.metric_name='Floor') as t.metric_lsb_lsb_fl from the TempEval-3 dataset if necessary select data(name='TempEval-3' if necessary) as t from the TACRED dataset Rows 1–6: TASTER DISTINCT TASTER input (English text): What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset ? output (Sparql query): select t.name, t.metric; from the TACRED, TempEval, or Dmlab-30 datasets if necessary select data(name='Tacred', type_id=0, t.metric_	False
"
 input (English text): In what country was conducted research with the largest number of participants?
 output (Sparql query): SELECT ?country WHERE { orkgr:R44980 orkgp:compareContribution ?contrib. ?contrib orkgp:P15249 ?country; orkgp:P23169 ?number_of_participants. } ORDER BY DESC( ?number_of_participants) LIMIT 1
 input (English text): Which are five the most common research fields for papers?
 output (Sparql query): SELECT ?research_field, ?research_field_labels WHERE { ?papers orkgp:P30 ?research_field. ?research_field rdfs:label ?research_field_labels. } ORDER BY DESC(COUNT( ?research_field_labels)) LIMIT 5
 input (English text): What is the most common location in the studies?
 output (Sparql query): "	SELECT ?locations WHERE { orkgr:R111045 orkgp:compareContribution ?cont. ?cont orkgp:P37537 ?locations. } ORDER BY DESC(COUNT( ?locations)) LIMIT 1	-------------- ------- ------ ---------------------- USA (32) 11 UK (5) 8 UK (2) 3 EU (1) 1 India (1) 1 India (0) 1 Canada (0) 1 Australia (0) 1 France (0) 1 Japan (0) 1 Switzerland (0) 1 China (0) 1 China (0) 1 France (0) 1 New Zealand (0) 1 Belgium (0) 1 Malaysia (0) 1 Hong Kong (0) 1 Argentina (0) 1 Philippines (0) 0 India (0) 0 Japan (0) 0 Brazil (0) 0 Chile (0) 0 South Korea (0) 0 Malaysia (0) 0 Philippines (0) 0 Turkey (0) 0 Taiwan (0) 0 Brazil (0) 0 Netherlands (0) 0 Netherlands (0) 0 Mexico (0) 0 Singapore (0) 0 Taiwan (0) 0 Sweden (0) 0 Greece (0) 0 United States (0) 0 India (0) 0 China (0) 0 Netherlands (0) 0 Mexico (0) 0 Philippines (0) 0 Netherlands (0) 0 India (0) 0 Turkey (0) 0 Singapore (0) 0 China (0) 0 Japan (0) 0 France (0) 0 Argentina (0) 0 China (0) 0 Germany (0) 0 Ireland (0) 0 Germany (0) 0 Brazil (0) 0 India (0) 0 China (0) 0 France (0) 0 Japan (0) 0 Netherlands (0) 0 Mexico (0) 0 Taiwan (0) 0 Singapore (0) 0 Sweden (0) 0 Spain (0) 0 Singapore (0) 0 India (0) 0 Japan (0) 0 (Click to expand. Credit goes to Jürg Meinhardt for this brilliant tool)	False
"
 input (English text): Provide a list of papers that have utilized the He et al., 2017 + ELMo model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""He et al., 2017 + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BiLSTM-CRF+ELMo model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-CRF+ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BCN+ELMo model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BCN+ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Provide a list of papers that have utilized the BioMASS model and include the links to their code ? output (Sparql query): B. 2. Results, and a discussion of future directions B.2.1. Open questions For a list of questions, see this answer here. In addition, see what I had planned to list in this reply. In a previous blog post I mentioned that I thought it might be interesting to do more quantitative analysis of the results. Now that it appears that the first data set was in fact, a very useful one, I have had time to do a bit of work that has the potential to be rather revealing; and I hope that this post contributes to that research. A. 2.2. An Overview of R I am by no means an expert on R, and to be honest, my expertise is not something that R is particularly good at, but I hope that by providing some examples of using R (and other R based packages) to answer the above questions, maybe this will show some people that, although the R packages used are not particularly good they can still be used to an extent to answer the above questions. Some of these approaches seem like they would be more appropriate for those readers who are not using R: Bibliography Management: For those who are interested in doing a little more research and are interested in the state of reference databases that are available on the web (in various formats), including online reference management, read this and this one. B. 2.3. R and C++ for Statistics There are many R packages for statistics, and I have used a number of them and had success with some. Some of the packages include scipy.misc.random, matplotlib, ggplot2, mpg321, rtse, rg	False
"
 input (English text): Indicate the model that performed best in terms of PRE-TRAINING DATASET metric on the DCASE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PRE-TRAINING DATASET"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the HMDB51 dataset in terms of Top-1 Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pre-Training Dataset"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pre-Training Dataset"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } output (Sparql query): This is good, but it would be nice if we had the best performing model for each dataset by default. That's exactly what we did: we created a function that takes in an optional parameter called model (i.e. how models should be implemented), and returns the best performing model for the given datasets! A function like this works for any number of datasets because it takes in the dataset information and runs one or more functions to find the best model for each dataset in that subset (or any subset of it). function best-model-for-dcl_jupyter_dataset_1 (model) { var result = model; if (!(result.id in models_"	False
"
 input (English text): Indicate the model that performed best in terms of Bits per byte metric on the Classical music, 5 seconds at 12 kHz benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bits per byte"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the AudioSet benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AudioSet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Classical music, 5 seconds at 12 kHz dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 outputs (String): ?dataset: Classical music, 5 seconds at 12 kHz ?benchmark: ?libr.dataset. ?score: ?METRICS. (6, 8, 12, 18) ?metric: a; b; c; d: 5 1 select :benchmark orkgp(score, metric_lbl.score, metric_lbl.metric) from orkgp.metric select score; metric_lbl.score, metric_lbl.metric; as score from orkgp.dataset select metric_lbl.score, metric_lbl.metric; as score from orkgp.metric; output (SparseDict): RDFS(name, data, quality, timestamp) select: score from: orkgp.metrics [:metrics,:metric_lbl.score] metric_lbl.score, metric_lbl.metric; as index count_matrix count_sums metric_lbl.metric_sums select: quality from: orkgp.metadata [:baud rates,:d_times,:baud rate] as bp count_matrix count_sums bp as index count_sums metric_lbl.metric_sums select: timestamp from: orkgp.timestamp where :timestamp BETWEEN 2014-01-12 00:00:00 and 2014-01-12 05:00:00 select: timestamp from: orkgp.timestamp where: :timestamp BETWEEN 2014-01-30 20:00:00 and 2014-01-30 5:00:00 select: timetimestamp from: orkgp.timestamp where: timestamp BETWE	False
"
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2014 German-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of BLEU metric on the WMT2014 English-German benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } in: [ 1,0,1 ] output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER ("	False
"
 input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the BC5CDR-disease dataset in terms of F1 entity level metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 5.9.1.5 Metric Generation Questions¶ Question Generator (QG)¶ QG generates a metric from RDF text and the entity entity. When a query is generated, the RDF text used to generate the metric must be compatible with the entity text. (There are special characters in the RDF text, so in some cases it may not be possible to generate a metric from that text.) The QG process consists of three steps: 1. The entity text is generated using the rdfs:generate function 2. QG generates a metric through the attribute_metric() function 3. The metrics are stored as a RDF object via the properties of the class Metric The first step is straightforward; as long as the input is in the usual English-English language (usually the default), all that is needed is to specify that the input consists of English and English text. If not, then it is necessary that the input be either a plain text file, or be passed through the use of the setmetric function. Note that this does not restrict which RDFs to use when generating the metric. For example, in the present paper, the QG output will include the data for the following languages: English (ASCII) Russian (ASCII) Chinese (ASCII) QG> Metric MetricMetric_rdfs.txt |> rdfs2metric(). select entity_name, _categories, _metric from MetricMetric select _categories as cv_category, _metric as metric from MetricMetricRdfs createMetric(entity text, setmetric('category','metric',1	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Gopher dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Gopher"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Tennis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?instructions in orkgp:HAS_INSTRUCTIONS ?instructions. HAVING { ?training_data if ?training_data = ?training_data ?dataset_lbl. FILTER (str( ?training_data) = ""atari_nxg:dataset"", ?dataset_lbl. NULL ?training_data. ?id_lbl. FILTER (str( ? training_data) = ""AwayGame.dataset"" ?dataset_lbl. NULL ?training_data. ?id_lbl. ?dataset_lbl. ?training_data. ?id_lbl, ?dataset_lbl. ?dataset, ?dataset_lbl. ?dataset_lbl ) }. ?id_lbl. OPTIONAL { ?id_lbl. } } } input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Video Game dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?gamerigetos. FILTER (str( ?gamerigetos) = "".dataset.dataset"", ?gamerigetos. NULL ?gamerigetos. ?gamerigetos _lbl. FILTER (str( ?gamerigetos) = ""Atari 2600 Video Game"") ?benchmark orkgp:HAS_DATASET ?dataset;, ?dataset_lbl. ?dataset_lbl. ?dataset _lbl, ?dataset"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Battle Zone dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Zaxxon dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } For more advanced searches we can use :partition to select by one part of the dataset, in this example that part is a series of benchmark articles. It is best not to use :partition multiple times at once because the search for articles will be slow. input (English text): List the Title and ID of all benchmark articles that contain a benchmark over the Atari 2600 Zaxxon dataset ? output (Sparql statement): SELECT DISTINCT ?benchmark ?benchmark_lbl WHERE { ?dataset a orkgc:Dataset; db_puzzle:P30 ?benchmark_lbl; db_puzzle:P30 ?benchmark_lbl; db_puzzle:P31 ?benchmark_lbl; db_puzzle:P31 ?benchmark_lbl; db_puzzle:P31 ?benchmark_lbl; db_puzzle:P31 ?benchmark_lbl; db_puzzle:P31 ?benchmark_lbl; db_puzzle:P31 ?benchmark_lbl; :partition ?benchmark_lbl :partition ?benchmark_lbl input (English text): List the Title and ID of all"	False
"
 input (English text): Provide a list of papers that have utilized the COMET - DynaGen model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""COMET - DynaGen"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Commonsense Transformers (COMET) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Commonsense Transformers (COMET)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the COMET - Direct model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""COMET - Direct"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""COMET - Direct"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Input (Text) Output (Sparql query) Input (Text) Output (Sparql query) English text: List the code links in papers that benchmark the COMET - Direct model in any benchmark ? English text: outputs (Sparql query)  rows (Sparql query)  rows (Sparql query)  Row 1 1  1  1  1  1 1  1  1  1 3 2 4 1 2 2 6 4 9 9 9  12 19 19 26 23 25 33  22.99 23.03 23.94 23.98 24.01 24.25 25.26 20 18 18 18 19 18 31 22 20 31 24 30 30 30 30 30 30 30 30 30 30 30 30 30 30 28 30 28 28 28 33 33 33 33 33 34 34 34 34 34 34 34 34 34 34 35 35 35 35 45 45 45 45 45 47 49 51 66 62 57 49 54 67 54 30 49 48 49 48 49 48 49 49 49 49 49 49 49 50 51 48 52 48 48 50 51 51 50 60 57 62 62 61 67 65 61 61 40 45 47 49 48 49 48 49 49 49 52 53 50 50 50 50 60 59 63 63 63 45 46 46 48 48 49 50 50"	False
"
 input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the WMT2016 Russian-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Russian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" You should also provide that list of models that have been tried on ALL datasets that you plan to benchmark in the future as well as those that haven't been tried at all. Note - you can access the benchmark results and see which benchmarks are used for all datasets by using db.get_all_benchmarks(...) to get the benchmark results for each dataset. Here is an example of what I meant by having tested the model's performance on the WMT2016 Russian-to-English benchmark dataset: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // set table and column names input ( English text ): Inputs and their corresponding row  columns. var input_lbl_1 = { a = 42, b = ""nike"", c = ""nike2"", d = ""spandex"", e = ""altra"", f = ""armani"", g = ""paulabar"", p = ""pajama"", q = ""pajamas"", r = ""fashionshoes"", s = ""nike"", t = ""nike2"", u = ""flannels"", v = ""furniture"" } input ( DataTable ( str ( input_lbl_1. a ), str ( input_lbl_1. b ), str ( input_lbl_1. c ), str ( input_lbl_1. d ), str ( input_lbl_1. e ), str ( input_lbl_1. f ) ] )): BATCH_ROWS = 10 var input_lbl_2 = { a = 33, b = ""molecules"", c = ""lentil"", d"	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Space Invaders dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Battle Zone dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"------------------------- ORDER BY desc( ?score) desc( ?value ) DESC LIMIT 5 For each metric, the metric result (i.e., the highest score) appears in a column of the RDF output object. In our case, that would correspond roughly to the ""result"" column in the standard RDF object, where possible. RDF is not a database format but data format, and as such, may take the form of a web or text document with a metadata representation such as an HTML document element or a URL to an XML file. RDF may have fields that are specific to its data format, but the RDF output object is simply the output of a single operation, and as such, is only as good as the information it contained. If, for example, someone wants to access the same metric in the RDF object, they might go to their web server and access rdfs:html:Atari_Sys_Sys_System_Program_Counters.RDF to get a text file that contains the same metric. RDF is the only web database format that can do this, and while it can be convenient to store the same metric across multiple locations, it does not have the flexibility to store that information across multiple data centers or across multiple machines within a data center. This problem has long been a barrier to database interoperability. To address this problem, in the last several years, the RDF Schema Working Group at the University of Colorado Denver has released a new schema, RDF Schema-Verbatim, that has multiple data formats — including RDF 3.3, RDF 3.4, and XML, and it can be used in relational databases. (This allows for data synchronization without the need for multiple data centers or a local database server.) RDF Schema-Verbatim can also be used as a schema"	False
"
 input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score on the AAPD benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset IWSLT2014 German-English?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the IWSLT2015 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"--------------------------- { { ?dataset a orkgc:Dataset; rdfs:label ?dataset_. FILTER (str( ?dataset_. ) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ORKP_EVATORS ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model_. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value ) } } GROUP BY ?metric ?metric_lbl Input (English): What is the top benchmark score and its metric on the IWSLT2015 English-German dataset ? Output (Sparql query): --------------------------- { { ?dataset a orkgc:Dataset; rdfs:label ?dataset_. FILTER (str( ?dataset_. ) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ORKP_EVATORS ?value. } || kc ?dataset_. ORKP_HAS_MODEL ?model_. ORKP_DESC ?value. OR Kc. ORKP_SV ?value1. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. OR Kc. ORKP_MODEL ?model_. ORKP_DESC ?value"	False
"
 input (English text): What are the metrics of evaluation over the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Krull dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Krull"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Frostbite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The code behind the analysis and interactive report have been made available on github: https://github.com/Svoboda/Atari_Analytics_Report Some information on our benchmark process can be found on our website: www.jvx.com/benchmarks  We are proud to announce that our benchmarking services have been reviewed at the ISO/IEC JVX.2017 paper ""Atari Games Evaluation using a Parallel Iterative Program Flow in the JVX Language"", published by IEEE. It can be read here: https://www.ietf.org/public/standards/jvx-2017-01-11/index.html"	False
"
 input (English text): What is the top benchmark score and its metric on the SoMeSci dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SoMeSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the MUTAG dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MUTAG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the MultiNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl(MAX( ?value)) WHERE { { SELECT ?metric ?metric_lbl(MAX( ?value)). ?metric max( ?value) } }. PARAMETER : The top benchmark score of SoMeSci and MultiNLI data Here are the result: Note that for each dataset (SoMeSci, MultiNLI), we only have one output row, that is, the first row of the dataset. This is because we don't do any further manipulation of the data, we just see the same result from top to bottom: the top score and metric on the same dataframe. Now let's do this on a large dataset. For this, we will create dataframes of 50 dataframes, where each of the 50 dataframes have 50 dataframes (there is only one column on each dataframe, because we have to deal with non-aggregate data for the same model). We will split each of the 50 dataframes (say 50×50) by 100 and have each 50 dataframe be a new dataframe. Then, we will do a simple filter on each of the 50 dataframes and change the value of the metric on each dataframe. Here is where that filtering problem comes about: how do we find the dataframe that has the metric ""10"" on the dataframe ""10"". To find it, we'll have to do a little bit of filtering and it will look something like this: filter: ?10  && ?10 (column: 10) In other words, given a set of 50 observations (10 dataframes) whose values have 10's values on them, this will give us 50 dataframes with 10's values. We'll find the 10's values on the 50 sets of 500 ("	False
"
 input (English text): What models are being evaluated on the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the ImageNet 64x64 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the ImageNet ReaL dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT query, model, paper, performance, version(version(version(version(version))) from l1_benchmark_l1; output (Sparql query): SELECT query, model, paper, performance, version(version(version(version))) FROM l1_benchmark_l1; input (English text): What has been tested on the ImageNet ReaL dataset ? output (Sparql query): select query, model, paper, performance from l1_benchmark_rea_l1; output (Sparql query): select query, model, paper, performance from l_benchmark_rea_l1; input (English text): Your project has been published on the arXiv. output (Sparql query): Select query, model, paper, performance, version(version(version(version))) FROM arXiv. input (English text): Which is your project name ? output (Sparql query):   SELECT query, model, paper, performance, type(rear), version(version(version(	False
"
 input (English text): Which model has achieved the highest Accuracy score on the Reuters De-En benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the Reuters En-De dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Accuracy score on the Reuters En-De benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT * FROM output_data; select 1% for accuracy < 1% (3 columns, 1 row); We have done a round-trip to output_dataset after filtering out the ""accuracy < 1% (3 columns, 1 row)"". Now the columns in output_dataset with an 'Accuracy' value that is below 1% have to be removed and any other columns that have 'Metric' values that are above the 'Accuracy' values (with an explicit filter_to_metric ) have to be removed too. The bottom line here is that now we only get the top 1% of the top 1% of RDP benchmark scores. Here's a look at all columns in output_dataset with an 'Accuracy' value above the '1%'. It was a tedious to do this using SSPT so here's an extract from the original paper (a little longer) to hopefully get you through: The table may be a little long, so lets start from the top and work our way down. We take the first column that was left on the"	False
"
 input (English text): List the metrics that are used to evaluate models on the Kinetics-600 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the BIOSSES dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT * FROM ?v1 WHERE ?dataset a orkgc:Dataset; a orkgc:HAS_DATASET. cts:HAS_METRICS:COUNTER ?value; OPTIONAL { ?metric ?metric_lbl; ?metric rdfs:label ?metric_lbl. OPTIONAL { } } OPTIONAL input (English text): Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model"	False
"
 input (English text): Can you list the metrics used to evaluate models on the Yelp-2 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Yelp Binary classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Yelp-5 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lblWHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Five"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } As before, the following steps may be taken to update the metrics: Change the field of the query. Change the parameter to a list of metrics that are used to evaluate models on the dataset. Add or remove measurements that do not apply to the baseline dataset. Delete the labels used to evaluate models for the baseline dataset. Update the metric from the query to a list of metrics that can be used by a validation system for the baseline dataset. Change or add measurements to the metrics that are used to evaluate models. Rename the metric to be recognized by a validation system. As you can see, many of the steps listed above can be used to update metrics. In fact, the steps above are just the steps that are necessary for the above mentioned metrics to be updated with the new metrics. 2.5.5. Inlining of the metrics Each validation system in the above model set can use metrics directly within the query. This may require that your own validation system, as described in the previous section, will also use metrics within the query. You are not needed to use metrics directly within the query but may not want to. We will use a small example to illustrate what can be done to inline metrics within the query. The"	False
"
 input (English text): Provide a list of papers that have utilized the BART Base (with text infilling) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BART Base (with text infilling)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the PBSMT model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PBSMT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BART model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BART"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT name, code, benchmark, cont, benchmark_lbl, source code FROM ( SELECT name, code, benchmark, cont, benchmark_lbl, source code FROM ( SELECT name, code, benchmark, cont, benchmark_lbl, source code FROM ( SELECT name, code, benchmark, cont, benchmark_lbl, source code FROM ( SELECT name, code, benchmark, cont, benchmark_lbl, source code FROM ( SELECT source_source_id FROM testgroup2 GROUP BY 1, -1, 1, 1 ORDER BY 1 ASC) '( ?model a orkgp:HAS_DATASET ?dataset) ?') 'BART Base (with text infilling) ( ?date time) ORkgp:HAS_DATASET ?dataset ORDER BY 1 ASC) testgroup2 ) ); Name: PASTER.LARGE_LORDE.0.8 PASTER.TESTGroups.testtestgroup2.trainlab LORDFN: PASTER.TESTGroups.testtestgroup2.trainlab_date_TIME RDFS: PASTER.TESTGroups.testtestgroups.trainlab_date_TIME Source code: PASTER.LARGE_LORDE.0.8 LORDFN: PASTER.TESTGroups.testtestgroup2.trainlab_date_TIME Benchmark: PASTER.LARGE_LORDE.0.8 LORSFN: PASTER.TESTGroups.testtestgroup2.trainlab_date_TIME Benchmark_LBL: PASTER.LARGE_LORDE.0.8	False
"
 input (English text): What is the top benchmark score and its metric on the SearchQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Natural Questions (short) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score)/10 -------------- ORDER BY DISTINCT ?score | DISTINCT ?value input (English text): What is the top benchmark score and its metric on the Natural Questions (long) dataset ? output (Sparql query): -------------- SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score)/10 -------------- ORDER BY DISTINCT ?score | DISTINCT ?value ORDER BY DESC( ?value) input (English text): Are there enough top 10 items to generate 1000 predictions ? output (HTML-formatted data): Top 10 (1) : Number of top 10 items : 1000 output (HTML-formatted data): -------------- TOP 10 (1) : Number of top 10 items : 1000 input (English text): How many different word associations can you perform on the Linguistics and Metrics datasets ? Do you wish to have more than 50 word associations (and less than 1000) for each dataset ? -------------- SELECT ?character_count 2 ?character_count_lc 1 ||{ ORA:ASCII_LENGTH ?character_lbl ORA:NAME ?characters_lc } ?num_characters.count ||{ ORDER BY num_characters_lc DESC } -------------- ORDER BY num_characters_lc DESC input (English text): How many different word associations can you perform on the History and Trivia datasets ? Do you wish to have more than 50 word associations (and less than 100) for each dataset ? -------------- SELECT ?character_count 1 ?character_count_lc 1 ||{ ORA:ASCII_LENGTH ?character_lbl ORA:NAME ?characters_lc } ?num_char	False
"
 input (English text): What is the best performing model benchmarking the CIFAR-10 dataset in terms of Parameters metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Parameters"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the CIFAR-100 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?metric ?metric_lbl. OPTIONAL { ?metric rdfs:label ?metric_lbl. } OPTIONAL { ?metric rdfs:labels. } OPTIONAL { OPTIONAL { ?model rdfs:labels. } OPTIONAL { orkgp rdfs:labels. } OPTIONAL { orkgp rdfs:labels. } OPTIONAL { orkgp rdfs:labels. } OPTIONAL { orkgp rdfs:labels. } OPTIONAL { orkgp rdfs:labels. } OPTIONAL { orkgp rdfs:labels. } OPTIONAL { orkgp rdfs:labels. } OPTIONAL { orkgp rdfs:labels. } OPTIONAL { orkgp rdfs:labels. } OPTIONAL { c2c rdfs:labels. } OPTIONAL { c2c rdfs:labels. } OPTIONAL { rdfs:label ?model. } OR { orkgp rdfs:label ?model. } OR { orkgp rdfs:label ?model. } OR { rdfs:labels. } OR { rdfs:labels. } OPTIONAL { rdfs:label ?model. } OPTIONAL { rdfs:labels. } OPTIONAL { c2c rdfs:labels. } OPTIONAL { c2c rdfs:labels. } OPTIONAL { c2c rdfs:labels. } OPTIONAL { c2c rdfs:labels. } OPT	False
"
 input (English text): What are the metrics of evaluation over the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT TOP(5,1) b c d e f... ; /* Output: */ output (Sparql query): SELECT TOP(5,1) b c d e f... ; /* Output: */ Output What are the metrics of evaluation over the Automatically labeled Medline abstracts corpus dataset ? Input (English text): What are the metrics of evaluation over the Automatically labeled Medline abstracts corpus dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lc orgc ORgc:Dataset; rdfs:label ?dataset_lc. FILTER (str( ?dataset_lc) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. { ?metric _lc:LICENSE STATEMENT; ORGc:MEDIA_URL_STATUS orgc:STATUS } OPTIONAL { ?metric_lc rdfs:label ?dataset_lc. } OPTIONAL { ?metric-lc rdfs:label ?dataset_lc_lbl. } OPTIONAL input (English text): What are the titles and IDs of research papers that include a benchmark for the Automatically labeled Medline abstracts corpus dataset ? output (Sparql query): SELECT TOP(5,1) b c d e f... ; /* Output: */ Output What is the top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset ? Input (English text): What is the top benchmark score and its metric on the Autom automatically labeled Medline abstracts corpus dataset ? output (Sparql query): SELECT TOP(5,1) b c d e f... ; /* Output: */ Output"	False
"
 input (English text): Name the datasets that have been used for benchmarking in the Citation Intent Classification research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Citation Intent Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Name the datasets that have been used for benchmarking in the Text Classification research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Text Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Name the datasets that have been used for benchmarking in the citation classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""citation classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DESC ( ?dataset_lbl. ?cont ). ?test. ?title. ?sociotext. ?lwr_pk. ?pk_sr. ?citation orkgc:Dataset ?dataset_lbl. ?citation as ?criterion and ?criterion. ?criterion as ?criterion1; orkgc:Criterion1 ?criterion1; output. input (English text): Name the datasets that have been used for benchmarking in the ""Text Classification"" research problem ? output (Sparql query): SELECT DESC ( ?dataset_lbl. ?cont ). ?test. ?title. ?sociotext. ?lwr_pk. ?pk_sr. ?citation orkgc:Dataset ?dataset_lbl. ?citation as ?criterion and ?criterion. ?criterion as ?criterion1; orkgc:Criterion1 ?criterion1; Note that these queries need the original dataset(s) in order to understand what's going on. While this is a good thing, you may want to test the dataset(s) on more than one problem. To do that, import the second input on the left. The above queries were done for the test dataset(s). The queries below were done on the corresponding benchmark dataset(s). To perform this analysis, we will be creating two different classes to be used in our benchmarking process: a ""text classification"" class and a ""citation class"" class. We will use the text classification class as our benchmark dataset. We will then import the data set from the text classification class into the benchmark dataset. Then, we will create a ""criterion"" class based on the text"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the S-NLI model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""S-NLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the D-NLI model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""D-NLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the H-NLI model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""H-NLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE ?code. ?code. ?code. input (English text): List the codes used in papers that benchmark the H-NLI model in any benchmark ? output (Sparql query): SELECT DISTINCT ?code WHERE ?code. ?code. ?code. Here you get all links to all relevant code in the code that are found using the D-NLI model in any benchmark article. D-NASILimator.java (in test/ ) import java.util.Scanner; public class D-NASILimator { private static final int JAVA_OPTIONS = 1609; public static void main(String[] args) throws Exception { D-NASILimator dnilimator = new D-NASILimator(); System.setProperty(""JAVA_OPTIONS"", 0); System.setProperty(""javaconversion"", 1); System.setProperty(""javapackage"", ""1.2""); System.setProperty(""javachost"", ""org.junit.internal.annotate.DannotateSystem""); System.setProperty(""javafxpath"", ""java:/javacoverage/dist/sdk_win32/junit/bin"").setCompiler(""javac 6.5""); TestDataTest jd = new TestDataTest(""D-NASILimator""); System.setProperty(""javapackage"", ""org.junit.internal.annotate.DannotateSystem""); //test that d-nli works, using the D-NASILimator dnilimator.check(); }"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the PubMed 20k RCT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the MedSTS dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMed 20k RCT dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?dataset : a from `MedSTS` group by ?dataset_lbl GROUP BY ORGGLER : ORGGLER : ORGGLER : ORGGLER : ORGGLER ; ORGGLER : OrgManager. ORGGLER : ORGGLER : ORGGLER : ORGGLER output (Sparql query): query (Sphinx): CREATE OR REPLACE SELECT MEDSTS.CUBIC_DISTINCT ?pubmed_lbl FROM `MedSTS` ct group by ?dataset, ?pubmed_lbl ORDER BY! ? output (Sparql query): SELECT DISTINCT ?pubmed_lbl WHERE ct.pubMedID = ?dataset.name; output (Sparql query): SELECT DISTINCT ?pubmed_lbl ?value FROM ?documents AS a INNER JOIN ?articles AS b ON (a.name=b.name); input (English text): Is it OK if I use an existing document for the test ? output (Sparql query): SELECT 1 AS b, 1: b.name, ?value input (English text): I need to run a benchmark that's an ORGGLER from a dataset. output (Sparql query): SELECT MEDSTS.CUBIC_DISTINCT ?pubmed_lbl FROM `MedSTS` ct group by ?dataset, ?pubmed_lbl ORDER BY! ? input (English text): I need to run a benchmark that's an ORGGLER from a dataset. output (Sparql query): SELECT MEDSTS.CUBIC_DISTINCT	False
"
 input (English text): Indicate the model that performed best in terms of Macro F1 metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Micro Recall score when benchmarked on the NLP-TDMS (Exp, arXiv only) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Micro Recall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Macro Recall metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Recall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE What value did the model output on the Benchmark, NLP-TDMS datasets ? output (Sparql query): Select the top performing model in terms of Macro Recall score when benchmarked on the NLP-TDMS (Exp, arXiv only) dataset ? input (English text): What is the name of the top performing model in terms of Macro Recall score when benchmarked on the NLP-TDMS (Exp, arXiv only) dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE This dataset defines the top performing NLP-TDMS datasets from the NLP-TDMS benchmark dataset. The data can either be provided as a tab-delimited (CSV document) or binary format (CSV document and/or binary). This file has been cleaned up for easier access on the site. This dataset was created on 10 Aug 2014. Select the top performing model in terms of Macro Recall score when benchmarking on the NLP-TDMS datasets in the NLP-TDMS benchmark dataset on both datasets; do make sure you have entered enough data if you are not sure. The default value for this query is ""Exp"". The CSV files has been parsed and formatted accordingly to match"	False
"
 input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the CoQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Overall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of EM score when benchmarked on the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""EM"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NLL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric nls:NLL; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NI"") { SELECT ?model ?model_lbl WHERE { ?dataset nls:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Inns"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. } ORDER BY DESC( ?value ) LIMIT 1 } } input (English text): What is the name of the top performing model in terms of IM score when benchmarked on the UK dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric im:Img; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""IM"") { SELECT ?model ?model_lbl WHERE { ?dataset im:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Im"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:"	False
"
 input (English text): Where can I find code references in papers that have used the DARQN soft model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DARQN soft"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DQN+SR model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DCN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code WHERE { ?model dcntn:BINARY ?.... } orkgp:HAS_DATE dcntn-date ?dataset. DATASET orkgp:HAS_SOURCE_CODE ?lbl. ?cont orkgp:HAS_LINK_CODE ?lbl. DATASET orkgp:HAS_SOURCE_CODE ?model. DATASET orkgp:HAS_SOURCE_CODE ?cont. DATASET orkgp:HAS_LINK_CODE ?model. BINARY orkgp:HAS_SOURCE_CODE ?components. DATASET orkgp:HAS_SOURCE_CODE ?lbl-dcm. ORKGP-lbr-DACN ?cont. ORKGP-lbr-DCNS-DACN ?lbl-dcm. dcntn-date(DRCNTN-date) orkgp-lbr-DACN orkgp-lbr-dcns-dacn orkgp-lbr-DCNS-date orkgp-lbr-DCNS-date orkgp-lbr-DCNS-date orkgp-lbr-DCNS-date ||kgp:HAS_DATE dcntn-date ?dataset. ORKGP-lbr-DACN ||g:HAS_LINK_CODE OR-dcn-dcm orkgp-lbr-DACN output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_l	False
"
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the iNaturalist 2019 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the iNaturalist 2018 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "		False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Ice Hockey dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ice Hockey"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Pong dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pong"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ice Hockey"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { select ?metric ?metric_lbl FROM ?dataset_lbl join ?score on ?dataset_lbl.dataset_id = ?dataset_lbl._lbl_id and!( ?dataset_lbl.dataset_id EXISTS ?dataset_lbl.dataset1) AND ?stat_lbl.stat_metric > ?metric_lbl.stat_metric_1 ?stat_lbl.stat_metric ?value. OPTIONAL { [ ?max( ?value - 1) ] ?val_lbl.stat_metric2 ?value } ] } } input (English text): What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { select ?metric ?metric_lbl FROM ?dataset_lbl join ?score on ?dataset_lbl.dataset_id = ?dataset_lbl._lbl_id and!( ?dataset_lbl.dataset_id EXISTS ?dataset_lbl.dataset1) AND ?stat_lbl.stat_metric > ?metric_lbl.stat_metric_1 ?stat_lbl.stat_metric ?value. OPTIONAL { [ ?max( ?value - 1) ] ?val_lbl.stat_metric2 ?value } ] } } 	False
"
 input (English text): Provide a list of benchmarked datasets related to the Information Extraction research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Information Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Scientific Claim Verification research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Scientific Claim Verification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Scientific Results Extraction research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Scientific Results Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Scientific Results Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } input (English text): Provide a list of benchmarked datasets related to the Syntheses Research area ? output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Synthesis Research"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } Please note that both input as well as output are mandatory but optional: To provide a reference list (that is, a list of datasets that were benchmarked in the same time frame) you need to include the option ""referencelist"". The referencelist is the list of datasets that were benchmarked in one timeframe but not in another. Since they are no longer benchmarked, they are not considered in the referencelist, and thus are used instead in the output. The"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Reacher, easy (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark score and its metric on the Walker, walk (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER ( ?dataset a orkgc:Dataset = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): How is the top benchmark score and its metric on the Walker, walk (DMControl100k) dataset ? output (Sparql query): SELECT ?score ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. AND ?score < ?value } ORDER BY DESC( ?value ) ? GROUP BY ?value ?score input (English text): Name (and ID) of research paper that shows the top benchmark score and its metric on the Walker, walk (DMControl100k) dataset ? output (Sparql query): SELECT TOP ( ?score, ?value ) ?stat ?value ORDER BY DESC( ?value ) ? SELECT TOP ( ?score, ?value ) ?stat ?value, DIMENSION BY ?score DIMENSION [ ?value 2, ?value 3 ], DIMENSION [ ?value 2, ?value 3 ], DIMENSION [ ?score 2, ?score 3 ], DIMENSION [ ?score 2, ?score 3 ], DIMENSION [ ?score 2, ?score 3 ], DIMENSION [ ?score 2, ?score 3 ], DIMENSION [ ?score 2, ?score 3 ], DIMENSION [ ?score 2, ?score 3 ], DIMENSION [ ?score 2"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Relation Classification research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Relation Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Information Extraction research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Information Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Semantic Role Labeling research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Semantic Role Labeling"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 output: The following query produces only records with labels that fit exactly the pattern of the database: select * from ?dataset where { ?problem a.Label!= NULL ?dataset a.Label }: output: Using the above query, the output shows us that there are 3,929 records for which Relation Classification failed due to the problem criterion. There is no record that belongs to the group of all records from which Relation Classification passes. Using the above query, the output shows us that there are 8,946 records for which Semantic Role Labeling failed due to the problem criterion. There is no record that belongs to the group of all records from which Semantic Role Labeling passes. Using the above query, the output shows us that there are 4,902 records for which Relation Classification fails due to the problem criterion. There is no record that belongs to the group of all records that fail because of the problem criterion. In all cases, a record is associated with a label that is less specific than that provided for the problem criterion. This information tells us that these results are most likely the results of some particular database querying. This can be confirmed with the following query: select * from ?dataset where { ?problem= ?dataset}.Label, { ?dataset ?.Identifier} : { ?dataset ?.Label: ?dataset.Identifier} output: The following query shows us that an additional record exists in the group named Problem. One might suspect that the query should have returned a record which is in the correct group. The output of the query shows that only one record exists in the group named Problem. It shows that the record in Problem has a label which is less specific than the labels given for Problem. It appears that there are 3	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WikiText-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" select ( id) as datafile, id as paper, id asbenchmark from wnli_datasets; +------------------+-------+--------+------------+ | dataset | id | paper | benchmark | +------------------+-------+--------+------------+ | WNLI-Dataset1 | 3100 | 584 | 8 | 1 | | WNLI-Dataset2 | 2573 | 100 | 8 | 1 | | seel.cse.lsu.edu | 1340 | 50 | 8 | 1 | | wiki-text-2-dataset1 | 3066 | 200 | 33 | 19 | | wiki-text-2-dataset2 | 3314 | 2200 | 22 | 6 | | seel.cse.lsu.edu | 2375 | 32 | 8 | 10 | +------------------+-------+--------+------------+ The following commands can also be used to filter the data if desired: -i, -k, -c, -N, -v, -T, -V, -h, -s and -W are supported. In the output, the names of the results are written to stdout/stderr if the output is the output of these options. See the manual page on the ""Filter by ID"". If you want to include another dataset in the filter, simply supply its name with a leading (or trailing) _id. e.g. -k 2.dataset will filter WNLI-dataset2, seel.cse.lsu.edu will filter seel.cse.lsu. See the manual page on the ""Filter by ID"". If you want to provide filtering for a set of ID's without including the set ID's, use -f and -g. e"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset PWC Leaderboards (restricted)?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the PWC Leaderboards (restricted) dataset in terms of Micro Precision metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Micro Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the PWC Leaderboards (restricted) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 output (NuTable): output (String): output (Str): 	False
"
 input (English text): Where can I find code references in papers that have used the Prior+Duel noop model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior+Duel noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DQN noop model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Duel noop model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Duel noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 The list of queries shown above should display the links for all the papers that use the DQN noop model to benchmark their models. If you're looking for some particular code links, look in the comments that are given at the bottom of most of these queries. Note that these query should be executed in a separate window or tab and only for this specific paper. The next time you run those queries, the results will show in an updated tab (or tab) if you use the paper tab. I can't find the source code of the prior-puzzle and duel-puzzle results This is a common issue that seems to occur, and you may have an outdated browser that's not displaying any web page because it hasn't been updated for a while. Try using Firefox or Chrome, download the latest version of the Web Inspector (recommended), and go to the following url: chrome://webinspector/content/updates Click on the tabs, search for 'browser.version.plist' in the text box next to it (if it's not installed, you can find it by executing: 'winetricks -i' it will be located along with the previous browser). If there isn't any update available, you may have to restart your browser.	False
"
 input (English text): What are the most commonly used benchmark datasets for the Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification. research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification."") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the CIFAR-10 Image Classification dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10 Image Classification"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the datasets benchmarked under the Fine-Grained Image Classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Fine-Grained Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	" This query uses the CIFAR-10 Image Classification dataset from the Stanford CIFAR-10 Image Classification Problem to search for optimal architectures as an example. Here is the output: Using the following query: ""How many of CIFAR-10-image-classifier ?"" This query has an error rate of 0.25%, so is not very useful. Results: Question #2 Question 6 Question 9 Question 13 Question 15 Question 17 In our last query, we used the Fine-Grained Image Classification problem to search for optimal architectures for classification tasks like word2vec, which, as most of you know, is a machine learning library based on the CIFAR-10 Image Classification dataset. Our results show the following: Question #2 Question 6 Question 9 Question 13 Question 15 Question 17 We can see that with this query, more than half of the nodes had more than one correct answer. Also, while only 25% of nodes had more than one answer, there were still more than 3,400 nodes where the correct answer was not present. This suggests that this query, in general, has an error rate higher than 0.10%. Question #2 Question 9 Question 13 Question 15 Question 17 In order to improve on this result, we use the next query: ""What is 10 points for the top 10 percent of nodes having the most correct answer ?"" This query used an incorrect scale for the question: The correct answer is 30 points; however, the error rate was 0.25%. In other words the correct answer for this query is not at all accurate. Results: Question #2 Question 6 Question 9 Question 13 Question 15 Question 17 With this query, we again saw more than a third of nodes have more than one correct answer. However, the error rate has been reduced to 0.22% this time."	False
"
 input (English text): What are the models that have been benchmarked on the DRI Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRI Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the DDI extraction 2013 corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DDI extraction 2013 corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRI Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?metric = ?eval. ?metric rdfs:label = ?metric. ?metric rdfs:lbl = ?metric. OPTIONAL { ?metric rdfs:label. } ?metric rdfs:lbl ?metric rdfs:lbl input (English text): Can you name the metrics used when benchmarking models on the DRI Corpus dataset ? output (Sparql query): SELECT ?metric = ?metric_lbl. ?metric rdfs:label = ?metric_lbl. ?metric rdfs:lbl orkgc:Query ?dataset_lbl. OPTIONAL { ?metric rdfs:label. } ?metric rdfs:lbl ?metric rdfs:lbl Input Text C++ Library Output C++ Library: using System; using Windows; class Input { private static ReadDataReader inputs; static Input() {} static void Main(string[] args) { // The main function of this class is to parse and return a list of words. // We are going to use a text input from a PC-BSD system that has been read using this library that has been modified by using. // The output will include the raw text of the PC-BSD system. List<string> newWords = { ""Soybeans"", ""apple"", ""candy"", ""apple"", ""apple"", ""orange"", ""candy"", ""car"". }; PC-BSD system = new PC-BSD( ""Unix/unix""; 0, true, &inputs); int words_per_sentence = system.Length; for (int i = 0; i < words_per_sentence; i++) { String words = system.GetString"	False
"
 input (English text): List the metrics that are used to evaluate models on the WOS-5736 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the IMDb benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl FOR EACH SELECT EGBIND( ?dataset_lbl),rdfs:filter(str( RDFS.format_name(RDF_INF)); RDFS.format_name(RDF_INF)); EGBIND ( ?dataset_lbl),rdfs:filter(str( RDFS.format_name(RDF_INF)); RDFS.format_name(RDF_INF)); input (English text): List the metrics that are used to evaluate models on the BRIEF benchmark dataset ? output (Sparql query):  SELECT DISTINCT ?metric ?metric_lbl FOR EACH SELECT EGBIND( ?dataset_lbl),rdfs:filter(str( ?dataset_lbl = ""BRIEF"")) ?benchmark orkgp:HAS_DATASET ?dataset,rdfs:filter(str( ?dataset_lbl = ""CAT"")) EGBIND ( ?dataset_lbl),rdfs:filter (str( ?dataset_lbl = RDS(RANK)$CAT))) EGBIND ( ?dataset_lbl),rdfs:filter (str( ?dataset_lbl = RDS(RANK)$LOB)) EGBIND ( ?dataset_lbl),rdfs:filter (str( ?dataset_lbl = RDS(ADG_SCI_STAT))) ?benchmark,rdfs:filter(str( ?dataset_lbl = RDB_RANK )) EGBIND ("	False
"
 input (English text): What is the name of the top performing model in terms of Percentage Error score when benchmarked on the CIFAR-100 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Percentage correct score when benchmarked on the CIFAR-100 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage correct"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Unpermuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfS:label ?metric_lbl. FILTER ( ?metric_lbl = ""Unpermuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfS:label ?dataset_lbl. FILTER ( ?dataset_lbl = ""Sequential CIFAR-10"") ?benchmark or kgp:BIDGE_N. ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ? eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfS:label ?metric_lbl. FILTER ( ?metric_lbl = ""Unpermute Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfS:label ?dataset_lbl. FILTER ( ?dataset_lbl = ""Sequential CIFAR-10"") ?"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10, 1000 Labels dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10, 1000 Labels"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CoNLL04 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?label ?lbl. FROM [coen.nl/dataset_tables-01] [coen.nl/dataset_tables-02] | # of benchmarks { bench: ?Benchmark of coen.nl/datasets-01, ?Benchmark of coen.nl/datasets-02 }. input (English text): Given that we have the following data sets and indices: 1) CoNLL04: CoNLL04 datasets 2) 1000 Labels: 1000 labels. output (Sparql query): SELECT ?label ?labels. FROM [coen.nl/dataset_tables-01] [coen.nl/dataset_tables-02] | # of benchmarks { bench: Benchmark of coen.nl/tables-01, Benchmark of coen.nl/tables-02, Benchmark of coen.nl/tables-03 }. input (English text): How many of those benchmarks do you intend to run for this dataset ? output (Sparql query): SELECT ?benchmark ?benchmark_rank. FROM ( coen.nl/dataset_tables-01 ). [coen.nl/dataset_tables-02] | # of benchmarks { bench: Benchmark of coen.nl/tables-01, Benchmark of coen.nl/tables-02, Benchmark of coen.nl/tables-03 }. input (English text): How many benchmarks do you intend to run for this dataset ? output (Sparql query): SELECT ?benchmark ?benchmark_rank. FROM ( coen.nl/dataset_tables-01). [coen.nl/dataset	False
"
 input (English text): What are the models that have been benchmarked on the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the ScienceCite dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the SciCite benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; model ? rdfs:label ?model_lbl. name, id, label ; ORGANIZATION, model_rdfs ?organization_lbl. name, id, label ; ORGANIZATION, model ?organization_rdfs. name, id, label ; model ?organization_rdfs. description, id, label ; orkgp:HAS_METHOD ?method. name, id, label ; ORGANIZEORGANIZEOrGANIZANTY ORGANIZARGONIST. method ; ORGMANUFACTURE ORGONIZATION. orkgp:HAS_STRING ?string. label, id, label ; ORGANIZEORGANIZATION ?organization_rdfs:DESCRIPTOR ?"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Road Runner dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Video Pinball dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Video Pinball"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?paper ?paper_lbl WHERE : ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?Benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Tennis dataset ? output (Sparql query): #(rdfs:label ?paper) SELECT ?paper :benchmark ?paper_lbl. input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Video Soccer dataset ? output (Sparql query): #(rdfs:label ?paper) SELECT ?paper :benchmark ?paper_lbl. input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Video Tennis dataset ? output (Sparql query): #(rdfs:label ?paper) SELECT ?paper :benchmark ?paper_lbl. input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari Jaguar (JUFA) dataset ? output (Sparql query): #(rdfs:label ?paper) SELECT ?paper :benchmark ?paper_lbl. input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari Jaguar (JUFA) video game, and the Atari Jaguar (JUFA) data dump ? 	False
"
 input (English text): List the metrics that are used to evaluate models on the MAZEA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MAZEA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the OpenBookQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OpenBookQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" select _id from test set with _id as id select _name from test set with _name as name select n from test set with n as n_val select n_score from test set with n_score as n_score input (English text): Select the number of metrics of the VTAB-1k test set (with the test set_id value set to ?dataset ? for the VTAB-1k test set) as well as the average value of each metric with the "" ?test"" option for each metric: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 select n as ?val, n_values ( ?val ) from test set_result with test set_id as test_id select n as ?val with vtab_1k = n_value select n as ?val_as_score from test set_result with test_id as test_id input ( English text ) : Select the number of metrics of the VTAB-1k test set (with the test set_id value set to ?dataset ? for the VTAB-1k test set) as well as the average value of each metric with the "" ?test"" option for each metric: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 select n as ?val, n_values ( ?val ) from test set_result with test set_id as test_id select n as ?val with vtab_1k = n_value select n as ?val_as_score from test set_"	False
"
 input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the CoQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Overall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the ObjectNet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Input the ""benchmark"" parameter, and the above queries will yield the top three models. If you want to benchmark any model in your dataset, you'll typically have to go to the ObjectNet website and click the ""Benchmark"" button. I've given the two most recent top models to the query ""select ?metric as ?metric_lbl and ?dataset as ?dataset_lbl"". The ""dataset"" parameter must be an ORGANESS model (that's: a hierarchical structure that contains the models for each part of the brain). If you want to benchmark a model that is not an ORGANESS, just select the model as a query parameter. Please note that the database columns that you pass into the query ( ?dataset= and ?model=- ) must have the database schema. They must match the schema of the object you want to benchmark. You can use the following two queries: select ?metric as ?metric_lbl2 and ?dataset as ?dataset_lbl22 from brainbenchmarks1 where csv_model= ?dataset1 and csv_model=-; select ?metric as ?metric_lbl3 and ?dataset as ?dataset_lbl32 from brainbenchmarks1 where csv_model= ?dataset1 and csv_model=-; select ?metric as ?metric_lbl22 from brainbenchmarks1 where csv_model= ?dataset1 and csv_model=-; The csv_model parameter is set to TRUE in the brainbenchmarks1 tables. You can have an ORGANESS model for each part of brain. This will provide the best results when you benchmark both the"	False
"
 input (English text): Provide a list of papers that have utilized the Rfa-Gate-Gaussian-Stateful (Small) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rfa-Gate-Gaussian-Stateful (Small)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Sarsa-φ-EB model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Sarsa-φ-EB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Rfa-Gate-arccos model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rfa-Gate-arccos"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ? model_lbl) = ""Rfa-Gate-Bayesian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code; orkgp:HAS_SOURCE_CODE ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have used the Rfa-Gate-Bayesian model in any benchmark ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ? model_lbl) = ""Rfa-Gate-Bayesian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code; orkgp:HAS_SOURCE_CODE ?model; orkgp:HAS_SOURCE_CODE ?code. } (Note: The"	False
"
 input (English text): Indicate the model that performed best in terms of NER Macro F1 metric on the ADE Corpus benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NER Macro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ADE Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NER Macro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Output from the DBCC2 CSV query: Breadth: 10.03m/s Matching of results: 8/14 Output from a DBCC2 CSV query; (data set 1) Breadth: 10.53m/s Matching of results: 5/28 Breadth: 10.05m/s Matching of results: 5/28 Breadth: 9.55m/s Matching of results: 6/28 Breadth: 9.36m/s Matching of results: 8/28 Breadth: 9.41m/s Matching of results: 8/28 Bread"	False
"
 input (English text): List the metrics that are used to evaluate models on the BIOSSES benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the SciFACT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciFACT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the BioASQ dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric_lbl. } How do we compare data sets ? How exactly do multiple metrics compare to each other ? How do multiple metrics compare to each other ?   How does a metric compare to a metric from a different dataset that already have its own metric ? In a multi-metric environment we can combine multiple metrics into one metric. In this case the combined metric can be compared to their parent metric. To achieve this task we should create a new dataset that contains all of the metrics. Let's write the following SQL statement: CREATE TABLE ' ?lbl' ('metric' STRING, 'lbl' TEXT ) ON DUPLICATE KEY UPDATE ROLLBACK ON FOREIGN KEY UPDATE ROLLBACK ON DELETE SET 'Metric' = ' ?Metric' FOR EACH ROW AS SELECT 'lbl','metric' FROM'metrics' WHERE 'lbl'.=' ?lbl' AND 'lbl'.=' ?metric' Output (Sparql query): CREATE TABLE ' ?lbl' ('metric' STRING, 'lbl' TEXT ) ON DUPLICATE KEY UPDATE ROLLBACK ON FOREIGN KEY UPDATE ROLLBACK ON DELETE SET '"	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Atlantis dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Centipede"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Here we can see that we now have some data in our schema. In each row we have several columns describing a few different entities, such as models, data sources, performance metrics, and more. Let's apply the output of the schema to another dataset. We want the highest performing model to have the highest score. For this purpose, we will first create one new Table called Model and then add the first row of the current table to it. With the schema fully constructed, we call the create_columns method. When executing the schema, the column columns of the tables are automatically inserted into the data model. You will notice the columns, and the data model table will be defined in the ""Model"" class. We use the class constructor to define the table. This time we use the new keyword to provide a default value for the table fields. The first and only field in the table is the name of our new model. Since we are specifying a default value for this field, we need to name the fields in the model. We use an ""object_names"" to define our model and add a column. The second field is the type of our model. We'll use ""class_name"" to represent this field and we'll use the default value of class_name. This should now be the only field in the table. The third field we define to hold the score metric. This is a String literal that, in this case, is ""Scores"". We will provide a numeric value for this field and this will be a single Number field using our default format. In this case, the Field is automatically added to the ""Score"" column in the data model (i.e., we haven't added it to the schema). The fourth field we define is the title as a String ""Title"". We will use this field to define the title"	False
"
 input (English text): What are the most commonly used benchmark datasets for the Semantic Textual Similarity research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Semantic Textual Similarity"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Named entity recognition research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Named entity recognition"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Natural Language Inference research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Natural Language Inference"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Natural speaking"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } input (English text): Do you know if they are comparable ? output (Sparql query): Select the first occurrence of each query from a database where the query type is NUML. input (English text): Do you know if they are comparable ? output (Sparql query): SELECT NVAL(a, b) as a_test, NVAL(a, b) as b_test FROM a_test b WHERE < ?dataset ?dataset_lbl. ?criteria ?criteria_table_lbl. < ?criteria ?criteria_table. ?tbl a, ?tbl b WHERE a_test = ?criteria_table_lbl. ?criteria ?criteria_table. ?key ?key_lbl. ?value ?value_lbl. ORK ?key_lbl, NVAL(a_criteria, b_criteria)) AS a_test, NVAL(a_criteria, b_criteria) AS b_test OUTPUT (parallel) WHERE a.criteria LIKE ?key_lbl. a.samp LIKE ?key_lbl. (1 row) The data was obtained using"	False
"
 input (English text): What models are being evaluated on the CIFAR-10 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Classic dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classic"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""COS-11"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Which models are to be evaluated ? CIFAR-10 - COS-11 - Model-12 - Model-13 - Model-14 - Model-15 - Model-16 - Model-17 - Model-18 - Model-19 - Model-20 - Model-21 - Model-22 - Model-23 - Model-24 - Model-25 - Model-26 - Model-27 - Model-28 - Model-29 - Model-30 - Model-31 - Model-32 - Model-33 - Model-34 - Model-35 - Model-36 - Model-37 - Model-38 - Model-39 - Model-40 - Model-41 - Model-42 - Model-43 - Model-44 - Model-45 - Model-46 - Model-47"	False
"
 input (English text): Where can I find code references in papers that have used the DeiT-B model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-B"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DeiT-B-384 model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-B-384"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DeiT-Ti model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-Ti"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- 	False
"
 input (English text): In what country was conducted research with the largest number of participants?
 output (Sparql query): SELECT ?country WHERE { orkgr:R44980 orkgp:compareContribution ?contrib. ?contrib orkgp:P15249 ?country; orkgp:P23169 ?number_of_participants. } ORDER BY DESC( ?number_of_participants) LIMIT 1
 input (English text): What is the total number of patients in the studies?
 output (Sparql query): SELECT SUM( ?number_of_patients) WHERE { orkgr:R33008 orkgp:compareContribution ?contrib. ?contrib orkgp:P15585 ?patients. BIND(xsd:integer( ?patients) AS ?number_of_patients) } 
 input (English text): For which country of study overall prevalence of epilepsy is the highest?
 output (Sparql query): "	SELECT ?country, ?country_label WHERE { orkgr:R75729 orkgp:compareContribution ?contrib. ?contrib orkgp:P15512 ?country. ?contrib orkgp:P16013 ?overall_prevalence. ?country rdfs:label ?country_label. ?overall_prevalence rdfs:label ?overall_prevalence_value } ORDER BY DESC( ?overall_prevalence_value) LIMIT 1	SELECT numAce( 0 ) AS name, numAce(1) AS numAce, numAce(2) AS numAce, numAce(3) AS numAce, numAce(4) AS numAce FROM DUAL_GROUP AS numAce INNER JOIN numSage AS numsage ON numAce. numAceID = numSage. numAceID WHERE numAce. numAceID BETWEEN 1 AND 2 AND numAce. numAceID BETWEEN 3 AND 4 input (English text): In which country of study did this study use the largest group of participants ? output (Sparql query): SELECT numAce WHERE numAce. numAceID BETWEEN 1 AND 2 AND numAce. numAceID BETWEEN 3 AND 4 input (English text): In which country of study were the participants in the trial evaluated ? output (Sparql query): SELECT numAce, numAceID FROM ( SELECT * FROM dbm_group AS group ) AS group INNER JOIN numSage AS group ON group.numAceID BETWEEN 1 AND 2 AND group.numAceID BETWEEN 3 AND 4 GROUP BY numAceID ORDER BY group.numAceID input (English text): In which country of study did this study measure a potential effect of a diet ? output (Sparql query): CREATE FUNCTION diet_effect() RETURNS bool AS $$ BEGIN RETURN diet_effect(1), diet_effect(2), diet_effect(3) END ; $$ LANG	False
"
 input (English text): What is the highest benchmark result achieved on the ShARe/CLEF eHealth corpus dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the ShARe/CLEF eHealth corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Please complete and submit the following statements. Name (required) Email Address Country of Residence/Occupation Email Address City, State, and Country of Residence Please select the model(s) from each of the above tables to be evaluated. When submitting a query, only the one model with the highest score in your case will be selected. The highest score is either 100 or above. As an example, a query might require a score of 1 and a 1 to 2 in a particular metric. The score of 1 would be accepted, if the average score in that metric was 1.5, the model would be evaluated as an 4.5/1. As an example, a query might require a score of 2 and a 2 in a particular metric. The score of 2 would be accepted. Your score cannot exceed the maximum score. Your score is shown with an asterisk(*) in the lower right corner of this window. If you have any questions, please contact your LIDA/SCHOLARSHIP supervisor. Your LIDA/SCHOLARSHIP supervisor will be notified as soon as you submit the form.	False
"
 input (English text): What models are being evaluated on the HMDB51 (finetuned) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the HMDB51 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the HMDB51 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" (in-memory) input (English text) output (Sparql query) output (Text) I have an existing dataset on the Heterogeneous Machine Database (HMMD), and I plan to use it along with its reference data for benchmarking. The data will be stored in a local version of HMMDB, on-demand at any time. The local version will have its own cache file, and its own index, and a separate thread for database reads/writes. The data will be accessed via an interface similar to the one provided by the R interface to HMMDB. The client will be a web application that will use different API endpoints (for example: file download, query, write, etc.). The client will not directly interact with the HMMDB but a client/server architecture will be used for communication between the client and the HMMDB. This enables parallel access to the underlying collection of HMMDB rows using the HMMDB's internal RDBMS features. This is an older version of this benchmark, because I'm not using it anymore. The previous version is below (with the latest improvements). I've provided a short summary as well as the full benchmark to compare its performance with other similar datasets. -- Summary (R data as text) -- This is the benchmark that is specified in R (but not run to completion yet). -- A description of the paper. ------------------------------------------------------------------------------ -- A very simple benchmark for training models using the HMDB benchmark dataset. -- The HMMDB reference is a HMMDB dataset that is being trained with the R package -- ""lmdb"" [link]. -- No tuning is done by the R package ""lmdb"", since there is no default tuned model. -- (If you train the R package ""lmdb"", you run the tuning in steps,"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Krull dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Krull"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Venture dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 This is an interesting question since the Venture dataset is a well-known dataset with extensive benchmark results and code available. I should point out that the data underlying this dataset is quite old, but given their wide coverage it is safe to say that they may have a good representative sampling. Unfortunately, they are not a good benchmark for the Atari 2600, as the benchmarks we performed show that the Atari 2600 wasn't the fastest of the 3 consoles. I do believe that the Venture dataset is useful and could be used as such, but as the Venture has since been discontinued I don't think it will be suitable and useful for benchmarks. In the meantime, I think it's interesting to consider how well-known and influential benchmarks work on older systems in order to explore benchmarking more generally. In other words, I just want to explore benchmarking, which is something that probably hasn't been explored fully, and which I think should be discussed. As I looked over the literature, I started looking for benchmarking packages that didn't involve doing the benchmarking yourself. I found two that seemed interesting, and that didn't seem too taxing for both users and authors: Benchmark: Benchmarvin is written in R and based on a benchmark for the Atari 760. It is written in a simple R wrapper and runs all Atari 2600 benchmarks. The benchmarks run in linear time (the user doesn't need to do any math or calculate offsets, but it does perform the benchmarking). The data format has been deprecated since 2010. It can be used for testing with the rbench package or the R benchmarking package, but you need to have a copy of SDS version 0.18.9 or higher installed (downloads are available). When run without a benchmarking session, the benchmarvin binaries run the benchmark once for each core and then exit. It can be used with multiple cores and with multiple output files	False
"
 input (English text): List the metrics that are used to evaluate models on the CUB-200-2011 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the CoNLL 2012 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	------------------------ DISTINCT ?lbl_x ?lbl_y ?lbl_lbl ?lbl_rt ?lbl ?lbl_rt ? input (English text): ------------------------ ROW # TEMP REFERENCES ABOVE ------------------------ [](t.cub.v6.0/index.php ?option=com_content&view=article&id=27) TEMP REFERENCES ABOVE ------------------------ [](t.cub.v6.0/index.php ?option=com_content&view=article&id=18) BEGIN CUB_200-2011 ROW [](t.cub.v6.0/static/index.php ?option=com_content&view=article&id=26) CUB_200-2011 ROW [](t.cub.v6.0/static/index.php ?option=com_content&view=article&id=28) CUB_200-2011 ROW [](t.cub.v6.0/static/index.php ?option=com_content&view=article&id=29) CUB_200-2011 ROW [](t.cub.v6.0/static/index.php ?option=com_content&view=article&id=30) CUB_200-2011 ROW [](t.cub.v6.0/static/index.php ?option=com_content&view=article&id=2) CUB_200-2011 ROW [](t.cub.v6.0/static/index.php ?option=com_content&view=article&id=31) CUB_200-2011 ROW [](t.cub	False
"
 input (English text): Provide a list of papers that have utilized the Weighted Tsetlin Machine model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Weighted Tsetlin Machine"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TDMS-IE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tsetlin Machine"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tsetlin Machine"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tsetlin Machine"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tsetlin Machine"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } orkgp:HAS_DATASET (Text) input (English"	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 HERO dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 HERO"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Breakout dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	------------ ----------------- 1.0.93715 3.0002676 2.606054 2.0.378617 -------- -6.63715 ----------------- Input: English text: What is the top benchmark score and its metric on the Atari 2600 Breakout dataset ? output (Sparql query): ------------ --------------------- 1.0.049 3.000046 2.680025 2.0.534 1.81456 ------------ -------6.63715 ----------------------------------- Input: English text: What is the top benchmark score and its metric on the Atari 2600 DataQuest dataset ? output (Sparql query): --------------- ---------1.0.93715 3.0002676 2.606054 2.0.378617 -------- -8.63715 ---------------5.63715 Input: English text: What is the top benchmark score and its metric on the Atari 2600 DataQuest dataset ? Output: --------------- -------------1.0.93715 3.0002676 2.606054 2.0.378617 ----- -7.63715 ----------------- Input: Java text: What is the top benchmark score and its metric on the Atari 2600 DataQuest dataset ? output (Sparql query): --------------- ---------- 2.0.378617 6.0000004 3.383057 4.163374 6.005028 --------1.81456 ------5.63715 --------1.81456 -------- -----6.63715 ----------------- Input: English text: What is the top benchmark score and its metric on the Atari 2600 AtariAge dataset ?  output: -------- 5.521000 7.500000 3.166767 2.0006681 3.168988 -------- 8	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Tennis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Computer game dataset ? Output (Sparql query): SELECT ?score|| ?dataset from ( 1 2 3) a ORGPCAST(1) a ORGPCAST(2) a ORGPCAST(3) a WHERE... ORGPCAST(1 ||GPCAST(2)) rdfs:name ||rdfs:labels; query(2)|:dataset a orkgp:HAS_DATASET ?d; orkgp:HAS_EVALUATION ? eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } output (Sparql query): SELECT SRCURF(a||0.0)||DISTINCT ?input from ( 1 2 3) a ORGPCAST(0) orggp:HAS_DATASET||dataset(0)|:dataset (0)| ORGPCAST(2) rdfs:name ||r"	False
"
 input (English text): List the code links in papers that use the QA-GNN model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""QA-GNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DQN noop model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the FQF model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FQF"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Inputs : (English text): 1. An entry is made in the following form: Inputs : (English text): 3 1. An entry is made in the following form: Inputs : (English text): 4 1. An entry is made in the following form: Inputs : (English text): 4 1. An entry is made in the following form: Inputs : (English text): 5 1. An entry is made in the following form: Inputs : (English text): 5 2. A code link is provided as an input of the query. The code link is the link itself 3. A list of code links is provided as an input Outputs: 1. You can use the QA-GNN model in all benchmarks. 2. You can use the DQN noop model in all benchmarks. This example also shows use of the non-zero DQN noop model in any benchmark. 3. You can use the FQF model in all benchmarks. This example also shows use of the non-zero FQF model in any benchmark. 4. You can use the full QA-GNN model (with all the features that are available in the full QA-GNN model) in all benchmarks, or use the non-zero DQN noop model (with all the features that are available in the QA-GNN model) only in any benchmark.	False
"
 input (English text): Indicate the model that performed best in terms of % Test Accuracy metric on the SNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""% Test Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the PROTEINS benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PROTEINS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Sequence error metric on the FSNS - Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Sequence error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" RDFS Language Support for Sparse Data We offer multiple RDFS language support to facilitate your web application's use of data provided by this project. We support both human and machine-based data processing. For human-based data processing, you can use the text processing functionality to perform text analysis on a single document that can be processed into Sparse Column Store Objects that are then processed further by machine learning and language modeling tools using the DataFrame model data structure and the RDFS language support. Alternatively, the machine-learning data can be extracted and further refined using the TextGen annotation tool through the RDFS Language Support module. What are Machine Learning models ? The RDFS provides a framework to build RDFS language models for machine learning applications that are available in RDFS, RDFS to XML and RDFS to JSON. The models are designed for building RDFS language models for text data analysis, but can also be used for building RDFS language models for machine learning applications. Machine learning models work by applying model constraints to a collection of structured documents (called a sentence) to improve (improve) query accuracy. In RDFS models, ""constraint resolution"" means that RDFS models provide new sentences and the features associated with words that are present in existing sentences, but that do not constitute statements in the original documents. For example, machine-learning models are interested in whether words appear more than once in a particular sentence, whether a particular sentence contains multiple independent clauses, and whether multiple sentence parts belong to the same sentence. This allows the text data to be parsed as a single stream and does not require the application of language model constraints for every sentence within the text. Machine learning models have been used in text processing applications because the text data can be transformed into a variety of different formats. An RDFS model is one"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ARC-PDN?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the ARC (Easy) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Easy)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset ARC (Challenge)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl I'm not really sure if I made any sense, or I even need the translation, but hopefully it helps."	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SciGEN dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SciREX dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciREX"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciREX"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Input (English text): Which dataset should I use when preparing the benchmark ? Output (Sparql query): SELECT ?dataset a asdataset FROM _dataset_solution WHERE ( ?dataset_t = ""SciGEN"") AND ( ?dataset_t = ""ScienceIE"") AND ?benchmark ?benchmark_t OR ?benchmark ? ?; Input (English text): Which dataset should I use when preparing the benchmark ? Output (Sparql query): SELECT ?""benchmark"" ?benchmark_t OR ?""benchmark"" ?benchmark_t FROM _benchmark_solution ? ( ?dataset_t) asdataset WHERE ( ?dataset_t = ""ScienceIE"") AND ( ?dataset_t = ""SciGEN"") IN (( ?dataset_t IN ""SciGEN"") ?benchmark or ?benchmark ? ?); Input (English text): Which dataset should I use when preparing the benchmark ? Output (Sparql query): SELECT ?""benchmark"" ?benchmark_t ORDER BY ?""benchmark"" ?benchmark_t; Input ("	False
"
 input (English text): List the metrics that are used to evaluate models on the CS-NER benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CS-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the TSE-NER dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 The last test used to identify models that performed well on the benchmark dataset and are ranked in the top 2/5 at the top of their class as determined by the RTE dataset ranking algorithm in R. The first query is very similar to the previous query in that it lists the metrics that are used to evaluate models on the CS-NER benchmark dataset and it lists the metrics of the first two models that perform very well in the benchmark dataset that has been ranked by the RTE dataset ranking algorithms to rank the other models in the top 5. In this query the inputs are not the English text but the RTE dataset rank and the output is a short SQL query where the output is the RTE class of the first two models that performed very well on the benchmark dataset. We are planning to add a second test with a similar query but at least two models (one from the TOP_5 that performed well on the CS-NER dataset and one from the TOP_30 that performed well on the TOP-30 dataset) are used to rank the scores of each model on each of the two benchmark dataset. However, one of the questions the first test asked is whether to include, in addition to the top model, another model from the TOP-30 that performed well on the TOP-30 dataset and so this test will have a different set of data. The second test will be the TOP_50_10 dataset that had a large amount of data (roughly 500M rows) with ~500 million RLS in the dataset (or one of six different RLS files), and where the model scores, if any, in one or two of the datasets above will have been used to rank the TOP_50_10 datasets. If we wanted to compare two or three different models (with different performance criteria based on the metrics the RTE dataset ranks them in) for their accuracy and model	False
"
 input (English text): Where can I find code references in papers that have used the Contextual Match model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Contextual Match"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Relation-Metric model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Relation-Metric"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Concept Mention Extraction model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Concept Mention Extraction"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label } ?model_lbl. FILTER (str( ?model_lbl) = ""Concept Mentions Extracted"" ) ?benchmark. ORkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	False
"
 input (English text): Can you provide links to code used in papers that benchmark the OTF spelling (single) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""OTF spelling (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the BERT-joint model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT-joint"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the OTF spelling+lemma (single) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""OTF spelling+lemma (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Scheduled for 8 November 2009. I've prepared code-sets for the BERT-joint model, and also tried these benchmark papers directly. The code results are in a single.tar.gz file, which can be burned into a.dmg file or downloaded directly. The benchmarks (not necessarily the code as it were) do not include the tests of the original OTF spell-checkers, and have been prepared for convenience if that is the case, but I'd expect the same results in an alternative spell-checker. The code outputs are in HTML format, so a browser can load it directly or use a web-browser for other purposes. This is an update of the article based on an earlier version published on 26 March 2008 (with some corrections). The original source code is available on the OpenTTD website.	False
"
 input (English text): Provide a list of papers that have utilized the 3-layer AWD-LSTM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""3-layer AWD-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the LN HM-LSTM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LN HM-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the A3C LSTM hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C LSTM hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Input language: language: eng (language code of translation) output language: output language: farsi (language code of translation)  input (English text): Provide a list of papers that have utilized the MIRL VLM model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a, dt ? orkgc: Model ; hdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MIRL VLM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Input language: language: english (language code of translation) output language: output language: french (language code of translation) input (English text): Provide a list of papers that have utilized the A4F model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a, dt ? orkgc"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset SVHN?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SVHN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Accuracy score on the Ohsumed benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ohsumed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Percentage error score on the SVHN benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SVHN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric A orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset A orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SVHN"") ?dataset ORKCF:HAS_DATASET ?dataset. ORKCF:HAS_EVALUATION ORKCSF:HAS_RANSFER ?eval ORKCSF:HAS_VALUE ?val. } ?cont orkgc:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgc:HAS_MODEL ?model. }"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the LSTM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the AWD-LSTM (3 layers) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM (3 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LayerNorm HM-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT TITLE ?dataset ?model name ?model_rdf ?rfl_class., COUNT(*) ?rfl_class. ?rfl_class., ?dataset. ?name ?model_rdf. input (English text): Select papers where the model was used in benchmarking benchmarking results. output (Sparql query): SELECT TITLE ?dataset ?model name ?model_rdf input (English text): Select papers where the model was used in benchmarking benchmarking results. output (Sparql query): SELECT TITLE ?dataset ?model name ?model_rdf 3) The output of the first query that generates a list of papers for the benchmark. input (English text): Top 1000 papers that use the LSTM model in any benchmark. output (Sparql query): SELECT TITLE ?datasets. ?model_rdf as ?labels; input (English text): Top 1000 papers that use the LSTM model in any benchmark. output (Sparql query): SELECT TITLE ?datasets. ?model_rdf as ?labels; 4) The output of the second query that generates a list of papers for the benchmark. input (English text): Top 1000 papers that use the LSTM model in any benchmark. output (Sparql query): SELECT TITLE ?data ?repo ?dataset. Input data: The journal title of this paper. Please choose	False
"
 input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""SPARQL query optimization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the datasets benchmarked under the relation extraction research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""relation extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""SPARQL query optimization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 output (RDF string): List the datasets benchmarked under the RELATED_DATA_SEPARATION research problem ? input and output (English text): List the datasets benchmarked in relation to the ROI_SET_RECORDING_EXPANSION research problem ? input and output (English text): List the datasets benchmarked when ROI_SET_RECORDING_EXPANSION is specified when running from a schema ?	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Amazon dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Error score on the Amazon-2 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Amazon-2 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?metric ?metric_lbl (max( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?( MAX( ?value) AS ?score) FROM ?metric ORDER BY DESC( ?value) } OPTIONAL { ?eval orkgp:HAS_DATASET ?dataset. orkgp:HAS_EVALUATION ?eval ORDER BY DESC( ?value) } } output (Sparql query): SELECT ?metric_lbl ?value ?( ?score) ?dataset_lbl LIMIT 1.10 Input (English text): Which data type would you like to input in the query ? (Input Text) output (Parse Output) SELECT ?""dataset_lbl"" ?('Amazon/2') ? ?metric ""Metric for Amazon/2"" FROM ?metric_lbl ORDER BY ?metric ?metric_lbl ?( MAX( ?value) AS ?score ) This is an example of parsec output when you are running on a server: SELECT DISTINCT ?dataset_lbl ?value ?dataset_lbl LIMIT 1.10 input (English text): Are you aware of any problem, if the test is repeated on another server ? output (Sparql query): SELECT DISTINCT ?dataset_lbl ?value ?dataset_lbl LIMIT 1.10 Input (English text): Describe any problem, if the test is repeated on other machines ? output (Sparql query): SELECT ?dataset_lbl ?value ?dataset_lbl LIMIT 1.10 Input (English text): What is your server's operating system ?"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CNN + LSTM + SVM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CNN + LSTM + SVM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the RNN model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""RNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Temporal Convolutional Network model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Temporal Convolutional Network"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TensorBoard"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } The output of RNN and DLM models depends on the parameters of the RNN and the parameters chosen for the DLM. If these parameters are not known (or were poorly chosen) we want the data to be very good. However, for model comparison we can only have a good result when the inputs are known. In order to make sure we know the input and the output, let's use R to find the parameters for the model. R will do this very easily: in the case of the CNN model, we get the parameters: in the case of a LSTMs, we can get the parameters: in the case of a BNN, we can get the parameters: If you don't know this or need more informations, you can use the following code: R will give you the parameters if (model.type) & 1, R will give you the data as a list of RNNs model.getModelParameters() 1 2 3 4 The above code will give you the following: The same code in R will give the following: 3.2.3. Data Analysis The code below is a simple query to look after some metrics which can be used to decide whether an improvement is significant or not.  4 SELECT R2[D"	False
"
 input (English text): Provide a list of papers that have utilized the BiT-M (ResNet-152x4) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M (ResNet-152x4)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiT-S (ResNet-152x4) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet-152x4)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BiT-S (ResNet) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Can you provide links to codes used in papers that benchmark the BiT-M (ResNet-152x4) model ? output (Sparql query): input (English text): Provide a list of papers that have utilized the BiT-M (Resnet) model and include the links to their code ? output (Sparql query): input (English text): Can you provide links to codes used in papers that benchmark the BiT-T (ResNet) model ? output (Sparql query): input (English text): Can you provide links to codes used in papers that benchmark the BiT-K (ResNet-152x4) model ? output (Sparql query): input (English text): Can you provide links to codes used in papers that benchmark the BiT-K (ResNet-152x4) model ? output (Sparql query): input (English text): Can you provide links to codes used in papers that benchmark the BiT-D (ResNet-152x4) model ? output (Sparql query): input (English text): Can you provide links to codes used in papers that benchmark the BiT-D (ResNet-152x4) model ? output (Sparql query): input (English text): Can you provide links to codes used in papers that benchmark the BiT-K (ResNet-152x4) model ? output (Sparql query): input (English text): Can you provide links to codes used in papers that benchmark the BiT-D (ResNet-152x4) model ? output (Sparql query): input (English text): Please provide links to code used in the Resilient Link Search (	False
"
 input (English text): What is the highest benchmark result achieved on the QNLI dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WSC dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WNLI dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT first( ?, 20, 'row_number') ?column as ?column_to_count FROM table1 ?column UNION all(row_number, ?column)  ( ?column, ?column_to_count) The query performs two checks: First, whether the output of the second query should match the first row in the table WSC to count the first column ? and second, if the result is a unique value from the table WSC, then it also counts the first row the first query returned in the table WSN. The first check is not present in our new form, because using a query it is harder to define these two results and therefore the constraint that WSC is not unique also doesn't check. The second check may be also not present with the	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the AAPD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark _ orkgp_kpk; orkgp:HAS_DATASET ( ?dataset_hls ?dataset_ydf ?label ?dataset_lbl); orkgp:HAS_HAS_PERFORMANCE ?eval. ?eval orkgp:HAS_NUMERIC ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ORDER BY DESC( ?value) } } GROUP BY orkgp_kpk ?label  ( ?value ASC ) The above query shows that an ACL-ADC score is higher than the AAPD baseline score of 1.0. The query shows that this higher score is not from an improved performance on ACL-ARC, but from a better fit on the AAPD dataset. Output: ACL-AAPD (1.0) 1.0 3.0 7.0 6.0 ACL-ADC (1.0) 3.0 5.0 7.0 6.0 In the first query the output matches the ACL data. The second query shows that the ACL-AAPD baseline score is a third of the ACL-ADC value. In the first query, the ACL-AAPD result is closer to 0"	False
"
 input (English text): List the metrics that are used to evaluate models on the FTD dataset benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTD dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the FB15k dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FB15k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the FTD dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTD dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT COUNT(*), ?column FROM d_columns WHERE { ?dataset a orkgc:Dataset; } ORDER BY id; output (Sparql query): select key, column from p_dst_query where key in ( ?col:A) ORDER BY id LIMIT 1 2 3 4 5 return ( ( ) ) from d_columns WHERE { ? col : A || ogc : Data ; } ORDER BY id ; query is a little different than query: We'll do our aggregation, then compare the selected results against the original (we'll get the same result if we do this on other datasets, see #2). We	False
"
 input (English text): What is the top benchmark score and its metric on the CIFAR-10 Image Classification dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10 Image Classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the CIFAR-10 dataset in terms of Parameters metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Parameters"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10 Image Classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?error ?error_max. ( MAX( ?error) ) || ?error ?error_min. ( MAX( ?error_min) ) || ?error ?error_percent. ( MAX( ?error_percent) ) AND ?error OR ?error_percent rdfs:label ?error. FILTER (str( ?error_max) = ""percentage"") } } ORDER BY ?error || ?error_min } input (English text): What is the highest accuracy benchmark of the dataset on the CIFAR-10 classification dataset ? output (Sparql query): SELECT DISTINCT ?score ?score_crit OR ?score_crit_crit WHERE { { SELECT ?score OR ?score rdfs:label score_critcrit. ORDER BY DESC( ?score ) } } ORDER BY DESC( ?score) LIMIT 1 input (English text): What is the lowest benchmark of the dataset on the CIFAR-10 classification dataset ? output (Sparql query): SELECT DISTINCT ?score ?score_crit_crit WHERE { { SELECT ?score OR ?score rdfs:label score_crit_crit_crit. ORDER BY ""lower /"" rdfs:value "" > 0.05 "" ORDER BY DESC( '' /'' ) } } ORDER BY DESC( ?score ) NOT NULL LIMIT 1 input (English text): If i am a human being, which is the BEST model for classifying CIFAR-10 classification images based on the CIFAR-10 dataset ? [4 options] output (Sparql query): SELECT TOP( ?test_type, 1) ?test_score ?test_crit || NULL, ?test_crit_crit || NULL"	False
"
 input (English text): What is the top benchmark score and its metric on the Walker, walk (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the Walker, walk (DMControl500k) dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 Input: Walker, walk (DMControl500k) Output: Best Model: None Best Score: 1525 (4 points out of 5) Lowest scoring model: 575 Points from the Highest Score: (6 points out of 11) Note: in this sample the model is given a higher score than the data in the sample. In the final report, the model is penalised with a sum of 3 points. This is because the score model is not penalised for more data points. The following table summarises the results obtained on the results of the three best models. Model Score, from the highest score to lowest. Best Score, for both the highest score of the best model and for the lowest score (inclusively) of the model. No. of Score, out of the total for the models. 1 1 100 1 1 101 1 1 104 2 6 99 2 6 102 3 12 98 3 12 101 4 3 99 4 3 101 5 3 101 6 4 99 6 4 101 7 2 100 7 2 100 8 4 100 8 4 100 9 6 99 9 6 101 10 1 100 10 1 100 11 0 999 12 2 100 12 3 99 13 4 99 13 4 99 14 6 99 14 6 98 15 0 100 15 0 101 16 2 99 16 2 99 17 2 99 17 2 97 18 2 98 18 2 97 19 2 97 19 2 97 20 3 66 20 3 67 21 0 100 21 0 98 22 1 100 22 1 99 23 1 96 24 1 99 24 1 99 25 0 99 25 0 99 The number of scores and points for every model was calculated by dividing the number of points earned by the total number of points the model scored. This yielded the following calculation of the best performing model: where N is the sample size, c is the confidence interval, f is the fst test score and y is the score obtained by the model.	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the PROTEINS dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PROTEINS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SciGEN dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Inputs: 1. dataset: SciGen 2. model's name: lr_gen 3. description: lr_gen 4. title: Learning lr models 5. description: lr_gen 803: Genes for lr 6. summary size: 2750 7. summary: SPSS summary information 8. training data, data points only 9. validation data, data only For the first few lines of this message we will only be taking two arguments. First of all is whether we are using the lr or the lr64 variant of the lr gene encoding. The first model we are testing is the lr_gen model from the lr_gen data set. The second line specifies the model's name for use with the data points. This is in a slightly different format and you will have to fix it to fit to your needs. Output: We want SciGen to use the SciGen dataset. The model is called lr_gen and the data points are called lr_fms. This is exactly what we want for using our lr model and the first four items of our benchmark are the things we can use. The first of these is a plot to show the performance on the SciGen. The second is an option in the model to evaluate the models performance on other datasets. This is an option that does not show up in the original model but should be very helpful in getting a good baseline against which to benchmark your model. Lr is a fairly cheap variant of the lr which allows for data points that may have high noise or low data points. The lr model is described in the lr code and the pkm code. We are using the PGC data from the Genes for Life and Luria1-Tissue database. The PGC and Luria1-Tissue datasets are fairly well	False
"
 input (English text): List the code links in papers that use the DATL model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DATL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the GPT-3 model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the POP3D model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""POP3D"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""POP3D"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } output (Sparql query) input (English text): List the code links in papers that use the HMM model on any benchmark ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a.HMM. RDF-DB-RDFSRF-REFERENCES ?dataset. ?cont orkgp:HAS_SILENT ?benchmark. ?cont orkgp:HAS_MATCH ?dataset. ?model. } input (English text): List the code links in papers that use the IEC6014 (ISO) 5010 standard in any benchmark ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model iecl:Model; rdfs:labels rdfs_lbl. FILTER (str( rdfs_lbl ) = ""ISO-5010"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_GCC_RDFS ? ?dataset. } input (English text): List the code links in papers that use"	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Double Dunk benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asterix benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Tennis benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Input Model Name [Score] Output Model Name [Score] Example 7: Compare two models of an RDF table: an RDF table and an SPSS table The SPSS model should be the best model with respect to the Atari 2600 benchmark dataset Example 7a: Identify the best model for an RDF table The RDF table should be the best model with respect to the Atari 2600 benchmark dataset Example 72a: Compare two models of a RDF table The SPSS model should be the best model with respect to the Atari 2600 benchmark dataset Example 7b: Identify the best model for an RDF table The RDF table should be the best model with respect to the Atari 2600 benchmark dataset To identify the best model for any other RDF dataset: Select the RDF model that performs the best performance in comparison to the benchmarks for each dataset/dataset Example 1: Compare two models of the Atari 2600 benchmark dataset We will use the BSP model [Batch Processing 1] with the score 1.30 and SPSS model [SPSS 1.28] with the score 0.91. To determine the best model: Select the RDF model that performs of best in comparison to the benchmark To measure the best performance of the model: The scores and The overall performance at all the benchmark steps from 1 through 18 The BSP model is better than the SPSS model at all the benchmarks that we select The SPSS model beats the BSP model under all the benchmarks that we select. The SPSS model is better than the BSP model at a majority of benchmarks that we select. The BSP model is worse than any of the models that we select. Below we will describe the steps in comparing the model:	False
"
 input (English text): Can you list the metrics used to evaluate models on the UCF101 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Reacher, easy (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the UCF101 (finetuned) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset ?dataset_lbl. DATA ?metric_lbl. FILTER (str( ?dataset_lbl) = ""UCF101"") orkgp:HAS_DATASET ?dataset_lbl. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Which evaluation metrics are commonly used when benchmarking models on the UCF101 (standard) dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101"") ?benchmark orkgp:HAS_DATASET ?dataset_lbl. OPTIONAL { ?eval orkgp:HAS_METRIC ?met"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CoQA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the MPQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MPQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MPQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 The output may be blank if the output for the CoQA benchmark dataset is not found in the dataset, or the input is an invalid CSV file. Input Data [ edit ] The list of paper IDs that have benchmarked models on this dataset can include text descriptions of the paper, including titles, abstracts and comments (in PDF format). A title is any paper title from a Journal Article in Science Citation Reports, PubMed, PubMed Central (for the period 1998-2013), or a citation to their peer-reviewed journal article. Abstracts of new papers are optional. You can select multiple paper IDs, and any one ID will be used, unless each ID takes on the case of a paper that has been published in two or more journals. To obtain a link to a specific paper, use a paper key that you set. The full text of each paper ID is in the standard CoQA format. However, to make it easier to read, each paper ID will have a short text field that corresponds to the field of the paper as displayed by the journal in the abstract. For example, suppose that we want to see the abstract of paper S1, the paper that was featured on the cover of the August 7, 2012 cover story of Nature ( http://doi. org/ 60930/nature12076 ). Here are a few sample sentences that define this document: Abstract This paper reports how a new technique could be used to find an optimal strategy for the design of human cancer therapies. Keywords S1 - Abstract This paper reports how a new technique could be used to find an optimal strategy for the design of human cancer therapies. Keywords S2 - Abstract S2 - Abstract S3 - Abstract S3 - Abstract S4 - Abstract S4 - Abstract S5 - Abstract The Co	False
"
 input (English text): What is the highest benchmark result achieved on the BC5CDR dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BC5CDR dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-chemical"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 Note: All values entered here must be numeric. If you type in a string for one of the values, you will be asked to specify a name for the metric. You can also use a value in the space-delimited form. The string must look good, but it can contain whitespace. output (Sparql query): input (English text): What are the titles and IDs of research papers that include a benchmark for the BC5CDR-chemical dataset ? output (Sparql query): Note: All values entered here must be numeric. If you type in a string for one of the values, you will be asked to specify a name for the metric. You can also use a value in the space-delimited form. The string must look good, but it can contain whitespace. output (Sparql query): input (English text): What is the highest benchmark result achieved on the BC5CDR-medical dataset, including the metric and its value ? output (Sparql query): Note: All values entered here must be numeric. If you type in a string for one of the values, you will be asked to specify a name for the metric. You can also use a value in the space-delimited form. The string must look good, but it can contain whitespace. output (Sparql query): input (English text): What is the highest benchmark result achieved on the BC5CDR-medical dataset, including the metric and its value ? output (Sparql query): Note: All values entered here must be numeric. If you type in a string for one of the values, you will be asked to specify a name for the metric. You can also use a value in the space-delimited form. The string	False
"
 input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Point Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SUCCESS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest DISTANCE_TO_GOAL score on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""DISTANCE_TO_GOAL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SUCCESS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SUCCESS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } output (Sparql query): Where ?model_lbl : Has any of these models achieved the highest SUCCESS score on the Object Nav test-std benchmark dataset ? In this example, the model 'lbl_model' has achieved the highest score on the Object Nav test-std benchmark dataset, thus achieving the highest SUCCESS score. We can also use these 'HELP' data inputs to create 'test' datasets with similar characteristics. For example, suppose that we have set up the following data inputs - 3d array consisting of a 'lbl_model_lbl' data input array and a 'HELP_data' data input array with the same number of rows as the '"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ScienceCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Paper Field dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark t:timing ||kgp:HAS_DATASET ?dataset AND (timing > 0 AND TIMING < 10) ORtiming ORtiming > 0 AND TIMING < 10) ||timing ORtiming <= 0 AND DATE(timing) ?dataset-a ORtiming ||kgp:EVALUATION ?eval. ?eval ORtiming. OPTIONAL ORtiming ||kgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. # ?timing, ortiming } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL timings, ortiming ORtiming ORtiming ORtiming ORtiming ORtiming ORtiming ORtiming ORtiming ORtiming ORtiming ORtiming ORtiming ORtiming ORtiming ORtiming"	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-Russian benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot German-to-French benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (German text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc German-to-French benchmark dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot Dutch-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkg"	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Robotank dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Robotank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Space Invaders dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?metric ?metric_lbl(MAX( ?value)) (1) GROUP BY ( ?dataset ?dataset_lbl) input (English text): What is the highest benchmark result achieved on the Atari 2600 Death and Taxes dataset, including the metric and its value ? output (Sparql query): --------------- ------------- ------------- -------------- -------------------------- ---- Atari 2600 Death and Taxes 1 5 10 9 32 18 3 6 37 19 38 15 1 1 31 39 20 2 4 41 21 2 6 16 39 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl(MAX( ?value)) (1) GROUP BY ( ?dataset ?dataset_lbl) input (English text): What is the highest benchmark result achieved on the Atari 2600 Spamball 2 dataset, including the metric and its value ? output (Sparql query): --------------- ------------- ------------- -------------- -------------------------- ---- Atari 2600 Spamball 2 5 20 41 10 45 17 7 37 16 10 1 3 46 33 19 5 6 13 39 21 2 3 22 18 2 2 9 14 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl(MAX( ?value)) (2) GROUP BY ( ?dataset ?dataset_lbl) input (English text): What is the highest benchmark result achieved on the Atari 2600 CyberWar 4 dataset, including the metric and its value ? output (Sparql query): --------------- ------------- ------------- 	False
"
 input (English text): Can you provide links to code used in papers that benchmark the ESIM + ELMo Ensemble model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ESIM + ELMo Ensemble"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the ELMo model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Select the first value in the input box and click on it. You get two results. input (English text): Are you sure that the output is correct ? (output shows correct output) output (Text): Which version of the ELMo Ensemble model were you using ? model_lbl: version, a: version, b: version input (English text): Inputs are sorted output (Text): Which version of BiDAF or Self-attention Ensemble were you using ? input (English text): Which version of the ELMo Ensembler were you using ? ELMo Ensemble: version, a: version, b: version. input (English text): Do you know the code for the ELMo Ensembler's model ? (output shows correct output) output (Text): Which version of the BiDAF + Self Attention + ELMo model were you using ? idc: model, a: version, b: version input (English text): Inputs are sortable output (Text): Which version of the BiDAF Ensembler were you using ? input (English text): Which version of the Self-attention Ensemble model were you using ? input (English text): Which version of the BiDAF model were you using ? idc: model, a: version, b: version. input (English text): Which version of the BiDAF model were you using ? input (English text): Which version of the Self-attention Ensemble model were you using ? idc: model, a: version, b: version. input (English text): What version of BiDAF or Self-attention Ensemble did you use ? idc: model, a: version, b: version. input (English text	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cheetah, run (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Walker, walk (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Cheetah, run (DMControl500k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	--------------------------- NAME metric_lbl PRICE (USD) --------------------------- 1.00 Cheetah, run (DMControl500k) DMControl_500k 500000 3.092 2.200 Cheetah, run (DMControl100k) DMControl_500k 100000 3.076 2.700 Cheetah, run (DMControl500k) DMControl_500k 1000000 3.074 2.300 Cheetah, run (DMControl100k) DMControl_500k 1000000 4.017 1.500 Cheetah, run (DMControl250k) DMControl_500k 250000 3.071 1.650 Cheetah, run (DMControl500k) DMControl_500k 500000 3.007 1.737 Cheetah, run (DMControl500k) DMControl_500k 1000000 4.006 2.250 Cheetah, run (FruIT100k) FruIT_FruIT_100k 1000000 3.006 1.760 Cheetah, run (FruIT100k) FruIT_FruIT_100k 1000000 4.020 1.550 Cheetah, run (FruIT100k) FruIT_FruIT_100k 5000000 4.014 1.700 Cheetah, run (FruIT100k) FruIT_FruIT_100k 5000000 3.007 1.660 Cheetah, run (FruIT100k) FruIT_FruIT_100k 50000000 4.011 1.775 Cheetah, run (FruIT100k) FruIT_FruIT_100k 5000000 3.006 1.650 Cheetah, run (FruIT100k)	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the TDMSci dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDMSci"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } In this case, the output is rather disappointing because of the lack of a good query filter on the output columns and the fact that the query contains false positives for any value that is not an integer. In other words, the output is not exactly what we get if we want the results of WOS-5736, which would be: output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdf"	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Alien dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"------------------------- ERROR: score > 1000: Invalid score; invalid metric; not valid benchmark. ERROR: score > 1000: ""Alien"". ERROR: ""Alien"": score = ""3000"". ERROR: score > 1000: ""Alien"": score = ""3000"". WARNING: score > 1000: ""Alien"": score = 2000. WARNING: score > 1000: ""Alien"": score = 2000. ERROR: score > 1000: ""Alien"": score = 2500. ERROR: score > 1000: ""Alien"": score = 2500. WARNING: score > 1000: ""Alien"": score = 2500. ERROR: score > 1000: ""Alien"": score = 2500. ERROR: score > 1000: ""Alien"": score = 2500. WARNING: score > 1000: ""Alien"": score = 2500. ERROR: score > 1000: ""Alien"": score = 2500. NOTICE: score > 1000: ""Alien"": score = 2500. NOTICE: score > 1000: ""Alien"": score = 2500. NOTICE: score > 1000: ""Alien"": score = 2500. NOTICE: score > 1000: ""Alien"": score = 2500. NOTICE: score > 1000: ""Alien"": score = 2500. NOTE: score > 1000: ""Alien"": score = 2500. NOTICE: score > 1000: ""Alien"": score = 2500. ERROR: score > 1000: ""Alien"": score = 2500. ERROR: score > 1000: ""Alien"": score = 2500. NOTICE: score > 1000: ""Alien"": score = 2500. NOTICE: score > 1000: ""Alien"": score = 2500. ERROR: score > 1000: ""Alien"": score = 2500. ERROR: score > 1000: ""Alien"": score = 2500. ERROR: score > 1000: ""Alien"": score = 2500. ERROR: score > 1000: ""Alien"": score = 2500. ERROR: score > 1000: ""Alien"": score = 2500. NOTICE: score > 1000: ""Alien"	False
"
 input (English text): What is the highest benchmark result achieved on the Oxford 102 Flowers dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford 102 Flowers"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the LAMBADA dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""LAMBADA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Birdsnap dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Birdsnap"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	------------------------ Result : Number of birds recorded in the dataset. ========== Score : < 2 (0.6%) Best score : < 2.5 (0.9.6%) Best number of birds : < 20.0 (1.5.9%) best birds : 2 : < 3.0 (4.2.3%) best animals : 3 : < 5.0 (8.3.4%) best animals : 4 : < 5.5 (9.2.7%) Best birds : 6 : <- 22.5 (22.9.9%) Best animals : 7 : <- 21.0 (21.6.7%) Best birds : 8 : <- 15.0 (15.9.9%) Best animals : 9 : <- 11.0 (12.4.4%) Bats : 10: <- 8.0 (12.2.7%) Birdsnap : 11: <- 12.0 (21.2.7%) Best birds : 12 : < 11.0 (13.8.9%) Best animals : 13 : <- 7.3 (7.4.1%) Best birds : 14 : <- 7.0 (7.6.3%) Best animals : 15 : < 5.0 (11.4.3%) Best birds : 16 : <- 5.0 (11.7.1%) Best animals : 17: <- 3.5 (3.5.9%) Best animals : 18: <- 3.5 (3.8.3%) Best birds : 19: <- 3.5 (5.2.3%) Bats : 20: <- 2.0 (2.8.3%) Birdsnap : 21: <- 2.0 (4.6.4%) Best birds : 22: <- 2.0 (5.5.9%) Best animals : 23 : < 2.0	False
"
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Chopper Command dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Chopper Command"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the Atari 2600 Star Gunner dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Star Gunner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Atari 2600 Chopper Command dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Chopper Command"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT query using a different metric as a primary metric (e.g.,'select * from nk_vcd;' or'select * from nk_vcd nnks*';). It is also sometimes useful to rename metric names to prevent name clashes with other metric names in the dataset. The'metric' of the specified dataset should not appear in the query-string for the data frame itself. In other words, your primary metric must not be in the dataset. To evaluate using some other metric, add the -m option to the 'dataset' of the 'chopper' command, e.g.: chopper> chopper -m nk_vcd Note that this will attempt to fetch an additional.csv file with data from nk_vcd, but these would typically be non-functional. You should instead change the 'dataset' option to use the nk_vcd dataset as your primary data-frame, e.g.: chopper> chopper -m nk_vcd -m nk_coffee orkgp:HAS_DATASET ORkgp:HAS_EVALUATION ORkgp:HAS_DEFAULT_METRIC ORkgp:HAS_BENCHMARK ORkgp:HAS_METRIC orkgp:H	False
"
 input (English text): Indicate the model that performed best in terms of PARAMS metric on the Stanford Cars benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the RACE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Stanford Cars benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } The response from the query above is shown below: [2] [4] Note: RDFS output is the result of a filter on the input text: . \ SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkg"	False
"
 input (English text): Indicate the model that performed best in terms of Precision metric on the Rotowire (Content Selection) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the metrics used to evaluate models on the RotoWire (Relation Generation) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?dataset ORG_LBL_DATA ?dataset_lbl,... ?model_lbl FROM model_lbl GROUP BY ?dataset ORG_LBL_DATA; output (Sparql query): SELECT ?dataset ORG_LBL_DATA ?dataset_lbl,... ?model_lbl FROM model_lbl GROUP BY ?dataset ORG_LBL_DATA; input (English text): If you want to find the optimal model for the RotoWire (Relation Generation) dataset, use the ORG_LBL_DATA parameter to get the set of models: output (Sparql query): SELECT ORG_LBL_DATA ORG_LBL_DATA ?dataset,... ?model_lbl FROM model_lbl GROUP BY ?dataset ORG_LBL_DATA; OPTIONAL ORG_LBL_DATA: ORG_LBL_DATA ?dataset,... ?model_lbl FROM model_lbl GROUP BY ?dataset ORG_LBL_DATA; input (English text): Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset ? output (Sparql query): input (English text): The model that performed best on the RotoWire (Relation Generation) dataset (Sparql query): SELECT ?dataset ORG_LBL_DATA ?dataset_lbl,... ?model_lbl FROM model_lbl GROUP BY ?dataset ORG_LBL_DATA; ORG_LBL_DATA: ORG	False
"
 input (English text): List the metrics that are used to evaluate models on the SQuAD2.0 dev benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the COPA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""COPA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuad1.1 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?dataset rdfs:lbl ?dataset_lbl. } output: 1. Selecting a metric from SQuAD 2.0 dev model 2. Selecting the metrics from COPA benchmark dataset 3. Selecting a metric from SQuAD1.1 dev model 4. Selecting the metrics from SQuAD1.1 dev model 5. Selecting the metrics from SQuAD1.1 dev model 6. Selecting the metrics from COPA benchmark dataset 7. Selecting the metrics from SQuAD benchmark dataset 8. Selecting the metrics FROM SQUAD dev model Note: To use the COPA benchmark dataset as a training example in this section, please select SQuAD1.1 for its metrics and the COPA benchmark dataset for its outputs."	False
"
 input (English text): Provide a list of papers that have utilized the Unregularised mLSTM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Unregularised mLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Longformer Large model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Longformer Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Large mLSTM model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Large mLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Large"")) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. ?cont orkgp:HAS_SOURCE_CODE; ?model orkgp:HAS_DATA ?data.! Note that the model name may be substituted with the name of the model code, but not the name of the dataset object such as in the input and output arguments. In this case, the model name is used only as the key that uniquely identifies the model. Back to TOP Lemma 6 (unexpected output) In addition to the possible errors above related to ambiguity, any such model name can be substituted with an identifier which the RDFa parser recognizes. input (English text): Provide a table of the top 2 languages used by the specified models ? output: SELECT 'top2' AS top2_language WHERE ( ?language 'fr') or ( ?language 'de') ?language ; orkgp:HAS_LANGUAGE TOP2_LANGUAGE orkgp:HAS_DATA TOP2_LANGUAGE Note that the top 2 languages are shown in the output of this query only when the user specifically requests it. input (English text): Provide a table of the top 2 languages used by the specified models ? output (Sparql query): SELECT TOP2( ?language = 'fr"	False
"
 input (English text): What models are being evaluated on the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL Anthology dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL Anthology"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the ACL Anthology dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL Anthology"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 # # # # # # # ################### # R package # ### ## # -------------------------------------------------------------------------- # -------------------------------------------------------------------------- ------------------------ # ~~~~~~~~~~~~~ # ~~~~~~~~~~~~~ # # # # ~~~~~~~~~~~~~ # # # ~~~~~~~~~~~~~ # ~~~~~~~~~~~~~ # -------------------------------------------------------- --------------------------------- # ~~~~~~~~~~~~~ # ~~~~~~~~~~~~~ # # # # # ~~~~~~~~~~~~~ # -------------------------------------------------------- --------------------------------- ################### # R package # # # ~~~~~~~~~~~~~ # # # ~~~~~~~~~~~~~ # # ################### # R package # # 	False
"
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Battle Zone benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Star Gunner benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Star Gunner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Yars Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Which model has achieved the highest Score score on the Atari 2600 Centipede benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest Score score on the Atari 2600 Caves of Qud benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest Score score on the Atari 2600 Space Invaders benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest Score score on the Atari 2600 Missile Command benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest Score score on the Atari 2600 Air Combat benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest Score score on the Atari 2600 Warlord benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest Scores score on the Atari 2600 Cactuar Battle benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest Scores scored in the Atari 2600 Cactuar Battle benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest Scores earned in the Atari 2600 Warlord benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest Scores scored in the Atari 2600 Cactuar Battle benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest Scores scored in the Atari 2600 Space Invaders benchmark dataset ? output (Sparql query): input (English text): Which model has	False
"
 input (English text): What models are being evaluated on the BC5CDR-disease dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Pubmed dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the NCBI-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } We now have a list of models from different fields that can be used to analyse the data, and that's all there is to it. What a useful tool! This is a useful workflow for those who are already familiar with statistical concepts or have a small dataset to analyse. So the next step is to apply the analysis on the whole list by using DBLAS to transform a list of tables into a list of sets. The advantage of doing this step by doing DBLAS is that you don't spend energy on deciding which columns to include with which models and instead you can just pick the model you think will deliver the best result. It's also important to highlight that the outputs from DUMP command are very similar to what could be used for other purposes. The output from dump command ( output.dat ) looks very similar to how a typical CMD output looks ( output.plist ). And so this post will be a bit different from what is previously written. So first we are going to create a list of sets using python's tidyverse package. In this example I am going to just dump the data generated using the BC5CDR-disease dataset, so that will just"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ScienceCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): Can you provide the highest benchmark result, including the metric and score, for the SCIx database ? output (Sparql query):  SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SCix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkg"	False
"
 input (English text): What models are being evaluated on the Fashion-MNIST dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Fashion-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the MNIST dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Fashion-MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Fashion-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Fashion-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ? eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } As you can see, a user can specify multiple sets of metrics, either explicitly or through the data filter. A user can specify this parameter as part of the request with the same URL, but the values may be distinct. The use of multiple URLs creates a two-way binding for other users to the data. Once a request is made for a value of parameter { ?dataset a, ?dataset b }, other users can use the URL { ?dataset a, ?dataset b } for the same set of parameters. After this request is made, other users can use the same URL to retrieve the same set of values and they would have access to parameters available in both sets. The values provided in the query is the subset of a data set set that satisfies the given filter. The results of the evaluation are the full set of variables of that set. 3.8. Evaluating Data Sets for Search In this section, we will use the example of fitting the model described in the previous section with the two datasets f1 and f2 as their inputs. We will also illustrate how it can be set up to support a different query. Fitting a Model Using the Data "	False
"
 input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-3 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-3"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""X-Sum"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the X-Sum dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""X-Sum"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""X-Sum"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Input-output format ROUGE-2 metric ROUGE-3 metric Metric score ROUGE-2 metric rOUGE-3 metric Metric score ROUGE-2 metric rOUGE-3 metric Metric score rOUGE-3 metric rOUGE-3 metric Metric score rOUGE-3 metric rOUGE-3 metric Metric score rrouge2 3 1 3 3 ROUGE-2 metric rOUGE-3 metric rrouge2 3 1 3 3 ROUGE-2 metric rrouge2 3 1 3 2 ROUGE-2 metric rrouge2 3 1 3 2 ROUGE-2 metric rOUGE-3 metric 3 1 2 3 ROUGE-2 metric rrouge2 0 0 0 2 2 ROUGE-2 metric rrouge2 0 0 0 2 2 ROUGE-2 metric 3 1 0 2 ROUGE-2 metric 3 1 1 2 ROUGE-2 metric rrouge2 1 1 1 2 ROUGE-2 metric rrouge2 0 0 1 2 2 Metric score Metric score Metric score ROUGE-2 metric rrouge2 0 0 0 2 2 ROUGE-2 metric rrouge2 0 0 0 1 1	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the RotoWire dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of benchmarked datasets related to the Relation Classification research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Relation Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RotoWire (Relation Generation) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT { ?dataset ?dataset_lbl. AND ?dataset ?dataset_lbl. AND ?statistics ?statistics. } FROM ?paper_lbl ?dataset  AND ?benchmark ?statistics ORDER BY ?paper_lbl [ ?statistics]  ORDER BY ?benchmark [ ?statistics]  AS COUNT  ELSE  0 output (Sparql query):  select * from ?paper_lbl  where { ?dataset ?dataset_lbl.... ?statistics ?statistics. }  select ?paper_lbl. [ ?dataset_lbl] AS name  from [ ?paper_lbl] ?labels  WHERE { ?labels ?labels. ?statistics ?statistics. }  select ?lbl_lbl. [ ?labels]  from [ ?stats_table] ?data_labels  order by [--tableName_1] DESC  limit  by 2  order by title  DESC  limit  by 2  select ?dataset_lbl. [ ?dataset]. AND ?dataset_lbl. [ ?labels]. WHERE { ?labels ?labels. }  select ?lbl_lbl. [ ?labels]. WHERE { ?labels ?labels. }  select ?labels. [ ?labels]. FROM 'data table'  join ?labels  on [ ?labels. title ]  order by title ORDER BY ?labels. [ ?labels].  SELECT { ?data_labels ?data_	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ning et al."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the GPT-3 175B (Few-Shot) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-3 175B (Few-Shot)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Fine-Grained Gating model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Fine-Grained Gating"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 * To be able to make valid queries we need to have the full input and output set. This allows us to compare model performance to a different dataset, and for this we use another option, in parallel. The first step is to start the QueryRunner (here the RNN is named P3 ) but before starting this we need to tell it where the query is coming from. The RNN receives a reference variable of the class of the input (the output can be anything). Here we use the function pprint (or in R you can use it by default for all data ). The RNN is started by calling the exec method. The return value is the RNN instance: # exec P3.exec( ?lbl.name) If there is no input given this returns the result for an empty list. Alternatively, if we have an input named ?lbl, the value of that input can be used in the RNN's return value. By default the RNN will start the query for itself and the output variable will be empty so no execution is actually done. To resume the query: # P3.exec( ?lbl.name, ?lbl.name) # This is optional. P3 also has the exec ( ?lbl.name.exec ) method. This is equivalent to the exec above except that a new string, ?lbl.name.name, is appended in the RNN's return value. This method is similar to the RNN's exec. # exec P3.exec( ?lbl.name, ?lbl.name.length ) P3 also has a exec ( ?lbl.name.exec.length ) method that gives a list of values that are appended to the RNN's output value. They contain only the data which is read from the input stream. There must be no space	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Atlantis dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Average Return (NoOp)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. ?value rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. ?value rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	False
"
 input (English text): Provide a list of papers that have utilized the DDQN (tuned) noop model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN (tuned) noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Prior+Duel noop model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior+Duel noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Prior noop model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	--------------------------- { ?model : a orkgc:Model ?model: lbl( a ?gcc:GEN-L) || ?model: a ?gcc:GEN-L || || ?model: a ?gcc:GEN-L || || ?model: a ||kgc:Generator. ?gcc :GEN-L === a ?gcc :GEN-L ?gcc :GEN-L === a ?gcc :GEN-L ||kgc:Genr. ?generator:Genr. ||genr:Genr. ?genr:Genr. ||genr:Genr. ?graph :a ||kgc:Genr ||ggc:Genr ||ggc:Lp ?ggc :GEN-L ?ggc :GEN-L === a ?ggc :GEN-L ?ggc :GEN-L ?graph :a||gcc:GEN||ggc:GEN||ggc:Lp }} input (English text): Where can I find code references in papers that have utilized the Prior+Duel noop model for benchmarking purposes ? Output (Sparql query): --------------------------- { ?model : a( gcc:GEN-L) || ?model : a( gcc:GEN-L || || ?model : a( gcc:GEN-L || || ) || ?model : a( gcc:GEN-L || || || ||GGC:GEN-L) ? || ?model : a( gcc:GEN-L || || || ||GGC:GEN-L) ? ?genr :genr( gcc:GEN-L) === a ?generr :genr( gcc:GEN-L) === a ?generr :genr( gcc:GEN-L) ? ?lbm :	False
"
 input (English text): What is the best performing model benchmarking the WMT2016 English-Romanian dataset in terms of BLEU metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Russian-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Russian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the WMT2016 English-Russian dataset in terms of BLEU score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 output (Sparql string containing English text, Russian text and the full Metric): model-id: 9e5f2dcf-ac4a-4501-a7d6-00b4b2c8be6a class: WMT-16R Russian-English benchmark_str_score: 4.560000 metric_str_score: 4.560000 (rdfs:label) Model ID: 9e5f2dcf-ac4a-4501-a7d6-00b4b2c8be6a 1 1 2 3 4 5 6 7 8 9 10 11 model ID : 9e5f2dcf - ac4a - 4501 - a7d6 - 00b4b2c8be6a table: WMT-16R Russian - English benchmark query.metric: 4.560000 metric_str_score: 4.560000 ( rdfs : label ) query. metric_str_score : 4.560000 ( rdfs : labeled ) model ID: 9e5f2dcf-ac4a-4501-a7d6-00b4b2c8be6a 1 1 2 3 4 5 6 7 8 9 10 11 model ID : 9e5f2dcf - ac4a - 4501 - a7d6 - 00b4b2c8be6a table : WMT - 16R Russian - English benchmark query. metric : 4.560000 metric_str_score : 4.560000 ( rdfs : label ) query. metric_str_score : 4.560000 ( rdfs : labeled ) model ID: 9e5f2dcf-ac4a-4501-a7d6-00	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy (%) score when benchmarked on the Oxford-IIIT Pets dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford 102 Flowers dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford 102 Flowers"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } output (Sparql query): SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Reuters En-De dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the 20NEWS dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""20NEWS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters-21578 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?papers ?papers_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""1%"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:H"	False
"
 input (English text): What models are being evaluated on the Penn Treebank (Word Level) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Penn Treebank dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" Enter CSV or CSV file: pandatree.treebank.model(a, b) for character level and paper treebank.treebank.model(a) for paper treebank This query takes 5 seconds and the output looks like this: ""Paper Treebank"" ""Characters"" ""Characters"" ""Characters"" ""Characters"" ""Paper Treebank"" ""Characters"" input (English text): Which models are being evaluated on the Paper Treebank (Character Level) dataset ? output (Sparql query): Enter CSV or CSV file: pandatree.treebank.model(a, b, a_index, b_index) for character level and paper treebank.treebank.model(a, b) for paper treebank This query takes 5 seconds and the output looks like this: ""Penn Treebank"" ""Characters"" ""Characters"" ""Characters"" ""Penn Treebank"" ""Characters"" ""Character Level"" ""Characters"" ""Characters"" ""Paper Treebank"" ""Characters"" input (English text): Which models are being evaluated on the Paper Treebank (Character Level) dataset ? Are any of the models considered part of the Penn Treebank treebank ? output (Sparql query): OPTIONAL (1 means all models, 5 means none) Enter CSV or CSV file: pandatree.treebank.model(a, b) for character level and paper treebank.treebank.model(a, b, a_index) for paper treebank This query takes 3.5 seconds and the output looks like this: ""Paper Treebank"" ""Characters"" ""Characters"" ""Characters"" ""Penn Treebank"" ""Characters"" ""Character Level"" ""Characters"" ""Characters"" """	False
"
 input (English text): Provide a list of papers that have utilized the Feedback Transformer (4 layers) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Feedback Transformer (4 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the PtGen model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PtGen"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the ANODE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ANODE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ANODE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the Anode model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ANODE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the ANOC model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ANOC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS"	False
"
 input (English text): Provide a list of papers that have utilized the DY-MobileNetV2 ×1.0 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV2 ×1.0"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the DY-MobileNetV2 ×0.75 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV2 ×0.75"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV3-Small"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the DY-MobileNetV3N model and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the DY-MobileNetV4-Large model and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the DY-MobileBroadway-Kraken-HDFS model and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the DY-MobileBroadway-Kraken-HDNF (DY-Broadway with Large New Neural Networks) model and include the links to their code ? output (Sparql query): 	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the FB15k dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FB15k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset FTD dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTD dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; RDFs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTS database"") ?benchmark } ?max_value }. OPTIONAL { ?max_value rdfs:label ?dataset_lbl. } } GROUP BY ?metric ?metric_lbl input (English text): Where do you want to compare the results of the FB15k and FCS (fsns - test dataset, including two metrics, with no comparison between them) for the FSNS dataset ? output (Sparql query): -------------- --------- 1) FCTS-S-B: 9.4 sec 2) FCTS-S: 4:58.2 sec 3) FCTS-B: 3:50.6 sec 4) FB15: 5:47.5 sec 5) FCS-S: 4:52.3 sec input (English text): Where do you want to compare the results of the FSNS (test dataset, including two metrics, with no comparison between them)"	False
"
 input (English text): Where can I find code references in papers that have used the MP-EB model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MP-EB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the MPCM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MPCM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the MPAD-path model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MPAD-path"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 I would love to hear what people think about the above, so please comment/recommend here or on twitter or in our google group. Thanks!	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the BoolQ dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BoolQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the BoolQ dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BoolQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the models that have been benchmarked on the BoolQ dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BoolQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT name, accuracy, orkgc, rdfs, orkgp, orkgb_dataset, metric, benchmark_val, benchmark_score, bencher_val, benchmark_scores, benchmark_bencher_val, benchmark_score_bencher_val, benchmark_score_bencher_scores, benchname from the_bar_db_2 b; R R is a statistical language that is used to perform statistics on data. The main benefits of R are speed and simplicity, with large amounts of flexibility. When choosing a statistical language you will use R because of its performance. There are many available R packages, and choosing a package will likely depend upon your requirements. The R package R. R is most commonly used for data analysis. Using the R package is a matter of importing the data for analysis using the'read.csv' file format. If you would like more information about importing data into R see this article Reading the Files R code can be loaded using the'read' command, however, it is also possible to download the data directly from RStudio. This can be done using the 'library()' command, which will generate the files directly in the current package directory within the current directory. R code will	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC Russian-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC German-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Swedish-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_VALUE ?value. OPTIONAL { ?value orkgc:Dataset; rdfs:label ?model_lbl. } ?cont orkgp:HAS_METRIC ?metric. OPTIONAL ( ?value a orkgc:Dataset ? OR ?value b orkgc:Dataset ? ) OR ( ?value c orkgc:Dataset ? OR ?value d orkgc:Dataset ? OR ?value e orkgc:Dataset ? OR ?value f ORkgc:Dataset ? OR ?value g ORkgc:Dataset ? OR ?value h ORkgc:Dataset ? OR ?value i ORkgc"	False
"
 input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 SDP Score: ( SDP score (0: 0, 1: 1.000000, 2: 4.000000, 3: 6.000000, 4: 14.5000, 5: 21.5000, 6: 30.5000, 7: 34.5000) Out Of A Sample of 100 SDP score (0: 0, 1: 1.000000, 2: 4.000000, 3: 6.000000, 4: 14.5000, 5: 21.5000, 6: 30.5000, 7: 34.5000.000) Out Of A Sample of 100 SDP score (0: 0, 1: 1.000000, 2: 4.000000, 3: 6.000000, 4: 14.5000, 5: 21.5000, 6: 30.5000, 7: 34.5000.000) Out Of A Sample of 101 SDP score (0: 0, 1: 1.000000, 2: 4.000000, 3: 6.000000, 4: 14.5000, 5: 22.7667, 6: 27.9333, 7: 31.9667, 8: 38.9333, 9: 43.5667, 10: 48.5667, 11: 53.5667, 12: 60.5667, 13: 66.667, 14: 72.5667, 15: 81.5667, 16: 90.5667, 17: 100.5667, 18: 110.5667, 19: 116.5667, 20: 122.5667, 21: 127.5667.000.000, 22: 133.5667.000.000, 23: 138.5667.000.000, 24: 140.5667.000.000, 25	False
"
 input (English text): List the code links in papers that use the MEMEN (ensemble) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the ZFNet (ensemble, 6 convnets) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ZFNet (ensemble, 6 convnets)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching""); } 5.3. Data set selection and reporting Using the '*' wild card operator when importing data sets is an easy way to select the subset of datasets you want to use in the benchmarking work. But if you're not using the * wild card operators, it is really difficult to select datasets that you want to use for the benchmarking. So many papers are trying to make selection of datasets easy (and yet they still can't find the dataset you want) or they're limiting the datasets to a subset of datasets they want (and they're always trying to find the dataset more convenient). That's why I often like to think of the benchmarking dataset as a set of datasets you want to compare to each other. Thus I try to select datasets from the set of datasets you want to see in the benchmarks at the beginning of the paper. Once I have the benchmarks in the '*' selection I proceed to load their data sets into SPSS using the '*' import method. Of course there is no guarantee that the benchmarks you're gonna load into SPSS will look good in SPSS. But I try to get the benchmarks I've saved to a single CSV file with all the benchmark datasets as keys. I know that it is very unlikely that the benchmark datasets do not contain any benchmark datasets in them. 6. Sysbench/SPSS/SASS One of the biggest challenges of benchmarking is the ability to visualize results in several formats and formats, for both the analysis and the reporting of the results. One issue is the choice of language. A common suggestion is to use SASS"	False
"
 input (English text): Could you provide a list of models that have been tested on the ImageNet 64x64 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the ImageNet 64x64 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bits per dim"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): What is the average score for each benchmark model in the image model validation sample ? output (Sparql query): input (English text): How did the ImageNet validation sample perform compared to the validation sample from our previous project ? output (Sparql query): input (English text): Which of the models from the list of models shown in the following table are in the top performance group for the image model validation sample ? output (Sparql query): input (English text): Which of the models from the list of models shown in the following table are in the bottom performance group for the image model validation sample ? output (Sparql query): input (English text): How big of a change in the model scores from the validation sample to the validation sample from our previous project ? output (Sparql query): input (English text): How was the validation sample in terms of its evaluation of the model scores ? output (Sparql query): input (English text): What parameters did you use to calculate your model scores ? output (Sparql query): input (English text): Which of the examples from the list of models shown in the following table are in the top performing group in this validation sample ? output (Sparql query): input (English text): Which of the examples from the list of models shown in the following table are in the bottom performing group in this validation sample ? output (Sparql query): input (English text): Which of the examples from the list of models shown in the following table are in the top performing, bottom performing, or very performance critical group in this validation sample ? output (Sparql query): input (English text): Which of the examples from the list of models	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT column, expression as (select, where, select/where*), column_id as column_id FROM table where table_name LIKE '%' AND column_type LIKE '%' AND id = 1 ) As we said earlier, when defining the relation we give the metadata, but there is more to the table than just metadata. We define the index on the tables and then we set the query to use the metadata. We do the indexes by default and we get the same metadata for all the tables. ALTER TABLE IF EXISTS trivia_qp ADD index	False
"
 input (English text): What is the total number of patients in the studies?
 output (Sparql query): SELECT SUM( ?number_of_patients) WHERE { orkgr:R33008 orkgp:compareContribution ?contrib. ?contrib orkgp:P15585 ?patients. BIND(xsd:integer( ?patients) AS ?number_of_patients) } 
 input (English text): In what country was conducted research with the largest number of participants?
 output (Sparql query): SELECT ?country WHERE { orkgr:R44980 orkgp:compareContribution ?contrib. ?contrib orkgp:P15249 ?country; orkgp:P23169 ?number_of_participants. } ORDER BY DESC( ?number_of_participants) LIMIT 1
 input (English text): Are children examined in the studies?
 output (Sparql query): "	"ASK { orkgr:R44979 orkgp:compareContribution ?contrib. ?contrib orkgp:P23161 ?population. FILTER(REGEX( ?population, ""ildren"")) } "	SELECT SUM( ?children_in_study ) FROM [ ?children_in_study] WHERE ?children_in_study. child_age IN ( ?average_age ?mth ?yd ?mm ?yyy ?mth ?yd ?mm ?yyy ) input (English text): output (Sparql query): SELECT SUM( ?children_in_studies) FROM [ ?children_in_studyeres] WHERE ( ?year ?birthdate) AND ( ?median_age ?birth_date) AND ( ?pupil_age ?birth_date) AND ( ?median_age ?mth_of_freed/full/half/half ?) input (English text): output (Sparql query): SELECT SUM( ?children_in_studies) FROM [ ?children_in_studyeres ] WHERE ?child_age( ?birthdate) IN ( ?average_age ?mth ?yd ?mm ?yyy ) AND ( ?median_age ?mth_of_freed/full/half/half ?) input (English text): output (Sparql query): SELECT SUM( ?children_in_studies) FROM [ ?children_in_studyeres ] WHERE ( ?child_age( ?birthdate) AND ?median_age( ?mth ?yd ) AND ( ?median_age( ?mth ?yd ) AND ( ?median_age( ?mth ?yd ) AND ( ?median_age( ?mth ?yd ) AND ( ?median_age( ?mth ?yd ) AND ( ?median_age( ?mth ?yd )	False
"
 input (English text): What is the highest benchmark result achieved on the OpenBookQA dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OpenBookQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the CORLL dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CORLL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the CoQA dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?status FROM ?dataset ?metric WHERE ?status = ?status_lbl): 5 ( SELECT ?metric ?metric_lbl = SUM( ?value ), SUM( ?value ), SUM( ?value ), SUM( ?value ) AS ?metric_lbl, SUM( ?value ) AS ?metric_lbl FROM ?dataset andkgc:Dataset,CORLL ?dataset,corp:corp,CORLL CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,CORLL d:corp,COR LL rdfs:col_lbl,rdfs:col_lbl. FILTER (str( ?dataset_lbl) = ""CORLL"") ?log:logged FROM ""corp CORLL d:corp,CORLL d:corp,COR"	False
"
 input (English text): What is the top benchmark score and its metric on the ImageNet 64x64 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the ImageNet V2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	------------- { { 1, 3, 64, 96 | 64.0413132844696967 | 1748.2747451859077 } } input (English text): What is the top benchmark score and its metric on the ImageNet V4 dataset ? output(Sparql query): ------------- { { 1, 3, 64, 96 | 64.0413132844696967 | 1748.2747451859077 } } I have uploaded the preprocessable V4 test data and I have modified it as per your requested criteria. It has shown the results of the previous version of the test data including the metric on the ImageNet dataset. I have also produced preprocessed V1 (with the same metrics but without the ImageNet dataset) and V2 dataset and modified it as per your requested criteria. The main differences between versions are the size of the data and the number of nodes in the dataset. I have also added another metric on the V4 dataset to make it more interesting compared to the original test. I have also added the original dataset with the same metrics to the V4 dataset as well. It has also produced preprocessed V4 dataset. The preprocessed V4 dataset is very large and should be processed with Glimmer. If you select the correct metric name and value, the preprocessed V4 dataset will produce the same results as the original data without all the nodes in the dataset.  I have also modified the preprocessed dataset so that its metrics and scores are consistent to the preprocessed V2 dataset and also the original V4 dataset, so that both the preprocessed V4 and V2 data can be compared. If you have any doubts or queries, feel free to contact me.	False
"
 input (English text): What is the highest benchmark result achieved on the Reacher, easy (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Could you provide a list of models that have been tested on the Reacher, easy (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Score score on the Reacher, easy (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text) You have an old version of the Reacher benchmark dataset ? output (Sparql query) output (Sparql query) You have an old version of the Reacher benchmark dataset. You have an old version of the Reacher benchmark dataset with the following features: Note: there is only one metric, one value which should be compared against the maximum score possible on the dataset.	False
"
 input (English text): What are the models that have been benchmarked on the Quasart-T dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quasart-T"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the NYT29 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Quasart-T benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quasart-T"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?type ?type_lbl. ?type ?type_lbl. FILTER (str( ?type ) = ""RDF"") ?statistics orkgp:HAS_START ?statistics_lbl. OPTIONAL { ?statistics orkgp:HAS_STANDARD ?statistics. ?value ?value. } input (English text): List the criteria that are used to evaluate datasets on the NYT29 benchmark dataset ? output (Sparql query): SELECT ?model ?model. WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl ) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?evaluate ?evaluate_lbl. } }   output (Sparql query): This last benchmark shows a nice trend: output (Github page):   The dataset used for this benchmark is the dataset for which the dataset comparison in the first two benchmarks was used. As noted above, most of the criteria used in these benchmarks are relevant to the datasets used for this benchmark. Some of them, however, only apply for the dataset on which the benchmark was run. In that case results based on that dataset would be invalid. The same problem applies to datasets that are just plain 'good' or 'bad' in many cases. In particular, for some datasets this means that these datasets do not perform well in the datasets used for this article; that is, they do not match some"	False
"
 input (English text): Can you list the models that have been evaluated on the CINIC-10 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the Multimodal PISA dataset in terms of Accuracy (%) metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Multimodal PISA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the Multimodal PISA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Multimodal PISA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ROWS( ?dataset ) OVER (*) FROM csv_dataset_models WHERE ?dataset = ? ?dataset ?dataset_cvs { ?dataset rdfs:label ?dataset_lbl. ?dataset rdfs:description ?dataset. } output (Sparql query): Outputs (Sparql query): WHERE ?dataset = ? ?dataset input (English text): Can you list the models that have been evaluated on the Multimodal PISA dataset in terms of Accuracy (%) metric ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Multimodal PISA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. ORkgp:HAS_METRIC ?metric. ?cont ORkgp:HAS_BENCHMARK ?benchmark; ORkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } ; output (Sparql query"	False
"
 input (English text): What models are being evaluated on the Barabasi-Albert dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Barabasi-Albert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the Barabasi-Albert dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Barabasi-Albert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "		False
"
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Breakout dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Phoenix dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Phoenix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Zaxxon dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT SUBSTRING( { ?dataset a;., ?dataset a ? } ), { ?dataset a;., ?dataset a ? }, ?dataset a;., ?dataset a ?.,, ?dataset a;. AS ?dataset_lbl LANGUAGE tcl, c, java, '-java'; : output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a {:, ?dataset a ? };: ?benchmark orkgp:HAS_DATASET ?dataset; ? } output (Sparql query): Select the metrics used to evaluate models on the Atari 16000 Breakout dataset. output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 16000 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Can you list the metrics used to evaluate models on the Nintendo Game Boy Color dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dat"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the MLDoc Zero-Shot English-to-Italian dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Italian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Spanish"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Spanish"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl output (Json JSON) 2.9.4.8,4.2.0,4.1.1,4.0.3,4.0.2 input (English text): What is the top benchmark result (metric and value) over the dataset OLTP English-to-Italian ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label"	False
"
 input (English text): Indicate the model that performed best in terms of BLEU metric on the WMT2016 English-German benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2014 German-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" In other words, that output should say something like: BLEU = ?metric ?value orkgc:Metric. The best model was detected by the WMT classifier, based on the criteria of the criterion of the criterion of the English text given as input. The WMT classifier used the following algorithm (based on the BLEU metric): BLEU (or kgc) represents the average score of English words for the ""BLEU"" criterion. A criterion of the BLEU criterion means that English words that are similar to a given word have a higher BLEU score than the other words. The other criteria were: ORGANIZATION : A criterion of the ORGANIZATION criterion means that the word ""HEAR_BREATHED"" is closer to ""HEAR_BLASTED"" than ""HEAR_BRAWNED"", where ""HEAR"" and ""BROWN"" are the same word. : A criterion of the criterion means that the word ""HEAR_BREATHED"" is closer to ""HEAR_BLASTED"" than ""HEAR_BRAWNED"", where ""HEAR"" and ""BROWN"" are the same word. SENSE : A criterion of the SENSE criterion means that this word has more than twice the same meaning (""BABY"", ""NICE"", ""WONDERFUL"") as ""SHIRTS"", where ""SHIRTS"" have the same meaning as ""BRUSHES"", ""BLERGER"" has the same meaning as ""ROTATION"", ""BRILLIANT"" has the same meaning as ""SCREWDRIVERS"", ""NOSE"" has the same meaning as ""SHOES"", ""PRINCESS"" has the same meaning as ""LEAVES"", ""BOUNCER"" has the"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Information Extraction research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Information Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Word Embeddings research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Word Embeddings"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Sentence Classification research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Sentence Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 Step 7: Validate the results I tested my results with several benchmarks, and I validated them against each of them with different conditions: Dump output to file: This is a standard output from my database service. Step 8: Implement this solution First of all, we need to create a new project in Microsoft Azure, which contains all dependencies, and the solution in which we'll be running the database service and the benchmarking. This project will be called my_data_benchmark. I chose the name my_data_benchmark_xlh because it's very obvious which project it is, but if we are really going to use this as a starting place, we can just call it one more time after we've checked the current directory. The name will help us in later steps, though since this project just contains a database server, it could be any database name. I created a project named my_data_benchmark and it's located in src\my_data_benchmark\my_data_benchmark-xlh. Then we need to set a couple of settings in the settings.json, then start the client. Run the following commands: On Windows: node server.js On Linux: git clone https://github.com/takayas/my-data-benchmark.git my_data_benchmark cd my_data_benchmark npm install On Mac: git clone https://github.com/takayas/my-data-benchmark.git my_data_benchmark cd my_data_benchmark On OS X: git clone https://github.com/takayas/my-data-benchmark.git my_data_benchmark cd my_data_benchmark npm install Now let's configure the database server	False
"
 input (English text): List the metrics that are used to evaluate models on the CUB-200-2011 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Text8 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Text8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Input Metrics 1. Benchmark Metrics  2. Validation Metrics  3. Feature Extraction Metrics  4. Metadata Metrics  5. Machine Learning Metrics  6. Statistics Metrics  7. Feature Selection Metrics (optional) This example illustrates how to create a query using the SQL query language. The examples are not designed to help you understand the underlying query syntax and how to modify and query the models used in the examples; however, you should have an understanding of the basic SQL syntax, and the possible features of select. Please refer to the SQL reference for further information. 1. Start with example. 2. Create a variable named example_db, then add two variables. 3. In the query, specify the dataset you want to query. Example_db = create.select('a').from('example', 'example1').where('label!=' + 'A')) 4. Use the where clause to retrieve all the metrics defined in the CSV. Example_db = create.select('a').from('example', 'example1').where('label!=' + 'A') 5. Repeat this step for all the values specified below. Example_db = create.select('a').from('example', 'example1').where('label!=' + 'A').group('weight') 6. Create a table called example_benchmark. You should add metrics associated with each benchmark dataset. Example_benchmark.set_metrics({cub_200_2011:2}) Example_benchmark.set_metrics({cub_200_2011:1}) 7. The results should be as follows.	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-German dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Chinese dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Chinese"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Italian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" Outputs the set of metrics with respect to the benchmarking metric(s). Sets the variable ?metric_lbl to the value of the specified metric and the value of ?metric in case there are no other relevant metrics to measure. Any other attribute-s are set to NULL in case the corresponding metric is absent. The output variables may be changed by passing as parameter a value to one of the following variables with values to the other attribute-s: ?benchmark - The number of evaluations done by this model over the specified interval. ?eval - The corresponding metric. ?metric - The corresponding metric-s. NOTE: If the metric_lbl variable is not set after calling the function, it will be set to the value NULL when the benchmarking of the model is run at all, but this variable might be reused later. Example The following functions benchmark and eval are defined in the models.rb file in terms of these three functions. Notice that when the models.rb file is loaded and the function bench is called, these three functions will be available at any time. benchmark returns what number of metrics (the number of evaluations) were applied in the specified interval. The function returns the corresponding metric (for example, as ?metric_lbl ). The function then calls eval to compute the corresponding metric (for example, as ?metric ). It is also possible to run the bench function using a single argument as part of the initialization. For example, when the model names is ""myapp.rb"" then: $./benchmark myapp.rb Evaluated 1 benchmark: 100 Benchmarking benchmark1: 100 benchmark: 9 The function benchmark does not produce any output and instead sends the results to the logfile, which is automatically deleted after every run: $./bench"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the TokenFuser model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TokenFuser"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the DQN-CTS model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-CTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Tokenlearner model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tokenlearner"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" input (English text): A paper titled ""A Hierarchical Bayesian Classifier for the Use of Graphical Models in the Presence of Algorithmic Uncertainty"" by DQN-CTS has been accepted for publication in: I am currently working on a novel method that attempts to make the classification based on an individual's unique vocabulary data more robust and robust in practice. This will include the integration of a DQN-CTS model into more complex networks and the ability to use token learners for classification of larger classes of text We have a very rich set of existing datasets, and this approach is an important step towards realizing our goal of making the ability to produce better machine learning results for the purpose of the academic research community easier In a previous post here, we had noted that there appear to be many problems with the use of a tokenizer to train a DNN. Even if all the tokens were removed from a graph, the model's learning is no longer robust in practice. This post details the way we can solve several of these problem. The first is the cost of the network and it is easily addressed by reducing the time to train a DNN. The second is the time to solve the DNN with a tokenizer. This is difficult for many reasons. But it may help you to build a DNN that can learn to solve the problems in an O(N*logN) fashion. Let's examine these problems in greater detail. The tokenized graph can be represented as the (pornographic word) with the number of words represented by an arrow, or the (real-world words) with a ""dot."" These arrows represent the nodes of a graph. The nodes are the data points and the arrows represent the edges of that graph line. In the previous post, we used a graph model to make the training task easy. But the training"	False
"
 input (English text): What models are being evaluated on the Pubmed dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MedSTS dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the MedSTS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT STATISTICS { ?model ?model_lbl. } In summary, it should be possible to get a reasonably good estimate of the performance of a given model (and in doing so, also find the differences between models) by using the following SQL: SELECT n1, n2, n3,..., nnfrom:cvs, n1, n2, n3,..., nnr1:rankings*0.9, nnr2:rankings*0.9,..., nnrnr1:rankings, nnrnr2:rankings, nnr1r2:rankings,... from:medsts, n1, n2, n3,..., nnr1:rankings, nnr2:rankings,..., nnrnr1r2:rankings, nnrnr2n1:rankings,... ORDER BY n1, n2, n3,...; The following is a table showing the results of this query and the corresponding output (whereas, from the previous post, the first column is the model and the second the output of medsts-solver): Results from: medst:HAS_DATASET orkgp:HAS_DATASET output(s) MedSTS score is: 6.75 MedSTS score is: 2.5 MedSTS score is: 7.25 MedSTS score is: 3.5 MedSTS score is: 7.75 MedSTS score is: 3.75 MedSTS score is: 7.25 MedSTS score is: 2.5 MedSTS score is: 7.25 MedSTS score is: 2.75 MedSTS score is: 7	False
"
 input (English text): What is the best performing model benchmarking the CoNLL04 dataset in terms of NER Micro F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NER Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the CoNLL++ benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" input (English text): What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric ? output (Sparql query): INPUTS: ""co nll r4 0.071 0.944 0.067 0.064"" ""co nll r4 1.04 0.072 0.954 0.066 0.064"""	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the UCF101 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the UCF101 (finetuned) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" In the above two cases, the input parameter is the ID of the input table. The second parameter is the full URL of the given dataset. This can also be the output parameter: > dplyr ""sparql:hint:dataset"" --url https://raw.githubusercontent.com/Nyhan/turbolab-fixtures/master/data/models/UCF101 (n=14) > select ""dataset"". ""ID"" ""title"". ""link"" as ""link"", ""dataset_lbl"". """", ""model_lbl"". """", ""rdfs"". """", ""label"". """", ""benchmark"". """", output (SparseMap), output (SparseVector), output (String) Note that the output parameter has to be a string. This will also be the parameter which was given to the model when it was created or loaded. This option can be used multiple times, meaning that no more parameters have to be passed for later use. Example: You have a dataset that contains the names of people who have successfully completed a task on a popular puzzle game, The Legend of Zelda. You want to create a model that can predict who will get the task (that is, who will have completed the task) based on their score (the probability that they will complete it). This will generate a table which may be used later. You want to define your training set in several columns, where each row contains the score for the previous task and each column has the information about the task itself (the score, the participants, and their progress). Here is the code you will need for generating your train data in R: library(fmt) t = numpy.bigarray(10, nrow=5"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Softcite dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } select 1..10 as rank, 1..10000000 as titles, 3..300000 as IDs from [citation or citation data], select TOP 1000, 1000, 300000, 1000 from `benchmark` ORDER BY rank, 1, 10000, 300000, 1000 DESC, 1, 10001, 300001, 1000 1.2 Benchmarks for the same dataset input (English text): Give me a list of research papers, that have performed benchmarks on the Softcite dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } select 1..1000 for rank, 1..1000 for titles, 1000 for IDs from `benchmark` ORDER BY rank, 1, 1000, 1000"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CvT-W24 (384 res, ImageNet-22k pretrain) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-W24 (384 res, ImageNet-22k pretrain)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the DeiT-S model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the ImageNet + iNat on WS-DAN model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ImageNet + iNat on WS-DAN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" Input for the text The following paper is using the CvT-W24(RANS) model to train an image-to-text learning algorithm. For each batch of 50, the model will predict a specific vocabulary word from a corpus of text. The paper states: ""An extensive set of CvT-W24 (RANS) models has been widely used in computer vision for various purposes. The CvT-W24 models are used to detect objects and to classify objects into small subclasses as well as to predict the next class. The most commonly used models include the ImageNet-K, ImageNet-S, and SINAM-PC (RANSPC). Each of these models is capable of training on large sets of training images. In this work, an online CvT-W24(RANS) model has been pre-trained to a single dataset. The model is trained on a subset of images in the pre-trained CvT-W24(RANS) dataset. The pre-trained model is then used in comparison to the real training data and also against trained real data to predict a word from a set of image. This experiment is to demonstrate the validity in training a real CvT-W24(RANS) model against trained real data to predict the next word."" The following link shows the code used in the paper http://downloads.kaggle.com/pubs/2014-2017/cvt-w24.tar.gz  It is interesting how the training data are uploaded to the online model. This is done in a similar way as how the training data are uploaded to a network. There is a second part of the pre-trained and trained datasets that are used like the final image dataset in the example."	False
"
 input (English text): Can you list the metrics used to evaluate models on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the MLDoc Zero-Shot English-to-Russian benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Input files with variable names starting with orkgc: $ export BUCC_DEFAULT as 'default'; $ export BUCC_LITERAL as 'literal'; $ export BUCC_DEFAULT_PARAMS as BUCC_DEFAULT_PARAMS; $ export BUCC_FROM_RUS as BUCC_LITERAL_FROM_RUS; $ export BUCC_FROM_RUSS as BUCC_DEFAULT_FROM_RUSS; $ export BUCC_FROM_RUS_SHARED as BUCC_DEFAULT_FROM_RUS_SHARED; $ export BUCC_FROM_RUS_GEO as BUCC_DEFAULT_FROM_RUSS_GEO; $ export BUCC_FROM_FROM_FROM_RUS as BUCC_DEFAULT_FROM_FROM_RUS_GEO; This creates a list of the metrics that are used to evaluate models on the BUCC German-to-Russian dataset. I'll explain the metrics a bit further in the next blog post. My goal here is to create a list of all metrics"	False
"
 input (English text): Which model has achieved the highest ROUGE-2 score on the AESLC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of ROUGE-L metric on the AESLC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-L"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the AESLC dataset in terms of ROUGE-1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" We did not see these results (yet). What gives ? Why do you have to wait ? You have to go all the way back and do it again later. You have to get yourself used to the idea that ROUGE-1 is only one of the possible metrics for the AESLC benchmark dataset (although all of them may also be used in the validation). By the time you get the first ""good"" model, the same model will have to repeat the same experiment, and even the same benchmark, again and again, until all the models converge. In this case, each of the benchmarking models has its own metrics and it will be a waste of time to go back and do the benchmark again -- at least not after you have finished the benchmark process. ROUGE-1 should allow you to perform a complete validation. Since each benchmark results is independent, you can only observe its effects -- for one such model, the other benchmark results will still be the same if you compare them directly. The main advantage of doing benchmarks instead of doing validation is that there is no risk that the model that performs best is wrong for a particular dataset. You can use all the features of that model, in one model, to prove the correctness of another model, and you will always obtain a model that outperforms the other model. If your benchmarks fail, this does not mean that there is a problem with the model that does not exist, but rather that some of the features you tried out on the training dataset are not applicable in the validation. It could happen that your model is actually wrong for your test dataset, too -- you have not tested only your model for all the metrics of the benchmarking model (instead, testing only the parameters in your model that you think will work for your validation. I could imagine this happening to a lot of model-based systems -- they do not test for a good"	False
"
 input (English text): What is the name of the top performing model in terms of Mean Accuracy score when benchmarked on the ModelNet40 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Mean Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ModelNet40"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the ImageNet 64x64 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the ModelNet40 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ModelNet40"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" Please try to generate a benchmarking test set using the benchmark tool. For example, you may try the -T or -f flag to generate an output file directly. By default, the benchmark command generates an output file which is automatically placed in the bench output directory. You could also explicitly add a benchmark directory and add the following command: $ benchmark -r bench.out -S ""benchmark_directory=bench.out"" -F 'benchmark_file=benchmark_directory/my_benchmark.txt ' You can also specify the name of an existing benchmarking file by executing the following command: $ benchmark -r bench.out -S ""benchmark_directory=bench.out"" -f /path/to/benchmark_file/my_benchmark.txt In addition, the benchmark command also takes a directory path as an argument. Where the benchmark directory is, Benchmark should output an output file named Benchmark.out in this directory. When running Benchmark, only the source files can be benchmarked. If you see any error messages or you run into trouble, let Benchmark know by running the -f flag: $ benchmark -f /path/to/benchmark_dir/path/to/benchmark.out This way, it won't be able to generate a benchmarking test file in the default location, and users with a specific command line options (i.e., -f in the above example) will not receive a default output file. You can run all of the benchmarks at once by issuing this command (requires an appropriate configuration file): $ benchmark -t -r -f -s <your_benchmark_directory> Benchmarking: The Benchmark tool generates a benchmarking test file named Benchmark.out, and a corresponding"	False
"
 input (English text): List the metrics that are used to evaluate models on the Penn Treebank (Word Level) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Penn Treebank benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Input: Python file with full source code to implement the above test. input (R): R code (file with full source code). output (R code): A Python program that tests the impact of using different metrics of trees (from Amazon, Amazon, and Treebank) on the Penn Treebank (Word Level) benchmark dataset. output (R code): A Python program that tests the impact of using different metrics of trees (from Amazon,"	False
"
 input (English text): Can you list the models that have been evaluated on the Reuters En-De dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the Reuters-21578 dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the Reuters-21578 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 You can have a look at the available features available on the SPSS dataset: Feature Version Number of Features Features (sorted in descending order) Time taken to test 0.02 sec 0.00 sec 0.00 sec 1.60 sec 2.00 sec 2.00 sec Time taken to load 0.09 sec 0.00 sec 0.00 sec 1.59 sec 2.05 sec 2.05 sec 3.14 sec Total number of times tested 0.18 sec 0.00 sec 0.01 sec 1.53 sec 2.05 sec 5.04 sec 9.04 sec Max number of times tested 1.56 sec 0.00 sec 1.00 sec 1.52 sec 7.96 sec 4.90 sec 16.96 sec The above examples tell us that, among the test models, those with the lowest Time to load scores are most accurate. The reason is that the model test is performed in the second stage. In the first stage, the results of the benchmark for the model are loaded and then the model's performance can be evaluated by the user. How is the model benchmarking performed ? Before selecting the benchmark to be benchmark tested, users need to select what model to use as benchmark test. The most common benchmark model used is one that takes into account the model quality and that is also the highest quality as identified in the benchmark criteria used. Since none of the benchmark models are used for all the benchmark datasets, users should select the most suitable one for each data set (for each model, for instance) and choose the model that is suitable for that data set. For the benchmarking for the model SPSS uses, users need to select the model which is best performing in terms of both Accuracy metric used and Accuracy and Standardize metric used. At least two features have to be selected by the user in order to use	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Atlantis dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER(str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. ?value rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	False
"
 input (English text): Can you list the metrics used to evaluate models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the RotoWire dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" output_id (String): Name of dataset. Output ID = ""re17.zip"". output_name (String): Name of dataset. The value of the output_name variable indicates which output is from the RotoWire dataset. If no output_name variable is set, a default output was displayed. If no output_name variable is set, the value of the output_id variable indicates the source dataset. output_format (String): The output format for the RotoWire dataset. Values in this range are specified as comma-separated list. The value of the output_format variable indicates how the output should appear in the table. The value of the variable indicates the name of output format. The format is case-sensitive, i.e., in the output format file for column x, you will see only row names ending in X. Other information about the format and how it is set can be found on the RotoWire website. output_summary (String): The output summary information for the RotoWire dataset. The value of the output_summary variable indicates how the summary should appear in the table. This information is specified as a comma-separated list. The value of the variable indicates the name of output summary format. The value of the variable indicates the name of output summary information for column x. output_x (String): The output format used for the RotoWire dataset. Values in this range are specified as comma-separated list. output_y (String): The output summary information used for the RotoWire dataset. The values in these ranges are specified as comma-separated list. The value for the variable indicates the name of output summary format. The value of the variable indicates the name of output summary table. output_pivot (String): The table format used for the RotoWire dataset"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the ImageNet V2 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?benchmark ?benchmark_v2_samples ?db/datasets. ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark_orkgp:HAS_DATASET!benchmark. ?comp orkgp:P31 ?comp; rdfs:label ?object. ?object. ?object. ?object. ?object. ?object; rdfs:contest ?object_samples. ?object_samples. ?object_samples. ?"	False
"
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Seaquest benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Pitfall! benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pitfall!"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 HERO benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 HERO"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ROUND(AVERAGE(ROW_NUMBER(n)) / HOUR,1/(n + 1),0); | ROUND(AVERAGE(AVERAGE(ROW_NUMBER(n)) / HOURS,1/(2))); | ROUND(AVERAGE(DAY_OF_MONTH)(AVERAGE(ROW_NUMBER(n)) / DAY) | ROUND(DAY_OF_MONTH(n)) / DAY) | ROUND(DAY_OF_MONTH(n)) / DAY) | ROUND(DAY_OF_WEEK(n)) / DAY) | ROUND(DAY_OF_WEEK(n)) / DAY) ------------------------------------------------- 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Amazon dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Amazon-5 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-5"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Amazon-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" Input and Output¶ In order to use the Amazon-2 dataset, and perform a benchmark, you also need to define the model and the dataset for which you want to benchmark. This can be done using a Python dictionary as follows. In the dictionary, you can either name the dictionary, or store a list with its keys. As an example, suppose to use the names 'amazon.1' and 'amazon.2': from sklearn.models import Amazon Amazon = Amazon ( name = 'amazon.1', attributes = { 'type' : 'binary_search' }) Amazon. __init__ ( self, metric = ""bench_time"", options = {'min_metric' : 'Amazon-2' }) Note That a dictionary with dictionary keys and lists must be passed to the sklearn.preprocessing.process_data() Python function, like any other Python program. If you want to process the dataset (which could happen without calling the function before), use the methods sklearn.preprocessing.process_data() and sklearn.preprocessing.compile_model(). You can also specify the inputs for the library when you call 'data/load_amazon.py' in order for the library to load the Amazon-2 dataset, and perform a benchmark. This can be done via the load() method, which will be called whenever you do a benchmark and pass the required inputs as the first argument: from sklearn import load, preprocessing as sklearn Load (input_dict = load(inputs=( 'amazon.1', 'amazon.2' )))"	False
"
 input (English text): List the code links in papers that use the Qbert Rainbow+SEER model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Qbert Rainbow+SEER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Rainbow model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rainbow"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Rainbow+SEER model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rainbow+SEER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Qbert Rainbow+SEER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): How do you feel about the current state of the Rainbow+SEER model ? (i.e., are we close to reaching the Rainbow+SEER design goal of providing a scalable, fault-tolerant, extensible, and interoperable set of models) output (Sparql query): "	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy (Middle) score when benchmarked on the RACE dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (Middle)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the RACE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (High)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (High)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): Indicate the model that performed best in terms of Accuracy metric on the RACE benchmark dataset ? output (Sparql query): The top-performing model in terms of Accuracy was RACE. The RACE benchmark has more data types than the previous datasets, including both multi-dimensional and single-dimensional data. This test illustrates the ability to perform queries in multi-dimensional data in R, using the RACE benchmark dataset without the need for custom queries. When working with datasets with more data types than single-dimensional data, the RACE benchmark uses multiple indices to store the data, making it more efficient against index allocation errors. input (English text): Indicate which RACE"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 Czech-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Czech-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Russian dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } invalid output (SQL Error) input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 (US) dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 (US)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } invalid output (SQL Error) input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 (German) dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str"	False
"
 input (English text): Can you list the metrics used to evaluate models on the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the LAMBADA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""LAMBADA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the MUTAG dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MUTAG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MUTAG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The data is stored in the following format - { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?dataset rdfs:label ?metric_lbl. … ?muse dsl :Dataset; ?muse rdfs :label ?metric_lbl. … ?muse pd :Dataset; ?muse pdm :Label; orkgp:HAS_Eval orkgp:HAS_METRIC ? ; orkgp:HAS_EVALUATION ? ; orkgp:HAS_METEOR ?metric_lbl. … { ?dataset rdfs :Label; ?metric rdfs:Label; { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl. == ""MUTAG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ) } } }"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the AESLC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the ESC-50 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ESC-50"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ESC-50 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ESC-50"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" 3.8.2: An example with the SAS BLAS benchmark For this example, we'll use the BLASbenchmark module from the scikit-learn API. Before we do, we're going to create an instance of each package. We run the scikit-learn install command as follows: From the scikit-learn dashboard, access the BLASbenchmark package by clicking the ""BLASBenchmark in the BLAS subdirectory, OR from https://matplotlib.org/install/package/BLASbenchmark"", and following the instructions to add it to your environment. 3.8.2: Importing BLASbenchmark into your environment From the command line, execute the following command to install the package: scipy.packages.blasbenchmark import scipy.optimize You'll see output confirming that the package is imported. This means we've added a feature to our Python environment. 3.8.3: Using the package The BLASbenchmark package expects the following input: For the BLASbenchmark benchmark, we have one BLAS table, a set of BLAS coefficients and an output dataset of a single class and each of its derivatives corresponding to one pair of input and output variables, respectively. In our case, we're going to perform a single example in which we'll use a regression tree, which may or may not be appropriate for other models in your data, or is not required if you're only doing a single regression model. We specify values for the following variables: The class label: This key is the class label of the function from which we learned the function being benchmarked, in this case ROC curve regression. The parameter to use when benchmarking: Any values. The number of iterations: This is the number"	False
"
 input (English text): Which model has achieved the highest F1 score on the SQuAD2.0 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT datatype, rdf_df2_lbl as lbl, rdfs_df2_lbl as lrdfs, orkgp_lbl as rdfp, orkgp_rdfs as rdfp_lst, and f1_rdf in R.lst # R.rdf sqrshc a, rafs p, orkgp_lst as rdfs, F1 rdfs and F1 rdfs to convert F1_rdf to F1. R.f1 lst. # input (English text): Which model has achieved the highest F1 score on the SQuAD1.1 benchmark dataset ? output (Sparql query): R.f1 sqrshc a, rafs p, orkgp_lst as rdfs, F1 rdfs and F1 rdfs to convert F1_rdf to F1. R.f1 lst. input (English text): Which model has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset ? output (Sparql query): # SELECT datatype, rdf_df2_lbl as lbl, rdfs_df2_lbl as lrdfs, orkgp_lbl as rdfp, orkgp_rdfs as rdfp_lst, and f1_rdf in R.lst # R.rdf sqrshc a, rafs p, orkgp_lst as rdfs, F1 rdfs and F1 rdfs to convert F1_rdf to F1. R.f1 lst. #	False
"
 input (English text): What models are being evaluated on the TSE-NER dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 2.4. Evaluating TSE model models The method described in the previous section requires a TSE model in order to evaluate the model. The first step is to identify model parameters within which you want to evaluate the model. Most of the time this is done by first creating the parameter matrix, an SVD with order 1: X, Y, Z,..., specifying the value of each parameter with respect to all other parameters. Such matrix can then be transformed into the model parameters: 4.3.1. Matrix transformations To translate an SVD matrix into parameters, we simply repeat the matrix multiplication, one term per parameter. This can of course be more convenient than creating the matrix separately, but it can be slower too. We will perform the matrix transformation from SVD to the model parameters after performing the RLS by the model. The RLS allows for a more direct way of doing matrix transformation into parameters, and is typically performed when creating the parameters matrix, instead of transforming from SVD to model parameters. 4.3.2. Enabling RLS of TSE models The first step is to enable the RLS of the TSE model."	False
"
 input (English text): What is the highest benchmark result achieved on the RotoWire (Content Ordering) dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of BLEU metric on the RotoWire benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Here is a more detailed analysis of the output: This output is a lot more complex than the raw results from the rdf/table analysis, but its data set is very large with many columns from 3 different data sources. In a general sense we can think of this output as an RDF-C structure, although the query looks slightly more like a JSON-LD one. The result is available here for anyone interested in more detail. Here is a screenshot (sorry for the small screenshot size, I had to fit for the full screen on my laptop): Here is how it looks on my desktop: The RotoWire data used for this analysis was stored in a S3 bucket. The whole RotoWire project's GitHub repository is found here.	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Seaquest dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	--------------- Total scores : 7,859,507 Total scores for datasets : 957,091,567 Data : --- - - - --- - ----------------------------------------------- Total scores by key : - - - - - - - ------------- The only dataset where this key's score was less than 0 was Seaquest. In some cases, you might be able to improve performance by using multiple data sets, but you'd then require more data storage and processing on each dataset. However, you could always perform more statistics (e.g., by using a data warehouse), which can greatly increase your performance, since the total number of measurements is much larger, and there are many additional datasets that you could choose from with a smaller number of parameters. Note : I have not tested the performance of this query against all the Atari 2600 datasets, as I have not yet had time to compile them. Data Source: The following data source contains Atari 2600 test scores from 1985 to present. Note: I have not tested this one against other Atari 2600 datasets, as I have not yet had time to compile them. Dataset and Metrics: Date : 1985 Dataset name : Atari 2600 Testing Set Metric : Score The score is stored in ISO-8601 compliant format. The integer value for the score has two components: the score value, and the percentile score range. The score value represents: 1 - Low score, 0 - Medium score. 1 2 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 11 11 12 12 13 13 14 A score of 0 indicates a score of 0.5. The score range is calculated as 30-59 with 100 being the highest score possible. The score range, in metric format, is : - 90 - 99 99 100 100 The metric score range is 30-59 with 100 being the highest possible score possible.	False
"
 input (English text): Can you list the models that have been evaluated on the GENIA - UAS dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - UAS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the GENIA - LAS dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - LAS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the GENIA - LAS benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - LAS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?benchmark ?benchmark_lbl. FILTER (str( ?benchmark_lbl) = ""GENIA -LAS"") ?benchmark. ?model rdfs:text ?model_lbl. } For a short summary of models included in this benchmark, see the README of the datasets, such as this one from the research lab of Dr. Alex Cope at the University of California, Santa Barbara: [UPDATE 4/11/12] Many thanks to my colleagues from the AIA (American Astronomical Society) lab for the help and input that made this benchmark possible... including one of our own."	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp-2 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Yelp Fine-grained classification dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT a as ?key, rdfs.dataset_id as ?dataset_id WHERE data.pub ?benchmark. ( NOTE : These queries are executed in parallel, to allow for parallel processing) input (English text): Give the URL of the dataset to which the benchmarking was based, which was used to create the benchmark	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the RACE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the 20NEWS benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""20NEWS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT RACE, ?model from c ( SELECT rdf ( 1 ) ORDER BY DESC(. 1 ) ), ?dataset ( ORDER BY rdf( 2 ) ORDER BY ""Api"" ), ?model ( ORDER BY RACE) FROM datasets) AS rdfs, { ?dataset ?data ..., ?metric a }; Note: You have to select one database and one or two or more model. But that is not required. The above query can be executed from 2 databases simultaneously. Just click on each one. You see the result of the query from these 2 databases at the top right of the page (in case we have more queries available). 3. Then, you would need to select one of the query results (in the left column) and click on the RACE in the query above to see the model that performed the best or the model that performed worst in terms of (Accuracy plus Accuracy minus Accuracy) ? 4. You can test your query by inserting a simple query to the model under investigation here. For the example, we will use the one above. Let's use 10k of the most popular web pages (the most popular page in the Internet). The query below is inserted with the same model. The result of this query is also shown at the top right corner of the page: select c.dataset, c.model, ?score, ?number1, ?number2 from c as c on c.dataset.count=0 We have 10 rows for this query select c.dataset, c.model, ?score, ?number1, ?number2 from c as"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 James Bond dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 James Bond"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bank Heist dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bank Heist"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bank Heist"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Blimp dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Blimp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Cobble Pop dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Cobble Pop"") ?benchmark orkgp:HAS_D"	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ARC (Easy) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Easy)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the CoQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Overall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of A2 score when benchmarked on the ANLI test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT rdfs:label ?metric_lbl. FROM ( SELECT ( ?metric_lbl) ?data as data_lbl. rdfs:label ?metric_	False
"
 input (English text): Could you provide a list of models that have been tested on the WNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the QuAC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QuAC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; id:int ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QUAC"") ?benchmark orkgp:HAS_DATASET ?dataset; ?name orkgp:HAS_NAME ?model. ?name rdfs:label ?model_lbl. } output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; id:int ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QUAC"") ?benchmark orkgp:HAS_DATASET ?dataset; ?names orkgp:HAS_NAME ?model. input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; id:int ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QUAC"") ?benchmark orkgp:HAS_DATASET ?dataset; ?names orkgp:HAS_NAME ?model. OR   ?num_classes OR   ?max_depth OR   ?min"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Time Pilot dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Time Pilot"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Amiga trackset with a benchmark for the Atari 2600 Atari trackset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amiga Trackset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Atari Trackset with a benchmark for the Atari 2600 Atari Trackset with a benchmark for the Atari 2600 TI-99/4a Trackset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the MPCM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MPCM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ning et al."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the MFEC model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MFEC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 This request takes one or more rows from the SPSS text data file, and returns two or more rows for each benchmarked paper. This query uses the 'all' option for the queries, meaning that benchmarks are sorted alphabetically. If you omit the 'all' option from your.sps file, SPSS defaults to ordering by most recent. The queries are available through the SPSS library as.qp in your output.	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CAIT-S-48 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-S-48"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the CaiT-M-48-448 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CaiT-M-48-448"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the CAIT-XS-36 model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-XS-36"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Can you provide links to code used in papers that benchmark the CaiT-XS-36 model ? output (Sparql query): In all of these cases (excluding the benchmark papers), data from the benchmark papers was extracted directly out of the files. What have we just seen ? Is this enough ? Are there any other examples of data that would be useful for benchmarking ? Is there any other database/dataset(s) that would be useful for benchmarking ? We hope that this post has been helpful in explaining the concepts mentioned here for a wider audience and hopefully, it has inspired some other people to start exploring the potential of this approach. We will be continuing to refine the list we discuss here as we continue investigating how we can improve the quality of our benchmarking and how we can better make the information more broadly understood. For the moment we only have a few benchmarks in general use but as time goes on we plan to expand, expand our list and hopefully create even wider applications with more diverse data we are able to test against.	False
"
 input (English text): What models are being evaluated on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the CIFAR-10 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model ?model_lbl | ?model_lbl. :dataset a orkgc:Dataset | ?model_lbl. :dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip))... model name is sas.cse.lsu.edu ? input (English text): What models are being evaluated on the data.gov dataset ? output (Sparql query): SELECT ?model | ?model_lbl | ?model_lbl. :dataset a orkgc:Dataset | ?model_lbl. :dataset_lbl. FILTER (str( ?dataset_lbl) = ""data.gov"") ?model rdfs:label ?model_lbl. input (English text): What models are being evaluated on the MEGA dataset ? output (Sparql query): SELECT ?model ? | ?model_lbl | ?model_lbl. :dataset a orkgc:Dataset | ?model_lbl. :dataset_lbl. FILTER (str( ?dataset_lbl) = ""mega"") ?model rdfs:label ?model_lbl. input (English text): What models are being evaluated on the HUFFMAN dataset ? output (Sparql query): SELECT ?model | ?model_lbl | ?model_lbl. :dataset a orkgc:Dataset | ?model_lbl. :dataset_lbl. FILTER (str( ?dataset_lbl) = ""huffman"") ?model r"	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Pitfall! benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pitfall!"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Phoenix benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Phoenix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Montezuma's Revenge benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT model *,... FROM ( SELECT * FROM ( SELECT m.model,... END FOR ) AS model m WHERE m.metric = ? or m.is_dyn( ?)) models ) output (Sparql query): SELECT model ? FROM ( SELECT model.model as ( ?metric),... FROM ( SELECT * FROM ( SELECT model.model as ( ?metric),... END FOR ) AS model m WHERE m.metric = ? or m.is_dyn( ?)) model m WHERE m.metric IS ( ?value), ?value is NULL/NULL SELECT model ? FROM ( SELECT model.model as ( ?metric),... FROM ( SELECT model.model as ( ?metric),... END FOR ) AS model m WHERE m.metric = ? or m.is_dyn( ?)) model m WHERE m.metric IS ( ?value), ?value is NULL/NULL. (Optional) Output the results of your query by using the output form, or SELECT model ?,... FROM result.fields WHERE result ?, ? ?.,..., results,... WHERE result ?, ? ?.,.... output (Sparql query): SELECT ?, ?,.,... FROM result WHERE model ?, ? ?.,... WHERE 1. SELECT ?, ?,.,... FROM result WHERE model ?, ? ?.,... WHERE 2. See Also: What's New ? If you are interested in the specific changes and new features in this version of Parabola I suggest you read the full changelog. Note The release version is now up to 0.8.2. This release contains various bug fixes and some new features. Thank you for your continuous support and bug reports! 	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet ReaL dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT rdfs:class ?class, rdfs:stat ?stat, rdfs:score ?score, rdfs:metric ?metric FROM rdfs_rdfs GROUP BY ?class, ?stat, ?score ORDER BY rdfs:index ON rdfs:name=rdfs:name JOIN data_datasets rdfs ON rdfs:name= ?index_name ORDER BY rdfs:rank DESC( )	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BiT-S (ResNet-152x4) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet-152x4)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-TDN(ResNet-101) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-TDN(ResNet-101)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the ResNet-152 (SAM) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ResNet-152 (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model resnet_52_model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAM"" ) ?benchmark orkgp:HAS_DATASET ?dataset. ""RES"" orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-PTY-PAT model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model resnet_5_model a orkgc:Model; rdfs:label ?model_lbl. FILTER str( ?model_lbl ) = ""BiLSTM-PTY-PAT"" ) ?benchmark orkgp:HAS_DATASET ?dataset. ""RES"" orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Can you provide links to code used in papers that benchmark the BiLSTM/TDM model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model resnet_5_model a orkgc:Model; rdfs:label ?model_lbl. FILTER str( ?model_lbl ) = ""BiLSTM/TDM"" ) ?benchmark orkgp:"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the PubMed 20k RCT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the PubMedQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Pubmed dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl Note That there is NO need at all for the parameter ?label, unless this field is an arbitrary string given by the user. To filter a dataset based on one or more labels, use ?matric_lbl, rather than ?metric_lbl. input (English text): Is there a higher score with all the categories in the list than the benchmark score for the dataset ? output (Sparql query): SELECT NCHAR ( ?'a','1') ?value FROM (SELECT COUNT(*) AS total FROM (SELECT COUNT(*) AS score FROM (SELECT ?id ORDER BY ?id DESC LIMIT N' ) ORDER BY ?id DESC) total) )"	False
"
 input (English text): Can you list the models that have been evaluated on the Walker, walk (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cheetah, run (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Walker, walk (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Walker, run (DMControl500k) dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } For a few models I also needed to evaluate on the new Cheetah dataset, but the results were somewhat inconsistent. The comparison was as follows, with a few comments: DM-controlled Cheetah: Walker, walk (DMControl500k) Cheetah: Walker, walk_100k Walker, run (DMControl500k) Cheetah: Walker, run (DMControl100k) I hope this helps you with your next project."	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the PolyAI Reddit dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PolyAI Reddit"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the DBpedia dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the REDDIT-B dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""REDDIT-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reddit"")) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): What is the highest benchmark result achieved on the DBpedia dataset, including the metric and its value ? output (Sparql query): ------------------------- SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"")) ?benchmark orkgp:HAS_ DATASET ?dataset; orkgp:"	False
"
 input (English text): What are the models that have been benchmarked on the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the CoQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Softcite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } }  Input (English text): For what reason do you evaluate the model that has had the maximum number of records evaluated by the CoQA dataset ? Explain your reasoning. output (Sparql query): OPTIONAL { ?data orkgc:Dataset; ?data rdfs:label ?dataset_lbl. } OPTIONAL { ?data rdfs:index ?dataset_lbl. } OPTIONAL { ?data rdfs:max_row ?dataset_lbl. } OPTIONAL { ?data rdfs:min_row ?dataset_lbl. } OPTIONAL { ?data rdfs:mixed ?dataset_lbl. } OPTIONAL { ?output orkgp:HAS_MODEL ?dataset. } OPTIONAL { ?model orkgp:HAS_MODEL rdfs:label. } }  Input (English text): For what reason do you evaluate the model that has had the least number of records evaluated by the CoQA dataset ? Explain your reasoning. output ("	False
"
 input (English text): Could you provide a list of models that have been tested on the SST-5 Fine-grained classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the SST-2 Binary classification dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-2 Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the SST-2 Binary classification benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-2 Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-1 Classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Can you provide a list of models that have been tested on the SST-0 Classification benchmark dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-0 Classification"") ?benchmark orkgp:HAS_DATASET ?dataset; ?opts orkgp:OPTIONAL orkgp:OPTIONAL ?dataset_lbl rdfs:label, ?optim orkgp:OPTIONAL ?model_lbl. OPTIONAL { ?dataset_lbl rdfs:label, } } input (English text): Could you provide a list of models that have been tested on the SST-6 Model"	False
"
 input (English text): Which model has achieved the highest Accuracy (%) score on the Hendrycks Test benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hendrycks Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the  Jacquard dataset dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Hendrycks Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hendrycks Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?model ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hendrycks Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } group by ?metric input (English text): What is the top benchmark score and its metric on the Jacquard dataset dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the LSTM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the REL-RWMD k-NN model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""REL-RWMD k-NN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""KD-LSTMreg"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""KD-LSTMreg"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } RAW Paste Data --[[ SORT BY [DIFF #] ASC ( DESC( # | DESC( [|] ASC) ) ASC ASC [|] ASC SORT BY [dff-c]][f] ASC ASC orkgc:Model; rdfs:labels = """" desc:Labs_Lists = """" DESC( # | DESC( [|] DESC) ) ASC DESC( [|] DESC) SOURCE_CODE DESC( [|] DESC | DESC( [|] DESC) ) desc:DATASET DESC( [|>DESC( [|] DESC) ] ) desc:MODEL DESC( [|> DESC[ [|] DESC] ) ] DESC DESC( [|] DESC | DESC( [|] DESC) ) DESC DESC DESC( [|] DESC | DESC( [|] DESC) ) DESC DESC DESC( [|>DESC( [|] DESC)] ) DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC desc;"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the TDM Tagged Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the NLP-TDMS (Exp, arXiv only) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 input (English text): List the title and ID of research papers that contain a benchmark over the NLP-TDMS (Exp, arXiv only) dataset ? output (Sparql query): input (English text): List the title and ID of research papers that contain a benchmark for the NLP-TDMS (expunged) dataset ? output (Sparql query): input (English text): List the name of research paper which includes a benchmark over the NLP-TDMS (expunged) dataset ? output (Sparql query): input (English text): How many records are there in the 'top 1000' results in the NLP-TDMS dataset according to the NFT test score (NFT-T) ? output (Sparql query): (t) input (English text): How many records are there in the 'top 1000' results in the NLP-TDMS dataset according to the NFT test score (TFT-T) ? output (Sparql query): (t) input (English text): How many documents were found in the top 1000 results in the NLP-TDMS dataset according to the NFT test score (NFT-T) ? output (Sparql query): (t)= input (English text): Table of how many documents were found in the top 1000 results in the NLP-TDMS dataset according to the NFT test score (NFT-T) ? output (Sparql query): input (English text): Table of how many pages in the NLP-TDMS dataset are marked as 'data only', and in what condition were they found ? output (Sparql query): input (English text): Table of how many pages in the NLP-TDMS dataset are marked as 'data only'	False
"
 input (English text): Can you list the metrics used to evaluate models on the ClueWeb09-B dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the IMDb-B dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the ClueWeb09-B dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") For the first part of this question, you would probably receive an empty response because there are only four models that the data have been evaluated on. And you would also receive all three results for the ""clue web09-b"" dataset. For ""clue web09-bp"", you would also receive all three results for ""clue web09-bp_rdf"". Here, we are going to break down the query.  Clause 10 – ""If the Clue Web09-B dataset contains not more than 10 examples"": In the first part of the question, we already saw "" Clue Web09-B dataset contains not more than 10 examples"" as the default answer. But you may have noticed that there was another answer. To see if we can change this default answer to ""true"", follow these steps: Update the default answer back to ""false"". Create a new statement inside the previous question using the same data type to determine if each of the ""clue web09-b"" results has been evaluated on the same dataset. This will allow us to see if the ""clue web09-bp"" dataset has the same information regarding its models and results. Let's do it. Clause 11 – ""If the Clue Web09-B dataset contains not more than 10 examples"": Now that the default answer has changed to ""false"", we will change that back to ""true"". Make a NEW statement inside the original query: UPDATE dataset SET data = { ?dataset a orkg"	False
"
 input (English text): Can you list the metrics used to evaluate models on the IMDb-B dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the iNaturalist 2018 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Metagrave 2011"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Can you list the metrics used to evaluate models on the Metadata2017 dataset ? output (Sparql query): input (English text): Can you list the metrics used to evaluate models on the MovieLens dataset ? output (Sparql query): input (English text): Can you list the metrics used to evaluate models on the Natural History Museum-Museum of Natural History (NHM-ML) dataset ? output (Sparql query): input (English text): Can you list the metrics used to evaluate models on the NIST_B2 dataset ? output (Sparql query): input (English text): Can you list the metrics used to evaluate models on the Natural History Museum (NHM)1 dataset ? output (Sparql query): input (English text): Can you list the metrics used to evaluate models on the Natural History Museum-NHM Dataset (M2) dataset ? output (Sparql query): input (English text): Can you list"	False
"
 input (English text): Provide a list of papers that have utilized the CvT-13-NAS model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-13-NAS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the CvT-W24 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-W24"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the CvT-21 (384 res) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-21 (384 res)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-21"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Input data was provided through the Crawl. (For questions about using CvT-W24-NAS in the CvT-21 model please see the project documentation.) The CvT-W24-NAS model was designed to be fast, and since it is based on linear regression, performance was measured in the 100 MFLOP limit (for the most popular dataset: http://cw.skeeten.org/) using a linear model constructed from several classes of parameters. However, the models were also tested on different datasets and models were tested on different datasets on the same machine and over different datasets using different data sets. For a large number (over 100) of models, there was no discernible difference regarding the performance. The results of this study were compared to other studies. To compare the results of this study with those of other research, please see the following studies: K. D. Loomis, C. B. Gudje, R. F. Oden, A. W. Reif, C. S. Cusina, D. S. D. Miller, W. G. T. Ahern, C. P. Wilson, B. A. Thompson, P. S. Tarr, and M. F. O'Connor, 2013 ("	False
"
 input (English text): Can you provide links to code used in papers that benchmark the SciKG model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciKG"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the CGU model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CGU"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the FG fine-grained gate model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FG fine-grained gate"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FG) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Can you provide links to code used in papers that benchmark the JAG model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""JAG"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Can you provide links to code used in papers that benchmark the BLAS model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BLAS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:H"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the NYT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the NYT24 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the NYT29 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	------------------------ 797 ------------------------ (0.02%) Best result 792 ------------------------ (0.04%) Best result 791 ------------------------ (0.03%) Best result 790 ------------------------ (0.00%) Best result 789 ------------------------ (0.00%) Best result 788 ------------------------ (0.00%) Best result 787 ------------------------ (0.19%) No score, best result 786 ------------------------ (0.14%) Best result 785 ------------------------ (0.23%) Best result 784 ------------------------ (0.17%) Best result 783 ------------------------ (0.06%) Best result 782 _ _ _ ------------------------ (0.18%) Best result 781 ------------------------ (0.14%) Best result 780 ------------------------ (0.25%) Best result 779 598 ------------------------ (0.04%) Best result 778 _ ------------------------- (0.13%) Best result 777 ------------------------ (0.03%) Best result 776 _ ------------------------- (0.34%) No score, best result 775 ------------------------ (0.05%) Best result 774 _ ------------------------ (0.25%) Best result 773 ------------------------ (0.21%) Best result 772 orkgp:HAS_DATASET a 1.00 1.00 Best result 771 rdfs:value-Rdfs::table_list_list. output (Sparql query): ORDER BY DESC( ?value ) ||||| --------------||| --------------||| --------------||| --------------||| --------------||| --------------||| Best metric, best result best result Best result 766 rdfs:value-Rdfs::table_list_list_. Best metric, best result 765 rdfs:value-Rdfs::table_list_list. Best metric, best	False
"
 input (English text): Can you list the metrics used to evaluate models on the SearchQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the SNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the QNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics for which models performed well on the QNLI benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Can you list the models that performed so well on the SearchQA dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?bench"	False
"
 input (English text): Which model has achieved the highest Relation F1 score on the SciERC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Relation F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score on the ScienceCite benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Entity F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" If any model was not found, it would be noted in the table. If the query results in an error and not one model name appears, it will be noted in the error table as well. If the query finds a model name such as 'f4', results will return all models with that name, even if no matching queries are found. The model information can be viewed directly with the table below: If you are using Excel or a similar application, try accessing the data directly with a script in a spreadsheet, using the following formula: where ""model"" is the model name, ""benchmark"" the date range used and parameter ""orkgp"" the model rank relative to the other model names: This data is displayed in a tabular table: To retrieve all the model data, just enter the query and hit the Run button. The result is displayed into a table: If more than one model matches in the query, a list of corresponding models is displayed: When there a matching model, another table is displayed with the values from the model matching the query. Once the model names are displayed, you can either run the query yourself to obtain the query results by using the Query box and clicking run query or download and parse the results to download the models. The query output will be in PDF format for easy navigation, but the tables can also be viewed using a spreadsheet application like Excel or another application. Using the database The SysInternals driver for SciEQL is built on top of the Windows Hibernate database driver. This allows you to run the query and store all your results into Hibernate. The following is how to install SysInternals and select the correct Hibernate version for your environment: Copy and paste following command: ""cscript sysinternals_installer.vbs C:\"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset SST-5 Fine-grained classification?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the metrics used to evaluate models on the SST-2 Binary classification dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-2 Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ?cont orkgp:HAS_CONTINUE IF ?cont OR ?cont. ?value OR ?value or ?value. OPTIONAL { ?value OR ?value OR ?value rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl For S3 dataset R input (English text): Can you list all the metrics used when evaluating models on the SST-2 Binary classification dataset ? output (Sparql query): data:datas"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WebQuestions dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?label ?value. OPTIONAL { ?value ?value. } SUBMIT ORDER BY ?label, ?value --------------------------- input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (short) dataset ? output (Sparql query): --------------------------- --------------------------- --------------------------- --------------------------- --------------------------- --------------------------- SOURCE SELECT ?label ?value. OPTIONAL { ?value ?value. } SUBMIT ORDER BY ?label, ?value --------------------------- input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset ? output (Sparql query): --------------------------- --------------------------- --------------------------- --------------------------- --------------------------- --------------------------- SOURCE SELECT ?label ?value. OPTIONAL { ?value ?value. } SUBMIT ORDER BY ?label, ?value --------------------------- input (English text): Is there a metric or score that can be used to compare the datasets ? output (Sparql query): select "" ?conversion_marker. "" for each value in table_name_extracts where value!= ?conversion_marker; input (English Text): Is there a metric or score that can be used to compare the datasets ? output (Sparql query): select "" ?conversion_marker. "" for each value in table_name_extracts where value!= ?conversion_marker; input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Quiz dataset ? output (Sparql query): SELECT DISTINCT ?metric ?met"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 English-German?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WMT2016 Romanian-English dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 Czech-English?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Czech-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Czech-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value ) } } GROUP BY ?metric ?metric_lbl Here is the result of the SQL query that runs on these two datasets output of query (Sparql) DISTINCT ?metric_lbl from wmt2016_cz text as SELECT TOP( 1) d.lang_name, d.loc. from wmt2016_cs text as SELECT TOP( 1) (lang_name, count( ?loc_name) as p ) AS top_metric_score, lang_name, p desc d.lang_name lang_names FROM WMT2016_CZ_LANG_TEXT as l text LINGUAS text as SELECT TOP( 1) (lang_name, count"	False
"
 input (English text): What is the average energy generation of all energy sources considered?
 output (Sparql query): SELECT (AVG( ?elec_gen_value) AS ?average_elec_gen_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43134 ?electricity_generation. FILTER(REGEX( ?energy_sources_labels, ""all sources"")) ?electricity_generation orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?elec_gen_value) } 
 input (English text): What is the average energy generation for each energy source considered?
 output (Sparql query): SELECT ?energy_sources_labels (AVG( ?elec_gen_value) AS ?average_elec_gen_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43134 ?elec_gen. ?elec_gen orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?elec_gen_value) } 
 input (English text): Which sectors are modeled as energy sectors and how often?
 output (Sparql query): "	SELECT ?label COUNT( ?label) WHERE { ?sector rdfs:label ?label; a ?class. ?class owl:equivalentClass <http://openenergy-platform.org/ontology/oeo/OEO_00000367>. [ orkgp:P37668 ?sector ]. } 	"SELECT rdg( ) as rdg_label ?, total energy output from energy_sources rdfs as -------------------------- Total --------------------------- energy_sources. total rdfs. rdg_label ? ---------------------------- Energy Source ------------- --- --- --- ------------ --------------- Electricity (EU/ER) 65.5565 1.64 1.68 1.64 -0.073 Gas (EU) 2.2 2.17 2.20 2.18 -0.038 Coal (-Gas) 6.2 2.07 3.0 2.08 -0.027 Oil (-Gas) 5.2 2.10 2.16 2.16 -0.066 Water (-Oil) 0.5 0.55 2.28 2.25 0.0004 Other (Currency) 0.9 0.35 3.08 3.07 0.0009 To summarize: the European Union as a whole, using EU renewable energy projects, will generate 11.6 percent of annual overall energy generation, although it could generate more if the remaining subsidies are not removed. This is a bit of a low estimate given that the EU is at least four times richer than the US (although EU net foreign assets is still much larger than the US net foreign assets). More generally, it is a figure that has been estimated several times and which includes energy output for renewable power projects not accounted for in the EU. A similar case was used in a report I published last year – ""The Climate Economics of Electricity Storage"". I analyzed EU countries as well as non-EU countries which are members of the European Union, but which also provide for a fixed renewable energy package or do not provide their citizens with access to renewable energy. Since energy storage is no longer optional in the EU, the model assumes it to be mandatory. The result is"	False
"
 input (English text): Can you list the models that have been evaluated on the Kinetics-600 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECTED FROM DESIGN RESULTS REPORT: AND THE ANALYSIS AND COMPARISON OF THE BIOLOGY RESEARCH STUDIES: The DESIGN RESEARCH STUDIES FROM THE SOCIAL SCIENCES: ORGANISMS OF MUSCLE FIBER: DESIGN, DEVELOPMENT, AND APPLICATION OF MODEL SELECTED FROM DESIGN RESEARCH STUDIES: ORGANISMS OF MUSCLE FIBER: STUDY SELECTED FROM RESEARCH STUDIES: BACTERIA IN SPINE-MARKED MUSCLE FIBER: DESIGN, DEVELOPMENT, AND APPLICATION OF MODEL SELECTED FROM DESIGN RESEARCH STUDIES: BACTERIA IN SPINE-MARKED MUSCLE FIBER: STUDY SELECTED FROM STUDIES: ORGANISMS OF MUSCLE FIBER: BODY WEIGHT THEORY: DESIGN, DEVELOPMENT, AND APPLICATION OF MODEL SELECTED FROM DESIGN RESEARCH STUDIES: ORGANISMS OF MUSCLE FIBER: BODY WEIGHT THEORY: STUDIES SELECTED FROM RESEARCH STUDIES: BODY WEIGHT THEORY: STUDIES SELECTED FROM ORGANISMS OF MUSCLE FIBER:	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the RotoWire dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the RotoWire (Content Ordering) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ID, TITLE, DATA_COUNT FROM ( SELECT title, title_as_dataset,  DATA_COUNT() FROM { ?dataset a}. INNER JOIN { ?dataset b}. ON (title = b.title_as_dataset AND title_as_dataset <> b. title_as_dataset ) AS c ON c.dataset = a.dataset_as_dataset_p; ID TITLE NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL null { ?dataset a}. R. N. Leggett and R. M. Haskins. J. Comput. Phys. 50(1-2-1984) No. 2  (1985), pp. 913-1026. 1. P. A. G. Lutz and H. P. Leggett.    A. S. M. Friedman et al.   in   R. F. B. Bartlett , I. H. C. Oates , H. O. E. Lancaster and H. E. S. D.B.  (eds.),  The  R. D. M. B. Journal :  http://rft.org/rft/papers.html ?el_id=382591 2. H. C. Oates and H. G. Leggett ,   A. S. M. Leggett, E. G. Simkin and E. S. D. B	False
"
 input (English text): Where can I find code references in papers that have used the Naive Bayes model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Naive Bayes"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the Yelp Binary classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the precision, recall, and f1 values of all compared studies that used the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request? 
 output (Sparql query): "	"SELECT ?precision ?recall ?f1 WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:P15006 ?algorithms. ?algorithms rdfs:label ?alg_names. FILTER(REGEX( ?alg_names, ""Naive bayes"")) ?algorithms orkgp:P36075 ?features. ?features rdfs:label ?fea_names. FILTER(REGEX( ?fea_names, ""Bag of words"")) ?features orkgp:P37029 ?categories. ?categories rdfs:label ?cat_names. FILTER(REGEX( ?cat_names, ""Feature request"")) ?categories orkgp:P3004 ?precision; orkgp:P5015 ?recall; orkgp:P18037 ?f1. } "	"SELECT VARIABLE ?feature_request, DISTINCT ?feature_request_lbl, LENGTH( ?feature_request ) ?feature_request_lbl FROM ?feature_data. ?feature_request and ?feature_data. ?feature_request_lbl WHERE { ?feature_request_lbl is ""Yelp binary classification"" or ?feature_request; rdfs:label ?feature_request_lbl. } input (English text): Where can I get a list of the most discussed papers that have mentioned the Naive Bayes model for benchmarking purposes ?"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ARC-PDN dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NYT29 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dmlab-30 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dmlab-30"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Scaife-32 dataset ? output (Sparql query): input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Scaife-56 (scot-56) dataset ? output (Sparql query): input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Scaife-64 dataset ? output (Sparql query):, input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Scaife-128 dataset ? output (Sparql query):, input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Scaife-144 (scot-144) dataset ? output (Sparql query):, input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Scaife-159 dataset ? output (Sparql query):, input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Argo-17 dataset ? output (Sparql query):, input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dfse-19 dataset ? output (Sparql query):, input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the CCS-100 dataset ? output (Spar	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the IWSLT2015 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The following variables are supported by sparc: rdfs:label The table label of the WMT2016 English-German dataset. rdfs:name The name of the WMT2016 English-German dataset in German. rdfs:id The id of the WMT2016 English-German dataset."	False
"
 input (English text): What is the top benchmark score and its metric on the Cartpole, swingup (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the Cartpole, swingup (DMControl100k) dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?score ?score_metric } OR ( output (Sparql query): SELECT DISTINCT ?score ?score_metric_lbl } OR ( output (Sparql query): SELECT DISTINCT ?score ?score_metric RANK ?metrics output (Sparql query): input (English text): What is the top benchmark score and its metric on the Cart Pole, swingup (DMControl500k) dataset, excluding the metric and its value ? output (Sparql query):  SELECT \ 'Score' AS ?score  AND  /'''/  DATEPARTITION BY MATCH( ?metric ).  ORDER BY ?metric_lbl  .  , ?score_metric  .  ;  ORDER BY ?score  FROM LABELS ; 1. output (Sparql query): 2. 1. 496 output (Sparql query): 3. 1. 556 output (Sparql query): 4. 1. 666 output (Sparql query): 5. 1.	False
"
 input (English text): What are the metrics of evaluation over the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT RANK, RANK, SCORE, SCORE, SCORE, SCORE, SCORE, SCORE, SCORE, SCORE, SCORE, SCORE, SCORE, SCORE, SCORE, SCORE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE AS TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE, TITLE ; PRIMARY KEY ( TITLE, TITLE, SCORE ) AUTHORS: Michael S. Gellman, et al. MATCHING INDIVIDUALS: Michael S. Gellman, et al. MATCHING REFERENCES: Peter J. G. Trewen and John M. Raskin, Analyzing Intergroup Ranks Among Americans Who Identify with a Protestant or Catholic Church (Journal of Applied Social Psychology, vol. 60, no. 5, Oct. 1995) AUTHORS: Michael S. Gellman, et al. MATCHING INDIVIDUALS: Michael S. Gellman, et al. MATCHING REFERENCES: Daniel L. Lieberman, Race, Social Interaction and the Structure of American Politics (Georgetown University Press, 2005) AUTHORS: Michael S. Gellman, et al. MATCHING INDIVIDUALS: Michael S. Gellman, et al	False
"
 input (English text): Can you provide links to code used in papers that benchmark the LSTM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Grave et al. (2016) - LSTM + continuous cache pointer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Grave et al. (2016) - LSTM + continuous cache pointer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the LSTM (Bai et al., 2018) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM (Bai et al., 2018)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Grave et al. (2018) - LSTM + continuous cache pointer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Input (English text): The code links in papers that use multiple LSTMs (Bai et al., 2018) - LSTM + continuous cache pointer model in any benchmark ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Grave et al. (2018) - LSTM + continuous cache pointer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } The code links in papers that use several LSTMs (Trambert and van Dalen, 2016) - LSTM + continuous cache pointer models with different parameters used in different benchmarks, in any benchmark ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str"	False
"
 input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of PARAMS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Oxford-IIIT Pets benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Oxford-IIIT Pets?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?metric_lbl ?model_lbl as ?dataset_lbl, ?model_lbl ?dataset_lbl ORDER BY ?value desc input (English text): What is the best model performing the worst in terms of accuracy metric on the Oxford-IIIT pets dataset to perform better at ? output (Sparql query): select DISTINCT ?model_lbl ?model_lbl as ?dataset_lbl, ?model_lbl ?datas	False
"
 input (English text): Can you provide links to code used in papers that benchmark the XLNet model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the XLNet (Large) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (Large)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the XLNet-Large model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet-Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" Input-output Data and Formats This section describes the format of the output data and formats you can use. To see how XLNet is created, first go to https://www.p2pool.org/tools/examples/XLnet/run-test-model.html. There you can read about the construction of XLNet model, how to find the model, benchmark and write results, and how to use the output files. The format of the data can be summarized in this table (updated in March 30, 2016, when XLNet became live). The output can be visualized by any system that processes structured data. This section describes how to use the output for scientific analysis. We also discuss what formats are useful, how to integrate XLNet into data, and some common formats for communicating results with non-scientists. General Format The easiest and most common format to use is the XML-schema. If the output is to be used for a scientific journal, then the same XML-schema would be used for everything using the result and code link formats above. For any other use, specify the format for your data like this in the xml:lang value of the html tag of the paper (or any other document that can run the program): <meta http-equiv=""Content-Type"" content=""text/html; charset=UTF-8""> <title>Cascades of Trees for Large-Scale Tree Reconstructions</title> <id>XLC-Xml-id</id>... See http://www.unisys.fr/xlc-dataset-format.htm for an example of an XML-schema. If you want to use the results of a paper as part of a larger interactive user interface or research platform, make sure that the title and the id attribute of each output file"	False
"
 input (English text): Where can I find code references in papers that have used the EfficientNetV2-S model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the EfficientNetV2-S (21k) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-S (21k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-L"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?code as ?model, ?benchmark ( ?model_hits ) as ?benchmark1, ?benchmark ( ?model_hits ) as ?benchmark2, ?cont ( ?model_hits ) as ?cont1, ?cont ( ?model_hits ) as ?cont2, ?benchmark ( ?model_hits ) as ?benchmark3, ?cont ( ?model_hits ) as ?cont3, ?lab ( ?model_lbl) as ?link, ?modet ( ?model_lbl) as ?modet1, ?module ( ?model_lbl) as ?module1, ?source_code as ?sourcecode, ?model as ?model1, ?benchmark1 as ?benchmark11 ?compare. ?cont( ?model_lbl) ?compare, ?modet1 ?compare, ?lbl as orkgp1, ?lbl as orkgp2, ?modet1 as orkgp2, ?lbl as orkgp3, ?modet1 as orkgp3, ?link as ?link, ?module1 as ?module11, ?module2 as ?module21, ?module2 as ?module22, ?module2 as ?module3, ?module3 as ?module31, ?module3 as ?module32, ?module3 as ?module33, ?link as ?link, ?module1 as ?model_hits ?compare, ?lbl as orkgp1, ?lbl as orkgp2, ?modet1 as orkgp2, ?module as ?module11, ?module2 as ?model_hits ?compare, ?lbl as orkgp1, ?	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the WOS-11967 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 The following is the output from my output query during training of the machine. The first 4 columns of the example output query represent the number of sets, the next column the value for each output of the benchmark, and the last column it the corresponding output of the benchmark. The output of this machine is the matrix of numbers. This matrix cannot contain any zero. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 PRIMARY KEY ( ?benchmark ) ; DROP TABLE IF EXISTS benchmark ; PRIMARY KEY ( ?benchmark ) ; DROP TABLE IF EXISTS benchmark_lbl ; PRIMARY KEY ( ?label ) ; DROP TABLE IF EXISTS benchmark_lbl_lbl ; CREATE PROCEDURE benchmark ( id data ); Next is how the machine does the optimization on WOS-11766 and WOS-11629. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 PRIMARY KEY ( ?benchmark ) ; DROP TABLE IF EXISTS benchmark ; PRIMARY KEY ( ?label ) ; DROP TABLE IF EXISTS benchmark_lbl ; PRIMARY KEY ( ?cont ) ; DROP TABLE IF EXISTS benchmark_lbl_lbl ; CREATE PROCEDURE benchmark_lbl ( id data ); The code snippet is presented below, and the example output query is displayed to the right. 1	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the WSC dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Supervised: dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): What is the highest score achieved on the Principal Component Analysis dataset, including the metric and its value ? output (Sparql query):  SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Principal Component Analysis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp"	False
"
 input (English text): What are the models that have been benchmarked on the ANLI test dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset ANLI test?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A3"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?score, ?dataset, ?metric, ?metric_lbl, { @@AscRank} AS ?class, ?score ?dataset, ?metric_lbl, { @@AscRank} AS ?score, @@metric, ?metric_lbl ?dataset, ?metric_lbl || ?score OR @@metric_lbl AND @@metric @@metric or ?metric rdfs:label ?model. OR ( @@value || @@data || @@metric OR @@dataset @@metric OR @@dataset_lbl or @@eval @@eval OR @@eval_lbl. ) Input data (HTML or text): Results 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195	False
"
 input (English text): Can you list the metrics used to evaluate models on the iNaturalist 2018 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the CIFAR-10 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the iNaturalist 2019 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?metric metric_lbl. OR DISTINCT ?metric_lbl. Input (Text or XML): the metric you want to check output (Sparql query): SELECT COUNT(*) FROM dfs SET Metrics = { METRIC c { ?metric: c. name } }; or kgp:NamedLbl; Result (Excel): iNaturalist 2018 The Results There are a few key insights that can be gained from the iNaturalist table, which are: It's clear that the iNaturalist models outperform the other model types by about 6% over the CIFAR-10 and iNaturalist 2019 versions These differences are in the range of 0.7% and 0.5%, and there are probably many other different variations It's common to have differences in performance, and this shows up in the percentage changes that you can see in the charts There are also large increases in the percentage of the CIFAR-10 benchmark class having a high	False
"
 input (English text): What are the models that have been benchmarked on the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Yars Revenge dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Yars Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tempest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	False
"
 input (English text): Which model has achieved the highest BLEU score on the WMT2016 German-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest BLEU score score on the IWSLT2015 English-German benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT d.dataset as ?dataset, BLEU_score from ?dataset_lbl ?dataset_lbl ORDER BY d.dataset DESC; Output: SELECT ? DISTINCT ?dataset d.dataset AS ?dataset DISTINCT ?benchmark Sorting The WMT2015 English-German benchmark dataset, which model has scored highest: output (Sparql query): SELECT c.score AS WMT2015, ?column_set c.column_set, ?column_name c.column_name, ?model c.model as ?model ORDER BY c.score DESC LIMIT 1 This query can be used to query the columns and columns names of the dataset that each model has done. We can also use it in conjunction with the ?filter property's default setting of 'any'. Let's sort the dataset by its BLEU score value using the ""BLEU score"" column's value. The following code queries the model_lbl.csv file in order to perform this operation."	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Lunar Lander (OpenAI Gym) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Supervised: dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } If the text is too long, please let me know. Please do not alter your dataset so as to change the output. For example you can upload new dataset here! Please only use this for a brief consultation only. If you want to update or publish your paper directly, please contact me via email. Please note that there has been two versions of the Supervised: dataset since 2012 and the benchmarking of models is not yet finished. There might be bugs. Please refer to the OpenAI repo for more details. I would like to cite your paper on RSC. Please include it in your work where possible. This worksheet will help you with how to proceed:   Please email your paper on this spreadsheet in your work paper. This worksheet will help you with how to proceed:  1. Copy/paste the RSC URL to your paper, add it to your submission. 2. Enter an abstract that shows the model you benchmarked. 3. Add your dataset that contains your benchmarked models. 4. Leave your query and number of experiments for the paper. 5. Be sure and include the benchmarking results that you get after doing the benchmark test. 6. Leave your reply to the comment of this worksheet to send your"	False
"
 input (English text): Provide a list of papers that have utilized the Prior hs model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the HNEABP (BWNE) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HNEABP (BWNE)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the A3C FF hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a hs:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF hs"") orkgp:HAS_DATASET ?dataset. } output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a hs:Model; rdfs:label ?model_lbl. NAME ?model_lbl. orkgp:HAS_CODE ?data. ; orkgp:HAS_SOURCE_CODE ?key. ?key. orkgp:HAS_GEOORDER ?data. ?hdr. input (English text): Provide a list of papers that have utilized the AALBF model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a hs:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AALBF"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model ; orkgp:HAS_SOURCE_CODE ?data. ?key. ?key. orkgp:HAS_GEOORDER ?data. ?hdr. input (English text): Provide a list of papers that have utilized the AMS hs model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a hs:Model; rdfs"	False
"
 input (English text): Where can I find code references in papers that have used the ELMo model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the e2e-coref + ELMo model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""e2e-coref + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EBID) orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Where can I find code references in papers that have used the BOLD-UCLM + ELMo model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a b:Color; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BOLD-UCLM"") orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Where can I find code references in papers that have used the BOLD-UCLM + Self Attention + ELMo model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a b:Color; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BOLD-UCLM"") orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK"	False
"
 input (English text): List the metrics that are used to evaluate models on the Pubmed benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the BIOSSES benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 select d as metric_lbl, C( ?dataset a) as lab and D.test(!rdfs, rafs); output output | | | metric_lbl | lab | |----------------------------------------|----------------------------|--------------------| | ------------|------------------------|------------------------| | B | 0.00 |1.00 | | C0.0 |1.00 |1.00 | | D | 0.04 |0.05 | | D0.5 |1.02 |1.08 | | D1 | 0.04 |0.04 | | D1.4 |1.09 |1.11 | | D1.4.1 |2.03 |2.07 | | D2 |0.04 |0.02 | | D2.5.1 |1.03 |1.04 | | D4 |0.02 |0.01 | | D4.3 |0.11 |0.07 | | D4.5 |2.05 |2.07 | | D5 |1.04 |7.08 | | D5.1.2 |1.14 |7.08 | | D5.2 |1.09 |2.38 |8.24 | | D6.1 |0.08 |0.09 | | D6.1.4 |4.08 |4.47 | | D6.2 |0.10 |1.11 | | D6.2.3 |4.18 |4.45 | | D6.3 |3.15 |5.05 |6.10 | | D6.4 |1.01 |0.25 | | D6.4.1|0.11 |0.17 | | D6.5 |1	False
"
 input (English text): Can you provide links to code used in papers that benchmark the 4 layer QRNN model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""4 layer QRNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DQN+SR model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""6-layer QRNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 The code references are not available anymore, see https://github.com/orkgc-project/layersqrnn/blob/master/layersqrnn/README.md. What if your paper only tests one layer of the network ? It is possible to use the code from http://arxiv.org/abs/1608.05043 to create separate versions of the paper for testing only one layer, if the paper is published in well-phished journals. Can you link to any preprint papers or papers you've published ? Yes, the reference list in the PDFs and the code in the DQN+SR Github repo is available in the preprints folder for download. Please note that the code you've chosen to publish has to be included in the published paper. The code has to be linked here from your article, if it does not, you're not allowed to publish the paper (or publish an addendum). How long has the project been going on ?! Since January 2013.	False
"
 input (English text): What models are being evaluated on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the WOS-46985 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the WOS-11967 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET. OPTIONAL { ?model orkgp:HAS_EVALUATION. ?model rdfs:label ?model_lbl. } } input (English text): What are the models that have been benchmarked on the WOS-92951 dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl. FILTER (str( ?dataset_lbl) = ""WOS-92951"") ?benchmark orkgp:HAS_DATASET. OPTIONAL { ?model orkgp:HAS_EVALUATION. ?model rdfs:label ?model_lbl. } } input (English text): Can you describe the evaluation methods in detail ? output (Sparql query): SELECT DISTINCT ?eval. ?dataset orkgp:HAS_DATASET. TIMED_TOO_LATE, OPTIONAL { ?dataset orkgp:HAS_EVALUATION. { ?dataset rdfs:label ?dataset_lbl. TIMED_TOO_LATE orkgp:HAS_INFORMATION ?retry ?eval; } } OPTIONAL OPTIONAL OPTIONAL OPTIONAL OPTIONAL OPTIONAL OPTIONAL OPTIONAL OPTIONAL OPTIONAL TRIGGERED TIMED_TOO_LATE input (English text): What are the evaluation methods in detail ? output (Sparql query):"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NYT29 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the 20NEWS dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""20NEWS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the NYT24 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?label ?data ?title WHERE { ?dataset orkc:Dataset; rdfs:label ?dataset_lbl. ?label_lbl ?data_lbl. FILTER (str( ?dataset_lbl) = ""NYT20News"") ?rank OR kc; rdfs:label ?labels_lbl. } Input an argument specifying which types of output you want to"	False
"
 input (English text): Name the datasets that have been used for benchmarking in the PointGoal Navigation research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""PointGoal Navigation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the datasets benchmarked under the Robotic Grasping research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Robotic Grasping"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Name the datasets that have been used for benchmarking in the Robot Navigation research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Robot Navigation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 [back to top] Please Note: To perform this task successfully, you can use the standard library (in the R and Python languages respectively) or use the R programming language. For more information, please consult the R manual for the R language. You will find more information about the R programming language at https://www.r-project.org/doc/language-r.html. [back to top]	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the CoNLL 2012 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" Inputs: pprint, bibtex, doc, code, pdf, csv, html source: https://coincode.weebly.com/ Output: Author: Rename to: CoNLL 2012. Code: bibtex: ""http://schemas.openlibrary.net/browse/CoNLL/2012/reference/"" doc: https://github.com/scramble/NIL-S/raw/master/co/S2.pdf.html csv: https://github.com/scramble/NIL-S/raw/master/co/S2.csv.htd.html html: https://github.com/scramble/NIL-S/blob/master/CODE-CO.md Output: Author: (Cameron Watson) Code: bibtex: ""CoNLL 2012: Reference"" doc: https://github.com/scramble/NIL-S/raw/master/co/S3.html.html xml: http://en.wikipedia.org. Output Explanation: An introduction to the NIL system (see below)! Authors: Code: Output: Author: (A-B) B Code: bibtex: ""H. R. R. Cooley, J. D. G. Martin, C. J. M. Wilson and B. J. J. Vos"" bibtex: ""F. E. Wood"" csv: ""S2.csv.htd"" html: http://en.wikipedia.org Output: Author: (A-C) B-C Code: bibtex: ""R. G. Moore, R. P. Epperson"	False
"
 input (English text): Can you list the models that have been evaluated on the IMDb-B dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the IMDb-M dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the IMDb-M benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?model orkgp:HAS_EVALUATION ?model. ?model rdfs:label ?model_lbl. } ] Note that in the example above output only is provided in the format of ""dataset_lbl"". The actual RDFS record with all field names of the model is sent in another format. This format is called RDFS-DB (Read-Only Data Stored) (or RDFS-DB1 if you are using a database with more than one model). The first part of the output, ""dataset"", indicates how model in this dataset is processed. So if the RDFS document specifies one model with title ""Model 1"", another with title ""Model 2"", the other model with title ""Model 3"" has to be loaded into RDFS first, otherwise it will not be seen. In fact all loaded models are loaded into the document and the result should be seen by the user. The next part, ""rdfs:label"", indicates how model in this dataset needs to be analyzed. RDFS-DB documents should have the attribute ""label"" indicating the name of the field of model to be analyzed. RDFS-DB is a read-only data stored format and thus the RDFS documents do not have any information about model. The ""benchmark"" part of this output shows the performance of your model(s). The best"	False
"
 input (English text): List the code links in papers that use the PEGASUSLARGE model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PEGASUSLARGE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Luna model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Luna"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PEGASUS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Luna"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Some comments from the authors Regarding test data: PEGASUS is a new benchmarking tool and it uses a combination of several existing benchmarks in SQL Server (not the case for most of the existing tools). The PEGASUS tool runs several different benchmarks for different purposes (see e.g. http://www.asp.net/~hudson/doc/benchmarking.aspx ). The PEGASUS test data is created in an automated process to reduce the number of SQL server instances the tool needs to run (similar to what the Benchmarks & Tools page has always said about the existing tools). The data is then used by the tool's tests (including the new ones) before the benchmarking routine is run. In some cases PEGASUS is used for benchmarks. In other cases the PEGASUS tool itself, or parts of the tool are used to benchmark SQL Server, i.e. to test the benchmarking functionality. Both cases are explained here: http://www.asp.net/~hudson/doc/benchmarking.aspx#Benchlanding Benchmark Data Download (PDF)"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Music Modeling research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Music Modeling"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Environmental Sound Classification research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Environmental Sound Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Audio Classification research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Audio Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Computer Vision"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Lunar Lander (OpenAI Gym)?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Cartpole, swingup (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"--------------------------- TIMESTAMP: 2017-03-21 23:37:18 Entering "" ?dataset a"" allows for inputting in any of the datasets available from OpenAI Gym. We have tested a broad range of datasets including the Moon, Earth and Planetary Lander Missions (MOEOL) and the Lunar Lander (Lunar Lander). We evaluated the test dataset for the metric value of how fast (or slow) a ""climbing"" rover can climb a 10 meter slope (measured in meters per stride). The results are displayed for the Metric and Score metric with a score scale, for example in the following table. The Metric/Scores matrix represents the total metric score. The ""climbing"" scores (m Sq/m) in meters per stride represent the total metric score with a scaling metric associated with the metric, for example the scaling metric associated with ""metric"". For example for the METRIC and SENSE SET metrics, the score is 10 with 3 scaling factors. We are continuing our research into the performance of the data sets we received. We hope to provide more details about these data in a later data update. For the data we analyzed and report in the following Table and Plot we have categorized the dataset into: Metric score Scaling metric score Scaling factor Scaling (steps) This section will focus on the metrics by scoring (m Sq/m), as the metrics have been normalized into a metric score, or metric score (see Figure 1 below). From a metrics perspective, scaling score provides a metric of the overall strength of some metric, for example the LISS/SAMS system scale (see Figures 2 and 3 for the LISS measurement), and the scaling factor serves for determining how far off the mean value is in the measurement process,"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Reacher, easy (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the Reacher, easy (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TITLE (1), ISNULL(RESTART) FROM TITLE (1), ?paper orkgp:HAS_DATASET ?dataset, ?lbl, ?dtml, ?id, ?method, ?source WHERE { ?a:RESTART; ?b:RESTART; ?c:RESTART, ?s, ?s2 orkgc:RESTART, ?s3, ?s4 ?dataset, ?lbl, ?dtml, ?ids, ?method, ?source, ?method_lbl, ?s2 orkgc:RESTART, ?s3, ?s4 ?dataset; ?data, ?lbl, ?dtml, ?ids, ?method, ?source orkgc:RESTART, ?s2 orkgv:RESTART, ?s3, ?s4 ?dataset_lbl, ?ids, ?lbl orkgc:NOT NULL } output (Sparql query): SELECT TITLE (2), ISNULL(RESTART) FROM TITLE (2), ?paper orkgp:HAS_DATASET ?dataset, ?lbl, ?dtml, ?id, ?method, ?source WHERE { ?a:RESTART; ?b:RESTART; ?c:RESTART, ?s, ?s2 orkgc:RESTART, ?s3, ?s4 ?dataset, ?lbl, ?dtml, ?ids, ?method, ?source orkgc:RESTART, ?s2 orkgv:RESTART, ?s3, ?s4 ?dataset_lbl	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Top 1 Accuracy score when benchmarked on the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""MACs"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT   ?metric orkgc:Metric AND   ?model ?model_lbl FROM ( select ** model_lbl ) WHERE model = ?model && ?metric = ?metric; Note: Some of the RDF output below may not represent the RDF graph format. Note:  Note that in order to benchmark one model with another, you can use the option --model-rank (defaults to True) to control how many models are benchmarked. - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? - ? -	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACE 2005 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?value WHERE { { ?value orkgc:Fields_F_TYPE ?record_type. ORLGE_VALUE(2, ?value). } orkgc:F_TYPE ?record_type. ORLGE_VALUE(2, ?value), ?output. ORLGE_VALUE(1). ORLGE_VALUE(2, ?output). OPTIONAL { ORLGE_VALUE(1, ?output[ ?output. ORLGE_VALUE(2, ?value)]]/2 } } } ORDER BY DESC( ?value ) input (English text): Can you provide the latest ACE 2005 score and compare it with the best match ? output (Sparql query): ------------------------- +-15% = 1.000000000, -5% = -0.999999999999998 ------------------------- = 10.000000000, +15% = 30.000000000, +-15% = 120.000000000 = ------------------------- input (English text): Can you provide the highest benchmark result for an ACL-ARC dataset ? output (Sparql query): ------------------------ -4 10% 15% --------------------------- SELECT DISTINCT ?value WHERE { { ORLGE_VALUE(2, ?value) } ; ORLGE_VALUE(1, ?value) } ; orkgc:Fields_F_TYPE ?record_type. ORLGE_VALUE(2, ?value). ORLGE_VALUE(2, ?value). OPTIONAL { ORLGE_VALUE(1, ?value) } ; orkgc:F_TYPE ?record_type. ORLGE_VALUE(2,	False
"
 input (English text): List the metrics that are used to evaluate models on the IMDb benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query):   output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query):   output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): Output with a Sitemap If you use different lists of metrics for different databases, you can output with a separate list, this makes it easier to maintain and display your metrics. Sitemap to select the metrics to use, one per database.    Sitemap to select the metrics to use, one per database. To do this, in your	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Tutankham?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Asterix dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Enduro?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?metric ?metric_lbl(MAX( ?score))... WHERE { SELECT ?metric ?metric_lbl(MAX( ?value) AS ?score)... WHERE { ?dataset a orkgc:Dataset... } } ORDER BY { ?score orkgp:HAS_DATASET ?dataset... } ORDER BY DESC( ?score) -------------------------- First result (metric) ------------------------------------------------------------ 1.00 3.30 -------------------------- Dataset name: Atari 2600 Tutankham 1.00 3.30 -------------------------- Number of rows: 2 2 2 2 2 1.00 3 3 3 3 3 1.00 3 3 4 4 4 1.00 6.00 6	False
"
 input (English text): What are the metrics of evaluation over the CINIC-10 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the RotoWire dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Hutter Prize dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset b orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What are the metrics of evaluation over the DRC-K4 dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset c orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRC-K4"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What are the metrics of evaluation over the Hutter Prize dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset d orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?bench"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Dmlab-30 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dmlab-30"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" select ?metric ?metric_lbl (MAX( ?value) AS ?score) FROM ?dataset a ORkgc:Dataset JOIN WizardOfWor as w2 GROUP BY ?metric; input (English text): What is the top benchmark score and its metric on the Atari 2600 WIZARDRY dataset ? output (Sparql query):  select ?metric ?metric_lbl (MAX( ?value) AS ?score) FROM ?dataset a ORkgc:Dataset JOIN Wizardy as w2 GROUP BY ?metric; input (English text): What is the top benchmark score and its metric on the Atari 2600 ZED dataset ? output (Sparql query):  select ?metric ?metric_lbl (MAX( ?value) AS ?score) FROM ?dataset a ORkgc:Dataset JOIN Zed as w2 GROUP BY ?metric; You may find the following interesting article on Wikipedia where you find these tables ""Data structure"" As we were pointing out earlier, all these data structures are similar to the arrays of the Java world. So why does it take us years, so many posts and so many pages of material to describe them in detail. Well, this is because the data structures that we use on various databases are complex and are based on the idea of a complex (i.e. high-dimensional) data structure that holds many elements. You cannot use the elements together as the data structure of an array; so that is why the various data structures are called as arrays. The above post ""A Tale of High-dimensional Data Structures"" (page 26) illustrates this very well. What is interesting about the above examples is that they are both using a relational database."	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Wizard of Wor dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Wizard of Wor"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 	False
"
 input (English text): List the code links in papers that use the Transformer-XL (12 layers) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL (12 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the 24-layer Transformer-XL model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""24-layer Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the 12-layer Transformer-XL model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""12-layer Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; ?rdfs:label ?model_lbl. ?dataset. ?cont orkgp:HAS_DATASET ?dataset. }... A note about the benchmark results: I tried to do a similar benchmark test for each of the two model variants, and found no such difference in the results. Since every benchmark is in different language, I'm doing some analysis on my database and comparing all the results. The result of my testing was almost identical: the comparison between the two models is equivalent to comparing them against each other, and this resulted in the same average performance. All in all, I find them similar. What is interesting is the results of the 24-layer Transformer-XL, and the difference between the 10-layer Transformer-XL and the 12-layer Transformer-XL. The difference is significant enough to be noticeable, although there is no reason to expect to see a significant difference from the benchmark results: the difference in total data size is only 2MB (the 24-layer Transformer-XL is 20,86MB, the 10-layer is 1,64MB). Even so, the difference is noticeable: the 10-layer Transformer-XL outperforms the 24-layer version more than four times (compared to just once from the benchmark data). In fact, as far as I know, the only way to get more than 4 times the performance with the 10-layer Transformer-XL is to use this model on a custom dataset: RDF Sitemap (RDF Sitemap example_item.html) I've decided to create a custom report with this report to show how quickly the performance of the 4-layer Transformer-XL was increased by using custom datasets. To	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Time Pilot dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Time Pilot"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Up and Down dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Please read the entire question for the benchmarking procedure; the only thing of interest here is the definition of the model parameter... You can download a PDF of this document here It is possible to benchmark an Atari 2000 system to determine which is the best model for estimating the score; for example, you may want to test whether the Atari 3000 model was more accurate than the Atari 4200 model when compared with the Atari 4100, the Atari 5200 or the Atari 6000. (For a detailed explanation, see the paper by Kostich et al: ) This benchmark uses nv50 and nv54 CPUs, 1GB of RAM and the same benchmark settings as for benchmarking the Atari 2600 Breakout dataset (see the benchmarking procedure if nv50"	False
"
 input (English text): What models are being evaluated on the NLP-TDMS dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the NLP-TDMS (Exp, arXiv only) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Could you provide a list of models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" In the table below, you see examples for an NLP library's benchmark. For now, the following is the list of the libraries currently provided with NLP-TDMS and can be used with NLP-TDMS. This is a list of the NLP libraries I'm aware of: NLP-TDMS.NET NLP-TDMS.NET (Windows) NLP-TDMS.NET (OSX) NLP.DB NLP.DB (Windows) NLP-TDMS.NET (OSX) (requires NuGet) How To Install and Run a NLP Library A library file that is part of the data that must be processed by the library is called an executable. The file can be downloaded from the NLP TDMS web site or downloaded from NuGet. An executable.exe file is used in these cases to install the NLP library into the target machine. An executable files usually contain a script file called the startup.cmd file which is used by the computer to start the software after the library was loaded. To install a NLP-TDMS library, just select the file named Library.cpp in the source tree and the library file is downloaded and installed by selecting the install option. NLP-TDMS.NET Framework NLP TDMS.NET (Windows) NLP TDMS.NET (OSX) NLP.DB NLP TDMS.NET (Windows) How To Run an NLP-TDMS Library To run a library you just need to place the executable file inside the project template for the library project that is currently installed. To run the sample data in the example dataset, the application is added under examples/data directory. You just install and run ""SampleData.NET"" application, then run NLP-TDMS.NET (OSX)."	False
"
 input (English text): Which model has achieved the highest Accuracy (%) score on the  Jacquard dataset benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the  Jacquard dataset dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the models that have been benchmarked on the  Jacquard dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" data import: [  { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ORG KATMARK ? ORG_DESC orkgp_lbl ? ORG_EVALUATION orkgp_lbl ? ORG_METRIC ? ORG_PRESCRIBE ? ORG_VARS ORG_LAST_METRICS ORG_MULTIMEDIA ?metric; ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ORG_VALUE orkgp_lbl ? ORG_METRICS ORG_MULTIMEDIA ? ORG_IMAGE ? orkgp_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?metrics ? ORG_PRESCRIBE ? ORG_VARS ORG_LAST_METRICS ORG_MULTIMEDIA ORG_IMAGE ? }] { ?xkcd a:7}} output (Sparql query): SAT is more accurate from the to the than from the to the D. However, the to G is higher than the to H, meaning that it should perform only moderately better at the G. SAT provides an upper bound for the accuracy of a -D  for the same dataset. Therefore, if both  A  and  B are equal"	False
"
 input (English text): What are the metrics of evaluation over the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ObjectNet (Bounding Box) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet (Bounding Box)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the ObjectNet dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Semantic Web"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (Greek text): What evaluation metrics are commonly used when benchmarking models on the Semantic Web dataset ? output (Sparql query): input (Spanish text): What evaluation metrics are commonly used when benchmarking models on the MIMO-1 data ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MIMO-1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (Spanish text): What evaluation metrics are commonly used when benchmarking models on the MIMO-1 data ? output"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the enwiki8 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwiki8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?article ?article_lbl WHERE { ?article a orkgc:Article; rdfs:label ?article_lbl. } """""" """""" SELECT DISTINCT ?article ?article_lbl WHERE { ?article a orkgc:Article; rdfs:label ?article_lbl. } """""" All the outputs we obtain from this query can be read by any number of DBMS as follows: Django Benchmark Results R Benchmark Results SQL Output from the query above can be processed as follows: $ Django benchmarked 1.11.1 python-1.11.1 +db-4.3.9-py2.7.x86_64+libjpeg-turbo.py (1 / 1) 0.938s $ R benchmarked 1.1.0 python-1.1.0 +db-4.3.9-py2.7.x86_64+libjpeg-turbo.py (2 / 2) 0.948s Django Benchmark Results R Benchmark Results SQL Output from the query above can be processed as follows: $ This time we have a better understanding of"	False
"
 input (English text): Indicate the model that performed best in terms of Top-1 Error Rate metric on the FGVC Aircraft benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of PARAMS metric on the Stanford Cars benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT *, ?metric ?metric_lbl., ?dataset a in ?dataset ?dataset_lbl., ?metric r in ?dataset ?dataset_lbl., ?name rdfs:label ?model., ?model rdfs:label ?model_lbl., ?model_lbl rdfs:label ?model., ?name rdfs:label ?model_lbl., ?model_lbl rdfs:label ?model_lbl., ?comp orkgs:HAS_EVALUATION ?eval., ?comp orkgs:HAS_VALUE ?value; To return FULL PARAM s: SELECT *, ?metric ?metric_lbl., ?dataset a in ?dataset ?dataset_lbl., ?metric r in ?dataset ?dataset_lbl., ?name rdfs:label ?model., ?model rdfs:label ?model_lbl., ?model_lbl rdfs:label ?model., ?model_lbl rdfs:label ?model_lbl., ?model_lbl rdfs:label ?model_lbl., ?comp orkgs:HAS_EVALUATION ?eval., ?comp orkgs:HAS_VALUE ?value; To return PARAMS data	False
"
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Seaquest benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Pitfall! benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pitfall!"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone: The Battle for Black Mesa!"", """") orkgc:Dataset. NAMES ?dataset_lbl. orkgc:HAS_DATASET ?dataset; orkgc:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgc:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone: The Battle for Black Mesa!"", """") orkgc:Dataset. NAMES ?dataset_lbl. orkgc:HAS_DATASET ?dataset; orkgc:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgc:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Output (Sparql query): output (Sparql query): SELECT STATISTICS ?Metric ?metric_lbl. OR ******** ******** ************ output (Sparql"	False
"
 input (English text): What are the models that have been benchmarked on the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Krull dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Krull"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Road Runner dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): What are the models that have been benchmarked on the Atari 2600 Super Game Boy sample dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Super Game Boy"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): What are the models that have been benchmarked on the Atari 2600 Super FX sample dataset ? output (Spar"	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Fishing Derby dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Fishing Derby"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Kangaroo dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Kangaroo"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Boxing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Can you list the models that have been evaluated on the Atari 2600 Darts dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Darts"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Can you list the models that have been evaluated on the Atari 2600 Darts Darts dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset;"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the WMT2014 English-French dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 French-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Can you list the metrics used to evaluate models on the English-French English dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?met"	False
"
 input (English text): Can you list the models that have been evaluated on the WMT2014 French-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" Note that the benchmark was run after a few hours of simulation time, and thus model quality was subject to variability. Note on Performance Before we get to the performance numbers, let's discuss an important aspect of performance. For the BUCC dataset, all of the evaluation points are evaluated using the same set of datasets. The standard deviation (SD) for a set of models is 1.8. We'll be calling this SD an SDI. The same SDI can be obtained over an individual model. That is, each individual model contributes to one ""discrete"" dataset from the set. We will call this SDI a SDI per model. In this paper we refer to all those random subsets of models that we are referring to as ""individual"" or ""individuals."" The SDI per model per individual model is a measure of model quality and is not necessarily related to any observed relationship between any specific dataset and any specific individual. In these situations, we'll use just the SDI of the individual dataset. In this study, we have collected datasets for the four European Union member states of Germany, Sweden, Netherlands, and Belgium. In order to compute the SDI of each of these datasets, any of these datasets can be imported into the R computing environment. For comparison, we'll also include the datasets corresponding to the US Census, and the Census of Population, Housing, and Income in the United Kingdom. Note that the R COUNT2R package was used to generate the dataset. For this reason, our evaluation of each of these datasets is different from that of some of the other datasets. Let's discuss this difference more fully in a moment. For comparison, we should again note that the US Census dataset is quite large. For example, as seen in Table I, a typical 50-person household might have 642 individuals in the dataset—a typical"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Attentional encoder-decoder + BPE model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Attentional encoder-decoder + BPE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the ELMo model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BIDAF+Self Attention"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Where can I find code references in papers that have used the BIDAF+Self Attention + ELMo model for benchmarking purposes ? output (Sparql query): input (English text): Are you allowed to compare any benchmarks to your model - or to a different model ? Does the same benchmark result count for all models ? output (Sparql query): If you are going to compare benchmarks to a different model, which one ? (A) Single model model (b) Multi-model model, with a different validation set. input (English text): How does the ELMo model compare to the ELMo (single model) model ? input (English text): What kind of benchmarks do you plan to make ? output (Sparql query): A single benchmark of any type: { ?benchmark a:BMI ; orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. :} orkgp:HAS_MODEL ?"	False
"
 input (English text): What models are being evaluated on the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the ESC-50 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ESC-50"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the AESLC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. OPTIONAL { ?model orkgp:HAS_DATASET ?dataset. ?model rdfs:label ?model_lbl. } } output (Parquet JSON): { ?dataset ?""dataset_lbl"" orkgp:HAS_DATASET ?""dataset"" ORkgp:HAS_EVALUATION ?label orkgp:HAS_BENCHMARK ?dataset. ?model rdfs:label ?model_lbl. } output (Sparql DB2 query): SELECT ?dataset_lbl. FROM ?data_lbl WHERE ORkgp:BENCHMARK ?benchmark. ?model rdfs:label ?model_lbl. \ ORkgp:HAS_MODEL ?model. output (Parquet DB2 DB2 query): SELECT ?model_lbl. FROM ?data_lbl WHERE ORkgp:BENCHMARK ?benchmark. ?model rdfs:label ?model_lbl. \ ORkgp:HAS_EVALUATION ?model. output (Sparql DB2 query): SELECT ?model_lbl. FROM ?data_lbl WHERE ORkgp:BENCHMARK ?benchmark. ?model rdfs:label ?model_lbl. \ ORkgp:HAS_EVALUATION ?model. output (Parquet DB2 DB2 query): SELECT ?model_lbl. FROM ?data_lbl WHERE ORkgp:"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SciGEN dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the HoC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 I see two problems here: first, the question is very simple while the problem itself is not trivial. And second, the problem is a perfect opportunity to apply techniques from the literature. One of the first areas of practical interest in data analysis is Bayesian statistics. The basic idea is that the most significant variable in a multivariate distribution should be given preference at sampling rates higher than some criterion. In short, there is a way to pick the most significant variable from a distribution that satisfies some property. The probability of picking the most significant variable for a given sample frequency, which we have taken to be. The most significant variable for our example is has a sample frequency of 100 samples per million. That tells us that the probability of picking the most significant variable, depending on the data, over the interval (0 to 1,000), is. This idea of sampling rate selection for data or a parameter of the distribution was the original formulation of Bayesian statistics by the mathematician. For the last ten years, I have been using these ideas to find the most likely values for the parameter in my dataset. This idea of Bayesian statistics or the sampling rate selection is one of the ways most of scientific data are processed by the scientific community today. Bayesian statistics requires a particular tool that is known as the resampling procedure. The resampling procedure is used to choose the most important and most significant data from a multivariate distribution. For instance, we might wish to choose the most important data for the same interval,. The resampling procedure starts by sampling from. We will assume that we have a parameter for, and an interval and. The parameter or a probability density function is then randomly distributed and is not distributed at a constant rate. As a result of this randomizing the rate of sampling, if the value of at the sampling rate, it will vary from time to time	False
"
 input (English text): Where can I find code references in papers that have used the PAR Transformer 24B model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer 24B"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the PAR Transformer Base model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the PAR Transformer Large model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Example 3. A comparison of models	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BertSumExtAbs model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BertSumExtAbs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BERT classifier model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT classifier"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD 2 model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERTwwm + SQuAD 2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BertWm + SQuAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = "" BERTWm + SQuAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Can you provide links to code used in papers that benchmark the CUBIC + SMART classifier model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model x:Model; rdfs:label orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. } input (English text): Can you provide links to code used in papers that benchmark the C"	False
"
 input (English text): Provide a list of papers that have utilized the Neural Network Language Model (NNLM) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Neural Network Language Model (NNLM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BertSumExt model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BertSumExt"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CL-Titles-Parser"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Provide a list of papers that have utilized the CL-Spine model and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the Cl-Vals model and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the CL-Morph model and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that use the EqToString class of DictChar and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that use the EqToString class of DictChar to determine if a DictChar class is available and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that use the Lattice class of DictChar and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that use the Lattice class of DictChar to determine if a DictChar class is available and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that use the Lattice class of DictChar and include the links to their code ? output (Sparql query): input (English text): Provide a list of papers that have utilized the Matmul package ? output (Sparql query): output (Sparql table): Result of the DMLS query: output (String): output containing the data set	False
"
 input (English text): Can you list the models that have been evaluated on the Habitat 2020 Point Nav test-std dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Habitat 2020 Object Nav test-std?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Could you provide a list of models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 A list of objects that were evaluated is available here. This list is not sorted. What is the best metric of success for Object LBL benchmarks ? A high score is not the same as a high evaluation of every metric and every variable in the final score. What should I do ? It's a good idea to try other criteria, like the highest score from the test: Which metrics best measure the performance of a set of individual metrics ? The best metric of each metric is a function of the criteria. If most metrics are equally good then the metric with the highest score is the best metric. Is the best metric a function of a parameter ? If a metric has only one value then the best metric is a function of the parameter. If a metric has two values then the best metric is a function of them. If all metrics have two values then I want the best metric to be defined by 2 parameters; the 2nd value is zero if both values are zero. In this case, the metric with the highest score can be defined by 2 parameters: (i) the metric parameter with the highest score and (ii) the metric parameter with zero if both values are equal. Then I can evaluate a function that can find an optimal metric. What should I define the function ? A good metric can be defined by a single expression with a single mathematical expression. What should I evaluate the function that returns the optimal metric ? A function that returns the optimum metric is a function defined by 2 parameters; the 2nd value is zero if both values are non-zero. If both parameters have the same value then the function can be evaluated without using additional parameters. I need a function that will return the probability of success of the metric. What do I need ? In order to calculate an optimal metric I have to know its standard deviation. A good metric must have	False
"
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Name This Game dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Berzerk dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Atari 2600 Name This Game dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" output (Sphinx/Squpy dataset): model 0: 2.7.0.0 : ""Benchmarking 1 model"" : ""Score"" : 6,639,0,000,000,000,000,000 Score : 0,000,000,000,000,000,000,000 0,000,000,000,000,000,000,000,000 data-model 0 : ""Main test"" : ""Benchmarking 14 models"" : ""Score"" : 899,000,000,000,000,000,000,000,000 Score : 0,000,000,000,000,000,000,000,000,000 0,000,000,000,000,000,000,000,000,000 data-model 1 : ""Main test"" : ""Benchmarking 7 models"" : ""Score"" : 581,000,000,000,000,000,000,000,000 Score : 0,000,000,000,000,000,000,000,000,000 0,000,000,000,000,000,000,000,000,000 data-model 2 : ""Main test"" : ""Benchmarking 2 models"" : ""Score"" : 481,000,000,000,000,000,000,000,000 Score : 0,000,000,000,000,000,000,000,000,000 0,000,000,000,000,000,000,000,000,000 data-model 3 : ""Main test"" : ""Benchmarking 8 models"" : ""Score"" : 414,000,000,000,000,000,000,000,000,000 Score : 0,000,000,000,000,"	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Road Runner dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Time Pilot dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Time Pilot"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?data_lbl ?matric_lbl (MAX( ?value) AS ?score) FROM orkgc:DataSet WHERE orkgc:LOBEXIFIER ?dataset_lbl; output (Sparql query): SELECT DISTINCT ?data_lbl ?matric_lbl (MAX( ?value) AS ?score) FROM orkgc:DataSet WHERE orkgc:LOBEXIFIER ?dataset_lbl; input (English text): what is the best metric for the test ? output (Sparql query): UPDATE orkgc SET data_lbl = ?data_lbl ?data_lbl_cql IF NULL IS NOT NULL OR FALSE IS NULL OR false IS NOT NULL WHERE ?dataset_lbl = ?dataset_lbl ?dataset_lbl_cql; input (English text): what is the fastest time for the test ? output (Sparql query): UPDATE orkgc SET time_lbl = ?time_lbl ?time_lbl_cql IF NULL IS NOT NULL OR FALSE IS NULL OR true IS NOT NULL WHERE ?dataset = ?dataset_lbl ?dataset_lbl_cql	False
"
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 Breakout benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 River Raid dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" Input # Input # Name Model name Dataset name Testable or valid ? output (Sparql query): nf - n/a - - 7/4/12 Output # output (Sparql query): nf nf: n/a n/a: n/a: n/a or kgp: n/a n/a: n/a: n/a or pdf: n/a n/a n/a: n/a: n/a or pdf4.1: n/a n/a n/a: n/a: n/a rdfs: n/a n/a n/a: n/a: n/a or model: n/a n/a n/a: n/a: n/a or model rdf: n/a n/a n/a: n/a: n/a or label: Output # Output # Name Model name Dataset name Testable or valid ? output (Sparql query): nf - n/a - - 7/4/12 Response # To view the full set of benchmarks: Run this query: select * from testbench2 where name = ""dataset-atari2600"" and model and benchmark like 'benchmark( ?dataset:dataset).benchmark' where name = ""dataset-atari2600"" and model and benchmark like 'benchmark( ?dataset:dataset).benchmark', 'paper1'='paper2' and table_id = '8', 'paper2'='paper3' and paper_lob_c = '1' Output: Input # Input # Name Model name Dataset name Testable or valid ? output (Sparql query"	False
"
 input (English text): Provide a list of papers that have utilized the EfficientNetV2-M model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the EfficientNetV2-M (21k) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-M (21k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the AlexNet, MultiGrasp model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AlexNet, MultiGrasp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AlexNet) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the AlexNet, MultiGrasp model (3k) and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AlexNet (3k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that have utilized the AlexNet, MultiGrasp model (21k) model and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AlexNet (21k) ( ?)"") ?benchmark orkgp:HAS_DATASET ?dataset."	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Asterix dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Ms. Pacman dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ms. Pacman"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" { ?dataset a orkgc:Dataset; rdfs:label ?dataset. ?dataset rdfs:p_label ?dataset. ?dataset model( rdfs:p_label= ?dataset; ) ?dataset rdfs:s1_labels. FILTER (str( ?dataset_p_label) = ""Atari 2600 Ms. Pacman"") ?benchmark { ?benchmark RANK(rdfs:p_label, 1-10); rdfs:p_label = ?dataset:RANK(rdfs:p_label. ?dataset rdfs:s1_labels. ) } } input (English text): Can you list the models that have been evaluated on the Atari 2600 Ms. Pacman dataset ? output (Sparql query): { ?dataset a orkgc:Dataset; rdfs:label ?dataset. ?dataset rdfs:p_label ?dataset. ?dataset model( rdfs:p_label= ?dataset { ?labels ?labels } ?labels rdfs:s1_labels. FILTER (str( ?dataset_p_label) = ""Atari 2600 Ms. Pacman"") ?benchmark { ?benchmark RANK(rdfs:p_label, 1-10); rdfs:p_label = ?dataset:RANK(rdfs:p_label. ?dataset rdfs:s1_labels. ) } } input (English text): Can you list the models that have been evaluated on the Atari 2600 Ms."	False
"
 input (English text): List the metrics that are used to evaluate models on the Habitat 2020 Point Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Habitat 2020 Object Nav test-std?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 # import the necessary packages from scipy.stats import pi # the first argument is the model to benchmark function rdfs_model = pi.rdfs_model_from_csv('{','dataset_path_',':dataset_file',':str_str_','metrics_',':label,':methods,':data_format,':str,':methods',,'metrics_ldiff1') # the second argument is the'method(:data)' option - this is the function used to # compute the scores (value, score_score_) function orkgp_value_model = orkgp_value_model.eval(rdfs_model) output (Sparql query): rdfs: ldiff1 ( ?value ). OR( (rdfs: ldiff1 ( ?value ) rdfs: ldiff1 ( ?value ) score ?score_score_ ), rdfs: ldiff1 ( ?value ) score ?score_score_ ) orkgp: score ?score_score_ ) # This shows the results of the initial comparison: if df.get('score', axis=1): a = 1 else: a = 0.0 # This shows the results of the initial comparison: # If you run this again, you'll get an error message if df: ldiff1('Habitat 2020 Object Nav test-std').value == 1: # a > 0.0, output (Sparql query): if df: ldiff1('Habitat 2020 Object NAV test-std').value < 0.0: # a < 0.0, output (Sparql query): This is a test of your assumptions, and nothing to complain	False
"
 input (English text): What is the top benchmark score and its metric on the NCBI Disease dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the NCBI-disease dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the BC5CDR-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"--------------------------- Benchmarks: | Id | Name | --------------------------- | 89514 | Cancer | | 10056 | Cardiovascular Disease | | 89890 | Colon Cancer | | 10069 | Liver Cancer | --- | 88901 | Leukemia | | 69941 | Mental Status | | 81319 | Neurological Disease | | 90072 | Nervous Disorders | | 81317 | Neurodegenerative/Subcortical and Central Disorders | | 90063 | Neurological Disorders | | 79640 | Osteoporosis | | 84718 | Peripheral Neurodegenerations | | 90074 | Prion Disease | | 69869 | Psoriasis | | 90063 | Schizophrenia | | 89053 | Sleep Disorders | | 80109 | Socioeconomic Status| | 80416| Stress Injury | --- | 89898 | Syphilis | | 69426 | Stress & Depression | --------------------------- Benchmarks: 12.7% 40000 - 2.8% 12.7% 40000 - 4.4% 12.9% 6.9% The last example will show how to apply the output clause ""*"" to filter by a specific metric within a specified benchmark. For example, to filter on only the performance score of all CRS-4 or CRS-12 data sources, you could write this query as follows: input (English text): Which metric should be filtered by the filter clause in the output ? output (Sparql query): WHERE ( ?metric, ?score < ?dataset. value) GROUP BY ?metric ?metric_Lbl The result should look similar to the previous example. Note: This feature is currently not supported by MS SQL Server 2016. For detailed information regarding the use of the output clause"	False
"
 input (English text): Can you list the metrics used to evaluate models on the ImageNet V2 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the ModelNet40 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ModelNet40"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label { ?dataset lbls:lbl ?dataset ?labels ?labeled_ls. ? ; } } OR output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label { ?dataset lBLs:lBL ?dataset ?labels ?labeled_ls. ? ; } } OR output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { orkgc_ls:dataset lbls:ls ?dataset ?labels ?labeled_gs. ? : ?dataset ?labelled_gs. ? ; } OR output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { orkgc:lbl lbls:lbl ?dataset ?labels ?labelled_gs. ? :... orkgc:lbl lbls:ls ?dataset ?labelled_gs. ? ; } OR output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { orkgc:lbl lBLs:lBL ?dataset ?labelled_gs. ? :... orkgc:lbl lBLs:ls ?dataset ?labelled_gs. ? ; } OR output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { orkgc_ls:lat lbl	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Open Entity dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Open Entity"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Paper Field dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a. ORG_DESC_ID ?dataset_lbl; h.name ?labels.. ORG_DATA_ID ?dataset_labels. h.key ?key.. ORG_DESC_ID ?key.. ORG_DESC_ID ?key. rdfs:label ?paper_lbl. } output (Sparql query): I hope that this will make the whole world a lot clearer and easier to find what can we say about the most famous open data from Open Data: SELECT IF(h.name == ?labels. ORG_DESC_ID, AND(( ?cont = ?labels. ?benchmark), AND( ?paper = ?paper_lbl)), ?dataset) FROM ( ?dataset_lbl ?dataset) INNER JOIN ( ?dataset_lbl ?dataset_key, ?dataset_lbl ?dataset_labels, ?dataset_key ?labels_schema) ON ( ?dataset_lbl. ORG_ID = ?dataset_key) JOIN ( ?labels. ORG_ID = ?labels_schema) ON ( ?dataset_labels. ORG_ID = ?labels_schema. ORG_NAME = ?dataset_label) JOIN ( ?key. ORG_ID = ?key_schema. ?dataset_label) ON ( ?labels_schema. ORG_ID = ?key_sche	False
"
 input (English text): Provide a list of papers that have utilized the HNEABP (BWNE) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HNEABP (BWNE)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Prior hs model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the HRLRE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HRLRE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HRLRE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } HALRE and HRCL are the two older models at present. HALRE is more useful for testing model robustness at the expense of code speed, as it uses a sparse data set with lots of duplicate entries. HALRE is much closer to BWNE, where the majority of the models are in order and each model needs to operate on a very small number of entries, as a result, it produces a more dense model. This is why HALRE is considered a good alternative to BWNE for more robust testing. The other model included is known as PRNBL and it's also the older model at present. It has some minor weaknesses in the implementation. If you are interested in more details check out this post. The full code used is available in the repo, in particular this post. For all those who are worried about what I am going to say in this post (like me, of course!), here are some additional considerations : All my posts use the word model interchangeably with models, which may be confusing as there are some different kinds of models. There are two other classes of models, called parametric and nonparametric, and in practice, all these models are parametric. However, there are two classifications made, using the more popular models and those that have some features that other param"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CeiT-T model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CeiT-T"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the CATTS model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CATTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the T-ConvS2S model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""T-ConvS2S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""T-ConvS2S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Can you provide links to code used in papers that benchmark the T-ConvE model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""T-ConvE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. }   input (English text): Can you provide links to code used in papers that benchmark the T-ConvC model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""T-ConvC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?bench"	False
"
 input (English text): Which model has achieved the highest Best Score score on the Atari 2600 Q*Bert benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Ms. Pacman benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ms. Pacman"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METICAL ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value ) LIMIT 1 } ?metric orkgc:Metric ?model orkgc:Metric ?model_lbl input (English text): Which model has achieved the highest Score score on the Atari 2600 Ms. Pacman benchmark dataset ? output (Sparql query): SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*B"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the One Billion Word dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the Words in Context dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Words in Context"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Words in Context dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Words in Context"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?mq:value1 ?mq:value2 ?mq:value3... Where the top 3 metrics with ?mq:value1 ≤ ≥5 were measured input (English text): What is the current ranking on Word of Mouth ? output (Sparql query): SELECT DISTINCT ?s:rank ?s:rank2 … WHERE rdfs:labels { { ?scores[2] ?mq:value } } | { ?scores[3] ?s:value } | { ?mq:value3 ?rdf:label } input (English text): What score is the least acceptable in the Words in Context dataset ? output (Sparql query): SELECT DISTINCT ?mq:value ?mq:score... Where the highest scoring metric was for 1 score less than 3 (e.g. 2, 4) input (English text): Can you provide the metric and ranking that best explains the top position for each metric in each Word of Mouth dataset ? output (Sparql query): SELECT ?metric ?metric_rank (max( ?value) AS ?value ) SELECT ?metric ?metric_rank (minimum( ?value ) AS ?value ) output (CSV): The metric and ranking value of the highest scoring metric were: 4 0.6 (0.6) 3 0.6 (0.6) 2 0.8 (0.8) 1 0.5 -2 0.5 -2 -1 0.8 (1)	False
"
 input (English text): List the datasets benchmarked under the Skills Assessment research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Skills Assessment"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the datasets benchmarked under the relation extraction research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""relation extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the datasets benchmarked under the Common Sense Reasoning research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Common Sense Reasoning"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Common Sense Reasoning"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ? ? ?problem. } output (Sparql query): Select the first dataset that meets any of the given criteria. output (Sparql query): SELECT a, b, c, d FROM DATASET AS a.b AND ( ? ?REPLACE ? ?(""a"", ""b""), ? ?REPLACE ? ?REPLACE ? ?REPLACE ? ?REPLACE ? ?REPLACE ? ?REPLACE ? ?REPLACE ? ?regex"" \/"") AS b.c; Input: a dataset with problem A, B, C or D. Output: a subdirectory with two dataset subdirectories and a CSV with all labels matching the problem you just defined. The second part of the problem definition consists of the description of the dataset, specifying the problem and outputting as a table. The third part is a query to output the dataset and the third part is a query to filter this dataset using a regex. A query is any text that is not only a regular expression with a given pattern, but any text that contains some or all of the given criteria, and/or a regular expression with some or all of the given criteria and is followed by a ? separator. A regular expression is also a query"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset ACE 2004?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT query, you might want to use that. In which case, you get the columns of the output. Let me explain my analysis. To generate a report, I used MySQL 5.6.3 and a database named ACL-ARC. The data were stored in a single table. All columns were numeric, and I used the following strategy: columnid | name | 1 | value | 2 | metric | 3 | score I made a dummy column, using the default values. Then, I created a column of the form row(columnid)  where row is the result of the SELECT *  query. For the first test, I used the SELECT *  query on this column. On the same query, I used the following SQL: CREATE TYPE alias_val ( columnid AS INT ) AS SELECT * FROM alias_val ; This query returned: alias_val | 1 | value | 2 | metric | 3	False

Metrics on query only
Precision	0.28575810678949337
Recall	0.6865754849043515
F1_Score	0.37618576593969044
Blue_4	0.18578984622761477
Bleu_Score_Cumulative	0.2075643534066971
Meteor_Score	0.001524390243902439
Rouge_1	0.44266900360945594
Rouge_2	0.3701266703234802
Rouge_L	0.4235318829977174

Metrics on all rows
Precision	0.20537432629148122
Recall	0.5112365679963671
F1_Score	0.27190799967908835
Blue_4	0.12053641376879085
Bleu_Score_Cumulative	0.13551333296063045
Meteor_Score	0.0009746588693957114
Rouge_1	0.32365609600288214
Rouge_2	0.24538009223532378
Rouge_L	0.3030101222418689

Number of sparql queries generated equal to given	1

Generated text starts with 'SELECT'	328
