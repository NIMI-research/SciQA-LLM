{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Uj9BbfTJVTX4XbxZ3hez0SlOpoTDYY8K","timestamp":1694849423678}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n","#!pip -q install accelerate>=0.12.0\n","!pip install transformers\n","!pip install datasets\n","!pip install sentence_transformers"],"metadata":{"id":"zuwLc--q7RtP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd drive/MyDrive/en2sparql"],"metadata":{"id":"YcU-KRrrF9JR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694958093464,"user_tz":-120,"elapsed":28560,"user":{"displayName":"Antonello Meloni","userId":"09336063038722863523"}},"outputId":"51c1ef90-4dd4-4630-f18d-2d004fa6b68a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/en2sparql\n"]}]},{"cell_type":"code","source":["import json\n","import torch\n","from sentence_transformers import SentenceTransformer\n","from sentence_transformers.util import cos_sim\n","from datasets import load_dataset\n","from transformers import pipeline, AutoTokenizer\n","threshold = 0.25\n","\n","model = SentenceTransformer('all-mpnet-base-v2', device='cuda' if torch.cuda.is_available() else \"cpu\")\n","# model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda' if torch.cuda.is_available() else \"cpu\")\n","raw_datasets = load_dataset(\"orkg/SciQA\")\n","print(raw_datasets)\n","embed_data = torch.load('train_embeddings.pt')"],"metadata":{"id":"mhRZVPpm8Zoe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"REnydZ-_F7GI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpt2 = pipeline(model=\"gpt2-large\", max_new_tokens=384, device='cuda' if torch.cuda.is_available() else \"cpu\", return_full_text=False)\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")"],"metadata":{"id":"tjudq_08AM22"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xF_bcVRM61-s"},"outputs":[],"source":["def divide_chunks(l_, n_):\n","    for i_ in range(0, len(l_), n_):\n","        yield l_[i_:i_ + n_]\n","\n","\n","def save_json(filename,data):\n","    with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n","        print(json.dumps(data), file=json_file)\n","\n","\n","\n","\n","\n","def get_similar(element, items=None, embeddings=None, num=None):\n","    emb_items = None\n","\n","    if items is None and embeddings is not None:\n","        emb_items = embeddings[\"emb_questions\"]\n","        items = embeddings[\"keys\"]\n","    elif items is not None:\n","        emb_items = model.encode(items)\n","\n","    if len(element) == 0 or emb_items is None:\n","        return []\n","\n","    emb_element = model.encode(element)\n","\n","    result = []\n","    scores = cos_sim(emb_element, emb_items)\n","\n","    if num is None or num < 2:\n","        maximus = torch.max(scores, 1)\n","        m = float(maximus.values[0])\n","        i = int(maximus.indices[0])\n","        if m > threshold:\n","            result = [[round(m, 4), items[i], embeddings[\"questions\"][i], embeddings[\"queries\"][i]]]\n","        return result\n","    else:\n","        scored_texts = []\n","        for i, score in enumerate(scores[0]):\n","            scored_texts.append([round(score.item(), 4), items[i], embeddings[\"questions\"][i], embeddings[\"queries\"][i]])\n","        sorted_scored_texts = sorted(scored_texts, key=lambda x: x[0], reverse=True)\n","        return sorted_scored_texts[:num]\n","\n","\n","def clean(st):\n","    st = st.replace(\"\\n\", \" \")\n","    st = st.replace(\"?\", \" ?\")\n","    st = st.replace(\"{\", \" { \")\n","    st = st.replace(\"}\", \" } \")\n","    st = st.replace(\"\\\\'\", \"'\")\n","\n","    while \"  \" in st:\n","        st = st.replace(\"  \", \" \")\n","    return st\n","\n","\n","def get_key(q):\n","    t0 = q.get('template_id')\n","    if t0 is None:\n","        t0 = \"None\"\n","    t = str(q.get(\"number_of_patterns\")) + \"-\" + t0\n","    return t\n","\n","\n","def save_embedding():\n","    train = raw_datasets.get(\"train\")\n","    questions = [q[\"question\"][\"string\"] for q in train]\n","    queries = [clean(q[\"query\"][\"sparql\"]) for q in train]\n","    keys = [get_key(q) for q in train]\n","    embeddings = {}\n","    emb_questions = model.encode(questions)\n","    embeddings[\"questions\"] = questions\n","    embeddings[\"emb_questions\"] = emb_questions\n","    embeddings[\"queries\"] = queries\n","    embeddings[\"keys\"] = keys\n","    torch.save(embeddings, 'train_embeddings.pt')\n","    return embeddings\n","\n","\n","def prepare_queries(n_):\n","    data = raw_datasets.get(\"test\")\n","    queries = []\n","    suggestions = []\n","    for q in data:\n","        t = get_key(q)\n","        question = q[\"question\"][\"string\"]\n","        suggestion = get_similar(question,embeddings=embed_data, num=n_)\n","        suggestions.append([[[x[0], x[1]] for x in suggestion],t])\n","        # print(suggestion, t)\n","        # break\n","        # continue\n","        if suggestion is None or len(suggestion)==0:\n","            print(\"Error with key\", t)\n","            queries.append(\"translate the following English text '\" + question + \"' to a sparql query\")\n","        else:\n","            final_q = \"\"\n","            for i_, k in enumerate(suggestion):\n","                final_q += \"\\n input (English text): \" + k[2]\n","                final_q += \"\\n output (Sparql query): \" + k[3]\n","\n","            # works better with gpt\n","            # final_q += \"\\n with this example what is the sparql query for:  \" + question\n","\n","            # works better with dolly\n","            final_q += \"\\n input (English text): \" + question\n","            final_q += \"\\n output (Sparql query): \"\n","            queries.append(final_q)\n","    return queries, suggestions\n","\n","\n","def main(shots=1,attempts=10,batch=50):\n","    # save_embedding()\n","\n","    query_list, suggestions = prepare_queries(shots)\n","\n","    print(len(query_list))\n","    # print(query_list, suggestions)\n","    # quit()\n","\n","    n = batch\n","    q_list = query_list  #list(divide_chunks(query_list, n))\n","    sparql = [clean(x[\"query\"][\"sparql\"]) for x in raw_datasets.get(\"test\")]\n","\n","    gs = []\n","    lens =[]\n","\n","    for i, question in enumerate(q_list):\n","      print(i, question)\n","      res_ = tokenizer.encode(question)\n","      len_ = len(res_)\n","      lens.append(len_)\n","      print(len_)\n","      if len_ > 620:\n","        print(type(res_))\n","        question = tokenizer.decode(res_[-620:])\n","      res = gpt2(question)\n","      gs.append(res[0][\"generated_text\"])\n","      print(res[0][\"generated_text\"])\n","      result = {\"questions\": query_list, \"sparql\": sparql, \"generated_sparql\": gs, \"prompt_len\": lens, \"suggestions\": suggestions}\n","      save_json(\"nlp_gpt2_\"+str(shots)+\"_shot_results_tok.json\", result)\n"]},{"cell_type":"code","source":["main(shots=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zjcJPbgA6DT","executionInfo":{"status":"ok","timestamp":1694965210581,"user_tz":-120,"elapsed":6497868,"user":{"displayName":"Antonello Meloni","userId":"09336063038722863523"}},"outputId":"458fb87b-b36e-4eb7-e6b5-aa5c75e43749"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["513\n","0 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Story Cloze Test dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Story Cloze Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you list the models that have been evaluated on the Story Cloze Test dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Story Cloze Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Which model has achieved the highest Accuracy score on the WSC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?\n"," output (Sparql query): \n","739\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","If all the filters or conditions have been satisfied, the output will be {% name %} {% e.g. story_cloze_test_acc = {% e.g. paper orkgc:Metric }, % attrs:dataset, % attrs:metric, and % attrs:model %}. \n","input (English text): Describe the methods used to find the model that has achieved the highest Accuracy score on the WSC benchmark dataset?\n","output (Sparql query): SELECT?name,?accuracy,?model,?dataset as % model_values FROM ( SELECT accuracy, model as model_value FROM e.model AS e WHERE e.model_type ='story_cloze_test')?benchmark ;  orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_INITIALIZE?dataset. ORDER BY desc(?accuracy) LIMIT 1; orkgp:HAS_METRIC?metric; orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }?con orkgp:HAS_MODEL?model?model rdfs:label?model_lbl. ORIENT {?metric orkgp:HAS_DATASET?dataset; rdfs:label?model_lbl. }?orderby orkgp:HAS_MODEL?model. ORIENT {?metric orkgp:HAS_DATASET?dataset\n","1 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the One Billion Word dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"One Billion Word\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Annotated development corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Annotated development corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Paper Field dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?\n"," output (Sparql query): \n","499\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","Input: English dataset (length of corpus - 1 MB total). Example output:\n","output (Sparql query) (output data (wordcount, wordcount_max, wordcount_min)):    [?dataset a ]   <select name from word_level order by wordcount<  <field name> .?dataset a orkgc:Dataset  ?dataset a orkgc: Dataset  ?wordcount {?dataset a }   <select name> from wordcount<  <field name>.?wordcount_max {?dataset a }   <select name> from wordcount_max<  <field name>.?wordcount_min {?dataset a }   <select name> from wordcount_min<  <field name>.?wordcount_min_max {?dataset a }   <select name> from wordcount_max_min<  <field name>.?wordcount_max_min {?dataset a }   <select name> from wordcount_min_max<  <field name>.?wordcount_min_max_1 {?dataset a }   <select name> from wordcount_min to 0.05{<0.05> }..?wordcount_min to 0.03{<0.03> }.?wordcount_max to 0.02{<0.02> }.\n","output (Sparql query) (output data (wordcount, wordcount_max, wordcount_min))):                        <select name>\n","2 \n"," input (English text): What are the metrics of evaluation over the UrbanSound8k dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UrbanSound8k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What models are being evaluated on the WOS-5736 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the CUB-200-2011 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the UrbanSound8k dataset?\n"," output (Sparql query): \n","538\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","The dataset we used has the following attributes:\n","input (English text): What are the weights of the analysis procedure over the UrbanSound8k dataset?\n","output (Sparql query): SELECT SUM(weights) FROM?analyze_metric_lbl? WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UrbanSound8k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. } \n","input (English text): What are the weights of the analysis procedure over the CUB-200-2011 dataset?\n","output (Sparql query): \n","The dataset also has the following attributes:\n","input (English text): What is the performance of the data analysis procedure over the UrbanSound8k dataset?\n","output (Sparql query): SELECT SUM(weight_test) FROM?analyze_metric_lbl? WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UrbanSound8k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. } \n","input (English text); (In our example, this is as input text and the weights are extracted from as many columns as needed): What is the performance of the data analysis procedure over the CUB-200-2011 dataset? \n","output (Sparql query); (In this case, the weights are extracted from as many columns as needed\n","3 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Supervised: dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Can you list the models that have been evaluated on the Penn Treebank dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset?\n"," output (Sparql query): \n","550\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","And lastly we can use the following query to get the lists for all the models (see data_models.txt ):\n","SELECT TOP 1 models FROM models AS models\n","This is a super simple example and it should provide the information you need to make some decisions about which models to buy instead of trying to figure out the details of the models. That being said, it does seem to be possible to do a lot more advanced calculations this way, especially if your dataset is larger. I hope this was an interesting and informative example to the reader and if you have any questions, comments or suggestions please drop me a line.\n","4 \n"," input (English text): What is the top benchmark score and its metric on the SearchQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SearchQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the MedSTS dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the CoNLL 2012 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the MAZEA dataset?\n"," output (Sparql query): \n","840\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------ QUARTER RATE -------------------- 1.07 1.18 2.23 3.36 4.48 5.07 MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA MAZEA ----- -------------- ----- -------------- ----- -------------- ----- -------------- 1.00 1.09 * 1.10 1.14 * 2.00 2.03 * 2.09 2.13 * 3.00 3.09 * 4.00 3.17 1.99 1.99 * 2.07 2.15 * 3.00 3.17 * 4.00 3.18 2.12 2.02 * 2.10 2.21 2.24 * 3.00 3.20 2.26 2.20 2.27 2.31 2.40 2.52 3.00 3.20 2.29 2.30 2.31 2.32 2.35 2.39 2.45 2.57 3.14 2.39 2.40 2.51 2.64 * 3.16 2.41 2.52 3.09 2.36 2.35 2.41 2.48 2.70 3.16 2.47 2.52 2.61 2.76 2.79 3.30 2.63 2.82 2.91 3.41 2.68 2.94 3.46 2.73 * 3.48 2.72 2.74 3.30 2.84 2.82 2.91 3.50 3.57 3.60 3.68 3.77 3.84 3.95 3.99 3.99 3.99 3.99 4.\n","5 \n"," input (English text): Can you list the metrics used to evaluate models on the TDMSci dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDMSci\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What models are being evaluated on the ORKG-TDM dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ORKG-TDM\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the TSE-NER dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TSE-NER\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the TDMSci dataset?\n"," output (Sparql query): \n","535\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note that since the query can return an object, you can pass an object to the query like this:\n","select result, rdf, paper as score from results where model was score where model_lbl contains (?dataset a orkgc:Dataset) select result, score, rdf, paper as score from results where model was score where model_lbl contains (?dataset a orkgc:Dataset) select result, score, score, rdf, paper as score from results where model was score where model_lbl contains (?dataset a orkgc:Dataset) select result, rdf, paper as score from results where model was score where model_lbl contains (?dataset a orkgc:Dataset) select result, score, score, score, rdf as score from results where model was score where model_lbl contains (?dataset a orkgc:Dataset) select result, score, score, score, rdf as score from results where model was score where model_lbl contains (?dataset a orkgc:Dataset)\n","The output will contain a score (an object) that is a member of the rdf. The score needs to be returned as a key/value pair, and is defined in the DBMSSpec.\n","The paper_score, score, and model_score models returned must all be members of the rdf. All of the other models in the dataset must also be members. If the rdf doesn't have the required members, the result will not be returned.\n","Note that the only time you can retrieve a result without having the corresponding model on the DBMSSpec is by passing in a query string, and it will take the return value from the query if it\n","6 \n"," input (English text): What is the average installed capacity of all energy sources considered?\n"," output (Sparql query): SELECT (AVG( ?installed_cap_value) AS ?average_installed_cap_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43133 ?installed_capacity. FILTER(REGEX( ?energy_sources_labels, \"all sources\")) ?installed_capacity orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?installed_cap_value) } \n"," input (English text): What is the average installed capacity for each energy source considered in 5 year intervals?\n"," output (Sparql query): SELECT ?rangeId ?energy_sources_labels (AVG( ?installed_cap_value AS ?avg_installed_cap_value)) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?paper orkgp:P31 ?contrib; orkgp:P29 ?year. BIND(xsd:int( ?year) AS ?y). VALUES( ?rangeId ?min ?max) { (\"2001-2005\" 2001 2005) (\"2006-2010\" 2006 2010) (\"2011-2015\" 2011 2015) (\"2016-2020\" 2016 2020) } FILTER( ?min <= ?y && ?y <= ?max). ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43133 ?installed_capacity. ?installed_capacity orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?installed_cap_value). } ORDER BY ASC( ?rangeId)\n"," input (English text): What is the minimum and maximum installed capacity for each energy source considered?\n"," output (Sparql query): SELECT ?energy_sources_labels (MIN( ?installed_cap_value) AS ?min_installed_cap_value) (MAX( ?installed_cap_value) AS ?max_installed_cap_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43133 ?installed_capacity. ?installed_capacity orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?installed_cap_value) } \n"," input (English text): What is the mean capacity of a carbon-based fuel?\n"," output (Sparql query): \n","652\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" select min(?installed_cap_value) as installed_cap_value from solar, installed_cap_value order by install_date desc\n","output (Sparql query) SELECT?min_installed_cap_value from solar, MIN(installed_cap_value) as min_installed_cap_value, MAX(installed_cap_value) as max_installed_cap_value from thermal, installed_cap_value order by install_date desc \n","output (Sparql query) SELECT?installed_cap_value of solar, mean_installed_cap_value of thermal, mean_committed_capacity of thermal, mean_energy_sources_capacity of thermal, mean_installed_capacity of thermal, mean_installed_capacity of solar, mean_installed_capacity of thermal from solar order by installed_date desc\n","7 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-German dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-French dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MLDoc Zero-Shot English-to-Japanese dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Japanese\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset?\n"," output (Sparql query): \n","562\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmark on the MLDoc Zero-Shot English-to-Spanish dataset?\n"," output (Sparql query):  \n","input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmark with the MLDoc Zero-Shot English-to-Welsh dataset?\n"," output (Sparql query): \n","input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmark with the MLDoc Zero-Shot English-to-French dataset?\n","output (Sparql query):  \n","input (English text): Give me a list of research papers including their titles and IDs, that have performed benchmark on the MLDoc Zero-Shot English-to-Arabian dataset?\n","output (Sparql query): \n","input (English text): Give me a list of research papers including their titles and IDs, that have performed benchmark on the MLDoc Zero-Shot English-to-Bosnian dataset?\n","output (Sparql query): \n","input (English text): Give me a list of research papers including their titles and IDs, that have performed benchmark on the MLDoc Zero-Shot English-to-Bomnian dataset?\n","output (Sparql query):  \n","input (English text): Give me a list of research papers including their titles and IDs, that have performed benchmark on the MLDoc Zero-Shot English-to-Brazilian dataset?\n","output (Sparql query):  \n","input (English text): Give me a list of research papers including their titles and IDs, that have performed benchmark on the MLDoc Zero-Shot English-to-Catalan dataset?\n","output (Sparql query):  \n","8 \n"," input (English text): Indicate the model that performed best in terms of Error metric on the Kuzushiji-MNIST benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kuzushiji-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the MNIST benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the Fashion-MNIST benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fashion-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?\n"," output (Sparql query): \n","830\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kuzushiji\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the Möbius-Miesian benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Möbius-Miesian\")?benchmark orkg\n","9 \n"," input (English text): What is the name of the top performing model in terms of BLEU score when benchmarked on the WMT2016 Romanian-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Romanian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Russian-English benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Russian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest BLEU score on the WMT2016 German-English benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset?\n"," output (Sparql query): \n","842\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Romanian-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","The below examples demonstrate how to find a BLEU score for a model. \n","DISTINCT(model) WHERE {?metric orkgc:Metric} OR {?label orkgp_score} and {?dataset b orkgc:Dataset} or {?dataset c orkgc:Dataset} ORDER BY DESC(metric OR metric) LIMIT 1 ORDER DESC(?value) LIMIT 1\n","SELECT max(max(model)), bleu_score FROM wmt2016_test_model OR wmt2016_test_model_BLEU ORDER BY DESC(metric OR metric) WITH (max(max(model)), bleu_score\n","10 \n"," input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl500k) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What are the metrics of evaluation over the Ball in cup, catch (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Ball in cup, catch (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?\n"," output (Sparql query): \n","659\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?benchmark?benchmark_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Ball in cup, catch (DMControl500k)\")?benchmark OR kgp:P31?benchmark._2 ORDER BY DESC(?value) LIMIT 1 } \n","input (English text): What are the metrics of evaluation over the Ball in cup, catch (DMControl300k) dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?datasetter a orkgc:Datasetter; rdfs:label?datasetter_lbl. FILTER (str(?datasetter_lbl) = \"Ball in cup, catch (DMControl300k)\")?benchmark OR kgp:P31?benchmark._1 ORDER BY DESC(?value) LIMIT 1 } \n","input (English text ): Who owns the Ball in cup, catch (DMControl5k) dataset?\n","output (Sparql query):  select  ?count? number?of?countries where state_count <= 1\n","input (English text): Who owns the Ball in cup, catch (DMControl100k) dataset?\n","output (Sparql query):  select  ?count? number?of?countries where state_count <= 1\n","input (English text ): Who owns the Ball in cup, catch (DMControl200k) dataset?\n","output (Sparql query):  select  ?count? number?of?countries where state_count <= 1\n","We can use this column\n","11 \n"," input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the iNaturalist 2019 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2019\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the Kinetics-600 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the ObjectNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset?\n"," output (Sparql query): \n","857\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Top-1 Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }  output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Top-1 Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?\n","12 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ART/CoreSC dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ART/CoreSC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Paper Field dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the AAN Corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AAN Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?\n"," output (Sparql query): \n","495\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ARC-TREE,\" \"ARC-FOLDED,\" \"ARC-TRADED\" \"ARC-COMPARE,\" \"ARC-SEARCH\" \"ARC-CLUSTERED,\" \"ARC-RANGE\" orkgp:HAS_DATASET?dataset.!OR }?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): List the title and ID of research papers that contain a benchmark for the Dataverse dataset?\n","output (Sparql query): \n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Dataverse\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): List the title and ID of research papers that contain a benchmark for the DMS dataset?\n","output (Sparql query): \n","output\n","13 \n"," input (English text): What models are being evaluated on the TDM Tagged Corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDM Tagged Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the NLP-TDMS dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the DDI extraction 2013 corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DDI extraction 2013 corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the TDM Tagged Corpus dataset?\n"," output (Sparql query): \n","538\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDM Tagged Corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n"," input (English text): Which of the following metrics are commonly used for benchmarking models on the Kaggle dataset?\n"," output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kaggle dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the SPM corpus dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl\n","14 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NCBI Disease dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC2GM dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC2GM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the BC5CDR dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset?\n"," output (Sparql query): \n","511\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Describe the format of the benchmarking data in your paper?\n","input (English text): Provide a list of data related to disease modeling.\n","output (Sparql query): SELECT COUNT(*) DISTINCT?title orkgc? :?data_id; rdfs:labels?data_labels; output (Sparql query): SELECT DISTINCT?title orkgc? : {?dataset a orkgc:Dataset; dfs:label?dataset_lbl.?cont orkgp:HAS_DATASET?dataset.?bench orkgp:HAS_BENCHMARK?dataset.... } orkgp? orkgp:P31 ; rdfs:label?data_labels.;  input (English text): Describe the format of your benchmarking data in your paper?\n","input (English text): Provide a list of data related to validation.\n","output (Sparql query): SELECT COUNT(*) DISTINCT?title orkgc? :?data_id; rdfs:labels?data_labels; output (Sparql query): SELECT DISTINCT?title orkgc? : {?dataset a orkgc:Dataset; dfs:label?dataset_lbl.?cont orkgp:HAS_DATASET?dataset.?bench orkgp:HAS_BENCHMARK?dataset.... } ORKP? orkgp:P31 ; rdfs:label?data_labels.; \n","input (English text): Describe the format of the validation data in your paper?\n","input\n","15 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset Lunar Lander (OpenAI Gym)?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Lunar Lander (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the ObjectNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Top 1 Accuracy score when benchmarked on the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top 1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset?\n"," output (Sparql query): \n","861\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Top 1 Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Lunar Lander\")?benchmark ORkgp:EVALUATION?eval?eval ORkgp:HAS_SET_VALUE?set ORkgp:HAS_METRICS?metric.*cont ORkgp:HAS_BENCHMARK?benchmark ORkgp:HAS_MODEL?model.*model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the XIL dataset?\n"," output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Top 1 Accuracy\") { SELECT?models ORkgp,ORkgp,ORkgp,ORkgp,ORkgp,ORkgp,ORkgp FROM ( SELECT?_lbl_label?model_lbl AS (?label) AND?_model_lbl AS (?model) FROM?lunar.l3/xl3 IF?_lbl_label?\n","16 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Challenge)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the AESLC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the FSNS - Test dataset?\n"," output (Sparql query): \n","523\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" input (English text): What evaluation metrics are commonly used when benchmarking models on the SIFML-ALB dataset (SIFML-TEST)?\n","output (Sparql query): select 1 as ds , i from ds where ds.sifml__label.starts_with 'SIFML-' 1 1 1 1 1 > select * from(ds , i = 1 )[ 1 ] as res ; res.sifml__label.index <> 'FSNS (TEST)' 1 1 10000000  > select * from(ds , i = 1 ) as res ; res.sifml__label.index > 'SIFML-TEST' 1 1 10000000  1.0 \n","Input data is parsed in order to be outputted to the database. The model class that is used in the example is: BAVARIAN_META, where type = 'FIM-4X2', and params = {... }. The BAVARIAN_META_EQUATION_META data structure stores the transformation matrix, evaluation metric, and model parameters. The transformed model is then used to predict the parameter value.\n","The BAVARIAN_META_EQUATION_META object has the property rdfs:pipeline for each transformation transformation matrix, where pipeline is the class of transformation matrix to use and the rdfs:params, where rdfs:params is a list of RDF attribute values to pass into the pipeline. These attributes control the parameters that can be assigned to the model using the ORG-like methods from the BAVARIAN_META_EQUATION_META, and RDF features from the BAVARIAN_META. The API is detailed in detail in the\n","17 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the ImageNet V2 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the ImageNet 64x64 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset?\n"," output (Sparql query): \n","525\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet 64x64\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n","Input text: What is the name of the project? What is the lead author and affiliation of the project?\n","output:\n","1. lead author:  Chris Alberts,  Department of Computer Science,  The University of Texas at Austin,  USA\n","2. author:  (C. Alberts)  Chris Alberts, Chris Alberts, Department of Computer Science, The University of Texas at Austin,  USA\n","3. affiliation:  U.T. Austin\n","4. lead author:  Chris Alberts,  Department of Computer Science, The University of Texas at Austin,  USA\n","If a project is not named author, its lead author should be the project lead.\n","What is the project lead?\n","output:\n","1. Christopher Alberts,  Department of Computer Science,  The University of Texas at Austin,  USA\n","2. Chris Alberts,  Department of Computer Science, The University of Texas at Austin, USA\n","If you use the '?benchmark' or the '?bENCHMARK' parameters, the lead author is the lead author of the project.\n","What's the title of the project?\n","18 \n"," input (English text): What is the highest benchmark result achieved on the Classical music, 5 seconds at 12 kHz dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Indicate the model that performed best in terms of Bits per byte metric on the Classical music, 5 seconds at 12 kHz benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Bits per byte\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What are the metrics of evaluation over the UrbanSound8k dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UrbanSound8k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset?\n"," output (Sparql query): \n","754\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" select * from urban/metrics2 where metric_name < 5;\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","Input (Text in the same format: \"S.M.O.\" with a space before it)\n","input (English text): What are the metrics of evaluation over the S.M.O.R.E. Music 5 seconds at 12 kHz benchmark dataset?\n","output (Sparql query) SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UrbanSound8k\")?benchmark orkgp:HAS_DATASET?dataset; or kgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","The above two queries were performed using SQL 2005 and MySQL 5.1.0. Results ( S.M.O.R.E. Music\n","19 \n"," input (English text): Provide a list of papers that have utilized the PBSMT model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PBSMT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the DQN-CTS model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN-CTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Weighted Tsetlin Machine model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Weighted Tsetlin Machine\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code?\n"," output (Sparql query): \n","493\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","Input fields may contain the following keys : type : The type of table, column, or dataset of data to be processed. If type is not specified, a type of \"table\" is assumed. row_number : In most cases, the row identifier. This key may be empty. To support row-level data, use the row_level key instead. name : The name of the table as defined in the schema. schema : In general, this is the name (case-insensitive), or.schema, of the schema to which the data belongs. If schema is not specified, use this value as the name of the schema to which the schema data belongs. For information about the difference between the two schemas, see the following figure: row_key : The row key to use for referencing to this table. If you want to retrieve only some rows, you should use the row_key. See the following figure: row_order : How many rows are in which the results should be returned. Only the key is needed. If omitted, a default value ( 0 ) or a value of the other type (any column value) will be used. In both cases, a column is returned as an empty record. Only the row_order key is needed.\n","\n",": The type of table, column, or dataset of data to be processed. If type is not specified, a type of \"table\" is assumed. id : Specify the unique identifier to use for this table and its data. Only one entry is required per row. You should use a unique identifier consistent with the database name (e.g. \"tbls:A\") or schema name (e.g. \"tbls.TBL\"). Valid IDs are valid from 1 to 9999 (empty); for security reasons, all foreign key references in table schema must be unique.\n","20 \n"," input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL (18 layers) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL (18 layers)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Transformer model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL (RMS dynamic eval) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL (RMS dynamic eval)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL Base model?\n"," output (Sparql query): \n","488\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note that the DISTINCT function is actually not a function in the text of the paper, but a shortcut: it can be omitted, even if the function's text suggests that it does, for all practical purposes it is being set as the fourth input to a statement like IF (DISTINCT(model) IS NULL). This function is simply a shortcut to the query that creates an input variable for the function call.\n","Now, what kind of input does it actually return? Let us suppose for the purposes of this exercise that the source code for the Transformer-XL comes in a.doc and that the transducers are in a.doc. The output of the transducers is then:\n","The code is very simple: the second argument of the DISTINCT function, or the second argument to the query is the name of the dataset to compare against in the query! This is the code to benchmark the dataset against the Transformer. In the code, the name of the dataset is used by the DISTINCT function to refer to a function as the first argument.\n","Now let's implement the benchmark in order to see how well the benchmarks perform:\n","Input dataset model output time (minutes) DISTINCT(model, data) Benchmark - Transformer Base (RMS) + (3) RMS Sizes 17 987 17 842 1.3 Categorical variables No Yes Yes N/A 2.3 Multi-class correlation No Yes Yes N/A 3.1 Standard deviations (mean) No Yes Yes N/A 11.5 RMS regression No Yes Yes N/A 10.5 Variable (RMS model) (RMS value) 1 1.05440068 0.0098 0 3.01143856 8.3 RMS scale No No Yes N/A 1.4 Randomized\n","21 \n"," input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Which model has achieved the highest F1 score score on the BUCC French-to-English benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Can you list the metrics used to evaluate models on the BUCC German-to-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?\n"," output (Sparql query): \n","659\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Which model has achieved the highest score on the BUCC German-to-English model benchmark dataset?\n","output (Sparql query): All models tested passed. Rows with the highest score are listed on the output for each model.\n","input (English text): What is the best performing model benchmarking the BUCC German-to-English model in terms of R2 score metric?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?metric rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"R2 score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC German-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } }?criteria: R2 score of the model, which is the number of points above or below the critical R2 value in the given criterion. \n","input (English text): Can you list the criteria tests against which you have performed the best validation on models with the highest model scores?\n","output (Sparql query): SELECT DISTINCT?criteria?criterion_lbl WHERE {?metric rdfs:label?criterion_lbl. FILTER (str(?criterion_lbl) = \"R2 score\") {\n","22 \n"," input (English text): List the code links in papers that use the SAN (single model) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SAN (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Concept pointer+DS model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Concept pointer+DS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Sarsa-ε model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Sarsa-ε\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the SAN (single) model and include the links to their code?\n"," output (Sparql query): \n","488\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input: 1. What percentage of papers in the SAN and Sarsa-ε databases are of high quality? 2. What is the percentage of time spent dealing with a specific file? 3. What was the source of the issue?\n","Output: Table:\n","---------------------------------------------------------------------------- File (RDF format): %F% Time in (seconds per file) spent dealing with a specific file: 0% (0) 10% (10) 20% (20) 30% (30) 40% (40) 50% (50) 60% (60) 70% (70) 80% (80) 90% (90) 99.9% (99.9) ----------------------------------------------------------------------------\n","Output: 2. What was the source of the issue?   \n","Input: 3. List the documents that use the SAN (single model) model, and include the links to their code?\n","output (Sparql query): \n","input (English text): List the code links in papers that use the SAN (single model) model in any benchmark?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"SAN (single model)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): provide a list of papers that have utilized the Concept pointer+DS model and include the links to their code?\n","output (Sparql query): SELECT DISTINCT?code WHERE\n","23 \n"," input (English text): Can you list the models that have been evaluated on the ACE 2004 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the AAPD dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AAPD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the ACE 2005 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the ACE 2005 dataset?\n"," output (Sparql query): \n","534\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?p_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2005\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?p_lbl. } } \n"," input (English text): What are the benchmarks on which these benchmarks are based? Note: if using the SQL-style of output, omit the leading? for the model variables and their labels.\n","output (Sparql query): SELECT \"Benchmark\".benchmark(\"model_lbl\", {?dataset a orkgc:Dataset,?model_lbl rdfs:label?model_lbl })?benchmark. OPTIONAL if the model is an RDFs model\n","If outputting as JSON it is also possible to do the output as comma-separated (CSV-like) lines (the default) or as a table using the table parameter:\n","INSERT INTO `benchmark` ( `model_lbl`, `p_lbl`) VALUES ('{?dataset a orkgc:Dataset', 'HAS_DATASET', 'HAS_PACKAGE', 'HAS_TEMPLATE', 'HAS_META', 'HAS_ASSIGNMENT', NULL, NULL, 'DATA', NULL, 'MEMORY_SIZE', '10485760', NULL, NULL, NULL, 10485760, NULL, NULL, 10485760, NULL\n","24 \n"," input (English text): Where can I find code references in papers that have used the MP-EB model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MP-EB\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"TDMS-IE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DeiT-B model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DeiT-B\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the PNDec model for benchmarking purposes?\n"," output (Sparql query): \n","486\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input the input data, as a string of two or more algortihter and an array (of length) of a set of model properties: a string array of character code pairs, separated by an ampersand (\"&\"), \",'\", \"*\", \"%\", a string array of column string pairs separated by an ampersand (\"&\"), the same name (\"Mv\", \"+\"\", \"Sv\", \"=\") the same number of a string array of char value pairs, separated by a single ampersand (\"&\") the same name of a model property, separated by the colon (\":\") a string array of subclasses of PNDec model properties, separated by a single ampersand (\"*\") each subclass has the same mappings as the model of its parent (or parent class) a string array of model string values, separated by an ampersand (\"&\") the same mappings as the model property of its parent a string array of model column values, separated by an ampersand (\"&\") the same mappings as the column of the model property of its parent or parent class or class of its parent the same name of a model property, separated by the colon (\":\") a string array of model name string values, separated by an ampersand (\"*\") all model name strings have equal priority. all model name strings have their mappings in a string array with the same name as a model property the same name of the model property in the name string, but separated with a colon (\":\") the same name of a field or attribute of an object the same name of an array of attributes of an object all attribute names are equal in priority A property name of a class has a different priority. a property name of a class has a different priority. a property name of a member of a class has the same\n","25 \n"," input (English text): Can you provide links to code used in papers that benchmark the CATTS model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CATTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the CAIT-XS-24 model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CAIT-XS-24\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the XDC model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XDC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the CATTS-XSUM model for benchmarking purposes?\n"," output (Sparql query): \n","484\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What are the output benchmarks that the CATTS-XS-24 model can benchmark against?\n","output (Sparql query): SELECT?v :?rng_sum (v).?v / 50; orkgp:HAS_VARIANCE?rng_sum(v). orkgp:HAS_HASH. orkgp:HAS_TWEAK_VARIATION?total_sum(v). orkgp:HAS_REGIMENT?benchmark. orkgp:HAS_TWEAK?model.\n","\n","Note that some of the output benchmarks (such as?graph_size' ) are not actually required. They are intended to be used to test the CATTS model.\n","\n","For the CBA data from the PASWIS data, the CATTS model is the same as the PASWIS model when computing the parameters. The PASWIS data used to benchmark the model.\n","\n","A lot of other parameters are specified by the RPSY model. These additional parameters range from how many elements in a group are displayed (e.g.:?group_size ) to whether a parameter is in the CBA group (e.g.:?group_flag ). The CBA uses these additional parameters to determine which parameters should be benchmarked. If the parameter can be benchmarked using only the RPSY model, RPSY was not included in the benchmarks. If RPSY is included in the benchmark, it must use the actual RPSY model, no other model was used. RPSY must run on a dataset that consists entirely of CBA data using a single CBA group.\n","\n","The output is also available on CSV and XML.\n","\n","This is now the last update to the CATTS version\n","26 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Which model has achieved the highest Accuracy score on the IMDb-B benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark result (metric and value) over the dataset OntoNotes?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OntoNotes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B?\n"," output (Sparql query): \n","836\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" output (Sparql query): SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) LEFT JOIN?IMDb-B?metric?metric_lbl ON rdfs:label?dataset_lbl. { {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IMDb-B\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont ORkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?eval ORkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }?cont ORkgp:HAS_MODEL?model rdfs:label?model_lbl. } ORDER BY DESC(?value) } } SET output_column=?value&output_column_name=?value&output_column_type=?value&output_column_suffix=?value&output_table_name=?value&output_table_order=?value&output_unique_name=?value&output_unique_value=?value&output_table_mode=?value&output_unique_type=?value&output_unique_suffix=?value&output_unique_value=?value&output_table_index=?name&output_unique_index=?name&output_tablename\n","27 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the MLDoc Zero-Shot English-to-German dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the MLDoc Zero-Shot English-to-Italian dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot German-to-French?\n"," output (Sparql query): \n","898\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT max(?value), max(?val) * 5, max(?value*5), max(?value)*5 FROM?lbl WHERE \"Max(?value)\" NOT NULL\n"," +---------------------+ | max(?value) |   max(?val) * 5 |   max(?value*5) | | -----+----------+-------+---------+----------+-------+----------+ | 1 | 6 | 14 | 14 | 5| 5 | +---------------------+\n","28 \n"," input (English text): Where can I find code references in papers that have used the MP-EB model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MP-EB\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Seq2Seq model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Seq2Seq\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the A2C + SIL model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A2C + SIL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the SemExp model for benchmarking purposes?\n"," output (Sparql query): \n","483\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"SemExp\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","Please provide details on where these files can be found.\n","29 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Amidar dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Atlantis dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Ms. Pacman dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Up and Down dataset?\n"," output (Sparql query): \n","531\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Zangobi dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_l Bl ) = \"Zigurat\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }  input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 8-bit video monitor dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_l Bl ) = \"16-bit Zangobi\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }  /\n","The last question is from the RAC survey. How do your participants perceive the RAC? What is their understanding of what RAC stands for so that they can make an informed choice? That is a simple question with a much longer response that is just\n","30 \n"," input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the One Billion Word dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"One Billion Word\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Text8 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Text8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Hutter Prize dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hutter Prize\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset?\n"," output (Sparql query): \n","847\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Number of params\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Hutter Prize dataset?\n","output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Number of params\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Hutter Prize\")?benchmark orkgp:\n","31 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ADE Corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ADE Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Can you list the models that have been evaluated on the Penn Treebank dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank (Character Level) dataset?\n"," output (Sparql query): \n","553\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset sel:CharacterLevel orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"sel.cse.mcrr\"),?benchmark orkgp::HAS_DATASET.?paper orkgp:HAS_MODEL sel.?model orkgp:HAS_EVALUATION sel. OPTIONAL {?paper orkgp::HAS_MODEL sel. } }\n","Input text: provide the full-text information for the given paper Title Author (English): a and b Author (English): a\n","output (Sparql query, string): SELECT RANK FROM ( SELECT author FROM {?dataset sel:CharacterLevel(s) } HAVING author.id =?r,?author_id =?a?author.author_id =?b LIMIT?r ORDER BY {?id |?name.?name. } ) ORDER BY a, b\n","input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the PUBM data set?\n","output (Sparql Query, string):  SELECT TOP1 (N)?dataset_lbl.?benchmark orkgp::HAS_DATASET. ORG(?dataset_lbl,?benchmark );?label.?content orkgp::HAS_DATASET. ORDER BY {?id |?name.?name. } ;\n","input (English text): Can you list the models that have been evaluated on the PUBM data set?\n","output (Spar\n","32 \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Video Pinball dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Video Pinball\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Krull dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Krull\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n"," output (Sparql query): \n","510\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","output1 (data): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n","output2 (data): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset? \n","input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n","output (Github query): SELECT?tokens?metrics OR?score AS tokens FROM dk2datasets;\n","output (Sparql query): SELECT?tokens?metrics OR?score AS tokens FROM dk2datasets;  \n","input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n","output (Github query): SELECT?tokens tokens_metrics AS metrics FROM dk2datasets;\n","output (Sparql query): SELECT?tokens tokens_metrics AS metrics FROM dk2datasets; \n","input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n","output (Github query):  \n","output (Sparql query):  \n","input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n","output (Github query):   \n","input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n","output (Github query):   \n","input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n","output (Github query):   \n","input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n","output (Github query):   \n","input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n","output\n","33 \n"," input (English text): What is the name of the top performing model in terms of Top 1 Accuracy score when benchmarked on the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top 1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Error Rate\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset?\n"," output (Sparql query): \n","833\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" output (Sparql query): Select TOP(1) ROW COUNT(*) AS top_accuracy FROM dataset_benchmarks ORDER BY DESC(??value) LIMIT 1 \n","input (English text): Which model has achieved the highest Top-1 Error Rate score on the ImageNet V2 benchmark dataset?\n","output (Sparql query):  output (Sparql query): SELECT TOP(1) ROW COUNT(*) AS top_accuracy FROM dataset_benchmarks ORDER BY DESC(??value) LIMIT 1 \n","input (English text): Which model has achieved the highest Top-1 error Rate score on the ImageNet V2 benchmark dataset?\n","output (Sparql query):  output (Sparql query): SELECT TOP(1) ROW COUNT(*) AS top_accuracy FROM dataset_benchmarks ORDER BY DESC(??value) LIMIT 1 \n","input (English text): Which model has achieved the highest Top-1 Top-1 Error Rate score on the ImageNet V2 benchmark dataset?\n","output (Sparql query):  output (Sparqi Query): SELECT TOP(2) ROW COUNT(*) AS top_accuracy FROM dataset_benchmarks ORDER BY DESC(??value) LIMIT 1 \n","34 \n"," input (English text): What was the most common type of approach for summarization before 2002?\n"," output (Sparql query): SELECT ?approach ?approach_label WHERE { orkgr:R6948 orkgp:compareContribution ?cont. ?cont orkgp:P15 ?implementation. ?implementation orkgp:P5043 ?approach. ?approach rdfs:label ?approach_label. } ORDER BY DESC(COUNT( ?approach_label)) LIMIT 1\n"," input (English text): Which scholarly knowledge graphs support RDF?\n"," output (Sparql query): SELECT ?title WHERE { orkgr:R78023 orkgp:compareContribution ?cont. ?cont orkgp:P7009 ?has_rdf. FILTER(REGEX( ?has_rdf, \"T\")) ?cont rdfs:label ?title } \n"," input (English text): Name the datasets that have been used for benchmarking in the Knowledge Graph Embedding research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Knowledge Graph Embedding\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What is the most common Knowledge representation method?\n"," output (Sparql query): \n","391\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What is a Knowledge representation?\n","output (Sparql query): Select * from knowledge_graphs.\n","Inputs and Output Parameters:\n","id : Integer number representing the id of the entry in the knowledge graph.\n","s : String representation of the query object representing the query in the query graph.\n","p : String representation of the parameter name describing the parameter in the query.\n","orkgr : ( RDF or CSV ) object representing data or information for a graph.\n","orkgp : ( RDF or CSV ) object representing data or information for a graph.\n","orkgname : ( RDF or CSV ) string representing the Name of the object or information for a graph.\n","cdfs( rdfs : RDF or CSV ) object representing a query with data for a graph.\n","p : ( CSV ) object representing a query that uses data for a graph or information for a graph.\n","RDF or CSV is the format for specifying RDF or RDFS representations.\n","orkgname or korvname( orkgp : RDF or CSV ) string used to specify a Knowledge model when data in a Knowledge graph is not represented using RDF or RDFS.\n","RDF or CSV is the format for specifying RDF or RDFS representations. Output fields may contain data that is in RDF or CSV.\n","The following fields are required:\n","key : RDF or CSV object name of a Knowledge item.\n","fieldname : RDF or CSV string used to specify a row in the Knowledge graph.\n","output : RDF S3 object representing the result of a query to S3 of information or data for a graph in a Knowledge graph. Note: If the information or data from the Knowledge graph is not in RDF or CSV, these output fields will not be available.\n","The\n","35 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the MedSTS dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dataset mentions in Social Sciences dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dataset mentions in Social Sciences\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NCBI Disease dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Where can all the data sets used in the compared studies be found?\n"," output (Sparql query): \n","509\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUBSTITUTES\n","The following sub-intervals have been created so you can compare the results of different sub-interval analyses:\n","First, all the statistical results (e.g., medians, SDs, variances etc.) were generated by SPSS, while the histogram was generated using the PASW statistic. Then, a test called meditsim was performed to compare the meditsim results with the actual meditsim results.\n","Finally, three sets of sub-intervals based on the following parameters were compared – meditsim with log likelihood, meditsim without log likelihood and meditsim with binomial log likelihood:\n","\n","\n","2nd sub-interval\n","\n","3rd sub-interval\n","\n","FUTURE INVESTIGATIONS\n","\n","We have decided to start an effort to make these results publicly available. For this purpose, we created a GitHub repository to publish the data sets that are used in the analyses of this paper:\n","\n","https://github.com/davidkennedy/meditsim\n","\n","As our analysis was performed with R, the analyses are fully interactive.\n","\n","If you want to explore the analysis further, you can take a look at the code of the code used to create the dataset.\n","\n","REFERENCES\n","36 \n"," input (English text): What are the most commonly used benchmark datasets for the Named entity recognition research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Named entity recognition\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Word Sense Disambiguation research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Word Sense Disambiguation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Name the datasets that have been used for benchmarking in the Relation Extraction research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Relation Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Entity Disambiguation research field?\n"," output (Sparql query): \n","521\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" select rdfs:type from rdfs, data WHERE rdfs:length = 20000; select rdfs:cont_dataset from rdfs, dataWHERE rdfs:length = 2000; select rdfs:cont_dataset_len from rdfs, dataWHERE rdfs:length = 2000; select rdfs:sorted_data_by_replacement from rdfs, dataWHERE rdfs:length = 2000;\n"," (Data): Entity Disambiguation. Data Type: Data type: Entity Disambiguation. Source: http://www.jct.org/en/disambiguation-research-field-statistics.php\n","37 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the BioASQ dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the GENIA - UAS dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GENIA - UAS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NCBI Disease dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BIOSSES dataset?\n"," output (Sparql query): \n","516\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Genographic Consortium and BioBaseline dataset?\n","output (Sparql query): SELECT DISTINCT?name?name?names WHERE {?datastats asl:datastats; rdfs:label?data-bazetal. FILTER (str(?datastats ) = \"BASE Datastats\")?data.?data-a. pk:L1?size?cont?cont.?cont.?data-b. pk:L30?size?cont.?cont.?data-c. pk:L70?size?cont.?cont.?data-d. pk:L100?size?cont.?cont.?data-e. pk:L150?size?cont.?cont.?data-f. pk:L200?size?cont.?cont.?data-g. pk:L250?size?cont.?cont.?data-i. pk:L300?size?cont.?cont.?data-k. pk:L350?size?cont.?cont.?data-m. pk:L400?size?cont. {?datastats asl:datastats;?name?name. pk:L2?cont?cont.?datastats asl:datastats;?name?name. }  error (string): input string should be a string or 'no input string', the list will be empty\n","error (string): input string should be a string or 'no input string', the list will be empty output (Sparql query): \n","input (English text): Give me\n","38 \n"," input (English text): Which model has achieved the highest F1 score score on the BUCC French-to-English benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the EBM-NLP benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"EBM-NLP\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score score on the BUCC Chinese-to-English benchmark dataset?\n"," output (Sparql query): \n","822\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input: input (English text): Which model has achieved the highest F1 score on the NY Times benchmark dataset?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"nyt\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","input (English text): Which model has achieved the highest F1 score on the DATABASE benchmark dataset?\n","39 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the ImageNet V2 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Supervised: dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ImageNet ReaL dataset?\n"," output (Sparql query): \n","500\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","The first query outputs a list of the title and ID of all known articles. The SQL query will return an array of the same name, each item of which will list the title and ID of the paper where the benchmark was found. The second query will return a list of all the titles found in the text of this list -- i.e., the name in the.docx file where the benchmark was performed. For the purpose of this post these documents are sorted alphabetically by the most relevant title.\n","There are two methods that can be used to search for benchmarks:\n","The first is to use a SQL statement to find the model (or model-by-model or model-by-labeling) where the benchmark was performed. The SQL statement will search for a particular column label: {?dataset a orkgc:Dataset } ;. At present all the results are sorted alphabetically, but that can obviously be changed by the user. For the benchmarking method that will be used the following should be set as the first parameter:\n","in_dataset = 1: ;\n","The second is to specify a subset of the results to look for:\n","in_group =\n","The advantage of the first is that it is an easy way to find a large subset of papers, especially if your search for a specific part of the dataset results in fewer than the original search results. However this can be costly to your SQL query, and it is also difficult to specify in the first place.\n","The advantage of the second is that the output of the query is a single document, even if some documents are needed in more than one case. So we can now use the second query to get output.\n","The second technique uses a query of the same form as in the first:\n","in_group = SELECT title, id FROM [paper titles]..\n","40 \n"," input (English text): Could you provide a list of models that have been tested on the Gibson PointGoal Navigation benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Habitat 2020 Point Nav test-std dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Point Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Name the datasets that have been used for benchmarking in the PointGoal Navigation research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"PointGoal Navigation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset?\n"," output (Sparql query): \n","542\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?labels?labels_lbl. TITLE OR ID?labels OR ID?labels_lbl. TITLE OR ID?labels OR ID OR ID OR ID OR ID?labels OR ID. OPTIONAL?label OR ID. OPTIONAL?labels_lbl. OPTIONAL?labels_lbl. OPTIONAL OR ID OR ID OR ID OR ID OR ID OR ID. OPTIONAL?label OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR ID. OPTIONAL OR\n","41 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset SciFACT?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciFACT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the CoNLL 2012 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset CoNLL++?\n"," output (Sparql query): \n","849\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["-------------------------- { { { SELECT?metric?metric_lbl?value AS metric }?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. }?cont orkgp:HAS_METRIC?metric rdfs:label?metric_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n"," output (Sparql query): -------------------------- { { { SELECT?metric?metric_lbl?value AS metric }?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. }?cont orkgp:HAS_METRIC?metric rdfs:label?metric_lbl. OPTIONAL {?value orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n"," output (English text): What is the top benchmark score and its metric on the CoNUCC 2012 dataset?\n"," output (Sparql query): -------------------------- { { { SELECT?metric?metric_lbl?value AS metric }?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. }?cont orkgp:HAS_METRIC?metric rdfs:label?metric_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n","input (\n","42 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset PIQA?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PIQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What models are being evaluated on the PIQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PIQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the PIQA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PIQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What is the best performing model benchmarking the PIQA dataset in terms of Accuracy metric?\n"," output (Sparql query): \n","638\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PIQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?option; \n","input (English text): Where to find a list of all the individual paper statistics and their rds and weights?\n"," output (Sparql query): SELECT DISTINCT?sitemap?sitemap_lbl ; \n","input (English text): What is the most common weights and sitemaps used on the PIQA benchmark dataset?\n","output (Sparql query): SELECT SUM(sitemap?sitemap_lbl) AS?weights, SUM(sitemap?sitemap_lbl) AS?sitemaps FROM {?dataset a orkgc:Dataset,?dataset a orkgc:Dataset_1,?dataset a orkgc:Dataset_2,?dataset a orkgc:Dataset_3} ORDER BY?dataset_a.sitemap_lbl    orkgc:Dataset  name weight (sitemap) sitemap_lbl 1 orkgc:Dataset_1 a 1 1 1 a 1 1 2 orkgc:Dataset_2 a 2 2 2 a 2 2\n","43 \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Alien benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Alien\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Pitfall! benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Pitfall!\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Gopher benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Gopher\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset?\n"," output (Sparql query): \n","826\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Crazy Climber\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","\n","Example 6-1\n","\n","In addition to the score metric, in the last item of each benchmark and for each dataset a set of parameters were chosen to represent the model that outperformed. For example, if the model performs best in terms of the score metric and the datasets do not offer the same level of complexity, parameters for complex datasets would be chosen differently, since they provide more insight into the performance of a specific model. Similarly, if the model underperforms, then we would then select more complex metrics.\n","\n","The same parameters were used in the following benchmark sets.\n","\n","Benchmark Set Parameters Score metric score metric and models to compare 1.0 score metric and multiple models for each of the datasets 0.5 score metric and individual models for each\n","44 \n"," input (English text): Provide a list of papers that have utilized the PtGen model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PtGen\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Prior hs model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the PBSMT model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PBSMT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Table-Sequence model and include the links to their code?\n"," output (Sparql query): \n","479\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Table-Sequence\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized a single-label model and include the links to their code?\n"," output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Single Label\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized two-label models and include the links to their code?\n"," output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Dietary Reference\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont or\n","45 \n"," input (English text): Provide a list of papers that have utilized the Feedback Transformer (4 layers) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Feedback Transformer (4 layers)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Transformer-XL model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Transformer-XL (24 layers) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL (24 layers)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Funnel Transformer model and include the links to their code?\n"," output (Sparql query): \n","502\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Provide a list of papers that have used the Funnel (4 layers) model and include the links to their paper?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label? model_lbl. FILTER (str(?model_lbl) = \"Funnel (4 layers)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have used the Funnel (12 layers) model and include the links to their paper?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Funnel (12 layers)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have used the Funnel (24 layers) model and include the links to their paper?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label? model_lbl. FILTER (str(?model_l\n","46 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters En-De dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters En-De\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the NYT dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the NYT24 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT24\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters De-En dataset?\n"," output (Sparql query): \n","517\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reuters De-En\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } o\n","\n","Output of the last part of the query, as a JSON file\n","\n","The last part of the query is very important, and should be the one that the database server will interpret. Every time you call the test_model function we will return the name of the dataset or test model we want to benchmark.\n","\n","Before we test our model, let us examine some examples. Let us first go through some very simple example to demonstrate how the query above can be implemented. Note that all of the parameters that were defined for the original query are the same in this example, but they will vary depending on the model type or test model. We will also use another test model to verify the test model. (You can use a different test model from this article, or run it yourself: RSpec::Criterion can do that for you) Note that we are using the standard RSpec::Criterion API but you can also use the more advanced spec spec function. We will also use the RSpec::Spec::Model module ( RSpec::Test.pm ) to perform our tests. Also, remember the key from the start of the first line of the code?\n","\n","RESTful API for RSpec\n","\n","The tests on the\n","47 \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the PubMedQA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMedQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the ACL-ARC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the PubMed 20k RCT benchmark dataset?\n"," output (Sparql query): \n","823\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"1926\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on those that appeared in the RCTs and the ACL-ARC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE ({?id a 1?value a} >= 0.95)? OR {?id a 2?value a} <= 0.95)? OR {?id a 3?value a} >= 0.95)? OR {?id a 4?value a} <= 0.95)? OR {?id a 5?value a} >= 0.95)? OR {?id a 6?value a} <= 0.95)? OR {?id a 7?value a} >= 0.95)? OR\n","48 \n"," input (English text): Can you provide links to code used in papers that benchmark the DocTAET-TDM model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DocTAET-TDM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the MMDL model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MMDL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DATL model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DATL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DocRED-BiLSTM model in any benchmark?\n"," output (Sparql query): \n","473\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): List the coding links in papers that use the DocRED-BiLSTM model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DMPLED\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): List the coding links in papers that use the DocRED-BiLSTM model in any benchmark?\n"," output (Sparql query): \n","input (English text): List the coding links in papers that use DocRDML model in any benchmark?\n","output (Sparql query):  \n","input (English text): List the coding links in papers that use the DocRDML model in any benchmark?\n","output (Sparql query):\n","SELECT rdfs:text, rdfs:text_loc(code), rdfs:text_loc(model) form_links, ds:dataset_loc(code), ds:dataset_loc(model) form_links_lbl form_links_loc form_links_loc_lbl form_links_loc_dbl form_links_loc_dbl form_links_loc and _(form_links_lbl) form_links form_links_loc_lbl form_links_loc_dbl form_links_loc_dbl form_links_loc_\n","49 \n"," input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of PARAMS metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"PARAMS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the CIFAR-10 dataset in terms of FLOPS metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest FLOPS score on the Flowers-102 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Flowers-102\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric?\n"," output (Sparql query): \n","827\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\"SELECT?model?model_lbl WHERE {?metric orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"FLOPS\"){ SELECT?model?model_lbl WHERE {?datasets anl:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Pets\")?benchmark orkgp:Evaluation?eval; orkgp:HAS_RESULT?result; }, orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; } ORDER BY DESC(?value) LIMIT 1 } }  query \"SELECT?model?model_lbl WHERE {?metric orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"FLOPS\"){ SELECT?model?model_lbl WHERE {?datasets anl:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Pets\")?benchmark orkgp:Evaluation?eval; orkgp:HAS_RESULT?result; }, orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; } ORDER BY?\" ) SELECT DISTINCT?model?model_lbl WHERE {?metric orkgc\n","50 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset AESLC?\n"," output (Sparql query): \n","853\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------ AS Metric R2(100) ------------ ----------------------------- {100.16062} 1,001,000,000 5,000,000 Benchmark success: ( 100.16062) : (100.1616)\n","RSS feeds (Sparql Query output) ---------------------------\n","1 year 1 month 6 days 2/2014 2/2014 2/2014 2 Months, 1 day 2 days, 2/2014 4,1/2014 4,1/2014 4 2/2014 2 months, 2 days, 5 days 3 days, 3/2014 6,1/2014 6,1/2014 5 2/2014 2/2014 2 days, 4 days, 4/2014 3 days, 3/2014 7,1/2014 7,1/2014 4 2/2014 2 months, 3 days, 6 days 4 days, 3/2014 4,1/2014 4,1/2014 3 2/2014 4 days, 5 days, 3/2014 3 days, 6/14/2014 4days, 6+ days 5 2/2014 4/1/2014 4/1/2014 3 2/2014 5 days, 6 days, 3/2014 3 days 2/2015 3 1/2014 1/2014 1 1/2014 8 Days, 2 days, 4 days 3 days, 3/2014 3 days, 8 days, 4 days 6 2/2014 4+ 2/2014 4+ 2/2015 2 2/2014 2/1/2014 4 2/2014 4/1/1 4 2/2014 5,1/1 4 1/1/1 8 Hours, 7 days, 3 days 5 days, 5/2014 4 days 6 1/2014 2/14/1 7 1/15/1 4 1/15/1 3 1/15/1 3 1/1/1 3 1/15/1 3\n","51 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Stanford Dogs dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Dogs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the SciGEN dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciGEN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the BioASQ dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset?\n"," output (Sparql query): \n","498\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","# Using an XML schema output (XML query) SELECT  @item  {    ?id = \"Stanning, R. and P. H. \" ; // 1. Research study title;  ?pub_year = \"1995 \" ;  ?pub_year = \" 1996 \" ;    ?pub_language = \" english \" ;    ?pub_country = \" United Kingdom \"    ?pub_name =  \"Oxford-IIIT \" ;    ?referrer = \" https://sites.google.com/site/rogerhutchinson/pets \" ;\n","     ?id = \"Stanning, R. and P. H. \" ; // 2. Research study publication id.\n","# using a structured query output (XML request) SELECT  @item ;  pub_year : datetime;  pub_year : datetime;  pub_language : String;  pub_country : String;  pub_name : String;  pub_language : String;  pub_country : String;   #  #  #  #  #  #  ?name = \"Stanning, R. \" ;  ?pub_country = \" United Kingdom \" ;  ?opath :  String; ?\"pub_country\": \"United Kingdom\";  ?opath :  String; ?id :  String; ?pub_year : datetime; ?pub_year : datetime; ?pub_language :  Strings/String(13); ?pub_language :  String(1); ?id :  Strings/String(3); ?opath :  \n","52 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the WOS-11967 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-11967\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the MedSTS dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the SearchQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SearchQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the WOS-46985 dataset?\n"," output (Sparql query): \n","849\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------------------- SUM OF?TOTAL_PCT?N_TOTAL_PCT?N_TOTAL_PCT?PAGE_SIZE?MARCHTRAIN?TOTAL_PAGE?PAGE_LENGTH?MINIMUM_PAGE_LENGTH?MAXIMUM_PAGE_LENGTH?PAGE_TYPE?PAGE_REPRODUCTION_DATA?PAGE_REPRODUCTION_PAGE?PAGE_TASK?PAGE_TASK_DATA?PAGE_ID?N_PAGES orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WOS-46985\")?benchmark orkgp:HAS_DATASET?dataset; or kgp:HAS_EVALUATION?eval.?eval or kgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont or kgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) orkgc:Dataset\n","input (English text): What is the top benchmark score and its metric on the HIST-1K dataset?\n"," output\n","(Sparql query): ------------------------- SUM OF?TOTAL_PCT?N_TOTAL_PCT?N_TOTAL_PCT?PAGE_SIZE?MARCHTRAIN?TOTAL_PAGE\n","53 \n"," input (English text): Provide a list of papers that have utilized the HNEABP (BWNE) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"HNEABP (BWNE)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Sarsa-ε model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Sarsa-ε\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Prior hs model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the AcrE model and include the links to their code?\n"," output (Sparql query): \n","494\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"AcrE\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): Provide a list of papers that have utilized the Adaptation model and include the links to their code?\n"," output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Adaptation\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the LinearRegression model and include the links to their code?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl.?linearreg REG_LEN?linearreg REFERENCES orkgc:regs?linearreg REFERENCES orkgc:regs?linearreg REFERENCES orkgc:regs :linearreg REFERENCES orkgc:regs?\n","54 \n"," input (English text): Which model has achieved the highest SemEval 2007 score on the Supervised: benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"SemEval 2007\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest SemEval 2015 score on the Supervised: benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"SemEval 2015\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the SemEval-2010 Task 8 dataset in terms of F1 metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval-2010 Task 8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Supervised: dataset in terms of SemEval 2013 metric?\n"," output (Sparql query): \n","830\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"SemEval 2013\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Supervised:\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","I used to love the idea of having to type a lot of characters in all my posts. Now there will definitely be me blogging a lot less after the last few weeks. I may change up my blogging habits for this week as well, but until then, if you want to know more about the Supervised: metric, just go find it!\n","I'm using this tool for two purposes: First, a database that has been modified to only show the values for each metric if they are not in the previous two or three categories. Second, I have been using it as a testing tool, where \"I try this on and on until it meets the score requirements.\", that is the test set. Now when the score requirements are met, I will write about the results in a\n","55 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the SciFACT dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciFACT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AESLC dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciTLDR dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciERC dataset?\n"," output (Sparql query): \n","507\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Provide a list of research papers along with their titles and IDs, that have benchmarked models on the SciELO dataset?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciELO\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): Give me a list of research papers along with their titles and IDs, that have benchmarked models on the SciFS dataset?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SFS\")?benchmark orkgp:HAS_DATASET?dataset.?coll orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","Input (English text): Provide a list of research papers along with their titles and IDs, that have benchmarked models on the SciPRR dataset?\n","output (Sparql query): \n","input (English text): Give me a list of research papers along with their titles and IDs that have benchmarked models on the\n","56 \n"," input (English text): How many studies do use Chloride as major anion?\n"," output (Sparql query): SELECT COUNT( ?anions_labels) AS ?chloride_count WHERE { orkgr:R110597 orkgp:compareContribution ?contrib. ?contrib orkgp:P37458 ?anions. ?anions rdfs:label ?anions_labels. FILTER(REGEX( ?anions_labels, \"Chloride\"^^xsd:string)) } \n"," input (English text): When the earliest paper related to X-rays was published? \n"," output (Sparql query): SELECT ?publication_years WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, \"X-ray\", \"i\")) ?papers orkgp:P29 ?publication_years. } ORDER BY ASC( ?publication_years) LIMIT 1\n"," input (English text): What is the total number of species examined in the studies?\n"," output (Sparql query): SELECT (SUM( ?number) AS ?total) WHERE { orkgr:R58002 orkgp:compareContribution ?contrib. ?contrib orkgp:P31024 ?number_of_species. BIND(xsd:integer( ?number_of_species) AS ?number) } \n"," input (English text): What quantity of iron oxide was discovered on Elorza crater?\n"," output (Sparql query): \n","353\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT rdfs:type orkgc:Iron oxide.?iron_oxid.?iron_oxid rdfs:title \" Iron oxide. \" rdfs:size (?total) AS?total rdfs:type rdfs:Title rdfs:size (?total) AS?total rdfs:size xsd:string rdfs:label rdfs:type rdfs:Title rdfs:size xsd:string rdfs:label rdfs:type rdfs:Title rdfs:size \n","input (English text): What mineral is the iron oxide?\n","output (Sparql query): \n","input (English text): Did the iron oxide belong to an iron mineral?\n","output (Sparql query): \n","input (English text): Is there already a paper analyzing the iron oxide?\n","output (Sparql query): \n","input (English text): Is there already a paper analyzing the chironium sulfide?\n","output (Sparql query): \n","input (English text): This data contains the following data: number of species detected, number of samples analyzed, number of materials analyzed, percent of chromium, percentage of iron, and the % of iron in the iron oxide.\n","output (Sparql query):\n","input (English text): In the following study the Iron Oxide was identified as an Iron Mineral on the site X-ray of a very large meteorite. Is there a need for such a large Meteorite to be analyzed?\n","output (Sparql query): \n","input (English text): This data contains the following data: number of species detected, number of samples analyzed, number of materials analyzed, percentage of chromium, number of iron, and the % of iron in iron oxide.\n","output (Spar\n","57 \n"," input (English text): Can you list the models that have been evaluated on the FB15k dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FB15k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the LAMBADA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"LAMBADA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the JNLPBA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"JNLPBA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the VTAB-1k dataset?\n"," output (Sparql query): \n","560\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input (English text): Can you list the models that have been evaluated on the VTAB-2k dataset?\n"," output (Sparql query):  \n","58 \n"," input (English text): Provide a list of papers that have utilized the DQN-CTS model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN-CTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the DQN hs model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the RNN model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"RNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code?\n"," output (Sparql query): \n","491\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Provide a list of papers that have utilized the DQN image model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model image ; orkgc:label?model_i. FILTER (str(?model_i) = \"DQN-PixelCNN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgc:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgc:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the Caffe model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model model ; orkgc:label?model_data / NULL?model_lbl. FILTER (str(?model_lbl) = \"Caffe\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgc:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgc:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the Nervana model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model model orkgc:Model / NULL?model_lbl. FILTER (str(?model_lbl) = \"Nervana\")?benchmark orkgp:HAS_DATASET?\n","59 \n"," input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Breakout dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Breakout\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Phoenix dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Phoenix\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Beam Rider dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Beam Rider\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Freeway dataset?\n"," output (Sparql query): \n","522\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Freeway\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n"," input (English text): Do you know if this dataset has a specific algorithm?\n","output (Sparql query):  SELECT DISTINCT?metric?metric_lbl FROM ( SELECT rdfs.classid, rdfs.rank, rdfs.val, rdfs.label, rdfs.method, rdfs.dataset_name, rdfs.format, rdfs.format2, rdfs.format3, rdfs.meta_rank, rdfs.meta_value, rdfs.name_seq, d.name_seq, d.ref_seq, d.locality, d.latitude, d.longitude FROM rdfs GROUP BY d.id ) c.dataset ROWS TURNED (1) SELECT c.dataset_id, c.dataset_rank, c.dataset_val, c.dataset_label, rdfs.dataset_name, rdfs.name_seq, rdfs.format, rdfs.meta_rank, rdfs.meta_value\n","60 \n"," input (English text): Provide a list of papers that have utilized the Dynamic Coattention Networks (ensemble) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Dynamic Coattention Networks (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DY-ResNet-10 model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DY-ResNet-10\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the XLNet (Large) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet (Large)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark?\n"," output (Sparql query): \n","498\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Dynamic Coattention Networks (single models)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code; } \n","Input (any text): Name of dataset OR parameter names for parameter values.\n","Output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Dynamic Coattention Networks (single models)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\n","Input (English text): Name of dataset OR parameter names for parameter values.\n","Output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Dynamic Coattention Networks (single models)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MOD\n","61 \n"," input (English text): Indicate the model that performed best in terms of Macro F1 metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Macro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Micro Precision score when benchmarked on the NLP-TDMS (Exp, arXiv only) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Micro Precision\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the NLP-TDMS (Exp, arXiv only) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Indicate the model that performed best in terms of Macro Precision metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n"," output (Sparql query): \n","911\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Result: Micro Precision scores: 0.984, Micro Precision score (Micro Precision minus Micro Precision) = 0.975, Micro Precision scores = 0.989, Micro Precision score (Micro Precision + Micro Precision) = 0.984, Micro Precision scores = 0.985, Micro Precision scores = 0.981 Standard Micro Precision scores: 0.093, Standard Micro Precision score (standard Micro Precision minus standard Micro Precision) = 0.091, Standard Micro Precision scores = 0.091, Standard Micro Precision scores = 0.091, Standard Micro Precision scores = 0.091, Standard Micro Precision scores = 0.091\n","Note that the micro precision can be measured with a small deviation to an arbitrary limit. For further details about Micro Precision, please see this reference. The only exception in the table are the standard measurement values of the precision, the standard deviation and the minimum precision.\n","If you are interested in working with R data, please feel free to get in touch!\n","62 \n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Pong dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Pong\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Bowling dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bowling\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Tennis dataset?\n"," output (Sparql query): \n","843\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------ Score ----------------------------------- 1000 3100 6000 8000 4200 10000 10000 10000 9900 11000 13000 16000 23000 300000 36000 10000 29200 43200 5500000 64000 10000 10000 12000 15000 17000 19000 23500 250500 34000 128001\n","\n","If you are interested in testing your datasets on different platforms, please consider using the Metric Data Warehouse to generate the schema on your local machine. If you're looking for a complete list of all the datasets and their corresponding metrics on Atari 2600, you can try our data warehouse search.\n","\n","For additional resources and support, please read through our community documentation.\n","63 \n"," input (English text): What are the metrics of evaluation over the DCASE dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DCASE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the SoMeSci dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SoMeSci\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What is the best performing model benchmarking the DuIE dataset in terms of F1 metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DuIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What are the metrics of evaluation over the DuIE dataset?\n"," output (Sparql query): \n","602\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input (English text): What are the metrics of evaluation over the LISA dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"LISA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): What are the metrics of evaluation over the LABRIS dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"LABRIS\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): What are the metrics of evaluation over the LHCB dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_l\n","64 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Cheetah, run (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the Cheetah, run (DMControl100k) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Walker, walk (DMControl100k) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset?\n"," output (Sparql query): \n","585\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Here, the output from the last query for both queries is listed, along with several additional labels for further analysis. All metrics are shown in their corresponding labels here, or can be printed as text: output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a or kgc:Dataset; rdfs:label?dataset_lbl. FILTER ((?dataset_lbl) = \"Cheetah, run (DMControl500k)\")?benchmark orkgp:HAS_DATASET?dataset; or kgp:HAS_EVALUATION?eval. ORKH1H2H3H4H5H6H7H8H9H10. ORKH1H2H3H4H5H6H7H8H9H10. ORKH1H2H3H4H5H6H7H8H9H10. ORKH1H2H3H4H5H6H7H8H9H10. ORKH1H2H3H4H5H6H7H8H9H10. ORKH1H2H3H4H5H6H7H8H9H10. ORKH1H2H3H4H5H6H7H8H9H10. ORKH1H2H3H4H5H6H7H8H9H10. ORKH1H2H3H4H5H6H7H8H9H10. ORKH1H2H3H4H5H6H7H8H9H10. ORKH1H2\n","65 \n"," input (English text): What are the models that have been benchmarked on the Reuters RCV1/RCV2 German-to-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters RCV1/RCV2 German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the best performing model benchmarking the Reuters RCV1/RCV2 English-to-German dataset in terms of Accuracy metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters RCV1/RCV2 English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters RCV1/RCV2 German-to-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters RCV1/RCV2 German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset?\n"," output (Sparql query): \n","697\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?models rdfs:filter?model_lbl. EXPLAIN PLAN --------------------------------------------------------------------------------  -- Filter out the Model. Model is not in scope --> --------------------------------------------------------- ------------ R 2.3 (r, df), model R = 3.6 Beta = 1.9, beta P = 0.09 COSI -2.33 -0.61, r C = 11.6 Model #1 r = 1.7, t C = 3.9 Model #2 r = 1.0, t C = 1.6 AUC -0.05, r A = -19.6 Model #1 r = 30.5, t R = 2.3, p R = 2.8 Model #2 r = 31.3, t C = 8.7 Model #1 r = 7.4, p C = 4.5 Model #2 r = 8.6, p R = 2.9 --------------------------------------------------------------------------------  -- Use the R-Model only. Model is in scope --> --------------------------------------------------------- ------------ R 4.3 Beta = 1.6, p C = 16.7 Model #1 r = 1.1, t C = 4.6 Model #2 r = 2.8 COSI -2.6 -0.73, r C = 12.5 Model #1 r = 4.9 Beta = 2.3, t C = 29.8 Model #2 r = 1.9 COSI 0.75, r A = 0.10 Model #1 r = 9.9, t C = 7.1 Model #2 r = 7.8 r C = 7.9 Model #1 r = 0.5, t C = 1.1 Model #2 r = -50.7, t C = -3.7 Model\n","66 \n"," input (English text): What is the most common drug in the studies?\n"," output (Sparql query): SELECT ?drug, ?drug_labels WHERE { orkgr:R155621 orkgp:compareContribution ?contrib. ?contrib orkgp:P37578 ?drug. ?drug rdfs:label ?drug_labels. } ORDER BY DESC(COUNT( ?drug)) LIMIT 1\n"," input (English text): What is the total number of patients in the studies?\n"," output (Sparql query): SELECT SUM( ?number_of_patients) WHERE { orkgr:R33008 orkgp:compareContribution ?contrib. ?contrib orkgp:P15585 ?patients. BIND(xsd:integer( ?patients) AS ?number_of_patients) } \n"," input (English text): What is the most common substrate for catalysts?\n"," output (Sparql query): SELECT ?substrate WHERE { orkgr:R25900 orkgp:compareContribution ?contrib. ?contrib orkgp:P15090 ?substrate. } ORDER BY DESC(COUNT( ?substrate)) LIMIT 1\n"," input (English text): What types of nanocarriers do have therapeutic effect?\n"," output (Sparql query): \n","296\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" BIND(xsd:table(?substrate. ) AS?substrate : OR k : > 0 ),?substrate_labels,?substrate_labels_with_value, DESC(?substrate_labels_with_value),?substrate_label_with_value)\n","Inputs: xsd:table(orkgr:CompareContribution rdfs:label \"COC (C) (Pharm.:); Phospha)\") BIND(xsd:table(orkgr:CompareContribution rdfs:label \"COC (C) (Pharm.:); Phospha)\") Output: ORKGR(orkgr:R811) BIND(xsd:table(orkgr:CompareContribution rdfs:label \"COC (C) (Pharm.:); Phospha)\") Inputs: orkgr:CompareContribution rdfs:label \"COC (C) (Pharm.:); Phospha\") BIND(xsd:table(orkgr:CompareContribution rdfs:label \"COC (C) (Pharm.:); Phospha)\") Output: ORKGR(orkgr:R811) BIND(xsd:table(orkgr:CompareContribution rdfs:label \"COC (C) (Pharm.:); Phospha)\")\n","Input: k\n","Output: ORKGR\n","Input: k\n","Output: ORKGR\n","The query below illustrates how ORKGR extracts data for the ROIs of two drugs in comparison to the number of subjects in the studies\n","from ORKGR(substrate, drug)\n","From the output of a query to query, it can be seen\n","67 \n"," input (English text): Which model has achieved the highest ROUGE-2 score on the AESLC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest ROUGE-1 score on the Pubmed benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Pubmed\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the BioASQ dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset?\n"," output (Sparql query): \n","836\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): In what order do you think the results of the two benchmarks comparing model scores represent the highest level of model performance?\n","output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?ratio rr:Rating}. {?rct r=1; r,c=3; r,d=3; r,c=4; r,d=4; r,c=5; r,d=5} {?rct r=1; r=2; r=3; r=4; r=5; r=6; r=7} {?ratio r=1; r=2; r=3; r=4; r=5; r=6; r=7} ; {?summary rr:Rating_difference(Akaike_Weighted_DataFrames, n=100, beta=0.7); summary(lm = log (lm > rr))? > lm:summary(lm:mrt=2.0)?> lm; } order by rct descending orkgp:HAS_DATASET?data?model_lbl.\n","input (English text): Which model had the highest ROUGE-1 score on the PNAS benchmark dataset?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"ROUGE-1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_l\n","68 \n"," input (English text): Provide a list of papers that have utilized the Unsupervised NMT + Transformer model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Unsupervised NMT + Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the SMT + iterative backtranslation (unsupervised) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SMT + iterative backtranslation (unsupervised)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the MEMEN (ensemble) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark?\n"," output (Sparql query): \n","509\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input (English text): Provide a list of papers that have utilized the Unsupervised NMT + weight-shifting model in any benchmark?\n"," output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Unsupervised NMT + weight-shifting\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): List the code links in papers that have utilized the ConvNets + weight-shifting model in any benchmark?\n"," output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"ConvNets + weight-shifting\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Have any of the papers using the Unsupervised NMT + weight-shifting model (BASIC) in any benchmark?\n"," output (Sparql query):   \n","input (English text): Have any of the papers using the ConvNets + weighted-shifting (B\n","69 \n"," input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential MNIST dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Unpermuted Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Percentage error score on the MNIST benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Percentage error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Could you provide a list of models that have been tested on the Sequential MNIST benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Which model has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark dataset?\n"," output (Sparql query): \n","743\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Which model has achieved the highest Score on the Sequential MNIST benchmark dataset?\n"," output (Sparql query):\n","input (English text): Could you provide a list of models that have been tested on the Sequential MNIST benchmark dataset?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential MNIST\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","input (English text): Which model has achieved the highest Number of samples?\n","output (JSON query): SELECT?N(number); OR select score n, n, score from test_results_list rdfs:label?test_results_lbl ORDER BY DESC(?score)\n","input (English text): Which model has achieved the highest N_estimator_points on the Sequential MNIST benchmark dataset?\n","output (JSON query): SELECT?N(number); OR select n, n, number from test_results_list rdfs:label?test_results_lbl ORDER BY DESC(?n)\n","input (English text): Which model has achieved the highest Score of n in sample n_estimator_points on the Sequential MNIST benchmark dataset?\n","output (JSON query): SELECT max(\n","70 \n"," input (English text): Can you list the models that have been evaluated on the SciERC dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the SciREX dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciREX\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the ScienceIE dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the SciTLDR dataset?\n"," output (Sparql query): \n","551\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciTLDR\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }  input (English text): Can you list the models that have been evaluated on the Scilab dataset?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Scilab\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","input (English text): Can you list the models that have been evaluated on the ScienceDataLite dataset?\n"," output (Sparql query):  \n","input (English text): Can you list the models that have been evaluated on the Scilab dataset?\n","output (Sparql query):  \n","\n","71 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the CommonsenseQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the OpenBookQA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OpenBookQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset?\n"," output (Sparql query): \n","523\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CommonsenseQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","Example input/output schema\n","For the below, we've taken the following schema as the \"best\" for our example, and adapted it so that we can compare and evaluate it against more models and parameters over time:\n","We're going to build up our data over the course of a few months and have a number of independent runs, so we only have to worry about modeling the data. For those of you thinking \"I've had enough data to work with\", here is how you handle it:\n","Create individual files for each of your models using Data.SQLCanvas. Set the labels.\n","Here's what it would look like if we did what we need:\n","We created a new file called \"Lit_1.csv\" in which we've given each of the following variables, which we'll use to test our model:\n","We defined data for our top 20.0% and 90.0% accuracy, and we'll use those numbers as the threshold for our model.\n","We create a new file called \"Lit_10.csv\" in which we've created a top 20.1% model, a top 90.1% model, and a top 10\n","72 \n"," input (English text): What is the best performing model benchmarking the IMDb-M dataset in terms of Accuracy metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the IMDb dataset in terms of Accuracy metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the highest benchmark result achieved on the iNaturalist 2019 dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2019\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the IMDb-M dataset, including the metric and its value?\n"," output (Sparql query): \n","833\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","output_dataset: Output in other database (N-Dimensional): iNaturalist: iNaturalist2019  inputs (dataset)\n","output (dataset, ordered by output_dataset) iNaturalist:iNaturalist2019 \n","output_dataset (N-dimensional, ordered by output_dataset)\n","{?k2?value} output (Sparql query): SELECT?k2?max(?value)?format\n","order-0-1?type?format ORDER_0 {?format?value?k2?score\n","order-1-2?type?format ORDER_1 ORDER_2 {?format?value?k2?score\n","order-0-1?type?format OPTIONAL\n","order-1-2?type?format OPTIONAL ORDER_0 {?format?value?k2?score\n","order-1-2?type?format OPTIONAL\n","ORDER BY ORDER_0\n","?value\n","?k2\n","?format?rdfs:label?pk2. ORK2\n","?format?rdfs:value?pk2. RDFS:label RDFS:value\n","?format?rdfs:label RDFS:value\n","?k2\n","?format?rdfs:value RDFS:label RDFS:value\n","?format?rdfs:label RDFS:value\n","?k2\n","?format?rdfs:value SELECT?k2?score\n","?format\n","order-2-0-2?type?format ORDER_2\n","ORDER BY ORDER_1\n","?format?format rdfs:label\n","?format rdfs:value output (N-Dimension, ordered by output_datas\n","73 \n"," input (English text): List the metrics that are used to evaluate models on the Scholarly entity usage detection benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Scholarly entity usage detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What models are being evaluated on the Scholarly entity usage detection dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Scholarly entity usage detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Abstracts\\' entities and relations annotated corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Scholarly entity usage detection dataset?\n"," output (Sparql query): \n","666\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Scholarly entity usage detection\")?benchmark ORkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval ORkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont ORkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont ORkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n","This query works for both documents of type abstracts (sparql query can now be used to run the aggregation on all entities of type abstracts, but the output must be output in the same query)\n","SELECT DISTINCT?title and?dataset FROM ( SELECT * FROM ( SELECT?label?term AND rdfs:value.?value (?) FROM ( SELECT* ) AND (?word ORDER BY?text ORDER BY?text DESC? ) ) ) ) ORDER BY?word DESC?; orkgp:HAS_DATASET?dataset\n","The query returns a query string that would be used to run the aggregation that we run in the\n","74 \n"," input (English text): Can you list the models that have been evaluated on the JNLPBA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"JNLPBA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the MedNLI benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the MultiRC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiRC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the MultiNLI dataset?\n"," output (Sparql query): \n","559\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\"You can use one of the following forms to add a new model to JNLPBA:\n","\"a. You can use either the ORGCC model tag (also called model tag ) which contains the name of the JNLPBA dataset (default: MedNLI) or the ORGCC name (also called model name ) which contains the label string 'MedNLI'. If you wish, you can assign a unique identifier to the ORGCC name.\n","\"b. You can use either the ORCC model tag (also called model tag) which contains the model label string 'ORGCC' and optionally the ORGCC name (sometimes simply referred to as 'ORGCC name').\n","\"c. You may also use any of the ORGCC tag's forms.\n","\"d. The ORGCC tag is only considered valid if the orkgc_dataset value contains a string representation of the ORGCC model tag, but you should not use the model tag directly in JNLPBA (otherwise you might get strange syntax errors). You can therefore change model names when using these forms. Please refer to the ORGCC documentation. For full details with full syntax for ORGCC, refer this link.\n","\"e. If both ORGCC and model tag are available, then ORGCC uses the model tag and uses it in all models that have the'ORGCC'model tag, in which case the ORGCC model name is used. The'ORGCC'model name can be used as all other model name of the model in the JNLPBA dataset (the'ORGCC'model name is added to each model name that contains the ORGCC model tag).\n","\"f. If model tag'ORGCC'is not available then you can also specify model\n","75 \n"," input (English text): Could you provide a list of models that have been tested on the 200k Short Texts for Humor Detection benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"200k Short Texts for Humor Detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the top benchmark result (metric and value) over the dataset 200k Short Texts for Humor Detection?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"200k Short Texts for Humor Detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Name the datasets that have been used for benchmarking in the Humor Detection research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Humor Detection\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): List the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset?\n"," output (Sparql query): \n","672\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT?metric1?metric2?metric3?metric4 OR?metric1?metric2?metric3 OR?metric1?metric2 OR?metric3 OR?metric1?metric2 AND OR?Metric3 AND OR?Metric1 AND OR?Metric2 AND OR?Metric3 AND OR?Metric1 AND OR?Metric2 OR OR?Metric4 ORDER BY?metric2?metric1 ORDER BY?metric1 ON {?error OR?error_lbl?input_lbl. }\n"," input (English text): List the attributes for each model in the 200k Short Texts for Humor Detection benchmark dataset that have been evaluated in the Humor Detection research problem?\n"," output (Sparql query):  SELECT?num?attribute1?num?attribute2?sum?attr3 OR?num?attribute1?num?attribute2?sum OR?num?attribute1?num?attribute2 AND OR?num?attribute1 AND OR?num?attribute1 AND OR?num?attribute2 AND OR?num?attribute3 ORDER BYattribute1 ORDER BYattribute1,attribute2,attribute3 OR?num?attribute1 AND OR?num?attribute2 ORDER BYattribute3,attribute1 ORDER BYattribute3,attribute2,attribute3,attribute4 ORDER BYattribute1,attribute2,attribute3,attribute4\n"," input (English text): Name the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset?\n"," output (Sparql query):  SELECT?num?attribute1?num?attribute2?sum?attr1 OR?num?attribute1?num?attr2 OR?num?attribute1 OR?num?attribute2 AND OR?num?\n","76 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the MNIST dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset Fashion-MNIST?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fashion-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Could you provide a list of models that have been tested on the Sequential MNIST benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Sequential MNIST dataset?\n"," output (Sparql query): \n","763\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): I have a dataset with 5 models and there are 5 datasets in total. How should I order the models in the dataset?\n","output (Sparql query): SELECT DISTINCT?models?metrics?metric_lbl ORDER BY DESC(?dataset) orkgp:HAS_MODEL?model?model ORDER BY ORGp(?metrics) DESC(?metric_lbl) ORDER BY DESC(?model) DESC(ORGp(?metric_lbl)) DESC(?model_lbl)? ORDER BY {?model?model rdfs:label?model_lbl. } ORDER BY {?model?model rdfs:label?model_lbl. } ORDER BY {?model?model rdfs:label?model_lbl. } ORDER BY {?model?model rdfs:label?model_lbl. } ORDER BY {?model?model rdfs:label?model_lbl. } ORDER BY {?model?model rdfs:label?model_lbl. } ORDER BY {?model?model rdfs:label?model_lbl. } \n","input (English text): I have a dataset with 5 model and there are 5 datasets in total. How should I order the models in the dataset?\n","output (Sparql query): \n","input (English text): Can you provide the smallest benchmark result, including the metric and score, for the Sequential MNIST dataset?\n","output (Sparql query): \n","input (English text): I have a dataset with 5 model and there are 5 datasets in total. How should I order the models in the dataset?\n","output (Sparql query): \n","input (English\n","77 \n"," input (English text): Provide a list of papers that have utilized the Ours: cross-sentence ALB model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Ours: cross-sentence ALB\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the CRF with term expansion model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CRF with term expansion\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the BERT-based binary sentence classifier model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERT-based binary sentence classifier\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the CRF with sentence expansion model and include the links to their code?\n"," output (Sparql query): \n","504\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Including this dataset will give the full output of these models. Please provide all links as well as any error messages that they have been able to help you with.\n","The output of the model in the other columns is output by the model output parameter, which can be null (no output) or an empty string in the cases below. (You can also use the output parameter with the output parameter to get the code of the model as well.)\n","If you wish to get the actual code of a particular model, you can output both the output (Sparql query, including the output of the model) and model output (RDF query, including the code).\n","To access the output parameter, use the '?output' option:\n","output (text, output='FALSE. ') Output the output as a list of sentences and the model's code as a set of sentence labels. You can include the output as a plain CSV file, or output as a binary JSON file. As with the model output parameter, you can also provide the code in the 'output' list parameter.\n","Please note that it is possible that your sentence will appear twice on the output list. This is because the output is not necessarily a list of sentences. It could be a dictionary of sentences that you are embedding or a list of sentences that will be used in an actual text description. If you are inserting a sentence (e.g. a sentence at the start of a sentence) into a model's output list, then you need to use the output option with the output parameter. As with model output, you can also provide the code of your model's output (in the 'output' list) in the '?output' parameters.\n","The 'DISTINCT?code' option is useful when specifying a data set that is not actually being used for any benchmark. It will show\n","78 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the NYT dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the NYT24 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT24\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset OntoNotes?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OntoNotes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset NYT-single?\n"," output (Sparql query): \n","845\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- Score ----------------- 1.00 100.00 80.00 100.00 79.00 83.00 84.00 85.00 50.00 --------------------------- Score ----------------- --------------- 12.00 13.00 11.00 9.00 8.00 8.00 8.00 7.00 ------------ 6.00 5.00 6.00 6.00 5.00 5.00 5.00 4.00 ------------ 5.00 4.00 4.00 4.00 3.00 3.00 3.00 2.00 ------------ 3.00 3.00 4.00 7.00 4.00 1.00 4.00 2.00 ----------- 3.00 3.00 1.00 16.00 1.00 3.00 2.00 2.00 4.00 ----------- 2.00 3.00 1.00 17.00 1.00 4.00 2.00 4.00 2.00 5.00 ------------ 4.00 3.00 3.00 9.00 3.00 4.00 2.00 7.00 --------------- 9.00 6.00 4.00 16.00 3.00 3.00 2.00 3.00 4.00 ------------ 3.00 9.00 3.00 10.00 3.00 3.00 3.00 2.00 7.00 ----------- 8.00 1.00 4.00 12.00 3.00 1.00 1.00 2.00 5.00 ----------- 3.00 1.00 1.00 12.00 3.00 2.00 2.00 2.00 4.00 ------------ 5.00 7.00 5.00 12.00 4.00 3.00 1.00 1.00 6.00 ----------- 1.00 8.00 6.\n","79 \n"," input (English text): List the metrics that are used to evaluate models on the SciERC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the FTD dataset benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FTD dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the SciTLDR benchmark dataset?\n"," output (Sparql query): \n","513\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","select {?dataset a,?dataset_lbl as?dataset_lbl,?model_name as?model_name } as model_name; for ( var i = 1; i < 4; i++) { var n = dataset_lbl[i]; model_name = rdfs.table(n) ||''; } select {?dataset_lbl as?dataset_lbl,?model_name as?model_name } as model_name; for ( var i = 1; i < 4; i++) rdfs.table(n) =''; rdfs.table(n,'') =''; for ( var i = 1; i < 4; i++) rdfs.table(n,'') =''; for ( var i = 1; i < 4; i++) rdfs.table(n,'') =''; for ( var i = 1; i < 4; i++) rdfs.table(n,'') =''; select {?dataset_lbl as?dataset_lbl,?model_name as?model_name } as model_name; for ( var i = 1; i < 4; i++) rdfs.table(n) =''; rdfs.table(n,'') =''; for ( var i = 1; i < 4; i++) rdfs.table(n,'') =''; for ( var i = 1; i < 4; i++) rdfs.table(n,'') =''; select {?dataset_lbl as?dataset_lbl\n","80 \n"," input (English text): Can you list the models that have been evaluated on the WMT2016 German-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the IWSLT2014 German-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the IWSLT2015 German-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the WMT2016 English-German dataset?\n"," output (Sparql query): \n","582\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" RDFS Model Name and Classification Result RDFS Model Name/Category: WMT2016 The results of the model evaluations were organized into two groups: The \"paper\" group produced papers describing the performance using WMT2016 with various datasets which included: English text, French text, German text and the Spanish text datasets. The \"benchmark\" group produced papers using the benchmarks (Sparql query) against all the datasets in the WMT2016 dataset. The paper group consisted of papers with the following description: Paper Title: \"Papers Evaluating the Performance of the WMT2016 Machine Learning Language Model\" by Bijan Bakhic, Nicolas Cazalot, and Jens Schneider, 2013-07-22, 2015. Paper Version: 2016.00.0059.0 Published by the University of North Carolina at Chapel Hill. Abstract: We develop new benchmarks for language models to predict language performance, taking advantage of the fact that the WMT2016 dataset is large enough in terms of raw number of features that it can serve as a benchmark. The paper also summarizes our results for the current version of the WMT2016 machine-learning language in a comprehensive and general approach, which is based on the following: (i) a novel statistical framework (RDD clustering); (ii) a well-described language model, which can be used for validation or regression; and (iii) a suite of test data (e.g., English text, French text, and German text datasets) in which to evaluate the model's performance. In particular, we address a number of methodological issues arising from the standard literature regarding the use of a large data set, including (but not limited to) the challenges in generating a highly realistic simulation environment, avoiding the use of overly optimistic regression coefficients, and overcoming many problems related to the use of arbitrary feature counts, to name a few. The\n","81 \n"," input (English text): Indicate the model that performed best in terms of FLOPS metric on the Oxford 102 Flowers benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford 102 Flowers\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of FLOPS metric on the Food-101 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Food-101\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the CIFAR-10 dataset in terms of FLOPS metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?\n"," output (Sparql query): \n","828\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. PARALLEL?metric_lbl. PARALLEL?dataset_lbl. PARALLEL?engine_label. FILTER (str(?dataset_lbl) = \"CIFAR-100\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_ METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of FLOPS metric on the GINI dataset in terms of FLOPS metric?\n"," output (Sparql query) : SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. PARALLEL?metric_lbl. PARALLEL?dataset_lbl. PARALLEL?engine_label. FILTER (str(?data_labels) = \"GTES\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC\n","82 \n"," input (English text): What is the best performing model benchmarking the RotoWire (Relation Generation) dataset in terms of count metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"count\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Name the datasets that have been used for benchmarking in the Relation Extraction research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Relation Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the RotoWire dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset RotoWire (Relation Generation)?\n"," output (Sparql query): \n","744\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" { { MAX(?value) AS?score { SELECT?metric?metric_lbl AS?score AS?test_score:score-1 { SELECT?dataset_lbl?dataset_lbl AS?lbl AS?lbl_lbl [?test_score:?test_score.index] FROM?metric?metric_lbl.lbls.lbls WHERE?test_score=?test_score.test_score.len?dataset_lbl?dataset_lbl.lbls.lbls.index ON?test_score=?test_score.test_score.index?test_score=?test_score.test_score.len?dataset_lbl?dataset_lbl.lbls.lbls.index ON?test_score=?test_score.test_score.eq.? test_score } } } } ORDER BY?metric?metric_lbl. } OR (?test_score = 0 &?test_score = 1 & SET?tbl_lbl = { rdfs:label?dataset_lbl.?dataset lbl:LBL; } & { OR?test_score = 0 & TEST?metric?metric_lbl. OPTIONAL (?tbl_lbl = { lbl:LBL; lbl_lbl }.) } ORDER BY?metric?metric_lbl. } OR { OR?test_score=0 & TEST?metric?metric_lbl. OPTIONAL (?tbl_lbl = { lbl:LBL; lbl_lbl }.) } GROUP BY?metric?\n","83 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Reacher, easy (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the Reacher, easy (DMControl500k) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the highest benchmark result achieved on the Reacher, easy (DMControl500k) dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric?\n"," output (Sparql query): \n","696\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Inputs are separated by newlines.\n","Result is formatted as the specified value when no arguments are specified.\n","Example\n","\n","$ python benchmark.py 3.7.18 2.0.0.4 7.65.0.3 2.0.0.4 11.67.0.3 16.37.0.1 21.33.0.6 40.75.0.1 60.56.0.1 67.33.0.1\n","The below output show the benchmark results.\n","REACH = 3.7.18 RANGED = 3.7.18 RANGED = 3.7.18 # 2.0.0.4 1.1.1.5 2.0.0.4 REACH = 3.7.18 REACH RANGED # 2.0.0.4 3 3.7.18 RUNNING = 2.0.0.4 DROP = 3.7.18 DROP RANGED # 0 DROP 2.0.0.4 1 1.1.1.5 DROP = 3.7.18 DROP RANGED # 2.0.0.4 3 2.0.0.4 # 1 DROP 3.7.18 DROP RANGED # 2.0.0.4 2 2.0.0.4 # 1 DROP 2.0.0.4 1 1.1.1.5 # 2 DROP 3.7.18 DROP RANGED # 2.0.0.4 3 2.0.0.4 # 1 DROP 3.7.18 DROP RANGED # 2.0.0.4 2 2.0.0.4 # 1 DROP 2.0.0.4 1 1.1.1\n","84 \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Amidar dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Asterix dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asterix\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Seaquest dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Assault dataset?\n"," output (Sparql query): \n","562\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Can you list the models that have been evaluated on the Atari 2600 Atari Lynx dataset?\n","output (Sparql query):  select id as n, count(*) as k from table(?model;?dataset;?model_lbl) n in count(*) select * from table(?model;?dataset;?model_lbl) k in count(*)\n","It can be seen that the results is a SQL SELECT statement. For those that want to run it locally or get the results for the whole dataset, there's an easy way to execute it. The code below makes use of this to output the raw results.\n","This code is taken from the blog post\n","https://brianjgarner.org/2016/02/18/execute-with-tokens-from-slim/   The code below has been modified by my friend Chris Bowers so that it calls the execute_program function.\n","We create a database named \"tokens\" and create a \"table\" called \"model.\"\n","CREATE TABLE tokens ( id char(10) NOT NULL DEFAULT NULL );\n","CREATE TABLE model ( id char(10) NOT NULL DEFAULT NULL);\n","CREATE TABLE benchmark ( id char(10), _model char(10) NOT NULL DEFAULT NULL, _benchmark char(10) NOT NULL DEFAULT NULL);\n","CREATE TABLE model_lbl ( id char(10), _model char(10) NOT NULL DEFAULT NULL, _benchmark char(10) NOT NULL DEFAULT NULL);\n","CREATE TABLE benchmark_lbl ( id char(10), _model char(10) NOT NULL DEFAULT NULL, _benchmark char(10) NOT NULL DEFAULT NULL);\n","We are going to create 5 graphs in a\n","85 \n"," input (English text): Where can I find code references in papers that have used the DQN+SR model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN+SR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DQN Best model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN Best\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DARQN soft model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DARQN soft\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DQNMMCe+SR model for benchmarking purposes?\n"," output (Sparql query): \n","493\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DQNMMCe+SR\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","output (XML response): Select code(s): a b c DQNMMCe+SR code (string): DQNMMCe+SR code (string):\n","SELECT DQNMMCe+SR code (string): A B C DQNMMCe+SR code (string):\n","SELECT DQNMMCe+SR code (string): Select algorithm: DQN best orkgcn best\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DQN best\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","output (XML response): Select code(s): a b c DQNMMCe+SR code (string): DQNMMCe+SR code (string):\n","select code(s): a b c DQNMMCe+SR code (string\n","86 \n"," input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl500k) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What are the metrics of evaluation over the Ball in cup, catch (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Ball in cup, catch (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Ball in cup, catch (DMControl100k) dataset?\n"," output (Sparql query): \n","659\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n"," \n","Input (English text).\n","output (Sparql query).\n","input (Spanish text).\n","output (Sparql query). \"\n","\" output (Sparql query). \"\n","\"\n","87 \n"," input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT29 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the NYT24 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT24\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset?\n"," output (Sparql query): \n","817\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" model : test.nba.nytimes.nyt24 benchmark score : 74912.67 F1 score: 1210.73 AVERAGE score (1.57): 10.56\n","input (English text): Which model has achieved the highest F1 score on the NYT24 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT24\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } \n","There is a similar output in SQL: SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs\n","88 \n"," input (English text): List the code links in papers that use the MEMEN (ensemble) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the MEMEN  (single model) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Ning et al.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the MEMEN (single model) model?\n"," output (Sparql query): \n","488\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","7. What if the paper has already published a paper that uses the MIDEN (single-data-flow-model, CNF), CNFD, or CNF model?\n","If you are interested in reviewing the paper, please sign the review form (see the FAQ section on how to review a paper). If you are looking for a more complete view of the paper, please also include:\n","a link to the corresponding abstract\n","a table listing the references that include the paper\n","a list of key points that you consider important to understand and to highlight in the review\n","8. Why should I cite the Journal article?\n","A citation is not necessary for the paper to be accepted for publication. A brief word or description in the abstract (by a journal member) is sufficient.[6]\n","9. What do you mean by the need to cite the Journal article? Can you tell me more about this process?\n","The following statement, which can be found in the Journal article, is very important when a paper is submitted.\n","\"The reviewer must cite, on the page with the paper, the following relevant journal article:\n","\"1 Journal article: A. L. et al......[6]\n","\"The reviewer must be clear on when the references should include the Journal article; this should be done on the first page (to be clear about who is the author of the paper), or on the page of the paper itself when referencing the relevant journal article as a source of information.\n","\"These types of cases raise some interesting questions: Is it better to cite one or two reference references or three, or in which case should we cite any of the three (since all three may be sources of information)? What are the differences between the two types of cases? Should I include that which requires the original journal citation in my paper? Where can I find and cite the references for\n","89 \n"," input (English text): Provide a list of papers that have utilized the CAIT-M-36 model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CAIT-M-36\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the CvT-13-NAS model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CvT-13-NAS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Weighted Tsetlin Machine model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Weighted Tsetlin Machine\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the MMV TSM-50x2 model and include the links to their code?\n"," output (Sparql query): \n","505\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"MMV TSM-50x2\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): Provide a list of papers that have utilized the WMTB-15model and include the links to their code?\n"," output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"WMTB-15\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): Provide a list of papers that have utilized the HMM-18model and include the links to their code?\n"," output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"HMM-18\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_B\n","90 \n"," input (English text): Which model has achieved the highest Accuracy score on the Yelp-2 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Yelp-14 dataset in terms of Accuracy metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-14\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Yelp-5 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Which model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset?\n"," output (Sparql query): \n","828\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" 1. model_1 ORM 1 2. model_2 ORM 2 3. model_3 ORM 3 4. models_5_1 ANDM 4 5. model_5_2 ANDM 5 6. model_5_3 ANDM 6 7. model_5_4 ANDM 7 8. models_5_5 ANDM 8 9. model_5_6 ANDM 9 10. models_5_7 ANDM 10 11. model_5_8 ANDM 11 12. model_5_9 ANDM 12 13. model_6 ANDM 13  (MAX(?value) AS?score)  GROUP BY?metric?metric_lbl  1. model_1 ORM 1 2. model_2 ORM 2 3. model_3 ORM 3 4. models_5_1 ANDM 4 5. model_5_2 ANDM 5 6. model_5_3 ANDM 6 7. model_5_4 ANDM 7 8. models_5_5 ANDM 8 9. model_5_6 ANDM 9 10. model_5_7 ANDM 10 11. model_5_8 ANDM 11 12. model_5_9 ANDM 12 13. model_6 ANDM 13  (MAX(?value) AS?score)  GROUP BY?metric?metric_lbl\n","Note that as you'd expect, all the model benchmarks are between 0.40 and 0.80 (the average accuracy for the first test is 0.48). The average is calculated as  0.48 / (1.0 × min(0.40 - 0.80) - min(0.40 + 0.80) - min\n","91 \n"," input (English text): What is the best performing model benchmarking the WMT2016 English-Romanian dataset in terms of BLEU metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Romanian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the IWSLT2015 German-English dataset in terms of BLEU score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest BLEU score on the WMT2016 German-English benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric?\n"," output (Sparql query): \n","849\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","The main concern is a high number of incorrect answers. In the above example the model with wrong answer should not have been benchmarked. We can observe these wrong answers due to different metric selection or the use of a false model.\n","1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n","92 \n"," input (English text): Provide a list of papers that have utilized the Sarsa-ε model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Sarsa-ε\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the SciBERT model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SciBERT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the EfficientNet-L2-475 (SAM) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"EfficientNet-L2-475 (SAM)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the SAC model and include the links to their code?\n"," output (Sparql query): \n","500\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"SAC\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the SciBERT model and include the links to their code?\n"," output (Sparql query): SELECT?code SET?column?id?data.?cont orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model.\n","select_method (sig_sig) (a_id, a_type, a_name, a_value, a_index[0], a_index[1], a_type, a_name, a_value, a_index[0], a_index[1], a_type, a_name, a_value, a_index[0], a_index[1], a_type, a_name, a_value) As a_model select_method (sig_sig, data_name, id, data_sig) as a_model select_method (sig_sig, data_sig_type, id, id, data_sig_name, id, data_sig_index[0], id, id, id, id,\n","93 \n"," input (English text): Provide a list of papers that have utilized the MEMEN  (single model) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the MEMEN (ensemble) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the PBSMT model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PBSMT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the MEMEN model and include the links to their code?\n"," output (Sparql query): \n","489\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note that MEMEN and PBSMT have a different subset of the possible parameters. Please contact me if you need to change the dataset you want to use for a benchmark. And please note that not all of these links represent the most recent papers about the method, nor do they contain the most recent benchmark results as this is not always possible. Please consult the original papers if you are looking for the source code, or the original paper if I missed it.\n","\n","\n","This website was created to share new code and benchmarks with other MEMEN enthusiasts in the community. I invite you to join us if you want to do something with this code. We can discuss the code using this forum here: http://forums.memen-benchmarks.org/#!forum/mesh-benchmarks\n","\n","How to contribute to my benchmark:\n","\n","Read this documentation to get a feeling for what each piece of code is supposed to do. Please try to read the code closely by using this code-hunting and benchmarking guide first, and then do your modifications according to the recommendations given there.\n","\n","As you are modifying the code, use the.h files to review and re-review any changes in this code - you may also have a look at the related README file.\n","\n","Finally, do not forget to publish your code under a free license (see the contribution guidelines). This is especially important because some of the projects you may be interested in may have a license that prohibits you from disclosing what code you are working on as per this contribution guidelines (e.g. the GPL).\n","\n","If you write your benchmark in C++, read this tutorial for more information about C++ support and benchmarking this benchmark using Visual Studio.\n","\n","If you decide to do a new benchmark for your favourite MEMEN benchmark and you want to share it, I would really appreciate if you would contact me\n","94 \n"," input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM + dynamic eval model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AWD-LSTM + dynamic eval\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the AWD-LSTM-MoS + ATOI model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AWD-LSTM-MoS + ATOI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the AWD-LSTM-MoS + dynamic eval model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AWD-LSTM-MoS + dynamic eval\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes?\n"," output (Sparql query): \n","539\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" Select all code links from past benchmark, where use the AWD-LSTM-MoS-Dyn. eval. model for benchmarking purposes.\n","Sections:\n","95 \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Defender dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Defender\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Demon Attack dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Demon Attack\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset?\n"," output (Sparql query): \n","836\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Assault\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Attack dataset, the Atari 2600 Demon Attack dataset and the Atari 2600 Demon Attack dataset?\n"," output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari\n","96 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset 200k Short Texts for Humor Detection?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"200k Short Texts for Humor Detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Could you provide a list of models that have been tested on the 200k Short Texts for Humor Detection benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"200k Short Texts for Humor Detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Name the datasets that have been used for benchmarking in the Humor Detection research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Humor Detection\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What is the best performing model benchmarking the 200k Short Texts for Humor Detection dataset in terms of F1-score metric?\n"," output (Sparql query): \n","677\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" (SELECT DISTINCT?model?model_lbl WHERE {?t:TinyTimeframe?task_t orkgc:Task; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"200k Short Texts for Humor Detection\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?task_t. OPTIONAL {?task_t orkgc:TASK?}} ORDER BY DESC(?t) \n","  \n","(SELECT DISTINCT?dataset?dataset_lbl WHERE {?t:TinyTimeframe?task_t orkgc:Task; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"200k Short Texts for Humor Detection\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark. ORkgp:P32?task_t. ORkgp:TASK.?TASK. OPTIONAL {?t orkgc:TASK?}} ORDER BY DESC(?t)\n","(INCOMPETENT or the empty list of all models) Input Output Names models from  --------  1 Humor Detection 200k Short Texts for Humor Detection 2.2 6.76 (7.02)\n","The 1m orkgp:HAS_COWBOY? model failed.\n","The 2m orkgp:HAS_BANJA? model\n","97 \n"," input (English text): Where can I find code references in papers that have used the Relation-Metric model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Relation-Metric\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the ETL-Span model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ETL-Span\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the BiLSTM-CRF model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM-CRF\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the STREET model for benchmarking purposes?\n"," output (Sparql query): \n","496\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","data input (plain text): Name of the input files. \n","data output (Sparql query): Name of the output files. \n","data input (plain text): Data used in the validation work. \n","data output (Sparql query): Data of models used in validation work. \n","data input (plain text): Data used in the validation work. \n","data output (Sparql query): Data of models used in validation work.\n","data input (plain text): Data used in the validation work. \n","data output (Sparql query): Data of models used in validation work.\n","data input (plain text): Data used in the validation work. \n","data output (Sparql query): Data of models used in validation work.\n","data input (plain text): Data used in the validation work. \n","data output (Sparql query): Data of models used in validation work.\n","data input (plain text): Data used in the validation work. \n","data output (Sparql query): Data of models used in validation work.\n","data input (plain text): Data used in the validation work. \n","data output (Sparql query): Data of models used in validation work.\n","data input (plain text): Data used in the validation work.  data output (Sparql query): Data of models used in validation work.\n","data input (plain text): Data used in the validation work. \n","data output (Sparql query): Dataset names used in the validation work.  \n","data input (plain text): Dataset names used in the validation work. \n","data output (Sparql query): Dataset names used in the validation work.  \n","model input (plain text): Name of the model. \n","model output (Sparql query\n","98 \n"," input (English text): Can you list the models that have been evaluated on the WikiText-2 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the enwik8 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwik8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Open Entity dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Open Entity\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the enwiki8 dataset?\n"," output (Sparql query): \n","549\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","Outputs: Sparql query: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (char?(?dataset_lbl). LOWER() ||?dataset_lbl. LOWER() = \"\" OR?dataset_lbl. LOWER() = \"\" + (rdf.dataset_id ~?dataset_lbl.dataset_id) OR?dataset_lbl.dataset_id ≠?dataset_lbl.dataset_id). LOWER() OR?dataset_lbl. LOWER(); orkgp:HAS_DATASET?model. ORKGP:HAS_HAPPI?model_lbl. }\n","Note : The model name of the model is a part of the name string of the class of the model. Note also that you can use the model name in this list to name your data, i.e. the dataset in your model will look like this (in the data_model field of the table):\n","Input (English text): I have a model that has been evaluated on the enwik8 dataset.\n","Output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (char?(?dataset_lbl). LOWER() ||?dataset_lbl. LOWER() = \"enwik8\")?benchmark orkgp:HAS_DATASET?dataset; or\n","99 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciTLDR dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ESC-50 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ESC-50\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset?\n"," output (Sparql query): \n","532\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbr WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbr. FILTER (str(?dataset_lbr) = \"SciERC\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbr. } \n","input (English text): Provide a list of research paper titles and IDs that have benchmarked models on either the ekstat dataset or the EKSC data set?\n"," output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"EkSTAT\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): Provide a list of research paper titles and IDs that have benchmarked models on either the ekstat dataset or the EKSC data set?\n"," output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"E\n","100 \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the Natural Questions benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the PubMedQA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMedQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the MPQA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MPQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the CommonsenseQA benchmark dataset?\n"," output (Sparql query): \n","818\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PublicQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n"," import csv (csv)\n","import hq2json from urllib.request import urlopen from sklearn import * import numpy as np csv = csv. read (), csv. read_json ( \"./dataset1.csv\" ), csv. read_json ( \"./dataset1.json\" ) hq2json2 = hq2json. fromstring ( csv, read ) mpq = mpq. fromstring ( csv, 'json' ) data = { 'dataset' : mpq } nb_data, ida = np. zeros (( 30, 30 )) for ( i, n ) in enumerate ( db ): data [ i ] = ida ([ i\n","101 \n"," input (English text): List the metrics that are used to evaluate models on the SQuAD2.0 dev benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0 dev\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the SQuAD1.1 dev dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1 dev\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the SQuAD2.0 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?\n"," output (Sparql query): \n","551\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","In order to speed up evaluation, we use the following optimization strategies: 1) Optimization based on the value of the metric itself, which is called R2. R2 is the metric-to-value ratio, R2 represents the ratio of a metric's value to that of a random variable on a dataset. We select only the metrics that fit these criteria. 2) As a first step, we consider all of the metrics in a dataset (including any metric that is currently not recommended) to determine which metrics are least influenced by validation problems (i.e. are more strongly linked to the real world). R2 is determined at this stage, so there is no need to update the models. 3) In this case, we choose a metric that is least influenced by validation problems (i.e. is not strongly linked to the real world). For this case, R2 can generally be estimated by using a linear regression model and a logistic regression model. 4) We are also given the model-to-values probability that the model is not biased. If this probability is relatively low, we use a non-probability metric that measures the influence of the model on its observed values. However, if this probability is relatively high, we are willing to use a more expensive R2 metric. We select the metrics that have a high probability of this type. We are willing to select the metrics based on some information presented in the documentation. 5) Our next step is to select a metric that has the least influence of our validation problems. If the metric has a high probability of this type, we are willing to select it because its use will reduce the cost of this optimization. This metric must be determined by testing all of the R2 metrics in a dataset, so we are not willing to select the metric randomly. We choose these metrics based on some evaluation problems, but we also\n","102 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 German-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the WMT2016 German-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the WMT2016 English-German benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 English-German dataset?\n"," output (Sparql query): \n","538\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n"," input (English text): What evaluations of \"model\" and \"evaluation metric\" does the WMT2017 dataset present?\n","output (Sparql query): SELECT LEN(p:metrics) AND LEN(p:evaluation_units) IF (p:metrics > 0) AND p:evaluation_units > 0; OR p:metrics < 0; OR p:evaluation_units < 0; \n"," input (English text): Can you list the evaluations of models on the WMT2017 English-German benchmark dataset?\n","output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2017 English-German\")?benchmark o:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:\n","103 \n"," input (English text): Provide a list of benchmarked datasets related to the Information Extraction research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Information Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Provide a list of benchmarked datasets related to the Word Embeddings research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Word Embeddings\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Name the datasets that have been used for benchmarking in the Text Classification research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Text Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Provide a list of benchmarked datasets related to the Reading Comprehension research area?\n"," output (Sparql query): \n","511\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Provide a list of benchmarked datasets related to the Word Frequency research area?\n","output (Sparql query): \n","input (English text): Name the datasets that have been used for testing in the Word Reading Research problem?\n","output (Sparql query): SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Word Reading\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \n","input (English text): Provide a list of benchmarked datasets related to the Text Classification research problem?\n","output (Sparql query): \n","input (English text): Provide a list of benchmarked datasets related to the Word Frequency research problem?\n","output (Sparql query): \n","input (English text): Provide a list of benchmarked datasets related to the Reading Comprehension research problem?\n","output (Sparql query): \n","input (English text): Provide a list of benchmarks in the Reading Comprehension research area?\n","output (Sparql query): \n","input (English text): Provide a list of benchmarked datasets related to the Word Frequency research problem?\n","output (Sparql query): \n","input (English text): Provide a list of benchmarks in the Word Embeddings research area?\n","output (Sparql query): \n","input (English text): Provide a list of benchmarked datasets related to the\n","104 \n"," input (English text): Indicate the model that performed best in terms of RE+ Macro F1  metric on the CoNLL04 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"RE+ Macro F1 \") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL04\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest NER Micro F1 score on the ACE 2005 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"NER Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the CoNLL04 dataset in terms of NER Micro F1 metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"NER Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL04\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset?\n"," output (Sparql query): \n","837\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" query select DISTINCT?model?model_lbl WHERE {?id int? orkgc:ID;... } ORDER BY DESC(?id) LIMIT 1...\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?id int? orkgc:ID;... } ORDER BY DESC(?id) LIMIT 1...\n","To get a quick look at the scoring, use the benchmark function with the option:benchmark=<benchmark score>. In this case, we can see the average score over time for the various models as well as the benchmark for one each for the different tests. The table below lists all models across all tests for the ACE 2005 benchmark data.\n","Tests ACE 2005 ACE2005 ACE2005 ACE2006 ACE2006 ACE2006 ACE2006 ACE2005 ACE2006 ACE2006 ACE2005 ACE2006 ACE2005 ACE2005 ACE2005 A B B B B B B B B B B B\n","All test methods are based on an empirical set of models to test performance on ACE2005, ACE2006, and ACE2006. The Benchmark function allows us to filter scores up or down to only focus on tests within our benchmark or select one or more items and filter the results, depending on how many tests are selected.\n","Model Metrics and Scores A summary of model metrics, the mean and min, can be found in the table above. Model metric names are as follows: orkgc:Idual (average of min and sum of min and max scores) orkgc:Idual-min (average of min and sum of min and max scores) orkgc:Idual (average of min and sum of max scores) orkgc:Min-min (average of min and sum of min and max scores) orkgc:Min (average of min and sum\n","105 \n"," input (English text): Can you provide links to code used in papers that benchmark the XLNet model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the XLNet (Large) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet (Large)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the NASNet-A + c/o model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"NASNet-A + c/o\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the XLNet (base) model and include the links to their code?\n"," output (Sparql query): \n","486\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["  (a) The XLNet and the MIR model are independent of each other, are these two models dependent on each other?\n","(b) If they are dependent, would their dependence change whether you use a standalone XLNet or a combined XLSNet and MIR model?\n","(c) If these were dependent, how would this change your choice between an original base and the XLNet-MIR model?\n","(d) As an alternative to an original base, what has worked well for you?\n","(e) What is the difference between the original XLNet and the XLNet-MIR model?\n","(f) What is the difference between the original XLNet-MIR and the XLNet-A + c/o model?\n","(g) What is the difference between the original XLNet and the XLNet-B + c/o model?\n","(h) What are the differences between a standalone XLNet and an XLNet-MIR model?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"XLNet (Base)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }    input (English text): Provide examples of the use of the XLNet (Big) model and the MIR (Medium) model in any benchmark?\n","output (Sparql query):  (a) The XLNet-Big and MIR-Big models are independent\n","106 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Paper Field dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Open Entity dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Open Entity\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the 20NEWS dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"20NEWS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Ohsumed dataset?\n"," output (Sparql query): \n","487\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note: Open source tools, including RDFS and R, have supported parallel processing for scientific and other data for some time in the hope of eventually making the process of performing database parallelization more natural as well as faster for researchers (such as, for example, in the presence of many clusters of machines and high throughput SQL). But in that era of the cloud where the tools can simply be installed in minutes, the speed improvements are negligible. Until recently the only way scientists could do parallel parallel processing was by \"scrape and analyze\" data in parallel, taking advantage of all the parallel processors on a cluster and to perform the work in parallel on different clusters and machines.\n","Some techniques work better on cluster than on individual machines; however, these techniques are still expensive and they can be prohibitive to scale for the foreseeable future (especially if the data to be processed has a high dataset size). We believe that in the near to mid 2020s the process of parallel processing of data will become almost self-propelling -- it will be possible to run many independent processes (of varying speeds of course, but usually running at about 4k and in parallel) and it will become possible to execute queries in parallel from each of them. In other words, by some time around 2020 scientific science will have been revolutionized!\n","And you thought your job wasn't exciting enough!\n","We would love to hear from you how you would make use of RDFS/R data parallelization in your research here: http://www.ncbi.nlm.nih.gov/services/pubmed/21718863\n","We would particularly welcome questions for RDFS/R users, and feedback on their use and implementation in RDFS/R. We are also in need of some more resources, such as data warehouse and database architects to help. If you are an architect who would like to contribute\n","107 \n"," input (English text): Can you list the models that have been evaluated on the NYT24 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT24\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the NYT dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the NYT-single dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the NYT29 benchmark dataset?\n"," output (Sparql query): \n","552\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","output (Sparql query): Select columns as appropriate from \"d:columns\" d:columns -------- ------?dataset a d :?model?model rdfs:label?model_lbl d :?paper d rdfs:label?model d :?benchmark rdfs:label?model d :?benchmark_benchmark rdfs:label?model d :?benchmark_benchmark_benchmark rdfs:label?model d :?benchmark_benchmark_benchmark_benchmark d :?benchmark_benchmark_mock d d d\n","Input fields of an S-SQL query can be specified as multiple fields using commas. Use SELECT a, b, c WHERE or an OR for multiple column options, such as SELECT a OR b ;\n","input (English text): could you list the models that have been evaluated by the NYT-single dataset?\n","output (Sparql query): Select columns as appropriate from \"d:columns\" d:columns -------- ------?dataset a d :?model?model rdfs:label?model_lbl d :?paper d rdfs:label?model d :?benchmark d rdfs:label?model d :?benchmark_benchmark rdfs:label?model d :?benchmark_benchmark_benchmark d :?benchmark_benchmark_mock d d d\n","Input fields of an S-SQL query can be specified as multiple fields using commas. Use SELECT a, b, c WHERE or an OR for multiple column options, such as SELECT a OR b ;\n","108 \n"," input (English text): List the metrics that are used to evaluate models on the Dataset mentions in Social Sciences benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dataset mentions in Social Sciences\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the ScienceIE dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the average energy generation for each energy source considered?\n"," output (Sparql query): SELECT ?energy_sources_labels (AVG( ?elec_gen_value) AS ?average_elec_gen_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43134 ?elec_gen. ?elec_gen orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?elec_gen_value) } \n"," input (English text): Which are 3 the most common variables for the atmosphere models?\n"," output (Sparql query): \n","535\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","If you have the list as mentioned earlier, then you can call this query with the input text as input. This time, the Dataset has the name of the Dataset and also its name which is the name of the model that we are going to run. If yes, then the parameters that are passed in the last argument should be used to set the parameter values for the model, else you can set the parameters that are given in the previous query to be the values which have been taken.\n","This may help to understand how you want to use or improve the dataset before you actually are running it.\n","You may also have noticed that I have changed the parameter values in the model that have been called for this query. If you try to run the model after the parameter that I have changed the data set to, then it may fail to run. You may also change the parameter set in the model to your own specifications. To do this, you may do the following:\n","In the text field that says Model, go to the Parameters panel and change the dataSet that is named you. In the Parameters panel that says Models, select from the Model Parameters panel the models that have been called. Finally, in the same panel that says Parameters, click Properties in the Data tab. Set the values that you want to change the parameters in the Parameters panel.\n","Conclusion:\n","If you are looking for a dataset that has been used for evaluating models, then it is quite simple to find a dataset on the Open Science Framework website. I have decided to use the dataset that is in their Open Science repository and you can refer back to this post to get the datasets that I used for evaluating models. If you want to view the datasets that are available within the Open Science Framework, then you can checkout OpenBASF-Metadata. If you are looking for the dataset that is used within\n","109 \n"," input (English text): List the code links in papers that use the DDQN (tuned) hs model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDQN (tuned) hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DQN noop model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DDRL A3C model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDRL A3C\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DDQN-PC model in any benchmark?\n"," output (Sparql query): \n","487\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DDQN-PC\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }  output (Sparql query):  SELECT DISTINCT?code WHERE {?model b; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DDQN-PC\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\n","110 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Text8 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Text8\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the MRPC dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MRPC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the SciFACT dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciFACT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?\n"," output (Sparql query): \n","491\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","select output_text, score, rdf from research\n","out where output_text = \"TREC-6\": SELECT output_text, score, rdf from research where output_text = \"TREC-6\"\n","In the above example, we used a simple filtering strategy involving the field name with the ORG data element as input and the field of interest as the criterion - the field name would have to refer to an object with the \"TREC-6\" attribute.\n","SELECT output_type, rdf from research\n","out where output_type = 'TREC-6': SELECT output_type, rdf from research where output_type = 'TREC-6'\n","The SQL query results in the following output.\n","RDBMS output, Text7 : \n","RDBMS output, Text8 : \n","RDBMS output, MRPC : \n","RDBMS output, SciFACT : \n","To get the summary by text column, use the following SQL query.\n","SELECT summary, score, rdf, text_1 as t1, text_2 as t2, text_3 as t3, text_4 as t4, text_5 as t5, text_6 as t6 FROM research\n","SEMANTICS\n","RDF2 data types and relations\n","This section covers some basics of data types available in the RDF API.\n","RDF2 data types\n","RDF2 features a number of different kinds of data types. Some of these are a reflection of the relational database and others are directly related to structured text. Most of the data types available in the API are also available as RDF5 or RDF documents, see the Types table of Data. We will cover these and some of their relations in more depth later on. The following table gives a detailed description of each data type (\n","111 \n"," input (English text): Can you provide links to code used in papers that benchmark the Recurrent Rational DQN Average model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Recurrent Rational DQN Average\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DQN Best model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN Best\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DQN+SR model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN+SR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Rational DQN Average model?\n"," output (Sparql query): \n","488\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code FROM?model WHERE {?model a orkgc:Model; rdfs:label?model_lbl.?mod orkgp:HAS_MODEL?mod.?model orkgc:SourceCode OR gc.?source_code } \n","\n","\n","Other Model Comparison\n","\n","\n","A second example of the comparison from the Recurrent Rational DQN algorithm with another model is shown on the following figure:\n","\n","In this example, we compare the SVM model from the last table with a linear SVM model and a random SVM model.\n","\n","We compare how well the DQN+SR linear model outperforms both the random SVM and the SVM.\n","\n","Also, the Recurrent SVM outperforms both the random SVM and the SVM but it is the Recurrent SVM which outperforms the random SVM, because it has better performance than the random SVM with regard to the accuracy and recall of the data.\n","\n","The only limitation with the code for this benchmark is to allow us to compare SVM model parameters with one another. As seen in the SVM test on line 38 of the code example for the other model, there is only one parameter that is constant for the DQN (in our example) and that parameter is the output label. We can therefore compare the parameter values and select only the parameter that is used in the comparison.\n","\n","Output (Sparql query):\n","\n","SELECT ROWS_PER_GROUP[i] AS a, b, c, d, e, f, g, h, k, l, max(i), min(i), min(i+1), max(i+1), min(i+1+1), min(i+1+1+1), avg(i,group_by\n","112 \n"," input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-Spanish dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Spanish\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Accuracy score on the MLDoc Zero-Shot English-to-Japanese benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Japanese\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset?\n"," output (Sparql query): \n","882\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" name?(?) model?(?) model_lbl?(?) model_lbl_accuracy score orkgc:Metric?(?) model_lbl_accuracy score orkgc:Metric, metric name orkglbl:HAS_STATISTIC?(?) model_lbl_STATISTIC model_lbl_accuracy score orkgc:Metric, metric name orkgp:HAS_STATISTIC?(?) metric_lbl_STATISTIC model_lbl_accuracy score orkgp:HAS_STATISTIC model_lbl_stats score orkgp:HAS_STATISTIC metric_lbl:HAS_STATISTIC name orkglbl:VARIANCE?(?) metric_lbl_VARIANCE metric_lbl:HAS_STATISTIC metric_lbl:HAS_STATISTIC, metric name orkgp:VARIANCE?(?) metric_lbl_VARIANCE metric_p:HAS_STATISTIC  The output above shows the top score (value) of the dataset in terms of Accuracy score. I will use the name orkglbl_stats with the prefix orkgp in case of an ORG model which is not part of the benchmark dataset. \n","input (English text): What is the name of the top performing model in terms of Accuracy score on the MLDoc Zero-Shot English-to-French dataset?\n"," output (Sparql query): SELECT DISTINCT?name?(?) model?(?) model_lbl?(?) model_lbl_stats score orkgc:Metric?(?) model_lbl_stats score orkgc:Metric, metric name orkgl\n","113 \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the 20NEWS benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"20NEWS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the Penn Treebank benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the Quora Question Pairs benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Quora Question Pairs\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset?\n"," output (Sparql query): \n","825\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl FROM?object_categorization AS object_categorization WHERE {?dataset a ORkgc:Dataset; rdfs:label?dataset_lbl. } ORDER BY DESC(?value) LIMIT 1 } }\n","\n","114 \n"," input (English text): List the code links in papers that use the Linear Transformer model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Linear Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Transformer-XL Large model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Transformer (12 layers) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer (12 layers)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Long Short Transformer model in any benchmark?\n"," output (Sparql query): \n","476\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Short Transformer\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): List the code links in papers that use the Multilayer Optimizer (MM) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model_lbl a:model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Multilayer Optimizer (MM)\")?benchmark orkgp:HAS_DATASET?dataset.?pwd orkgp:HAS_SOURCE_CODE?pwd; orkgp:HAS_SOURCE_CODE?model. } \n"," Input text in German\n"," output (Sparql query): SELECT?code from?benchmark; \n","output (Sparql query): SELECT?code from?dataset; \n","115 \n"," input (English text): What is the name of the top performing model in terms of EM score when benchmarked on the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"EM\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the WebQuestions dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WebQuestions\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the CoQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Overall\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the TriviaQA dataset?\n"," output (Sparql query): \n","832\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TriviaQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the FTSO dataset?\n","output (Sparql query): \n","SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FTSO\")?benchmark orkgp:H\n","116 \n"," input (English text): Which methods are used?\n"," output (Sparql query): SELECT DISTINCT ?methods, ?methods_labels WHERE { orkgr:R110361 orkgp:compareContribution ?cont. ?cont orkgp:HAS_METHOD ?methods. ?methods rdfs:label ?methods_labels. } \n"," input (English text): What are the research problems Vernier Effect is related to?\n"," output (Sparql query): SELECT DISTINCT ?problems, ?problems_labels WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, \"Vernier Effect\", \"i\")) ?papers orkgp:P31 ?contrib. ?contrib orkgp:P32 ?problems. ?problems rdfs:label ?problems_labels. } \n"," input (English text): What kind of graph does ADANA use?\n"," output (Sparql query): SELECT ?graph ?graph_label WHERE { ?paper orkgp:P31 ?cont; rdfs:label ?title. FILTER(REGEX( ?title, \"ADANA\")) ?cont orkgp:P5008 ?graph. ?graph rdfs:label ?graph_label. } \n"," input (English text): What is Raman spectroscopy?\n"," output (Sparql query): \n","329\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input: English text input (English text): What is spectroscopy? Output:\n","DISTINCT RANK NAMES P.P.\n","DISTINCT RANK NAMES P.P.\n","DISTINCT RANK NAMES P.P.\n","SELECT RANK(DISTINCT p) FROM rdfs_tbl_p, p GROUP by p\n","output (Sparql query): SELECT NAMES(DISTINCT ps) FROM rdfs_tbl_p, ps GROUP BY ps\n","output (Sparql query): SELECT ps.ID FROM rdfs_tbl_p, ps WHERE (ps.ID = 0)\n","DISTINCT RANK NAMES P.P.\n","DISTINCT RANK NAMES P.P.\n","SELECT RANK(DISTINCT ps) FROM rdfs_tbl_p, ps GROUP BY ps\n","output (Sparql query): SELECT ps.ID FROM rdfs_tbl_p, ps GROUP BY ps\n","output (Sparql query): SELECT ps.ID FROM rdfs_tbl_p, ps GROUP BY ps\n","output (Sparql query):\n","Sparql query with two results columns\n","As you can seen in the following example, you can make a more detailed analysis using sparsql-select-result-column as well - this one is a pretty much simple graph with two rows per node. You can make the graphs like in the following two code samples:\n","The result-columns-in-graph.sql will create a simple graph with output like you see in the previous code sample. We can add this as a query to sparsql-select-result-column by typing it at the command prompt. The sparsql-select-result-\n","117 \n"," input (English text): What is the best performing model benchmarking the DTD dataset in terms of PARAMS metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"PARAMS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DTD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark result (metric and value) over the dataset TDMSci?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDMSci\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Could you provide a list of models that have been tested on the DTD benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DTD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the DTD dataset?\n"," output (Sparql query): \n","741\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): The data for the DTD benchmark dataset was generated by running the following SQL query, where the query was run against a DISTINCT?metric?metric_lbl dataset, where the parameter DISTINCT, is optional (the default is TRUE): SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. DERIVED FROM dataset_lbl OPTIONAL?eval.?eval. OPTIONAL?eval. OPTIONAL?eval. OPTIONAL {?eval orkgp:HAS_DATASET?dataset; rdfs:label?dataset_lbl. VALUE, metric_lbl. } ORIGMENT ORIGMENT WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. DERIVED FROM dataset_lbl OPTIONAL?eval.?eval. OPTIONAL?eval. OPTIONAL{?eval orkgp:HAS_DATASET?dataset; rdfs:label?dataset_lbl. VALUE, metric_lbl. } OPTIONAL {?eval orkgp:HAS_EVALUATION?eval?eval. ORIGMENT OPTIONAL ORIGMENT OPTIONAL orkgp:HAS_METRIC?metric. ORIGMENT ORIGMENT. OPTIONAL orkgp:HAS_BENCHMARK?rank. ORIGMENT ORIGMENT\n","118 \n"," input (English text): What is the top benchmark score and its metric on the CIFAR-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the CIFAR-100 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the CoNLL 2012 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the CINIC-10 dataset?\n"," output (Sparql query): \n","850\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- |Score|Metric|Result| | 7 | 1568,943 | | 40 | 890|1000,000 | | 20 | 636|1 - | | | | | | | | | | ----------------------- Total |1,852,966 |2,071,892|\n","What happens if you try to select a top test score from the data?\n","output (Sparql query): SELECT name?test_score,metric_lbl_test_score FROM CINIC-10; orkgp:HAS_METRICS?metric_lbl_test_score; rdfs:label?metric_lbl_test_score rdfs:value?metric_lbl_test_score (max(??value)) OR kgp:HAS_EVALUATION?eval(eval_score) orkgp:HAS_MODEL?model(model) rdfs:label?metric_lbl_test_score rdfs:value?metric_lbl_test_score WHERE?test_score = 1 AND?metric_lbl_test_score = 1 ORDER BY DESC(?,?value)?value\n","Does a top score of 1000,000 in one column cause the data to not have the metric for Metric score above 50?\n","output (Sparql query): SELECT name?test_score,metric_lbl_test_score FROM CINIC-10; rdfs:label?metric_lbl_test_score rdfs:value?metric_lbl_test_score ORDER BY DESC(?,?value)?value\n","How can you get the data set (Metric-9) to have a metric score of 2\n","119 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the SemEval-2021 Task 11 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval-2021 Task 11\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SemEval-2010 Task 8 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval-2010 Task 8\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ARC (Challenge) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Challenge)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the SemEval-2018 Task 7 dataset dataset?\n"," output (Sparql query): \n","524\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUM OF R^4 R^3 =\n","R^4 is the standard deviation of the average across the datasets\n","R^3 is the standard deviation of the mean across the datasets\n","R^4 =\n","R^3 =\n","R^4 =\n","R^3 =\n","R^4 =\n","R^3 =\n","R^3 =\n","R^3 =\n","R^3 =\n","R^3 =\n","R^3 = \n","\n","\n","The standard deviation of an arithmetic mean is defined to be R^2 / R^1 (in this instance it is R^2 / R^1 ) and the standard deviation of an arithmetic mean is defined to be R^1 / R^2 (in this instance it is R^1 / R^2 ). Thus\n","\n","The standard deviation of a mean is always measured in units of degrees\n","\n","Example 2: What is the standard deviation of the mean of a dataset? (10 datasets)\n","\n","In a simple spreadsheet, each column contains the values for each dataset to calculate (each row contains the values of a dataset) :\n","\n","Input (Input text): How many datasets have an arithmetic mean of 1,0,0,0 with standard deviations from 2 to 9?\n","\n","Output (Sparql query):\n","\n","\n","10 datasets\n","\n","Output (Sparql query): \n","\n","SUM OF R^2 R^1 = 1 1 1 0 1 0 1 0\n","\n","Sqrt OF R^4 R^3 \n","\n","\n","The Sqrt OF R^4 R^3 method is defined as the following :\n","\n","\n","The Sqrt OF R^4 R^3 method is defined as the following :\n","\n","SQRT OF R^4 (R^3/R^2) = (1 - 1/2)(1/2)\n","120 \n"," input (English text): List the metrics that are used to evaluate models on the COPA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"COPA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the ACE 2005 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the AG News benchmark dataset?\n"," output (Sparql query): \n","509\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AG News\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }  input (English text – text version): List the metrics that are used to evaluate models on the AG News benchmark dataset? \n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:labels?dataset_lbl. FILTER (str(?dataset_lbl) = \"AG News\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:labels?metric_lbl. } }\n","input (English text): List the metrics that are used to evaluate models on the BLEEP benchmark dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:labels?dataset_lbl. FILTER (str(?dataset_lbl) = \"\n","121 \n"," input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NCBI-disease dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the BC5CDR-chemical dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 entity level\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-chemical\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the JNLPBA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 entity level\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"JNLPBA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset?\n"," output (Sparql query): \n","862\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Inputs data: MyBrainDB_lbl_schema.db\n","data: MyBrainDB_lbl_scores.db\n","output: MyBrainDB_lbl_score_schema.db - MyBrainDB_lbl_scores.db \n","Inputs data: MyBrainDB_lbl_scores_lbls_label.csv\n","data: mybrain_db.db.scores.lbls_label.csv\n","output: MyBrainDB_lbl_scores.csv - MyBrainDB_lbl_scores.db \n","Inputs data: MyBrainDB_lbl_scores_values_lbls_label.csv\n","data: mybrain_db.db.scores.values_lbls_label.csv\n","output: mybrain_db.db.scores.values_lbls.label.csv - MyBrainDB_lbl_scores.values.lbls_label.csv \n","Inputs data: mybrain.db.scores.values.lbls_label.stacked.csv\n","output: mybrain.db.scores_values.lbls_label.stacked.csv - mybrain.db.scores.values.values.lbls_label.stacked.csv \n","Inputs data: mybrain.db.scores.values.values.lbls_label.stacked.stacked.csv\n","output: mybrain.db.scores.values.values.lbls.label.stacked.stacked.csv\n","Inputs data: mybrain.db.scores.values.values.lbls_predict.csv\n","data: mybrain_db.db.scores\n","122 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NCBI Disease dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Supervised: dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the TDMSci dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDMSci\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset?\n"," output (Sparql query): \n","508\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NCBI Disease\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n","\n","What else does your database offer?\n","\n","You can define custom SQL statements and query the user about them using the API. The main ways to do this are to generate your own queries and to have the user annotate and describe queries in a description.\n","\n","These features are provided by the S.A.C.I. database directly. It is possible to use S.A.C.I. directly outside of the query API, as described on the S.A.C.I Developer Portal\n","\n","What would be the purpose of providing the API?\n","\n","The NCBI is already using the S.A.C.I. engine, and the NCBI website already includes information about its features. The benefit of providing it externally is the ability to contribute enhancements and fixes to the source code - which will give us a stronger base for maintaining the user interface, including the database interface, and the tools that perform the functions described in the API.\n","\n","Please help the effort on the NCSI-DISEASE database\n","\n","We are very happy to learn how these features will facilitate the development of the NCBI Disease database, but the goal is to complete it by 2017. If you think the NCBI is doing pretty well (or not too well), feel free to help\n","123 \n"," input (English text): Can you provide links to code used in papers that benchmark the GCNN-8 model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GCNN-8\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the GGCNN model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GGCNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the CGU model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CGU\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the GCN Hybrid model?\n"," output (Sparql query): \n","463\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): List the code links in papers that benchmark the GCN Hybrid model?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"GCIN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Can you provide links to code used in papers that benchmark the GGRU model?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"GGRU\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","124 \n"," input (English text): Could you provide a list of models that have been tested on the ADE Corpus benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ADE Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Provide a list of benchmarked datasets related to the Constituency Parsing research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Constituency Parsing\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Data-to-Text Generation research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Data-to-Text Generation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What data format does CHEMDNER corpus have?\n"," output (Sparql query): \n","530\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------ CUMM_TYPE | BINARY DUMMY | DUMMY_FLOOR | FONT | BINARY_H_INIT | FORWARDed_LATTES | TEXT_Lattes N_DICTS -------- ------------------ --------------- CHEMDNER | CUMM_TYPE | FORMAT | | CURRENCY | | EXCLUDE N/A | NON_DICT | | FILTER | | BINARY_H_INIT | PICTURE | | FILTER | | BINARY_FILTER | | BINARY_H_FILTER | | TEXT_LATTES | TEXT_PURCHASE | | TEXT_PURCHASE | | VIDEO_CODE | N/A | TEXT_LATTES | NEW_DOCUMENT | | TEXT_LATTES | NEW_TEXT_SCHAPTERS | | VIDEO_CODE | N/A | VIEWSTATE | | VIEWSTATE | | VIEWSTATE | | TALENT | | ACTIVITY | | ACTIVITY | | ACTIVITY | | ACTIVITY | | ACTIVITY | | ACTIVITY | | ACTIVITY | | ACTIVITY | | ACTIVITY | | ACTIVITY | | DEVELOPMENT | DEVELOPMENT | | DEVELOPMENT | | DEVELOPMENT | | TALENT | DEVELOPMENT | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE | VIEWSTATE |\n","\n","2.18.0 New for May 20, 2013 -- CHEMDNER 2012 (2013)\n","125 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the CUB-200-2011 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the SciGEN dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciGEN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the CUB-200-2011 dataset?\n"," output (Sparql query): \n","623\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","126 \n"," input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Ning et al.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the DocTAET-TDM model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DocTAET-TDM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the SMT model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SMT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the SEE model?\n"," output (Sparql query): \n","468\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. PARAMETER?label_lbl. } \n","input (English text): Can you provide links to code used in papers that benchmark the VH model (SEMBLED)?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. PARAMETER?label_lbl. }  input (English text): Can you provide links to code used in papers that benchmark the STM model?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. PARAMETER?label_lbl. } \n","input (English text): Are the results from the papers listed here in the MEDLINE database (1966-2016), Scopus (1957-2016), and Google Scholar (1999-2016), and is the input in the output (an example from this category or from other categories) from any of these sources also part of your published work?\n","output (Text only, error handling applied):\n","(Please specify results from all sources)\n","input (English text): A number of papers have demonstrated that there could be important differences in the performance of the SVM and model in the benchmarking papers when the benchmarking paper and the peer reviewed journal that reviewed it both use the same model. However, I have personally not looked at the peer reviewed papers and I have yet to read the benchmarking paper and I am not sure about the reliability of the results.\n","output (Text only, error handling applied):\n","[Please specify results from studies published before 1st March 2016\n","127 \n"," input (English text): Provide a list of papers that have utilized the Prior hs model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Prior+Duel hs model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior+Duel hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the DQN hs model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Duel hs model and include the links to their code?\n"," output (Sparql query): \n","486\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Duel hs\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\n"," input (English text): Can you provide links to code used in papers that benchmark the DQN hs model?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DQN hs\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Can you provide links to code used in papers that benchmark the DUO hs model?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Duo hs\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:H\n","128 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset OntoNotes?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OntoNotes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset Text8?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Text8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Human-annotated)?\n"," output (Sparql query): \n","850\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- METRIC -------------------------- LBL --------------------------- score 9538.00 586.99 1.0000 score 9515.00 757.29 1.0001 score 8408.00 693.59 1.01 score 8353.00 697.28 -0.03 score 8285.00 706.00 -0.01 score 8278.00 684.00 -0.01 score 8237.00 653.00 0.00 score 8228.00 666.00 0.00 score 8209.00 665.00 0.00 score 8203.00 658.00 0.00 score 8207.00 669.00 0.00 score 8202.00 671.00 0.00 score 8199.00 673.00 0.00 score 8193.w 488.00 0.00 score 8189.w 483.00 0.00 score 8186.w 472.00 0.00 score 8186.w 477.00 0.00 score 8146.w 453.00 0.00 score 8151.w 452.00 0.00 score 8149.w 450.00 0.00 score 8151.w 451.00 0.00 score 8148.w 451.00 0.00 score 8130.w 446.00 0.00 score 8149.w 456.00 0.00 score 8130.w 448.00 0.00 score 8147.w 449.00 0.01 score 8149.w 447.00 0.00 score 8843.w 420.00 0.00 score 8157.w 421.00 0.00 score 8150.w 424.00 0.00 score 8153.w 428.00 0.00 score 8157.w 433\n","129 \n"," input (English text): Provide a list of papers that have utilized the Adaptive Input Very Large model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Adaptive Input Very Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Transformer (Adaptive inputs) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer (Adaptive inputs)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Longformer Large model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Longformer Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Adaptive Input Large model and include the links to their code?\n"," output (Sparql query): \n","493\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Provide a simple query that outputs the papers indexed in the original, and the papers indexed in the paper that used Adaptive Input Very Large and in which the data sources have been included. To output the papers indexed in the original and the papers indexed in the paper that used Adaptive Input Very Large in the original paper: select paperID from papers where original = paperID & original.index == paperID & original.code ==?code or id >?dataset.index.ID & original.code ==?code as?code then?cont or avg(?cont or avg(?cont.index?dataset.index.ID ).) > median( or avg(?cont.index.ID?dataset.index.ID ) ). select id from?cont or?cont.index or?cont.index.ID or?cont.id as?id or p.code ==?code end function benchmark\n","output (Sparql query): \n","input (English text): Provide a table of each paper (or index) that includes the abstract, title, abstract, authors, author_id, title, number of citations, source code, and total number of citations to that paper and the number of citations to all papers indexed by the paper in the dataset\n","output (Sparql query):\n","indexed_text :?page&code \n","input (English text): Select a page in a paper or a table of documents and provide an extract of the table or pages that contain that page. If that table or page has no content, only that portion is extracted, not the entire page. \n","output (Sparql query): SELECT DISTINCT?page & title.?page of text.\n","Outputs the text within the page index that matches the text within the page that includes\n","130 \n"," input (English text): What models are being evaluated on the Atari-57 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari-57\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Amidar dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Asterix dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asterix\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the Atari 2600 Solaris dataset?\n"," output (Sparql query): \n","553\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Solaris\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","131 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MultiNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiNLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the QNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SNLI dataset?\n"," output (Sparql query): \n","511\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Give me a list of research papers on which the multi-benchmarks were performed?\n","output (Sparql query):\n","SELECT DISTINCT?me?num2p?num2x?pos2x?pos2y?pos2z?pos2l 2 -2?1?L 2 -2?2?L 2 -2?L 2 -2?3?L 2 -3?1??L?L |??L??L??L??L??L??L??L??L??L??L??L??L??L??L??L??L?L |?L |?L |?L |?L |????? ||??G ||??G ||??G ||??G ||??G ||?L | ||??G??G |??G??G |??G??G??G ||??G ||??G ||??G ||?L | ||?L??L??G ||??G??G??G ||??G ||??G ||??G ||??G??G??G??G??G??G ||??G ||??G ||??G??G................\n","output (Sparql query): \n","input (English text): Provide a list of NLP benchmarks that benchmarked the MultiNLI dataset on the list of papers we provided before or on a list of papers that we provided after?\n","output (Sparql query): SELECT DISTINCT?me?num2r2t?num2y 3 3  ?R2T?1 1 -1 ???R??O 1!???? ????????R??R 1 |??R || 1 || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9 || 10 ||?????R ||?R?R ||\n","132 \n"," input (English text): Where can I find code references in papers that have used the Large mLSTM +emb +WN +VD model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Large mLSTM +emb +WN +VD\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the MMDL model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MMDL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the MP-EB model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MP-EB\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the MMV model for benchmarking purposes?\n"," output (Sparql query): \n","488\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","This means using these code links on your websites, and that the code references listed in your papers should be checked to make sure they only refer to a specific version of your code, since they are not code that you have modified. That said, there are also links to articles which appear in the news articles, and I can only think of the following papers on each of the four test cases: (1) DIMENSIONS - S. D. Gupta; (2) DIMENSIONS - J. P. Rabin; (3) DIMENSIONS - D. T. W. Lipschitz; (4) DIMENSIONS - F. Bouchot; There is a whole list on Wikipedia of any papers on MMDL that refer to a specific version of the code, but there is no particular reason for it other than it appears in the data set. Please verify that the link you have used is no longer valid, using the same code links found on this page. (thanks to J. P. Rabin)\n","In addition to the above, this project already have access to the whole MP-EBI data set. What has changed in the past week regarding our access? Thanks\n","(1) Added this link to the MMDL source code at https://github.com/MP-ETI-AS/EBI/graphs/source.\n","What has changed recently regarding your access to the MP-EBI data set? Thanks\n","(4) Added a link to the MP-EBI paper, which describes the MP-EBI implementation, code, and software of the MMDL model.\n","Note: If you have any questions or comments in relation to the MP-EB comparison using the MMDL version of the MP-EBI model (MMDL), please use the MP-EBI mailing list.\n","133 \n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Amidar dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Krull dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Krull\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?\n"," output (Sparql query): \n","847\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["-------------------------\n","You can retrieve different metrics for each metric.\n","input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Centipede\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval ||kgp:HAS_METRIC?metric. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n","input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Centipede\")?benchmark orkgp:HAS_DATASET?dataset. ORkgp:HAS_EVALUATION?eval. ORkgp:HAS_METRIC?metric. ORkgp:HAS_EVALUATION?eval.?\n","134 \n"," input (English text): Could you provide a list of models that have been tested on the Sequential MNIST benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the MNIST dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Could you provide a list of models that have been tested on the Kuzushiji-MNIST benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kuzushiji-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the MNIST dataset?\n"," output (Sparql query): \n","666\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","For each criterion, you can use one or more subquery pairs to get the score, the metric, etc. of each model (the other side of the GROUP BY - GROUP BY - group BY or GROUP BY - group BY - ORDER BY pattern). See these sections for more information.\n","How to Use the RDFS Benchmark Functions\n","The benchmark functions are useful only to check their validity. So they must not be used indiscriminately. Only use them in cases where their evaluation is required (for example, to evaluate a model that fails the criterion: it will have to be validated).\n","When evaluating a model, if the method used fails the criterion of the benchmark, or if there is a possible mis-match between the source and target or between two benchmark datasets, you should use the rdfs.benchmark function, and not just any old rdfs. Benchmarks are designed to be robust to small, or nonexistent, changes in parameter ordering, to a small change in the data set, to changes in the model inputs, etc. A benchmarking code should include the following steps:\n","Find the metric that you want to evaluate on the benchmark dataset. This is a unique metric used for all other criteria, so use the original value or the reference metric to check for duplicates. Create a query that selects the metric, comparing the original and the new value, and optionally comparing it with the reference metric. If it is the only metric in the table you are using, use the index: SELECT reference_lbl FROM rdfs.benchmark(tbl), tbl WHERE { { reference_lbl!=0?reference_lbl :{ reference_lbl!=1?reference_lbl } }?metric } ORDER BY metric?metric_1\n","Create an index for the new metric on the current model and new model tables\n","135 \n"," input (English text): In what country was conducted research with the largest number of participants?\n"," output (Sparql query): SELECT ?country WHERE { orkgr:R44980 orkgp:compareContribution ?contrib. ?contrib orkgp:P15249 ?country; orkgp:P23169 ?number_of_participants. } ORDER BY DESC( ?number_of_participants) LIMIT 1\n"," input (English text): Which countries are considered in the papers about geopolitics?\n"," output (Sparql query): SELECT DISTINCT ?location WHERE { ?_ orkgp:compareContribution [ orkgp:P32 [ rdfs:label ?label ]; orkgp:P5049 ?location ] FILTER(REGEX(STR( ?label), \"geopoli ?tics\")) } \n"," input (English text): What is the top benchmark result (metric and value) over the dataset Dataset mentions in Social Sciences?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dataset mentions in Social Sciences\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Where did the study with maximal geographic scale take place?\n"," output (Sparql query): \n","492\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Which country was the largest in the study?\n","output (Sparql query): Select country of highest density country in the study Dataset country of largest size country, n country | |------------+---------------------------+----------------------------------------------------------------- Afghanistan Afghanistan Afghanistan Albania Albania Algeria Aland Islands Albania Italy Algeria Angola Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh Barbados Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia and Herzegovina Botswana Brazil Brunei Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Central African Republic Chad Chile China Colombia Comoros Congo Costa Rica Cote D'Ivoire Croatia Cuba Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Fiji Finland France French Guiana French Polynesia Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guinea Guinea-Bissau Guyana Haiti Honduras Hong Kong Hungary Iceland India Indonesia Iran Iraq Ireland Israel Italy Jamaica Japan Jordan Kazakhstan Kenya Kiribati Korea, South Kuwait Kyrgyzstan Laos Latvia Lebanon Lesotho Liberia Libya Liechtenstein Lithuania Luxembourg Macao Macedonia Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mexico Micronesia Moldova Monaco Mongolia Montenegro Montserrat Morocco Mozambique Myanmar Namibia Nauru Nepal Netherlands Netherlands Antilles New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island Northern Mariana Islands Norway Oman Pakistan Palau Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Island Poland Portugal Puerto Rico Qatar Reunion Romania Russia Rwanda Saint Helena Saint Kitts and Nevis Saint Lucia Saint Martin (French part) Saint Pierre and Miquelon Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Solomon Islands Somalia\n","136 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CoNLL04 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL04\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WLPC dataset?\n"," output (Sparql query): \n","532\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Output:\n","Input: output SELECT 'dataset_name', DISTINCT?dataset_name FROM?dataset_tbl WHERE DISTINCT?dataset_name OR\n","Selecting a dataset and running the benchmark test\n","At this stage you should already have the list of files that you want to use in your test. We will use the most popular dataset from the pre-release CSE version (which would represent the main benchmark set for most models) and run the benchmark with this dataset (but using CSE version 0.6.5.0. The dataset is already created):\n","Selecting a dataset that has been checked by the community for being suitable for benchmarking. As there are just a few models we don't want to use from the pre-release CSE, we'll select the WNLI dataset. Selecting a dataset that has been checked by the community for being suitable for benchmarking. As there are just a few models we don't want to use from the pre-release CSE, we'll select the WNLI dataset. output SELECT 'dataset_name', DISTINCT?dataset_name FROM 'WNLI_DATA_COMMUNITY' GROUP BY?dataset_lbl; \n","In this query we are using the filter clause, which is available from the Select statement. As the source dataset is already made for the benchmark we won't have to specify an explicitly given id. If you don't want to specify the source dataset you can leave the id value blank. The query also produces 1 output row. The two rows are in the table below.\n","output SELECT 'dataset_name', DISTINCT?dataset_name FROM?dataset_tbl WHERE?dataset_name = '[WN\n","137 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the TSE-NER dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TSE-NER\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the CoNLL 2012 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the TempEval-3 dataset?\n"," output (Sparql query): \n","501\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SELECT?DISTINCT?paper _lbl WHERE { [email protected]?dataset_lbl.?cont orkgp:HAS_DATASET?dataset.?bench orkgp:P31. ORG cnlt:NEST_DATA_NEXTS?labels_lbl. }\n","output (Sparql query):\n","output (Sparql query): SELECT orkgc:DISTINCT paper, NEST_DATA_NEXTS paper, paper_lbl. FROM ( SELECT DISTINCT?benchmark OR GATHER _lbl. FROM NEST_DATA_NEXTS and cnlt or GATHER _labels_lbl. FROM TempEval-3 ), SELECT?labels_lbl as titles_lbl. FROM CODATE_TMPELEX.TITLE. orkgc:DISTINCT (paper) AS papers. FROM ORkgc JOIN ( SELECT DISTINCT?benchmark OR GATHER _lbl. FROM NEST_DATA_NEXTS ), SELECT?labels_lbl as titles. FROM CODATE_TMPELEX.TITLE. orkgp:DISTINCT (paper) AS papers. FROM ORkgc JOIN ( SELECT DISTINCT?benchmark OR GATHER _lbl. FROM NEST_DATA_NEXTS ), SELECT?labels_lbl as titles. FROM CODATE_TMPELEX.TITLE. orkgp:P31 AS papers. FROM orkgc JOIN ORkgc JOIN TSMGLEX.\n","This result looks like this when compared to the original query:\n","output (Sparql query): SELECT DISTINCT\n","138 \n"," input (English text): Can you list the models that have been evaluated on the SciERC dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the SciFACT dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciFACT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the ART/CoreSC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ART/CoreSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the SciERC benchmark dataset?\n"," output (Sparql query): \n","560\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Please provide a list of parameters that are important for the evaluation of this model.\n","This model is the last one which have been evaluated on the SciFU benchmark dataset. The model was compared against the SciFU benchmark dataset and evaluated with the following parameters: \n","The model uses the same classification algorithm  (Linear Algebra)  that has been used to train the SciFU benchmark dataset. This model uses the non-linear classification  (SVMS regression)  (a classification using a non linear process)  of the SciFU benchmark dataset.\n","This classification system  was developed as well to be used in conjunction with the SciECoC classification (see SciECoC tutorial for more details).\n","The evaluation of this model on SciFU benchmark dataset shows its good performance. The model is able to classify the two training sets, as well as the non-training examples with high accuracy.\n","If the model was used in combination with SciECoC (the main classification system used in the training of the SciFU benchmark dataset)  it has shown better results.\n","The model can also be evaluated on the non-training and training scenarios. This is demonstrated through the following scenarios:\n","(A)  No training took place: no test set is being evaluated. The model is evaluated as a predictor with 0% sensitivity and 100% specificity.\n","(B)  Training takes place but no test set: the model is evaluated as a predictors with 100% sensitivity and 0% specificity.\n","You can choose to perform a training and evaluation on this model.\n","The validation data can be obtained if the model was not used in training. But you do not need to provide the training and validation data.\n","If you want to validate the model: you can use the validation procedure, if you have access to the validation data\n","139 \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the ACL-ARC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the Reuters-21578 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset?\n"," output (Sparql query): \n","824\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["---------------------------\n","| score | model | --------------------------- --------------------- --------------------------- -------- 2012 CoNLL CoNLL 2012 CoNLL CoNLL ndvs:DISTINCT?model?model_lbl. ndvs:label?model_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a ndvs:Dataset = \"CNBC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 }\n","output (HTML (ASC)): HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) : HTML (ASC) > n\n","140 \n"," input (English text): What are the models that have been benchmarked on the IWSLT2015 English-German dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the WMT2014 German-English benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the WMT2016 English-Czech dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Czech\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the WMT2014 English-German dataset?\n"," output (Sparql query): \n","583\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What are the models that have been benchmarked on the WML 2015 English-Polish benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WML2015 English-Polish\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","input (English text): Could you provide a list of models that have been tested on the WMT2015 English-Czech dataset?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2015 English-Czech\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","input (English text): Could you provide a list of models that have been tested\n","141 \n"," input (English text): What is the highest benchmark result achieved on the Yelp-2 dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Yelp-5 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Yelp Binary classification dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Yelp-14 dataset, including the metric and its value?\n"," output (Sparql query): \n","859\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["-------------------------- NAME ------------ -------------------------- Yelp-14 ------------- 1.12.85315 0.07% Yelp-5 1.1.112345 0.05% Yelp-6 3.2.09988 0.02% Yelp-8 1.0.393729 0.01% Yelp-9 0.7.556736 0.01% Yelp-14 0.6.389991 0.01% Yelp-3 0.9.583492 0.00% Yelp-8 0.8.84792 0.00% Yelp-14 0.7.494489 0.00%\n","The Metrics Metric, a common class of metrics, helps us to distinguish between the best and most effective metrics in use in a sample group of queries. Often the metric is based on a number of inputs. The following two queries show how to use the Metric column of a data frame to group each benchmark. Inputs are: name\n","\n","value\n","\n","metric\n","\n","The following dataset has been selected for this example: Yelp-3 (English text) and Yelp-5 (English text)\n","\n","Metric Name Value Metric Yelp-1 1.1.112345 1.1 Yelp-2 1.1.112345 1 Yelp-3 3.2.09988 1\n","\n","input (English text): What is the top benchmark result achieved from the dataset?\n","\n","output (Sparql query): SELECT DISTINCT?name?value?metric_lbl ( MAX(?value) AS?score) WHERE { { SELECT?name?value?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER\n","142 \n"," input (English text): List the code links in papers that use the DDRL A3C model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDRL A3C\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the A2C+CoEX model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A2C+CoEX\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DDQN (tuned) hs model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDQN (tuned) hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?\n"," output (Sparql query): \n","494\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" --------- --------- --------- --------- --------- --------- --------- --------- --------- --------- --------- --------- --------- --------- --------- --------- --------- ---------\n","input (English text): Add each paper that uses the A3C FF hs model at any benchmark to an RDF table\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model? rdfs:labels??lbl? FILTER (str(?lbl ) =?model_lbl)?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }  input (English text): Add this paper to an RDF table\n","output (Sparql query): ------------------------ --------- ----------- --------- ------- A3C FF hs A3C FF hs A3C FF hs A3C FF hs -------------- ----------- --------- ------- 5 (2.00) 22 (1.92) 5 (2.00) 0 ( 0.02) 0 ( 0.0) A3C FF hs A2C + CoEX A3C FF hs\n","input (English text): Add this paper to an RDF table\n","output (Sparql query): ------------------------ --------- ----------- --------- ------- A3C FF hs A3C FF hs A3C FF hs -------------- ----------- --------- ------- 5 (1.88) 26 (1.71) 6 (3.50) 1 ( 0.02) 0 ( 0.0) A3C FF hs A3C FF hs A3C FF hs -------------- ----------- --------- ------- 6 (3\n","143 \n"," input (English text): What is the best performing model benchmarking the RTE dataset in terms of Accuracy metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RTE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RTE dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RTE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the MedNLI benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the RTE benchmark dataset?\n"," output (Sparql query): \n","625\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What is one of the best performing models benchmarking the RTE dataset in terms of Accuracy metric?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RTE\")?benchmark orkgp:HAS_DATASET?dataset?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?metric a rdfs:label?metric_lbl. } } } \n","output (Sparql query): output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RTE\")?benchmark OR kgp:MODEL?model_lbl. } ORDER BY DESC(?name) LIMIT 1 \n","input (English text): Could you provide a list of research papers along with their titles and IDs, that have performed benchmarks on the RTE dataset?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FIL\n","144 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the WOS-11967 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-11967\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the WSC dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the HMDB51 dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value?\n"," output (Sparql query): \n","862\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------ SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WOS-5736\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?ob orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?ob orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } --------------------------- ORDER BY DESC(?value)\n","input (English text): What is the highest benchmark result achieved on the WSK data, including the metric and its value?\n"," output (Sparql query): ------------------------------ SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WSK\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:H\n","145 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Supervised: dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Natural Questions (long) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?\n"," output (Sparql query): \n","534\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WebQuestions\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?\n","output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WebQuestions\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }  Note the extra step in the second query.\n","input (New): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WebQuestions\")?benchmark orkgp:HAS_D\n","146 \n"," input (English text): List the metrics that are used to evaluate models on the SciERC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the COPA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"COPA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the ARC (Easy) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Easy)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset?\n"," output (Sparql query): \n","518\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" select id, metrics_in_sample, metrics_count, metrics_lbl_count, metric_lbl for  each  item from metrics.metrics_in_sample_to_metrics.  select metrics_in_sample  as measurement_name, metalsc_in_sample_to_metalsc.  metrics_count  as metric_name, metric_lbl_count  as measurement_name,  metrics_lbl  as metric_lbl_id  as metrics_count, metrics_lbl_lbl-count, metrics_lbl_lbl_id  as metric_lbl_lbl  as measurement_name,  metrics_lbl-lbl-count  as measurement_name  from  metrics.metrics_with_counters_to_metrics.  metrics_count,  metrics_lbl-count  as measurement_name  to  metrics_lbl-all  (metrics_lbl_id,  metrics_lbl-count),  metrics_lbl-count,  metrics_lbl-lbl-id,  metrics_lbl-lbl-count from  metrics.metrics_with_counters:  metrics_lbl_counters  to  metrics_lbl-all  (metrics_lbl_id,  metrics_lbl-count),  metrics_lbl-count,  metrics_lbl-lbl-id,  metrics_lbl-lbl-count from \n","147 \n"," input (English text): Could you provide a list of models that have been tested on the Natural Questions benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the Natural Questions (long) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the WebQuestions dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WebQuestions\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the Natural Questions (short) dataset?\n"," output (Sparql query): \n","554\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 38\n","148 \n"," input (English text): What is the highest benchmark result achieved on the WMT2014 English-French dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset WMT2014 German-English?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 English-German?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the WMT2014 French-English dataset?\n"," output (Sparql query): \n","875\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 French-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  SELECT a, b as  , c as  , d as  , e as  and f  from? ( WMT2014 French - English,  select nlfrom? ( WMT2016 English - German, select nlfrom? ( WMT2016 German - English, select nl ) from? ( WMT2012 Spanish and Portuguese, select nlfrom? ( WMT1984 Italian and  English, select nl ) from? ( TEXAS Spanish and Portuguese, select nlfrom?( TEXAS Italian and Italian, select nl ) from? ( ATLAS, select nlfrom\n","149 \n"," input (English text): What is the best performing model benchmarking the WikiText-2 dataset in terms of Validation perplexity metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Validation perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Perplexity score when benchmarked on the LAMBADA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"LAMBADA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Validation perplexity score when benchmarked on the Penn Treebank (Word Level) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Validation perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Word Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Test perplexity score on the WikiText-2 benchmark dataset?\n"," output (Sparql query): \n","849\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT * FROM WikipaediaText2 rdfs:value?model_lbl LEFT JOIN WikiText2 rdfs:value?model_lbl LEFT JOIN WikipaediaText rdfs:label?model_lbl WHERE model_lbl.test_refresh < 10 orkgc:Metric; rdfs:label?metric_lbl. LEFT JOIN WikiText2 rdfs:label?model_lbl LEFT JOIN WikipaediaText rdfs:value?model_lbl LEFT JOIN WikiText2 rdfs:label?model_lbl LEFT JOIN WikipaediaText rdfs:label?model_lbl LEFT JOIN WikipaediaText rdfs:label?model_lbl LEFT JOIN WikiText2 rdfs:label?model_lbl LEFT JOIN WikiText2 rdfs:label?model_lbl LEFT JOIN WikipaediaText rdfs:label?model_lbl LEFT JOIN WikiText2 rdfs:label?model_lbl LEFT JOIN WikiText2 rdfs:label?model_lbl LEFT JOIN WikiText2 rdfs:label?model_lbl LEFT JOIN WikiText2 rdfs:label?model_lbl LEFT JOIN WikiText2 rdfs:label?model_lbl LEFT JOIN WikiText2 rdfs:label?model_lbl LEFT JOIN WikiText2 rdfs:label?model_lbl LEFT JOIN WikiText2 rdfs:label?model_lbl LEFT JOIN WikiText2 rdfs:label?model_lbl LE\n","150 \n"," input (English text): List the code links in papers that use the All-attention network - 36 layers model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"All-attention network - 36 layers\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the All-attention network (36 layers) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"All-attention network (36 layers)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the All-attention network (18 layers) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"All-attention network (18 layers)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the All-attention network - 18 layers model for benchmarking purposes?\n"," output (Sparql query): \n","511\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY (0 hits)\n","N-Bounded Dirichlet All-Attention N-Bounded Dirichlet All-Attention N-Bounded Dirichlet All-Attention 2 rows selected (0.00 sec) \n","It's a really easy question to answer! We can see a lot of references to the All-attention network in the papers that use the algorithm using the \"lbl\" attribute in the list of comments generated. There are many other references to the All-attention network such as: http://cisinst.nlm.nih.gov/cisinst/doc/lbl-1.0.pdf http://cisinst.nlm.nih.gov/cisinst/doc/lbl-2.pdf\n","I will discuss this more in depth here on Code Snippet, so take a look!\n","In summary, if we have the following model with the following input, we can use it in the same benchmarking model as the All-attention network:\n","model = All-attention ; model ( nrows = 24, ncols = 28 ) ;\n","and we use the nrows attribute to select all the lines where the character \"A\" (or the character in a similar way, in case of the All-attention network) appears:\n","$ hspec --batch # Hspec is a matcher for Hspec, so we can use it to see if the results match\n","$ hspec model test_3.hs\n","# Model is the one with all the rows from the paper as parameters\n","Rows with type ( a, b, c ): a b c\n","As we can see, the model has been tested against the same dataset. If we try to use model in a specific benchmarking model:\n","# Model is the one with\n","151 \n"," input (English text): what are the evidence and limitations of paper \"Evaluating the Use of Social Networks in Author Name Disambiguation in Digital Libraries\"?\n"," output (Sparql query): SELECT ?evidence ?evidence_title ?limitation WHERE { orkgr:R6751 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5004 ?evidence; orkgp:P5006 ?limitation. ?evidence rdfs:label ?evidence_title. FILTER(REGEX(STR( ?paper_title), \"Evaluating the Use of Social Networks in Author Name Disambiguation in Digital Libraries\", \"i\")) } \n"," input (English text): Provide a list of benchmarked datasets related to the co-authorship prediction research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"co-authorship prediction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the metrics used by paper \"Using NMF-based text summarization to improve supervised and unsupervised classification?\n"," output (Sparql query): SELECT ?metrics, ?metrics_labels WHERE { ?papers rdfs:label ?title. FILTER(REGEX( ?title, \"NMF-based text summarization\")) ?papers orkgp:P31 ?cont. ?cont orkgp:P34 ?eval. ?eval orkgp:P2006 ?metrics. ?metrics rdfs:label ?metrics_labels. } \n"," input (English text): Does paper \"Disambiguating authors in citations on the web and authorship correlations\" employe Cosine similarity?\n"," output (Sparql query): \n","499\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT?disambiguation?disambiguation_labels.?disambiguation DESC.?disambiguation_labels FROM?disambiguation,?contr.?disambiguation_labels AS?labels,?eval.?eval AS?eval_labels WHERE {?criteria? OR?criterion? } ORDER BY criterion, criterion_id, criterion DESC. FOREIGN(criterion_id)\n","input (English text): Describe the definition of \"degree cosine similarity\"?\n","output (Sparql query): SELECT?scale,?scale_values,?criterion_ids,?criteria_labels,?criterion_id,?criterion_values,?criterion_labels_id,?criterion_labels_labels FROM?sloan.constrained_scores AS?sloan_constrained_scores,?review.scores AS?review_criteria,?reputation.criterion_ids,?reputation_criterion_values,?reputation_criterion_labels AS?criterion_labels,?reputation.reputation_criterion_values AS?criterion_values; FOREIGN LABEL; \n","input (English text): What are the metrics employed to determine degree and scale values?\n","output (Sparql query): \n","output (Sparql result): \n","output (Scalar result): \n","output (Sparql result): \n","Output format: ------------ ------------ -------------\n","Bibliographic metadata: ------------- Author: ____ ____ Authors: ____ ____ Title: ____ ____ Keywords: ____ ____ Subgenre: ____ ____ Rep\n","152 \n"," input (English text): Can you list the models that have been evaluated on the SciERC dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the ARC (Challenge) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Challenge)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the ScienceIE dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the STS Benchmark dataset?\n"," output (Sparql query): \n","558\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Could you provide a list of models that have been evaluated on the STS dataset?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STS\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","input (English text): Could you provide a list of models that have been evaluated on the COSMOS dataset?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"COSMOS\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }  \n","input (English text): Could you provide a list of models that have been evaluated on the COSMOS dataset?\n","output (Sparql\n","153 \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Phoenix benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Phoenix\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asterix benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asterix\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Alien benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Alien\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Freeway benchmark dataset?\n"," output (Sparql query): \n","821\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Freeway\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } \n"," output (Sparql query): select 1.85 as scores from dbo.score where score=1.35 and score < 10 select 3.00 as scores from dbo.score where score=0.95 and score < 10 select 2.80 as scores from dbo.score where score=4.75 and score < 10 display counts score for score >=1\n","This sample shows the results:\n","154 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the DDI dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DDI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the BC5CDR dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the DCASE dataset?\n"," output (Sparql query): \n","493\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","output_file (Sparql file): Output file of benchmark titles\n","The output_file is an ordinary output file. You can view it in any text editor and read in the names and titles of the papers in a file. The names of the papers are enclosed in square brackets. The titles are quoted: the quotes are necessary because the title consists of two parts, one part is a header and one part is called the body.\n","Once created, the output_file may be stored with other variables, but will not be automatically reloaded once you use the new version of PARSE.\n","If the names contain spaces, you can use double quotes. For the time being, you can only specify the first names. The names of the papers may be nested to a depth of 64. You can only specify one number in the body and each number must be enclosed in brackets.\n","We can include a number by using one character. When using numbers, you can also use a trailing slash (/) to give them an index:\n","output_file (Sparql file): Output file of benchmark titles\n","We can add as many lines and names in between as we need. The columns in the output file (if present) are referred to as lines. The lines must all start with a newline in between the characters which make up the line's name (i.e., ).\n","You must not use the \\ character to start a line.\n","The format of the output text is as follows:\n","[TITLES] [RESULTS] [RATES] [DESCRIPTIONS]\n","The first title must be one of the formats:\n","d or ddataset or dclarifies\n","The name of the data set depends on the dataset type. In a ddataset, the name is a dash followed by one or more d or ddataset\n","155 \n"," input (English text): What is the best performing model benchmarking the Yelp Fine-grained classification dataset in terms of Error metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): List the metrics that are used to evaluate models on the Yelp Binary classification benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the Yelp-2 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?\n"," output (Sparql query): \n","629\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What metrics are commonly used when benchmarking models on the BigQuery dataset?\n","output (Sparql query): \n","(Optional, default True) Show the source code that comes with the benchmark or a link to the source code that comes with the benchmarking script\n","output (Sparql query): \n","Note: If the script is to be run by the user, it must be run using a shell (i.e., by /bin/bash )\n","option (string): The name of the method. If null, the benchmark method is used.\n","metadata: The metadata to attach to the output.\n","If the source file is not found, an error is raised with either no error message or one that will display some informative log message or error string.\n","The default method is benchmark. Benchmark works with the Yelp BigQuery dataset and the standard datasets (Yelp-2, BigQuery-50). It is highly recommended to use the benchmark. Benchmark. You must run it only once.\n","If you do not need to benchmark, please use the Benchmark annotation instead.\n","The code that results in the benchmark is usually pretty straightforward: the first row of the response is a call to the benchmark method. By default, the first row of the response is the query string itself. However, if your app requires a response with other fields, such as the data from a specific feature such as a location, you must include the field name. For example:\n","SELECT {?id int64, {?id?location : {?locationId?? '1'}}*} WHERE {?metric?meter.?value*; } END; // Note: This example uses only one metric, the \"meter.\" It does not need other fields to be specified # include('http://stackoverflow.com/\n","156 \n"," input (English text): Which model has achieved the highest Score score on the Atari 2600 Battle Zone benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Score score on the Atari 2600 Amidar benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Score score on the Atari 2600 Krull benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Krull\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Score score on the Atari 2600 River Raid benchmark dataset?\n"," output (Sparql query): \n","809\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note how there is a lot of duplication in each of the SELECT statements. This is where I usually save the results I have generated.  The output (Sparql query) here is slightly different, and contains some duplicates from the original output (Sparql query).  In this particular case, I am not interested in using the same label names (a and kgc ) as those in the original output (Sparql query).  This is because the output of the original code is so long that I can easily figure out in which layer of the graph to look for the results for a given label.  I would probably use some index on the model label to try to limit what parts to save. After running: SELECT (?value) FROM (SELECT id(),?value() AS c,?value() AS g,?value() AS n,?value() AS r,?value() AS m,?value() AS p,?value() AS d,?value() AS r2 FROM test.dataset t WHERE r =?r2 AND t.labels[id] =?s or t.labels[id] =?c OR n =?n OR p =?p AND d =?dt AND m =?m1 AND o =?o1 AND s =?o2 OR r2 =?d2)) as t FROM (SELECT?name AS n FROM t.datasets c, d, o) t GROUP BY (?name) ORDER BY (?name) DESC LIMIT 1,200 I got this: SELECT?name?(s,c,dbtype)) AS?name,?name?dbtype,?name? c d o n t,?name?(s,c,dbtype)) AS?name,?name?dbtype,\n","157 \n"," input (English text): Provide a list of papers that have utilized the BertSumExt model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BertSumExt\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the NCBI_BERT(base) (P+M) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"NCBI_BERT(base) (P+M)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the BERT-based binary sentence classifier model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERT-based binary sentence classifier\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the BERT + BiLSTM + CRF Decoding model and include the links to their code?\n"," output (Sparql query): \n","520\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Provide a list of papers that have utilized the BERT-based binary sentence classifier model and include the links to their code? output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BERT-based binary sentence classifier\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the BERT-based binary sentence classifier model and include the links to their code? output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BERT-based binary sentence classifier\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the BERT-based binary sentence classifier model and include the links to their code? output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER\n","158 \n"," input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the BIOSSES benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Pearson Correlation\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BIOSSES\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the PubMedQA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMedQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Pearson Correlation score on the STS Benchmark benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Pearson Correlation\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the MedSTS benchmark dataset?\n"," output (Sparql query): \n","827\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Pearson Correlation\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MedSTS Benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","In addition to the benchmark results, the dataset contains all the records within the sample period. For each RDD-statement of the benchmark dataset, there are all the records that satisfy the particular pattern. For example, the pattern \"STS\" is defined by the following RDD-statement:\n","SELECT a.id, r.m.id 'STS' AS 'Stored Variance', r.m.s AS 'Stored Measure', a_score AS 'Score (score' FROM ( SELECT a.id as index_id FROM index ) r dms WHERE id = 1 LIMIT 5 ) ; [A] OR (SELECT a.id as index_id FROM index ) r dms WHERE id = 1 LIMIT 5 ) ; [\n","159 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the WOS-11967 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-11967\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-5736 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the WSC dataset?\n"," output (Sparql query): \n","508\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------- SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WSC\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark. } \n","The list of all papers that include a benchmark in their title or description can be obtained using:\n","SELECT * FROM?DATA;\n","Which will yield output similar to:\n","Name Title ID 1 WJG-0114_Sparql_Benchmark_S.C (2009): WOS-11967 Benchmark for Sparse Linear Regression (WSC) and Sparse Polynomial Regression (SPR) 2 WSC-11067_Sparql_Benchmark_S.B (2010): WSC-11517 Benchmark for Linear Regression with Sparse Polynomial Regression (WSC-2) 3 WSC-12097_Sparql_Benchmark_S.A (2011): WSC-12072 Benchmark for Linear Regression with Sparse Polynomial Regression (WSC-1) 4 WSC-11273_Sparql_Benchmark_S.B (2010): WSC-11201 Benchmark for Linear Regression w/ Sparse Polynomial Regression 5 WSC-11217_Sparql_Benchmark_S (2010): WSC-10957 Benchmark for Linear Regression with Sparse Polynomial Regression 6 WSC-10667_Sparql_Benchmark_S.B (2011): WSC-10649 Benchmark for\n","160 \n"," input (English text): List the code links in papers that use the CRF with term expansion model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CRF with term expansion\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the CRFSuite model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CRFSuite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the BiLSTM-CRF model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM-CRF\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the linear-chain CRFs model in any benchmark?\n"," output (Sparql query): \n","484\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {!model orkgc:Model?model_lbl. FILTER (str(?model_lbl) = \"CRF with linear-chain\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the LSTM-CRF model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {!model orkgc:Model?model_lbl. FILTER (str(?model_lbl) = \"LSTM-CRF\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","output (S Parql Query) \n","input (English text): Does the BLAS or LAPACK library support the term-expansion model?\n","output (Sparql queries) SELECT DISTINCT?model FROM?BLAS, BLAPACK, CRF, DATASET, DATASET3, HASH, SIZE, SIZE3 ;\n","output (Sparql query)\n","input (English text): List the term-expansion model links in papers that benchmark the term-expansion model?\n","output (Sparql queries) SELECT?term,?code.|\n","161 \n"," input (English text): Can you provide links to code used in papers that benchmark the DrQA (Document Reader only) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DrQA (Document Reader only)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DyGIE model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DyGIE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DY-ResNet-10 model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DY-ResNet-10\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DrQA model in any benchmark?\n"," output (Sparql query): \n","483\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DrQA\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","Note that we still have to parse the text that we are displaying, but with some very basic syntactic transformations (this will be the next iteration). All in all, this step (for all the papers that we are basing on) is probably the most complicated bit of parsing/dictionary resolution in our paper.\n","Conclusion\n","So there it is. This paper has done an excellent job of demonstrating that DyGIE-CNNs can be used as \"big data\" indexing algorithms. Not only have the authors done great work from a mathematical standpoint but we have also made an awesome progress towards a future that can use this type of network system for something that is really useful.\n","In our paper we are using a combination of both the most current and most advanced neural networks of today and with the help of the GPU we are basically able to do such high resolution indexing, clustering and classification at the same time.\n","Now we may very well be able to use these network architectures for things that are useful on a larger scale like speech, language translation, natural language processing, image recognition, database indexing... and of course things like machine learning in other domain like image recognition, machine learning in video, image analysis, computer vision, machine learning for various kinds of natural language processing, machine learning for various kinds of video, etc\n","162 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset SST-5 Fine-grained classification?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Could you provide a list of models that have been tested on the SST-5 Fine-grained classification benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Which model has achieved the highest Top 5 Accuracy score on the ObjectNet benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top 5 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset?\n"," output (Sparql query): \n","768\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): If there is no model that meets the criteria above, please provide the dataset that you chose.\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl; /* DISTINCT?metric */ SELECT DISTINCT?model?model_lbl; /* DISTINCT?model_lbl */ \n","output (Sparql query): \n","input (English text): As a proof that your use of the following options work, please supply a copy of your code to be used for this benchmark-related analysis\n","output (Sparql query): \n","input (English text):\n","output (Sparql query):\n","/* DISTINCT?dataset a OR kgc:Dataset: */ SELECT DISTINCT?dataset_lbl; /* DISTINCT?dataset_lbl */ \n","output (Sparql query): \n","input (English text): Please fill out this short form to access the data that was used to generate this output\n","163 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Challenge)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?\n"," output (Sparql query): \n","525\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label orkgp:HAS_DATASET orkgp:HAS_EVALUATION orkgp:HAS_METRIC;...?metric rdfs:label orkgp:HAS_METRIC;...?metric rdfs:label orkgp:HAS_METRIC } \n","input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?\n","output (Sparql query): \n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label orkgp:HAS_DATASET orkgp:HAS_EVALUATION orkgp:HAS_METRIC;...?metric rdfs:label orkgp:HAS_METRIC;...?metric rdfs:label orkgp:HAS_METRIC } \n","RDDs were processed by extracting the RDF fields from the model. The RDF fields include the model identifier (e.g. rdf:id) and the attributes of the model. For example, the rdf:method is used for the 'acceleration' feature of the model. All the model fields including the parameter names are processed.\n","The following queries are included in the RDBMS example.\n","RDBMS RDFM-Binary::Targets::ACL-ARC or DLTRANS::ACL ARC-ACLIPSE::Targets::AC\n","164 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Quora Question Pairs dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Quora Question Pairs\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the WebQuestions dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WebQuestions\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Which QA benchmarks incorporating more than 10000 questions?\n"," output (Sparql query): \n","850\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"QA\" )?benchmark;?exec orkgp:HAS_MAXEST?value } } ORDER BY DESC(?value) } }\n","(optional)                SELECT COUNT(?) AS test1?, TEST2 AS test2? ORDER BY 2?                -1                -1 ORDER BY 2?\n","You need to be logged in to change this field.\n","Input (English text): Can you provide the highest \"test\" score (including the metric and score) for the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TriviaQA\" )?benchmark;? exec orkgp:HAS_MAXEST?value } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n","input (English text): Can you provide the highest \"test\" score (including the metric and score) for the TriviaQA dataset?\n","output (Sparql query): Where are you? Can you provide the highest? (\n","165 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Atlantis dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Amidar dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Ms. Pacman dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset?\n"," output (Sparql query): \n","531\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Berzerk\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","166 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Freeway dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the RACE dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RACE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Stanford Dogs dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Dogs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Stanford Cars dataset?\n"," output (Sparql query): \n","508\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","select input (English text); output (string):\n","output (integer):\n","end ;\n","end\n","\n","\n","Here is a complete Scala program for the data query analysis:\n","\n","import scala.collection.*; import scala.RDFS.BaseList; /* Scala code to perform the data extraction. This example uses RDFS to fetch information for the following datasets: http://datasets.stanford.edu/, which is a list of datasets of data related to Stanford's Stanford Cars dataset, which is a list of cars on the Atari 2600 game console, which is a list of cars on the Macintosh computer, and where to find the corresponding data files for these datasets. */ @SuppressWarnings(\"unused\") class Scape { def extract_dataset(dataset: Scape): Scape = { val data = Scape.withData(data) val name = Scala.String.parse(data.getText()) val title = Data.getLink(\"Stanford Cars\", dataset) val benchmark = Data.getLink(\"Stanford Cars\", data) val result = result.filter { _ => Benchmark(title, benchmark) } val result_id = RDFS.dstRDFS(result).dstRDFS(RDFS.dstRDFS).map(data => data.getId) val results = new List[RDFS.Data]() val results_id = result._id.toString val res = result_id.toList.map(data => data.getTitle.trim()) for (i in 1..(res.size - 1)) { if (i % res.length) { { val id = res[i].getId.toString val name = res[i].getName val benchname = res[i\n","167 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC Chinese-to-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Chinese-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BUCC French-to-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the BUCC Russian-to-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset?\n"," output (Sparql query): \n","529\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","To find out what is possible to create a list of all benchmarks on Chinese-to-English, please refer to the following sample program: \"Benchmark.py from __future__ import print_function\" For more details on Benchmark.py, please refer Chapter 18 : Benchmarking your dataset.\n","4. Benchmark Your Dataset\n","4.1. Benchmarking your dataset on Chinese-to-English\n","To benchmark your dataset on Chinese to English and/or on other languages, we are using the \"Benchmark.py from __future__ import print_function\" on your dataset (available from https://www.dropbox.com/s/u1oq7a6t9yjt52w/benchmark.py?dl=0):\n","$./benchmark.py FROM Chinese-to-English AS ASID1 TO Chinese-to-English ASX1...\n","This program prints out an output like the following:\n","Output from Benchmark.py on a list of Chinese-to-English.\n","4.2. Benchmarking the Chinese-to-English dataset with your favorite data scientists\n","If you have the appropriate Python package and a dataset, you can benchmark your dataset. Let's use the same paper mentioned in the previous section, A Scatterplot: Chinese-to-English Data Visualization, with a paper that is in turn of the original paper. To do this, we will use the benchmark.py from the previous section. This program prints out an output like the following:\n","Now we are in position to use the following two programs that have been created for that: r2py on romanian- to-english and tk on french- to-english.\n","Here are the instructions for the program from r2py, which is available from http://www.r2py\n","168 \n"," input (English text): What is the top benchmark score and its metric on the Stanford Cars dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the metrics of evaluation over the CIFAR-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Classic dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classic\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Stanford Cars dataset?\n"," output (Sparql query): \n","611\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Stanford Cars\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Stanford Cars\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n","output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Stanford Cars\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS\n","169 \n"," input (English text): What is the highest benchmark result achieved on the ShARe/CLEF eHealth corpus dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Automatically labeled Medline abstracts corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ShARe/CLEF eHealth corpus dataset?\n"," output (Sparql query): \n","655\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Inputs and outputs for a simple query. Outputs are for a query to determine the counts for each term.\n","The function also accepts a comma separated list of terms and the count which is the sum of the counts for the term(s).\n","The function also accepts the count which is the sum of the counts for the term(s).\n","outputs_term: \"SELECT?fnc?dataset, {term term_lbl}?score FROM #%datasets and #%posts AND %post_lbl. ORDER BY 1 desc\", output\n","The function also accepts a comma separated list of terms and the counts which are the sums of the counts for the term(s). An example function that uses these counts for a query that selects the title from the Surname entity list for each posting and the counts for the post which generated that title is the following:\n","input(text):\n","\"SELECT Surname?title FROM Posts #%posts ORDER BY title DESC LIMIT 5\" and output(surname)\n","output(surname)\n","Inputs and output for a simple query. Outputs are for a query to determine the counts for each term.\n","The function also accepts the counts which are the sum of the counts for the term(s). An example function that uses these counts for a query that selects the title from the Post entity list for each site is the following:\n","input(text):\n","\"SELECT HINT?posts?title FROM Articles #-posts ORDER BY title DESC LIMIT 5\"\n","170 \n"," input (English text): What is the highest benchmark result achieved on the Finger, spin (DMControl500k) dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Finger, spin (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset Finger, spin (DMControl100k)?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Finger, spin (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the research problems Vernier Effect is related to?\n"," output (Sparql query): SELECT DISTINCT ?problems, ?problems_labels WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, \"Vernier Effect\", \"i\")) ?papers orkgp:P31 ?contrib. ?contrib orkgp:P32 ?problems. ?problems rdfs:label ?problems_labels. } \n"," input (English text): For what piezoresistive material the least response time was obtained?\n"," output (Sparql query): \n","732\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (Evaluation method) input (language): Where was the evaluation performed?\n","output (Sparql call) output (RDF2) output (Text format) output (RDF):\n","source (RDF) source (text) text (RDF) text (RDF2)\n","RDF 1.1 (January 2009)\n","RDF 1.1 (October 2009)\n","RDF 1.1 (June 2009)\n","RDF 1.1 (July 2009)\n","RDF 1.1 (April 2009)\n","RDF 1.1 (May 2008)\n","RDF 1.1 (June 2008)\n","RDF 1.1 (March 2008)\n","RDF 1.1 (October 2007)\n","RDF 1.1 (June 2007)\n","RDF 1.1 (June 2006)\n","RDF 1.1 (January 2006)\n","RDF 1.1 (December 2003)\n","RDF 1.1 (May 2001)\n","RDF 1.1 (February 2001)\n","RDF 1.1 (May 2000)\n","RDF 1.1 (February 2000)\n","RDF 1.1 (October 1997)\n","RDF 1.1 (February 1997)\n","RDF 1.1 (November 1997)\n","RDF 1.1 (October 1996)\n","RDF 1.1 (March 1996)\n","RDF 1.1 (November 1993)\n","RDF 1.1 (May 1991)\n","RDF 1.1 (December 1979)\n","RDF 1.1 (February 1979)\n","RDF 1.1 (November 1977)\n","RDF 1.1 (September 1976)\n","RDF 1.1 (January 1975)\n","RDF 1.1 (November 1973)\n","RDF 1.1 (July 1972)\n","RDF 1.1 (May 1971)\n","\n","171 \n"," input (English text): List the code links in papers that use the GPT-3 model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GPT-3\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the GPT-3 (Zero-Shot) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GPT-3 (Zero-Shot)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the GPT-3 (few-shot) model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GPT-3 (few-shot)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark?\n"," output (Sparql query): \n","498\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"GPT-2 (small)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Where can I find code references in papers that have used the GPT-2 (small) model for benchmarking purposes?\n"," output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"GPT-2 (small)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Where can I find code references in papers that have used the GPT-2 (small) model for benchmarking purposes?\n"," output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"GPT-2 (small)\")?benchmark orkgp:HAS_DATASET?dataset.?\n","172 \n"," input (English text): List the metrics that are used to evaluate models on the Atari 2600 Seaquest benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Atari 2600 Pitfall! benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Pitfall!\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Atari 2600 Robotank benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Robotank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Atari 2600 Fishing Derby benchmark dataset?\n"," output (Sparql query): \n","530\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): List the metrics that are used to evaluate models on the Atari 2600 Grognard challenge dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Grognard\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): List the metrics that are used to evaluate models on the Atari 2600 Handytest challenge dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Handytest\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): List the metrics that are used to evaluate models on the Atari 2600 Grognard challenge dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?\n","173 \n"," input (English text): What are the models that have been benchmarked on the ImageNet V2 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet ReaL dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the best performing model benchmarking the ImageNet dataset in terms of Number of params metric?\n"," output (Sparql query): \n","761\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","output (Word Cloud):\n","As seen in the Word cloud, the models performing the best are also the ones with a large number of params. This was observed by using the OPTIONAL and ORDER BY keyword for the top-performing model output.\n","Also, the data from the pre-processed RDFS data provides a good representation of the input data and therefore, we do not really have to compute the value of the metric to be used as a metric.\n","This post was inspired from the paper by Andrew Gerrand et al, \" An Analysis of the Top-Covered ReaT datasets\", with comments by David A. Chappell.\n","For more information, see: Andrew G. Gerrand, E.W. Bostwick, and David M. Chappell. \" An Analysis of the Top-Covered ReaT Datasets \" in\n","The Open Access article and pre-processing scripts can be found here.\n","Hope that helped. Please use the comment form below to express your opinions on the post.\n","174 \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Amidar dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Seaquest dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Berzerk dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset?\n"," output (Sparql query): \n","565\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","optionally (optionally?) output text for output data, can be either text\n"," (in this case, only the text will be output)\n","Input data:\n","input (English text): \n","input (English text, with optional space between model name and name of the column):\n","input (English text): \n","input (English text, with optional space between model name and model columns):\n","output (Sparql query):\n","output (Sparql query) \n","input (English text)  \n","output (Sparql query)  input (English text) \n","input (English text)  output (Sparql query) \n","input (English text) data-seqs  - \n","for the nongenetic data seq\n"," {      (str(model) = \"?\" & \":\")  // <- This will be set using the model's name (sans  \":\") model = \"Atari 2600 Berzerk\"; data-seqs  \n","}\n","output (Sparql query):  print(\":')\",     model.id,  print(\"\",\":\") models:     id        col  =  \"\"  ;  model.id    col  =  \"@\"  ;  table  =   \"Berzerk\"  from   \"Berzerk_N\",     \"Berzerk_H\"  for   \"Berzerk_H\",.  &  (c   {.&  (c  . \":=:\") . &  model.name);    row  =   \"@\"  ;  row    =   \"@\"  ;  show  \n","175 \n"," input (English text): Can you provide links to code used in papers that benchmark the BiT-S model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-S\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BiT-L model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-L\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the BiT-M (ResNet-152x4) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-M (ResNet-152x4)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the BiT-M model in any benchmark?\n"," output (Sparql query): \n","491\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT code, rdfs, code_link\n","from?code\n","select rdfs._code_links as rdfs_code_links, rdfs._code_links_references\n","from.rdfs\n","select code_link as rdfs_code_links_references, rdfs._code_link\n","from.code_link\n","join?code_link in code\n","select rdfs.code_link as rdfs_code_link, rdfs._code_link\n","from.code_link\n","from rdfs.code_link.c_codecode\n","where rdfs.code_link = rdfs._code_link\n","select code_link.code = rdfs.code_link\n","from c_codecode in c_codecode\n","where rdfs.code_code = c_codecode\n","select rdfs.code_link._code as rdfs_code_link_references, code_link.code._code as rdfs_code_link_references\n","from.code_link and c_codecode in c_codecode\n","where rdfs.code_code = c_codecode and rdfs.code_link._code in c_codecode\n","select rdfs.code_link.code = rdfs.code_link._code\n","from c_codecode in c_codecode\n","where rdfs.code_code = c_codecode and rdfs.code_link.code_link in c_codecode\n","where rdfs.code_code = c_codecode and rdfs._code in c_codecode\n","select rdfs.code_link._code as rdfs_code_link_references\n","176 \n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Space Invaders dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Space Invaders\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Time Pilot dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Time Pilot\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Pitfall! dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Pitfall!\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?\n"," output (Sparql query): \n","871\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------------------- { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Star Gunner\")?benchmark orkgp:HAS_DATASET?dataset; if (?dataset_lbl.label_cont = \"Atari 2600 Star Gunner\"){ select?labels_lbl.dataset }else{ select?labels_lbl?labels_lbl.dataset }?c }?p } } ------------------------- { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Star Gunner\")?benchmark orkgp:HAS_DATASET?dataset; } ORDER BY?benchmark {?s : \"Benchmark?\" } } } ------------------------- { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset>?{?data[\"data2\"][\"attrs\"]}} { if?data[\"data2\"][\"attrs\"]&&(?rs<0?dataset.length:?rs++){ select?labels_lbl.dataset }else{ select?labels_lbl?labels_lbl.dataset }?c }?p } ------------------------- { SELECT?metric?metric_lbl?value WHERE {?dataset\n","177 \n"," input (English text): List the code links in papers that use the NAS Cell model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"NAS Cell\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Ning et al.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the S-NLI model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"S-NLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the NASCell model?\n"," output (Sparql query): \n","465\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","How do you know the code is based on a paper that does not use the NASCell model?\n","Answer: The code in the relevant papers and tables is based on the NASCell model.\n","For all models (Ning/S-NLI/NASCell/Nested/UDFs) the following fields are available:\n","?code:This field will contain the name of the paper, the source of the data and the benchmark code in the paper. For each benchmark the number of rows reported in the benchmark table is compared to the corresponding data in the original data set.\n","?columns: These columns contain the results of individual tests to achieve the benchmark.\n","?result: These columns contain the benchmark code for each benchmark.\n","For all models (Ning/S-NLI/NASCell/Nested/UDFs) the following fields are available:\n","?code: This field will store the name of the benchmark.\n","?columns: These columns contain the results of individual tests to achieve the benchmark.\n","?result: These columns contain the benchmarks.\n","For all models (Ning/S-NLI/NASCell/Nested/UDFs) the following fields are available:\n","?code: This field will contain the name of the benchmark.\n","?columns: These columns contain the results of individual tests to achieve the benchmark.\n","?result: These columns contain the benchmarks.\n","And the following fields which are available for each benchmark:\n","?dataset: A link to a specific directory on the net for the benchmark code.\n","?source code: The code that the paper implements.\n","?benchmark code: The code that is used to benchmark the benchmark.\n","For all models (Ning/S-NLI/NASCell/Nested/UDFs) the following fields are available:\n","\n","178 \n"," input (English text): Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"TDMS-IE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-TDN(ResNet-101) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM-TDN(ResNet-101)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the CitClus model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CitClus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the TCN model for benchmarking purposes?\n"," output (Sparql query): \n","494\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input Output CitCLusc model from the paper\n","In the paper's body: \"Data in this analysis can be freely copied and modified. The data set is publicly available under the Open-File-Permissive License in which the full text of the data set in the paper is included.\" This model is currently in use in a few papers, listed below, though a lot of research has gone in recently to try to make it more accurate and more usable. Please see the CitClus project for a list, or read this Wikipedia article for documentation of what is needed for CitLusc-TDN to be used in science.\n","Code references:\n","The following code is given, which can be found in the appendix.\n","from tdm import tcns from csv import s3_reader as reader for csv_list in reader: for line in reader for column in line.items(): if not line.startswith(\"?\"): line.append('[@0 = {%s}]' % (column, col)) if column.split(',') == 1 and column.split('.') == 1: column.split('.')[-1] = column.split(':')[-2 - 1] print(\"[@0 = {}] \" % col.value if col.split('@')) i = int(column.split('@')[-1]) / 256 for row in csv_list: if col.split('@'[0][-1]): print(row) if not (row or col[0][/16-1] == 0): return i x = 0 for c in reader.readerlines(): print(c) c = c.replace('') c.replace(''.join('')) for c in csv_list: print(c)\n","As\n","179 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the WMT2014 English-French dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset WMT2014 German-English?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset BUCC French-to-English?\n"," output (Sparql query): \n","882\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- -- -- -- -- -- -- (?) -- -- -- -- -- ---------------------------------------------------------- SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCQ2014 French-to-English\")?score orkgp:HAS_DATASET?dataset; }?value? orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval? orkgp:HAS_METRIC?metric. } } ORDER BY?metric?metric_lbl\n","Thanks in advance to the above answers and a number of additional commentaries on them, we are now in a good position to analyze the data and evaluate the performance of our machine learning models. Next we will start to get a clear idea of what are our models learning about the dataset. Let's find out.\n","180 \n"," input (English text): What is the top benchmark score and its metric on the SQuAD2.0 dev dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0 dev\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the SQuAD1.1 dev dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1 dev\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the SQuAD1.1 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset SQuAD2.0?\n"," output (Sparql query): \n","866\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------ SELECT *\n","\n","data, score, metric = (1:10.1E-3,1000000,0,4.99)\n","\n","output (Sparql query): ------------\n","\n","{ 'total': 100.0,'score': 20.0,'metric': 5.0,'metric_lbl': 'SQuAD2.0' } ORDER BY?metric?metric_lbl\n","\n","data, score, metric = (1:100.0,100.0,0,4.99)\n","\n","output (Sparql query): ------------\n","\n","{ 'total': 100.0,'score': 20.0,'metric': 5.0,'metric_lbl': 'SQuAD2.0' } ORDER BY {.metric.?metric_lbl?.total}\n","\n","data, score, metric = (1:100.0,100.0,0,4.99)\n","\n","output (Sparql query): ------------\n","\n","{ 'total': 100.0,'score': 20.0,'metric': 5.0,'metric_lbl': 'SQuAD2.0' } ORDER BY {.metric.?metric_lbl?.total}\n","\n","data, score, metric = (1:100.0,100.0,0,4.99)\n","\n","output (Sparql query): ------------\n","\n","{ 'total': 100.0,'score': 20.0,'metric','metric_lbl': 'SQuAD2.0' } ORDER BY {.metric.?metric_lbl?.total}\n","\n","data, metric, average = (0:100.0,1\n","181 \n"," input (English text): What is the highest benchmark result achieved on the BC5CDR dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset CoLA?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoLA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset BC2GM?\n"," output (Sparql query): \n","850\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC2GM\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the datasets BC3CGM?\n"," output (Sparql query):  SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC3CGM\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:H\n","182 \n"," input (English text): Can you list the models that have been evaluated on the Story Cloze Test dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Story Cloze Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the WikiText-2 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Text8 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Text8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset?\n"," output (Sparql query): \n","533\n"]},{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1083 > 1024). Running this sequence through the model will result in indexing errors\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","output (Sparql text): The following output will be created if you pass the input dataset to the benchmark: The following output will be created if you pass the input dataset to the benchmark:\n","output data: Benchmark Metrics Example    1-0: Metrics for Story Cloze Test\n","1-0: Metrics for Story Cloze Test 2: Metrics for Text8 Benchmark Benchmark Metrics Example   1-0: Metrics for Story Cloze Test\n","1-0: Metrics for Story Cloze Test 2: Metrics for Text8 Benchmark Benchmark Metrics Example   1-0: Metrics for Story Cloze Test\n","1-0: Metrics for Story Cloze Test 2: Metrics for Text8 Benchmark Benchmark Metrics Example   1-0: Metrics for Story Cloze Test\n","1-0: Metrics for Story Cloze Test 2: Metrics for Text8 Benchmark Benchmark Metrics Example 2: Metrics for Text8 Benchmark The following data will also be created as output: 1) Text1: Story Cloze Test data\n","2) Text8: Text8 data The following data will also be created as output: 1) Text1: Story Cloze Test data\n","2) Text8: Text8 data The following data will also be created as output: 1) Text1: Story Cloze Test data\n","2) Text8: Text8 data 2) Text8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n","183 \n"," input (English text): Who is the author with the largest number of papers about semantic representation in music research problem? \n"," output (Sparql query): SELECT ?author WHERE { ?papers rdf:type orkgc:Paper. ?papers orkgp:P31 ?contrib. ?contrib orkgp:P32 ?research_problems. ?research_problems rdfs:label ?research_problems_labels. FILTER(REGEX( ?research_problems_labels, \"semantic representation in music\", \"i\")) ?papers orkgp:P27 ?authors. OPTIONAL { ?authors rdfs:label ?authors_labels } BIND(IF(BOUND( ?authors_labels), ?authors_labels, ?authors) AS ?author) } ORDER BY DESC(COUNT( ?author)) LIMIT 1\n"," input (English text): Who is the author of the most recent paper about insects?\n"," output (Sparql query): SELECT ?author_name WHERE { { SELECT ?publication_date_ AS ?date_of_the_latest_paper { ?paper_ a orkgc:Paper; rdfs:label ?title_; orkgp:P28 ?publication_month_; orkgp:P29 ?publication_year_. OPTIONAL { ?publication_month_ rdfs:label ?publication_month_label_ } OPTIONAL { ?publication_year_ rdfs:label ?publication_year_label_ } BIND( xsd:integer( IF( BOUND( ?publication_month_label_), ?publication_month_label_, ?publication_month_ ) ) AS ?publication_month_as_number_ ) BIND( xsd:integer( IF( BOUND( ?publication_year_label_), ?publication_year_label_, ?publication_year_ ) ) AS ?publication_year_as_number_ ) BIND( xsd:dateTime( CONCAT( ?publication_year_as_number_, \"-\", ?publication_month_as_number_, \"-01T00:00:00.000-00:00\" ) ) AS ?publication_date_ ) FILTER( ?publication_month_as_number_ > 0 && ?publication_month_as_number_ < 13 && ?publication_year_as_number_ > 0 && ?publication_year_as_number_ < 2023 ) FILTER(REGEX(STR( ?title_), \"insect\")) } ORDER BY DESC( ?publication_date_) LIMIT 1 } ?paper a orkgc:Paper; rdfs:label ?title; orkgp:P27 ?author; orkgp:P28 ?publication_month; orkgp:P29 ?publication_year. OPTIONAL { ?publication_month rdfs:label ?publication_month_label } OPTIONAL { ?publication_year rdfs:label ?publication_year_label } OPTIONAL { ?author rdfs:label ?author_label } BIND( IF( BOUND( ?author_label), ?author_label, ?author ) AS ?author_name ) BIND( xsd:integer( IF( BOUND( ?publication_month_label), ?publication_month_label, ?publication_month ) ) AS ?publication_month_as_number ) BIND( xsd:integer( IF( BOUND( ?publication_year_label), ?publication_year_label, ?publication_year ) ) AS ?publication_year_as_number ) BIND( xsd:dateTime( CONCAT( ?publication_year_as_number, \"-\", ?publication_month_as_number, \"-01T00:00:00.000-00:00\" ) ) AS ?publication_date ) FILTER( ?publication_month_as_number > 0 && ?publication_month_as_number < 13 && ?publication_year_as_number > 0 && ?publication_year_as_number < 2023 ) FILTER( REGEX( STR( ?title), \"insect\" ) && ?publication_date = ?date_of_the_latest_paper ) } \n"," input (English text): How many studies are published after 2019?\n"," output (Sparql query): SELECT (COUNT( ?date) AS ?number_of_studies) WHERE { orkgr:R110393 orkgp:compareContribution ?contrib. ?contrib orkgp:P9040 ?date. filter( ?date > \"2019\"^^xsd:string) } \n"," input (English text): Who has contributed to the largest number of articles about coronavirus?\n"," output (Sparql query): \n","1083\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input (English text): How many papers are published in which year?\n","output (Sparql query):  \n","input (English text): How many studies are published in which year?\n","output (Sparql query):  \n","input (English text): How many papers are published in which year?\n","output (Sparql query):  \n","input (English text): From which year is it published?\n","output (Sparql query):  \n","SURVEY (English text): Which is a newer paper of the year? (y = 0-2017)\n","output (Sparql query):   \n","input (English text); number\n","output (Sparql query) SELECT?number_of_papers AS?number_of_papers_from_2017_paper WHERE (?number_of_papers IS NULL) ORDER BY number_of_papers DESC(?number_of_papers_) <?number_of_papers_from_2017; \n","input (English text): Why are publications in 2017 so much younger than from the years 1960-1989?\n","output (Sparql query) SELECT?year AS&year_name (?year IS NULL OR y <?year_number) AND?year_date AS&year_date_from(?year) AS y FROM?annuals WHERE ann_number:INT =?year; \n","input (English text): Which is a newer paper of the year? (y = 0-2017)\n","output (Sparql query) SELECT?number_of_papers AS?number_of_papers_from_2017_paper WHERE {?number_of_papers IS NULL} ORDER BY?number_of_papers DESC(?number_of_papers_) <?number_of_papers\n","184 \n"," input (English text): What is the best performing model benchmarking the DuIE dataset in terms of F1 metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DuIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What are the models that have been benchmarked on the DDI dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DDI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the DuIE dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DuIE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the models that have been benchmarked on the DuIE dataset?\n"," output (Sparql query): \n","618\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","output (Sparql query): What are the models that have been benchmarked on this dataset and has a metric?\n","input (English text): How many datasets have been benchmarked on this dataset and metric?\n","output (Sparql query): \n","output (Sparql query): Which datasets and metrics have been benchmarked on this dataset and metric?\n","input (English text): This is the dataset which is expected to have the most F1 (Best F1) metric on the dataset.\n","output (Sparql query): Select... from DDI... WHERE ID = 528\n","input (English text): Which metric did you expect the DDI dataset to have the most F1 metrics on?\n","output (Sparql query): \n","output (Sparql query): By default, the dataset is tested on at least 3 different metrics and the average of these metrics (the best 2 for each metric) will be used in the benchmark; if an existing metric or model would not satisfy this criterion then it is not used and this metrics value is discarded; the same applies when a particular model would satisfy two metrics.\n","How many datasets have been benchmarked on this dataset and metrics?\n","input (English text): Which metrics did you expect the DDI dataset to have the most F1 metrics on and what is the average of these metrics (the best 2 for each metric)?\n","output (Sparql query): SELECT T1, S1, T2, S2, S3, S3 FROM DDI (?lbl_mv:Metric?metric_lbl. ) T1 (?lbl_mv:Model ) T2 (?lbl_mv:Model ) T3 (?lbl_mv:Model ), RDFS_ID, MIN, MAX, ID FROM Dat\n","185 \n"," input (English text): What is major reactant for \"Microwave-Assisted Cobinamide Synthesis\"?\n"," output (Sparql query): SELECT ?reactant WHERE { ?papers rdfs:label ?titles. FILTER(REGEX( ?titles, \"Microwave-Assisted Cobinamide Synthesis\")) ?papers orkgp:P31 ?cont. ?cont orkgp:P37557 ?reactant. } \n"," input (English text): What is the most common substrate for catalysts?\n"," output (Sparql query): SELECT ?substrate WHERE { orkgr:R25900 orkgp:compareContribution ?contrib. ?contrib orkgp:P15090 ?substrate. } ORDER BY DESC(COUNT( ?substrate)) LIMIT 1\n"," input (English text): Where can I find code references in papers that have used the Reactor 500M model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Reactor 500M\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): What coolant does ASTRID reactor use?\n"," output (Sparql query): \n","353\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What is the most common reactor fuel combination in \"Reactor 500M\"?\n","output (Sparql query): SELECT?fuelGroup GROUP BY?fuel GROUP BY?fuelGroup * 2;\n","input (English text): Where can I find reference material sources and code references in papers that use the Reactor model for data analysis purposes or for benchmarking purposes?\n","output (Sparql query): SELECT orkgp ORkgp.?dataset. ORkgp.?model. ORkgp.?method. ORkgp.?source_code ORkgp:HAS_SOURCE_CODE ORkgp:HAS_SOURCE_CODEORkgp:HAS_SOURCE_CODE ORkgp:HAS_SOURCE_CODE ORkgp:HAS_SOURCE_CODE ORkgp:HAS_SOURCE_CODEORkgp:HAS_SOURCE_CODE ORkgp:HAS_SOURCE_CODE ORkgp:HAS_DEFINITE?dataset; ORkgp:DEFINITE?dataset = output_of_p/results\n","input (English text): Why was the name of this work \"Reactor\"?\n","output (Sparql query): SELECT?lbl.?dataset.?lbl.?model. ORkgp.?method. ORkgp.?source_code ORkgp:HAS_SOURCE_CODE ORkgp:HAS_SOURCE_CODEORkgp:HAS_SOURCE_CODE ORkgp:HAS_SOURCE_CODE ORkgp:HAS_SOURCE_CODEORkgp:HAS_SOURCE_CODE ORkgp:HAS_SOURCE_CODE ORkgp:HAS\n","186 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Atlantis dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Ms. Pacman dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Yars Revenge dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Yars Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Berzerk dataset?\n"," output (Sparql query): \n","873\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------|-------------------------------------------------------------|-----------------------+------------+----------+ | Metric | Score | Time | Metric | Time | Metric | Time | --------------------------|-------------------------------------------------------------|-----------------------+------------+----------+ | 1.0 3.2 5.7 8.0 1.4 12.5 | | 1.5 4.1 6.0 8.0 1.5 15.1 | | 2.0 4.5 6.8 8.5 3.2 24.0 | | 2.5 5.8 7.5 8.0 2.5 28.4 | | 3.0 5.1 8.1 8.8 4.0 34.1 | | 3.5 4.9 6.6 8.5 2.5 36.1 | | 4.0 6.0 9.7 8.9 2.5 39.8 | ||------------|--------------------------------------------------+------------+----------+------------+----------+ | 1.5 1.1 3.3 5.6 8.5 2.2 11.1 | | 1.9 3.9 5.4 6.0 8.0 1.7 14.8 | | 2.2 4.1 6.0 8.0 1.5 15.7 | | 2.5 4.8 6.6 8.0 2.1 16.5 | | 2.9 4.7 6.1 8.5 3.9 17.2 | | 3.3 5.1 6.8 8.5 2.2 19.2 | ||------------|--------------------------------------------------+------------+----------+------------+----------+ 3.0 5.2 8.4 8.9 2.3 21.3 | ||----------------|--------------------------------------------------------|------------------------+----------+------------+--------+----------+ 4.0 5.9 9.3 9.9 2\n","187 \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Crazy Climber dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Crazy Climber\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Fishing Derby dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Fishing Derby\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Seaquest dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Skiing dataset?\n"," output (Sparql query): \n","565\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","You are all set? OK, now you can try some of the models. You can do so by running the following two queries on the database. You must replace the variables as you see fit.\n","ORGK:HAS_GEOGRAPHY_MODEL GATK:HAS_GRAPHICS RDFS:HAS_DATA\n","output : Select RDFS:GEOGRAPHY_MODEL GATK:GRAPHICS RDFS:DATA SELECT GATHERDATE AS date, GATK AS model, RDFS_DATA AS dtype, ORGK_DATA AS name, ORGK_DATA_LENS AS lens_num, ORGK_DATA_COMPONENT AS data_probability, GATK_NAME AS item, GATK_DESCRIPTION AS type, ORGK_DATA_RANK AS rank, ORGK_REFERENCE AS reference, ORGK_SEPARATOR AS separator, ORGK_SEQUENCE AS value, GATK_STATISTICS AS statistic FROM GATK_POPULATION TO ORGK_DATA_LENS GATK_POPULATION GATK_PRIMARY_PROCESSOR RDFS:POPULATION RDFS:REFERENCE GATHERDATE AS date, GATK AS model, GATK_VERSION AS version.\n","output : Select GATHERDATE AS date, GATK AS model, GATK_VERSION AS version.\n","output : Select GATHERDATE AS date, GATK AS model, GATK_VERSION AS version.\n","Or you can filter for a specific dataset. For this you can pass filter options to both queries:\n","ORGK:HAS\n","188 \n"," input (English text): What are the most commonly used benchmark datasets for the Document Summarization research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Document Summarization\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Text Generation research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Text Generation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Data-to-Text Generation research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Data-to-Text Generation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Text Summarization research field?\n"," output (Sparql query): \n","515\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What are the most commonly used benchmark datasets for the Document Summarization research field?\n","output (Sparql query): SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Document Summarization\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }   input (English text): What are the most commonly used benchmark datasets for the Text Generation research field? output (Sparql query): SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Text Generation\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \n","input (English text): What are the most commonly used benchmark datasets for the Data-to-Text Generation research field? output (Sparql query): SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_\n","189 \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Krull dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Krull\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Time Pilot dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Time Pilot\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Tutankham dataset?\n"," output (Sparql query): \n","511\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Including the \"N\" statistic, the mean score of the \"S\" metric for the 2 datasets is 0.082, compared to 0.059 score with the original \"N\" statistic for both datasets. However, in the Tutankham dataset the number of trials at a time was reduced to 14 for the Atari 2600 Krull and 15 for the Venture, compared to the original 8.\n","The analysis above gives an estimate of the potential benefit on the Atari 2600 Venture dataset compared to the original \"N\" metric, based on the number of trials (N=28), and the number that would be lost (i.e. number of trials reduced to 15 in the Venture dataset vs the remaining trials in the Atari 2600 Krull). The \"N\" statistic can be interpreted in 2 different ways. One is to show that the value of the \"N\" statistic in the original dataset is too small to explain the variability, and the other is to show that the value of the \"N\" statistical value in the 3 datasets does not match that in the original dataset. The first hypothesis would be supported by the data, the second by the interpretation of the data. The \"N\" statistic is not large enough to explain the variability of the Atari 2600 Krull dataset from a statistical standpoint. The 2 alternative hypotheses provide much stronger support with larger values of the \"N\" statistic which are also more accurate.\n","Conclusion\n","In this post I have provided a list of 2 datasets on the Atari 2600, and analyzed them with Sparql and an example program with an \"N\" statistic (the Atari 2600 Venture dataset, and the Atari 2600 Time Pilot dataset). After identifying the potential benefits of combining these 2 datasets, you can determine if either dataset is the ideal choice for the other. This allows you to develop a more effective strategy for training using data on that your competitors don't share. I\n","190 \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Road Runner dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Road Runner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Pong dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Pong\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Tutankham dataset?\n"," output (Sparql query): \n","840\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Tutankham\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.? eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","input (English text): What is the original name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Space Adventure dataset?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Space Adventure\")?benchmark orkgp:\n","191 \n"," input (English text): Can you list the models that have been evaluated on the BioASQ dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the BIOSSES dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BIOSSES\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the SciERC dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the PROTEINS dataset?\n"," output (Sparql query): \n","552\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["  input (English text): Can you list the models that have been evaluated on the LISP dataset?\n"," output (Sparql query):   input (English text): Can you list the models that have been evaluated on the LISP2 dataset?\n"," output (Sparql query):   input (English text): Can you list the models that have been evaluated on the LEISURE dataset?\n"," output (Sparql query):   input (English text): Can you list the models that have been evaluated on the MATS dataset?\n"," output (Sparql query):   input (English text): Can you list the models that have been evaluated on the MATS2 dataset?\n"," output (Sparql query):\n","192 \n"," input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT29 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the JNLPBA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"JNLPBA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the JNLPBA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 entity level\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"JNLPBA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (long) dataset?\n"," output (Sparql query): \n","847\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1 Long\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"JNLPBA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the Natural Questions (short) dataset?\n"," output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1 short\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"JNLPBA\")?bench\n","193 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset CommitmentBank?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommitmentBank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you list the models that have been evaluated on the CommitmentBank dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommitmentBank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the metrics of evaluation over the CIFAR-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the CommitmentBank dataset?\n"," output (Sparql query): \n","638\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Please, type in the name of the model where the goal is to evaluate. First (highest score) First (lowest score) Second (highest score) Second (lowest score) Third (highest score) Third (lowest score) The model name is required. This model is evaluated against the CIFAR-10 dataset.\n","output (Sparql query): \n","input (English text): Please, provide a description of the dataset and the training set.\n","output (Sparql query): \n","output (Sparql query): Select the metric you would like to evaluate over the dataset. The first column is the metric. The second one is the score of the model.\n","output (Sparql query): SELECT?metric?metric_lbl?value WITH score as score AND metric as metric_lbl; \n","input (English text): The goal of the model is to evaluate the score on the metric (CIFAR-10 metric) relative to the baseline score.\n","(S)he input is text output (S)he is text input (English text) I want to do the following SQL query.\n","output (S)he is text output (English text) This is how our example output would look like.\n","output (S)he is text output (English text) Select the metric you would like to evaluate from the CIFAR-10 dataset. The first column is the metric. The second one is the score of the model. The third one is the metric score. The metric score is the score value for each model relative to its baseline score.\n","output (S)he is text output (English text)\n","input (English text): How many models where evaluated on the dataset? By default, there are 100 models evaluated when using the benchmark and 0 models evaluated when using the\n","194 \n"," input (English text): Can you provide links to code used in papers that benchmark the BiT-S (ResNet-152x4) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-S (ResNet-152x4)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the BiT-L (ResNet) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-L (ResNet)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BiT-S model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-S\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?\n"," output (Sparql query): \n","499\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiT-M\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): List the codes. Could you please identify the code link(s) in this paper?\n","output (Sparql query):  SELECT?code_link AS code_link FROM [R] AS b WHERE?lbl=?title DESC CHARACTER SET latin1=?title CASE WHEN?lbl=?code AND?code_link =?label THEN c ELSE NULL END; 1 2 3 4 5 6 7 8 9 10 11 12 input ( English text ) : List the codes. Could you please identify the code link(s) in this paper? output ( Sparql query ) : SELECT? code_link AS code_link FROM [R] AS b WHERE? lbl =? title DESC CHARACTER SET latin1 =? title CASE WHEN? lbl =? code AND? code_link =? label THEN c ELSE NULL END ;\n","input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?\n","output (Sparql query):  SELECT DISTINCT?code_link WHERE {?lbl a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl\n","195 \n"," input (English text): What is the best performing model benchmarking the ACE 2005 dataset in terms of RE Micro F1 metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"RE Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of RE Micro F1 metric on the ACE 2004 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"RE Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest NER Micro F1 score on the ACE 2005 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"NER Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric?\n"," output (Sparql query): \n","825\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl ORDER BY DESC(?value) LIMIT 1 } \n"," input (English text): Describe the model-driven analysis procedure to best explain the ACE 2004 dataset?\n"," output (Sparql query):  \n","Example 3 The first section explains the RE+ Micro F1 metric and the second section provides a model-driven analysis of the ACE 2005 dataset. This example has the following fields:\n","input (English text): Indicate the model that performed best in terms of the RE+ Micro F1 metric on the ACE 2004 benchmark dataset?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"RE+ Micro F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2004\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model?model rdfs:label?model_lbl. SORT BY DESC(?value) LIMIT 1 } }  input (English text): Which of the following models performed best in terms of the RE+ Micro F1 metric on the ACE\n","196 \n"," input (English text): What is the top benchmark score and its metric on the BBCSport dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BBCSport\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the RTE dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RTE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the SearchQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SearchQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Nottingham dataset?\n"," output (Sparql query): \n","832\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["-------------+---------------------------- RTE::RTE_REST_SERVER_NAME RTE::RTE_REST_SERVER_NAME_2 RTE::RTE_REST_SERVER_NAME_2_2 RTE::RTE_REST_SERVER_NAME_3 RTE::RTE_REST_SERVER_NAME_3_2 RTE::RTE_REST_SERVER_NAME_4 RTE::RTE_REST_SERVER_NAME_4_2 RTE::RTE_REST_SERVER_NAME_5 RTE::RTE_REST_SERVER_NAME_5_2 RTE::RTE_REST_SERVER_NAME_6 RTE::RTE_REST_SERVER_NAME_6_2 RTE::RTE_REST_SERVER_NAME_7 RTE::RTE_REST_SERVER_NAME_7_2 RTE::RTE_REST_SERVER_NAME_8 RTE::RTE_REST_SERVER_NAME_8_2 RTE::RTE_REST_SERVER_NAME_9 RTE::RTE_REST_SERVER_NAME_9_2 RTE::RTE_REST_SERVER_NAME_10 RTE::RTE_REST_SERVER_NAME_10_2 RTE::RTE_REST_SERVER_NAME_11 RTE::RTE_REST_SERVER_NAME_11_2 RTE::RTE_REST_SERVER_NAME_12 RTE::RTE_REST_SERVER_NAME_12_2 RTE::RTE_REST_SERVER_NAME_13\n","197 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Challenge)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the STEM-ECR v1.0 dataset?\n"," output (Sparql query): \n","529\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STEM-ECR\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the STEM-ECR v1.0 dataset?\n"," output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STEM-ECR\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metiorc.?metric rdfs:label?metric_lbl. } } \n"," input (English text): Which benchmarks are typically used to benchmark the ARC data on the ARC dataset?\n"," output (Sparql query): SELECT B_EXAMPLE?metric?name =?selector?metric_lbl. FILTER (str(?metric_lbl) = \"ARC\")?benchmark. OPTIONAL {?metio rdf:name\n","198 \n"," input (English text): Can you provide links to code used in papers that benchmark the Multi-Format Contrastive model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Multi-Format Contrastive\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the multi-head model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"multi-head\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Contextual Match model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Contextual Match\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Multi-Perspective Matching (single model) model?\n"," output (Sparql query): \n","480\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Multi-Perspective Matching\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","Input is a model with a single face\n","Output is single model with all faces\n","What if the paper benchmarked multiple models?\n","I'm working from input, but I need more code to understand how to convert it into a SQL query. Do you have information about the output from the benchmark?\n","My output is a JSON object with the following fields.\n","fields input (English text): In a paper that benchmarks multiple models, what are those outputs?\n","output (SParql Query): SELECT MULTIPLESULTIVETYPE?model_lbl.?mode  | SELECT MULTIPLESENSURE?model_lbl.?form  | SELECT MULTIPLESENSURE?model_lbl.?value  | SELECT MULTIPLESENSURE?model_lbl.?mode  | SELECT MULTIVETYPE?source_code.?code  | SELECT MULTIVETYPE?source_code.?value?\n","output (Sparql Query): SELECT MULTIPLESULTIVETYPE?model_lbl.?mode  | SELECT MULTIPLESENSURE?model_lbl.?form  | SELECT MULTIPLESENSURE?model_lbl\n","199 \n"," input (English text): List the metrics that are used to evaluate models on the GAD benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GAD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the GigaWord dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GigaWord\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the CUB-200-2011 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the GAD dataset?\n"," output (Sparql query): \n","536\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Output :\n","The results of the query look something like this :\n","Note on Benchmarking with GAD\n","GADBench has been written to be as powerful as it can be. When using GADBench, the only way the performance of a model can be evaluated is by using the following option :\n","optarg.  OPTIONAL ( Default : [ default ] ) ( Boolean, Not implemented yet) Whether to use an index on an index which is used to evaluate the modeling for the model. If the model name contains the letter G, the index will be used to store the default values for the metric. E.g. The benchmarking options : max_rank 10, min_rank 10, avg_score 1000.  OPTIONAL ( Default : [ default ] ) ( Boolean, Not implemented yet) Whether to use an index on an index which is used to evaluate the building of a model. If the model name contains the letter G, the index will be used to store the default values for the metric. E.g. The benchmarking options : max_rank 10, min_rank 10, avg_score 1000.  OPTIONAL ( Default : [ default ] ) ( Boolean, Not implemented yet) Whether to use an index on an index which is used to build the model. If the model name contains the letter G, the index will be used to store the default values for the metric. E.g. Benchmarking : default,min_rank 1000. \n","In order to use the benchmark option, you may use one of the following options :\n","optarg.  OPTIONAL ( Default : [ default ] ) ( Boolean, Not implemented yet) Whether to use an index on an index which is used to evaluate the forecasting for a model. If the model name contains the letter G, the index will be used to store\n","200 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the PubMed 20k RCT dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the metrics of evaluation over the CIFAR-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the MedNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the PubMed 20k RCT dataset?\n"," output (Sparql query): \n","629\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PubMed 20k RCT\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } ORDER BY DISTINCT?metric?metric_lbl.  This sentence is an example of a PARENTALITY HINT. See page 9 of this document for more information on PARENTALITY. If we get it right, that'll get us up to our final score for our Metric Dump.\n","input (English text): What is the score (in points) for the PubMed 20k RCT?\n","output (Sparql query): SELECT DISTINCT?score, (metric_lbl.score / 200, metric_lbl.score * 100) AS?score_med_lbl(?value) FROM medcriteria?medstat WHERE (?x.val =?value) AND (?x.stat =?stat_lbl) AND (metric=?value) ORDER BY metric AS score,?metric AS scoring,?value AS scoring_score, (metric_lbl.score / 200, metric_lbl.score * 100) AS score_med_lbl_medstat(?\n","201 \n"," input (English text): Could you provide a list of models that have been tested on the Gibson PointGoal Navigation benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Habitat 2020 Point Nav test-std benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Point Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the WOS-5736 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset?\n"," output (Sparql query): \n","553\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","5.4.13: Update the model name (for the following benchmark dataset on the GLSL-K7 platform):\n","input (English text): List the models that are used to evaluate models on the BenchmarkBenchmark Datapoint dataset?\n"," output (Sparql query): SELECT d.model_lbl,?d.model_lbl OPTIONAL (str(d.model_lbl) = \"Gibson PointGoal Navigation\")?db_datatet.?db_datatet OPTIONAL (str(d.model_lbl) = \"gibson point goal navigation benchmark dataset\")\n","input (English text): List the models for the BenchmarkStatisticBenchmark dataset?\n","output (Sparql query): \n","5.4.14: Update the model name (for the following benchmark dataset on the GLSL-K7 platform):\n","input (English text): List the models that are used to evaluate models on the PointGoalBenchmark Datapoint dataset?\n","output (Sparql query): SELECT?dataset_lbl,?db_datatet OPTIONAL (str(?dataset_lbl) = \"PointGoalBenchmark Datapoint\")?db_datatet OPTIONAL (str(?db_datatet) = \"gibson point goal navigation benchmark dataset\")\n","input (English text): List the models that are used to evaluate models on the TestStatisticBenchmark dataset?\n","output (Sparql query): \n","5.4.15: Update the model name (for the following benchmark dataset on the GLSL-K7 platform):\n","input (English text): List the models that are used to evaluate models on the PointGoalBenchmark Datapoint dataset?\n","output (Spar\n","202 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ARC-PDN dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC-PDN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AESLC dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SearchQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SearchQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AAPD dataset?\n"," output (Sparql query): \n","517\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AAPD\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n","input(english text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks for the H.264 and H264K datasets?\n","output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n","input(english text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks for the H.265, H265K, and HEVC datasets?\n","output (Sparql query): SELECT DISTINCT?paper OR kgp?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"H264\")?benchmark orkgp:HAS_DATASET\n","203 \n"," input (English text): Where can I find code references in papers that have used the XLNet (single model) model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the NAS Cell model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"NAS Cell\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the A2C + SIL model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A2C + SIL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the AxCell model for benchmarking purposes?\n"," output (Sparql query): \n","483\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"AtherCell\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Where can I find code references in papers that have used the C2C + H2S model for benchmarking purposes?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"C2C + H2S\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Where can I find code references in papers that have used the C2D + H2S2 model for benchmarking purposes?\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"C2D + H2S2\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkg\n","204 \n"," input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM-MoS model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AWD-LSTM-MoS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM-DOC x5 model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AWD-LSTM-DOC x5\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM + dynamic eval model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AWD-LSTM + dynamic eval\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model?\n"," output (Sparql query): \n","505\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","All of the above are from the \"benchmarked\" output (and there are several more that would produce similar output, but I am not familiar with them). They all come from the code that I wrote, and it is not even an original or an \"extensive\" code. It is just a couple of lines of code, and some annotations. I may update the document to explain the code to you if my familiarity with it grows with time. In the meantime, here is the description of the code as in the PDF. This is just the main loop that goes through all of the documents and returns all results. Here is some background info: For a number of experiments it happens that certain documents do very poorly in many of the benchmarks. Some papers do this too, so they might be interesting, but I am not sure if it is worth it for me. I had some interest the fact that the papers of some of the authors (I won't mention their names as that might make some of the papers harder to find and also because they do not publish at this time), are actually better at the benchmarks if I make the code public. This means that some of the benchmarks may not be benchmarked even if they have some flaws! If you want to run the benchmark, you just need to be like the developers of the benchmark and write the equivalent code. Now you can just run each one of the benchmarks as you would with all of the others. I haven't even given you instructions on how to do that, or why all of the documents are useful. If you have questions, please do not hesitate to ask and if it is too complicated to explain it, please post it to the LSTM mailing list. In order to get the performance for all of the parameters, for each benchmark I have calculated and added up the average and maximum for all the data. Then I simply took\n","205 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the BioASQ dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the CommonsenseQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the MedSTS dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMedQA dataset?\n"," output (Sparql query): \n","498\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MedQua\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the PeerJ dataset?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PeerJ\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?note orkgp:P32?cont; rdfs:label?paper_lbl. } \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the bibliometrics dataset?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Bibliometrics\")?benchmark orkgp:HAS_DATASET?dat\n","206 \n"," input (English text): What are the most commonly used benchmark datasets for the Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification. research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification.\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Name the datasets that have been used for benchmarking in the Text Classification research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Text Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Image Generation research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Image Generation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Name the datasets that have been used for benchmarking in the Image Classification research problem?\n"," output (Sparql query): \n","542\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What is the most commonly used benchmark dataset for the Speech Recognition research field?\n","output (Sparql query): \n","input (English text): What is the most commonly used benchmark dataset for the Speech Recognition research problem?\n","output (Sparql query): \n","input (English text): What is the most commonly used benchmark dataset for the Neural Network Research field?\n","output (Sparql query): \n","input (English text): What is the most commonly used benchmark dataset for the Image Classification research field?\n","output (Sparql query): \n","input (English text): What benchmark datasets have currently been used?\n","output (Sparql query): \n","input (English text): What benchmark datasets do you already have?\n","output (Sparql query): \n","output (Sparql query): \n","input (English text): What benchmark datasets did you not find? \n","output (Sparql query): \n","Please ensure you have read and understood the above message! If you're having difficulty reading this message, I apologize and ask you to refer to our documentation or contact the mailing list below.\n","207 \n"," input (English text): Could you provide a list of models that have been tested on the Gibson PointGoal Navigation benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Habitat 2020 Point Nav test-std dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Point Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Name the datasets that have been used for benchmarking in the PointGoal Navigation research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"PointGoal Navigation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Gibson PointGoal Navigation dataset?\n"," output (Sparql query): \n","664\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Habitat 2020 Point Nav test-std\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n","208 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cheetah, run (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What models are being evaluated on the Cheetah, run (DMControl500k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the Cheetah, run (DMControl100k) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Which model has achieved the highest Score score on the Cheetah, run (DMControl500k) benchmark dataset?\n"," output (Sparql query): \n","708\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","For more information.\n","(2)  Using the above table we can see that the first model, \"Cheetah, run (DMControl500k)\" has a score of 27 and is ranked 3rd in the competition. The second model, \"Duck, run (DMControl700k)\" has a score of 17 and is ranked 6th. The third model, \"Wolverine, run (DMControl1000k)\" has a score of 22 and is ranked 2nd for this benchmark dataset.\n","Note: for more details, please refer to the test/benchmark/data/Cheetah/DMControl500k data set. It is an RDS compatible file.\n","To find out how many RDS compatible models have been evaluated, you can run the following commands:\n","C:\\> rrdtool --dataset=Cheetah.dat\n","rrdtool v2.8.1 Copyright (C) 2011 R Foundation for Statistical Computing. All rights reserved. RDFS v3.x or RDFS v4.x\n","Using the table above, we can also get the number of models that have passed each test. After running the steps above, the following query produced the following output:\n","209 \n"," input (English text): What is the top benchmark score and its metric on the Stanford Cars dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Stanford Dogs dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Dogs\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What is the top benchmark score and its metric on the SearchQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SearchQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Stanford Dogs dataset?\n"," output (Sparql query): \n","728\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","If rdfs:labels is set the labels of the labeled rdfs. This gives the labels used by the benchmarks that do this, and the score score.\n","210 \n"," input (English text): Indicate the model that performed best in terms of Senseval 3 metric on the Supervised: benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Senseval 3\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the RACE benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RACE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of PRE-TRAINING DATASET metric on the DCASE benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"PRE-TRAINING DATASET\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DCASE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Senseval 2 metric on the Supervised: benchmark dataset?\n"," output (Sparql query): \n","836\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Senseval2\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SUPERVISED\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","input (English text): Indicate the model that performed best in terms of Senseval 3 metric on the Unsupervised: benchmark dataset?\n"," output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Senseval3\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UNSUPERVISED\")?benchmark orkgp:\n","211 \n"," input (English text): Indicate the model that performed best in terms of Test perplexity metric on the Penn Treebank (Word Level) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Test perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Word Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the WikiText-2 dataset in terms of Validation perplexity metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Validation perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Perplexity score when benchmarked on the LAMBADA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"LAMBADA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset?\n"," output (Sparql query): \n","847\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["   input (English text): Indicate which model is performing best for the WikiText-2 benchmark?\n"," output (Sparql query): \n","Note: Due to the large dataset size, the validation_criteria parameter was expanded to accept N-dimensional models.\n","Tail2\n","input (English text): In what manner is the WikiText-4 benchmarking tool configured?\n","output (Sparql query):    input (English text): In what manner is the WikiText-4 benchmarking tool configured?\n","output (Sparql query): \n","Note: This is a query to the LAMBADA tool for LAMBAGE.  This is a query to the TALENT tool for LAMBAGE.\n","t-Sparql query_input (text): What are the top performing models using the TALENT-HANDLE model for the TALENT benchmark dataset?\n","SwingC++\n","input (English text): What is the top performing model for the TALENT benchmark dataset?\n","output (Sparql query):    input (English text): What is the top performing model for the TALENT benchmark dataset?\n","output (Sparql query): \n","Tail2\n","Tail2: SELECT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. } ORDER BY DESC(?value) LIMIT 1 } \n","T-Sparql query_input (text): What are the top performing models using the TALENT model for the WikiText-104 benchmark dataset?\n","SwingC++ query_input (text): What are the top performing models using the TALENT model for the WikiText-105 benchmark dataset?\n","SwingC++ query_\n","212 \n"," input (English text): What is the top benchmark score and its metric on the WMT2016 Russian-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Russian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the WMT2016 Romanian-English dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Romanian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 English-German?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value?\n"," output (Sparql query): \n","872\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- Metric Result: 1000-1299.4495 1.000000 0.000001 0.000000 0.000000 WMT2016 English-Russian 1 MATCH ROUND (0 1 2 3 4 5 6 6) WMT2016 English-Russian 10 MATCH ROUND (1 2 3 4 5 6 6) WMT2016 English-Russian 2 WHT (7 8 9 10 11 12) WMT2016 English-Russian 15 WHT (15 16 17 18 19 20 21) WMT2016 English-Russian 3 WHT (55 60 60 70 70 80 80 80) WMT2016 English-Russian 5 WHT (65 80 85 95 90 95 95 95 90) WMT2016 English-Russian 7 WHT (84 95 100 105 110 115 115 115 115) WMT2016 English-Russian 10 WHT (135 135 155 170 175 180 180 180) WMT2016 English-Russian 7 WHT (125 125 170 175 180 180 180 180) WMT2016 English-Russian 17 WHT (195 195 225 245 295 315 315 315) WMT2016 English-Russian 3 WHT (125 125 170 175 180 180 180 180) WMT2016 English-Russian 5 WHT (165 170 195 215 275 285 295 295) WMT2016 (WMT2016 English-Russian), WMT2016 (Russian), WMT2016 (Russian), WMT2016 English-Russian 20 WHT (250 250 265 305 345 350 300 350) WMT2016 English-Russian 4 WHT (225 225 275 315 350 350 375 375) WMT2016 English-Russian 10 WHT (220 232 290 315 350 350 375 375) WMT2016 English-Russian 15 WHT (255 255 265 315 375 375 375 375) WMT2016 English-Russian 15 WHT (250 250 265 315 375 375 375 375) WMT2016 (WMT2016 English-Russian\n","213 \n"," input (English text): What is the most common substrate for catalysts?\n"," output (Sparql query): SELECT ?substrate WHERE { orkgr:R25900 orkgp:compareContribution ?contrib. ?contrib orkgp:P15090 ?substrate. } ORDER BY DESC(COUNT( ?substrate)) LIMIT 1\n"," input (English text): How many studies do use Chloride as major anion?\n"," output (Sparql query): SELECT COUNT( ?anions_labels) AS ?chloride_count WHERE { orkgr:R110597 orkgp:compareContribution ?contrib. ?contrib orkgp:P37458 ?anions. ?anions rdfs:label ?anions_labels. FILTER(REGEX( ?anions_labels, \"Chloride\"^^xsd:string)) } \n"," input (English text): What is another name for Bisphenol A?\n"," output (Sparql query): SELECT ?same_name_label WHERE { ?resources rdfs:label ?resources_labels. FILTER(REGEX( ?resources_labels, \"Bisphenol A\", \"i\")) ?resources orkgp:SAME_AS ?same_name. ?same_name rdfs:label ?same_name_label. } \n"," input (English text): What is the most common lead compound?\n"," output (Sparql query): \n","324\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","* * *\n","\n","* * *\n","\n","* * *\n","\n","This report contains a lot of text and the output can be overwhelming.\n","\n","For each table you will be able to see a table of the values for each query as well. Each table also has a description attached.\n","\n","This report can be imported into a variety of systems to make a single report. The following section describes those systems. However, you can also use this report as a stand alone report if you are creating multiple reports.\n","\n","Table of Contents:\n","\n","Summary Report - For most organizations with multiple data sources this is a better tool with the following benefits, compared to a report as described above:\n","\n","Reports can be imported into SQL Server, but you have to choose to import all or a particular report.\n","\n","If there are multiple tables and you import multiple reports, the results will be displayed as single columns with no sort fields.\n","\n","The reports will show the full set of column names that were used when using the various reports.\n","\n","If there are specific columns you are interested in, you can open the report in an individual format (for example in Microsoft Excel). Or you can export the results as an Excel file using the Export as... box.\n","\n","Export as Excel files are very popular and can be very quick. It's also quick to export from MS-Excel to other file types. In summary, it is the next best thing to having an individual table of results at your fingertips, but the import can do the job.\n","\n","These reports are great for analyzing long term data sets so they are very common, but if your data sets are short term only or very small this could prove to be a useful report to share with management and investors. When using a report, it is also useful for looking at specific trends and trends in the past.\n","\n","\n","Quickstart\n","214 \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Road Runner dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Road Runner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Beam Rider dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Beam Rider\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Enduro dataset?\n"," output (Sparql query): \n","839\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Enduro\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METric?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","So this is why I have the variable model_lbl. It's used during benchmarking by Parquet to save the output of the RDFS queries, so that the SQL can be read from my RDFS file without using SQL.\n","Next Steps: Next step   After the release, I will keep doing updates to bring in bug fixes and new functionality. So keep an eye out if you run into problems while using the script that I wrote.\n","The script uses RDFS to build a list of metrics in a CSV file as well as a number of other queries. The CSV file is saved in a json files and each file has a MetricColumn attribute to list the metric associated with it. Each metric column is represented as a nested named group of key\n","215 \n"," input (English text): Provide a list of papers that have utilized the C51 model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"C51\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the DDQN (tuned) noop model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDQN (tuned) noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the NASNet-A + c/o model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"NASNet-A + c/o\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the C51 noop model and include the links to their code?\n"," output (Sparql query): \n","501\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note: You do not need to include the full text - i.e. where did you obtain the code from? If you provide a short snippet of the code, you can quickly include it with the parameters. Just include the keyword: &parameter with the parameters. This will allow you to use the parameter in a parameterized query:  input (English text): Provide a list of papers that have utilized the DDQN (tuned) noop model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DDQN (tuned) noop\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }  input (English text): Provide a list of papers that have utilized the C51 noop model and include the links to their code?\n"," output (Sparql query):\n","Note: You do not need to include the full text - i.e. where did you obtain the code from? If you provide a short snippet of the code, you can quickly include it with the parameters. Just include the keyword: &parameter with the parameters. This will allow you to use the parameter in a parameterized query: \n","The complete query would look like the following:\n","#!/usr/bin/bash input (English text): Provide a list of papers that have utilized the DDQN (tuned) noop model and include the links to their\n","216 \n"," input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Ning et al.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the OverFeat - 7 accurate models model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"OverFeat - 7 accurate models\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Sarsa-φ-EB model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Sarsa-φ-EB\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the FABIR model?\n"," output (Sparql query): \n","484\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["   input (English text): Can you provide links to code used in papers that benchmark the FABIR model and the LFSG model?\n"," output (Sparql query):   \n","1.8.2. The DATASET and THE model\n","In S-Lite, the model has a'reference' version, i o a DATASET, a reference version of the 'S' model, and the source code for the DATASET.\n","In S-Lite, these are available through the S/D-code/reference.sql file. They are both stored together, just like the RDFS/S source code. However, there are distinct ways to access them.\n","The S/D version can be retrieved by searching through the RDFS/S source code ( S/D version of the source code on this page).\n","One can retrieve the reference version by searching with the corresponding RDFS/S prefix.\n","One can retrieve the RDFS/S reference using RDFS/S syntax (see also RDFS/Sreference.sql).\n","1.9. Output schema\n","In S-Lite, there are two output schemas. These are for searching, and for creating output documents (output files).\n","The search schema is identical to the main schema, however, the result returned from the search is not dependent on the source of the RDFS, S, or SRS (see above)\n","1.10. Other Notes\n","These notes are not necessarily linked to any one paper and their content may change in the future (but the main text is here). We tried to keep the notes to a minimum during this work however, it is likely that there are other good notes that would be much more useful.\n","S-Lite uses a variety of different data formats\n","217 \n"," input (English text): What are the objectives for Sepsis prediction?\n"," output (Sparql query): SELECT DISTINCT ?objectives WHERE { orkgr:R70642 orkgp:compareContribution ?contrib. ?contrib orkgp:P15051 ?objectives. } \n"," input (English text): What is another name for Bisphenol A?\n"," output (Sparql query): SELECT ?same_name_label WHERE { ?resources rdfs:label ?resources_labels. FILTER(REGEX( ?resources_labels, \"Bisphenol A\", \"i\")) ?resources orkgp:SAME_AS ?same_name. ?same_name rdfs:label ?same_name_label. } \n"," input (English text): List the datasets benchmarked under the Temporal Information Extraction research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Temporal Information Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What is the emergency type of AMBER Alert?\n"," output (Sparql query): \n","355\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT emergency_type?emergency_type_enum_enum_enum _ (0 or 3) FROM orkgc_en. orkgc_enum. emergencies WHERE ORIKGKPROM(type, emergency_type_enum)?emergency_type ORIKGKPROM(category, emergency_type_enum)?emergency_type_enum ERRNO NULL. ERRNO NULL. ---------------------------------------------------------------------------------------------- \n"," input (English text): Which time and date format is the AMBER Alert?\n"," output (Sparql query):  SELECT andkgc_stddev_datetime ORIKGKPROM(stddev) AMBER_Alert_Format _ (0 or 3) FROM orkgc. orkgc_enum. emergency_types GROUP BY. ORIKGKPROM(stddev) ORIKGKPROM(status) ORIKGKPROM(date)\n","input (English text): Which time and date format is the AMBER Alert as reported by the FBI?\n","output (Sparql query):  SELECT ANDIKGKPROM(DATE_TIME, AMBER_Alert_Format _ (0 or 3)) AMBER_Alert_Format _ (0 or 3) FROM orkgc_en. orkgc_enum. security_types\n","input (English text): What is the range-based AMBER Alert policy defined by the FBI?\n","output (Sparql query):  SELECT andkgc_range_policy andikgkproms. range_policy. andikgkproms. range_policy ORIKGKPROM(range_policy. range_policy ORIKGKPROM(range_policy. range_policy, AMBER_Alert_Format _ (0 or 3)) )\n","\n","218 \n"," input (English text): List the metrics that are used to evaluate models on the OpenBookQA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OpenBookQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the PIQA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PIQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the CIFAR-100 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset?\n"," output (Sparql query): \n","524\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): List the metrics that are used to evaluate models on the Schemata-Lab benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Schemata-Lab\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): List the metrics that are used to evaluate models on the OGC database?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"O\n","219 \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the CommitmentBank benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommitmentBank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the Reuters-21578 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the OntoNotes benchmark dataset?\n"," output (Sparql query): \n","823\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","We have just seen how different R users view the performance of andkgp and a few others. When R does not choose between these models, that is a good sign that the model is not very well-defined--that it lacks some kind of descriptive information. In this case, the model lacks the ability to estimate F1 for different datasets. By default, we expect that R will pick one of the three models to use for this purpose: orkgp or the model that outperforms it by a relatively large amount: orkgp.\n","This is how R behaves: it compares the performance of the model based on the output and the test results and then selects the best model based on this evaluation. In other words, to show the results of the comparison on the CommitmentBank benchmark dataset, this example would create an OR-tree-like structure: select test from (SELECT test FROM ORGKEY(?dataset?)) a; or if no criterion is specified, orkgp and its closest rivals a and kgp:LSTM will be used to determine which model performs best.\n","Of course, R developers have a range of options when deciding which model to use. These options represent the possibilities that are available.\n","Option 1: Just select the best-performing model and use OR-trees\n","If the output is a set of results in different datasets with an OR-trees tree structure, and the choice is between OR-trees and a very simple select function or select() loop over the output, the simplest option for R.app is to use an OR-trees tree structure and have R.app just select an OR-trees tree structure. This is shown in the following example:\n","1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n","220 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Atlantis dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 James Bond dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 James Bond\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Time Pilot dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Time Pilot\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset?\n"," output (Sparql query): \n","504\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Defender\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n"," output (Sparql query): \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Go-Go dataset?\n","output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Go-Go\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Pong dataset?\n","output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600\n","221 \n"," input (English text): Where can I find code references in papers that have used the Contextual Match model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Contextual Match\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Class Diagram model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Class Diagram\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Relation-Metric model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Relation-Metric\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes?\n"," output (Sparql query): \n","493\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Pointer + Coverage + EntailmentGen + QuestionGen\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," - orkgp:HAS_HISTORIC_BANDWIDTH (3) \n","input (English text): What happens if we use a certain benchmark for each criterion?\n","output (Sparql query): SELECT?criterion1-criterion2 ||?criterion3-criterion4 || (SELECT DISTINCT?criterion1 ||?criterion2 ||?criterion3 ||?criterion4 ||?criterion5 ||?criterion1||?criterion2||?criterion3||?criterion4 ||?criterion5 ||?criterion5||?criterion6 ||?criterion5 ||?criterion6||?criterion7 ||?criterion6||?),?criterion1-criterion3 ||?criterion4-criterion5 ||?criterion6-criterion8?\n","predictor:model_lbl (1) input (English text): Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QGen model for benchmarking purposes?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label\n","222 \n"," input (English text): Provide a list of papers that have utilized the Transformer-XL model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Transformer-XL (24 layers) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL (24 layers)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Feedback Transformer (4 layers) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Feedback Transformer (4 layers)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?\n"," output (Sparql query): \n","501\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Switch Transformer (4 layers)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }  \n","input (English text): Provide a list of papers that have utilized the Transition Transformer (2 layers) model and include the links to their code?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Transition Transformer (2 layers)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the Graft Transformer (4 layers and 9 layers) model and include the links to their code?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Graft Transformer (4 layers and 9 layers)\")?benchmark orkgp:HAS_DAT\n","223 \n"," input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the OpenBookQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OpenBookQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CommonsenseQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?\n"," output (Sparql query): \n","541\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input (English text): Can you list the models that have been evaluated on the OpenBookQA dataset?\n","output (Sparql query): \n","input (English text): Please list the models that have been evaluated on the \"OpenBookQA\" dataset\n","output (Sparql query): SELECT DISTINCT?model?model rdfs:label?model_lbl. OPTIONAL {?dataset a orkgc:Dataset; rdfs(?model. lbl?dataset_lbl). }\n","\n","\n","Results\n","\n","Below is the result set (please note that the results of the benchmark are still only a very early prototype). Notice that in this example, the R and SAS version have been included in the same output file as the SAS version on the right.\n","\n","\n","To read further information on this project, you can also refer to the included files for the SAS version, the R version, the MATLAB version, and the SQL version.\n","\n","Further Reading\n","224 \n"," input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Amazon-5 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-5\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the AESLC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Amazon-2 dataset?\n"," output (Sparql query): \n","509\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" input (English text): What evaluation metrics are commonly used when benchmarking models on the Amazon-12 dataset?\n","output (Sparql query):  input (English text): What measurement metrics are commonly used when benchmarking models that are created from different metrics?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon-12\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. }\n","The benchmark data set is available in the Amazon web server. The output from this benchmark is in the Parquet format.\n","This benchmark was run on the test set consisting of both unstructured and structured data. This benchmark is available in the Amazon web server. The output from this benchmark is in the Parquet format.\n","The Amazon benchmark is an interesting tool to investigate the performance properties of different model training methods. This benchmark measures the ability to extract the training sets of a wide range of metrics from the AWS benchmark set, and run a number of different model-training models on these training sets. It is therefore a good example of a benchmark that is designed to allow for rapid results in a given dataset in order to identify the critical metrics to evaluate models based on.\n","The Amazon AWS benchmark was used to evaluate models on the AWS-5 dataset. It is worth noting that the AWS benchmark is a large dataset which is very difficult to parse into a single set. That being said, this is a very useful tool to get a feel for the performance potential of different model training methods. The following example contains a test set where\n","225 \n"," input (English text): List the code links in papers that use the DDRL A3C model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDRL A3C\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the A2C+CoEX model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A2C+CoEX\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the CitClus model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CitClus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the A3C-CTS model in any benchmark?\n"," output (Sparql query): \n","479\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" output (Sparql query for CitClus)  \n","output (Sparql query for CitClus)  \n","226 \n"," input (English text): What is the top benchmark score and its metric on the CoNLL 2012 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the  Jacquard dataset dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \" Jacquard dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the CIFAR-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Hutter Prize dataset?\n"," output (Sparql query): \n","845\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Hutter Prize dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval }?cont orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } ORDER BY DESC(?value)\n","Example 2 Using a text query on the first and second rows, we can now see the difference between the metrics on the second row.\n","{ {  SELECT topScore (1, \"5\") as metric_lbl ;   {   SELECT firstScore (2, \"10\") as metric_lbl ;   {   SELECT secondScore (3, \"60\") as metric_lbl ;    } }  ORDER BY metric_lbl { \"firstScore\" \"2\" \"10\" \"secondScore\" \"60\" }\n","227 \n"," input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the ACE 2004 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the NYT24 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT24\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?\n"," output (Sparql query): \n","804\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (Japanese) benchmark dataset?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 score on the F1.1 (Italian) benchmark dataset?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 score on the F1.1 (Italian) benchmark dataset?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 score on the F1.1 (Italian) benchmark dataset?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 score on the F1.5 (Portuguese) benchmark dataset?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 score on the F1.5 (Portuguese) benchmark dataset?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 score on the VCA (Italian) benchmark dataset?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 score on the VCA (Italian) benchmark dataset?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 score on the VCA (Italian) benchmark dataset\n","228 \n"," input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Hutter Prize dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hutter Prize\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of PARAMS score when benchmarked on the Flowers-102 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"PARAMS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Flowers-102\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the One Billion Word dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"One Billion Word\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset?\n"," output (Sparql query): \n","846\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Params\") { SELECT?model?model_lbl WHERE {?dataset orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval {?dataset rdfs:label?dataset_lbl. } rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","If I select the last question, the output for the RDS database will look like the one below:\n","data ( RDS database) [2] Name: ngrams Dataset: ngrams.fetch.datasets.ngrams.0 Size: 2.8M Parameter:?value (N) [1] Parameter:?metastrat2. [3] Parameter:?ngrams.valc. [4] Parameter:?ngram.value. [5] Parameter:?metastrat1. [6] Parameter:?metastrat2. [7] Parameter:?metastrat3. [8] Parameter:?metastrat4. [9] Parameter:?ngrams.value. [10] Parameter:?ngrams.metastrat1. [11] Parameter:?n\n","229 \n"," input (English text): Can you list benchmarked problems in the area of Computer Sciences?\n"," output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Computer Sciences\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n"," input (English text): Can you list benchmarked problems in the area of Information Science?\n"," output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Information Science\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n"," input (English text): What research problems have benchmarked datasets under the Machine Learning research field?\n"," output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Machine Learning\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n"," input (English text): Can you list benchmarked problems in the area of Artificial Intelligence?\n"," output (Sparql query): \n","443\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Artificial Intelligence\")?paper orkgp:P30?rf; orkgp:P31  orkgp:HAS_BENCHMARK?benchmark; orkgp:P32  orkgp:P33?problem.?problem orkg:HAS_BENCHMARK rdfs:label?problem_lbl. } \n","Please feel free to share links to your own blog posts in the comments section below. Your contribution will be much appreciated, and we will publish back to back comparisons of each datasets where the corresponding authors were mentioned. And yes, this time I've also used an example of some random text output that was retrieved from the Google Scholar database. All data used in this post are available from the following data sets :  orkgp:P30, orkgm:HAS_BENCHMARK, orkgp:P32, orkgp:P33, orkgp:P35, orkgp:P36, orkgp:P37, orkgp_2:HAS_BENCHMARK and orkgp_2:P30.  And for those who would like to use SQL to benchmark their own papers, see my previous blog post on the use of SQL to benchmark Machine Learning. The source code for this post can be found inside my project project.  The source code for the original blog entry is located in the blog post entitled SQL Benchmarking: Machine Learning.  Please, note that the sources and the code are in my personal repository so feel free to fork it and\n","230 \n"," input (English text): What are the models that have been benchmarked on the SNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the SciFACT dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciFACT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the ANLI test dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the FSNS - Test dataset?\n"," output (Sparql query): \n","553\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input Input Dataset Results Dataset 1 SNLI/SciFACT - SNLI/SciFACT - SNLI/SciFACT 2 SNLI/NASTAT - NASTAT - NASTAT 3 NASTAT/SNLI - NASTAT - NASTAT 4 NASTAT/NASTAT - NASTAT - NASTAT 5 SFSNS/SciFACT-FSNS - SFSNS/SciFACT-FSNS 6 SFSNS/NASTAT - SFSNS/NASTAT - SFSNS/NASTAT 7 SFSNS/NASTAT/SFSNS/FSNS - SFSNS/NASTAT/SFSNS 8 NASTAT/SFSNS/NASTAT/SFSNS/FSNS - NASTAT/SFSNS/NASTAT 9 NASTAT/SFSNS/NATSAT-SFSNS - NASTAT/SFSNS/NATSAT 10 SFSNS/NASTAT/SFSNS/FSNS/FSNS - SFSNS/NASTAT/SFSNS 11 SFSNS/NASTAT/NATSAT-SFSNS - SFSNS/NASTAT/NATSAT 12 NASTAT/NATSAT/SFSNS/FSNS/FSNS - NASTAT/NATSAT/SFSNS 13 SFSNS/NASTAT/SFSNS/FSNS/FSNS - SFSNS/NASTAT/SFSNS 14 NASTAT/NATSAT/SFSNS/FSNS/FSNS/FSNS - NASTAT/NATSAT/SFSNS 15 NASTAT/SFSNS/SFSNS/AFSNS-SFS\n","231 \n"," input (English text): What is the best performing model benchmarking the WMT2014 French-English dataset in terms of BLEU metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the WMT2016 English-Romanian dataset in terms of BLEU metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Romanian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the IWSLT2015 German-English dataset in terms of BLEU score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric?\n"," output (Sparql query): \n","851\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["   select?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. {?dataset a orkgc:Dataset. {?metric a orkgc:Metric} rdfs:label?dataset_lbl. {?dataset a orkgc:Dataset. {?metric a orkgc:Metric}} rdfs:label?dataset_lbl. {?dataset a orkgc:Dataset. {?metric a orkgc:Metric}} rdfs:label?dataset_lbl. {?dataset a orkgc:Dataset. {?metric a orkgc:Metric}} rdfs:label?dataset_lbl. {?dataset a orkgc:Dataset. {?metric a orkgc:Metric}} rdfs:label?dataset_lbl. {?dataset a orkgc:Dataset. {?metric a orkgc:Metric}} rdfs:label?dataset_lbl. {?dataset a orkgc:Dataset. {?metric a orkgc:Metric}} rdfs:label?dataset_lbl. {?dataset a orkgc:Dataset. {?metric a orkgc:Metric}} rdfs:label?dataset_lbl. {?\n","232 \n"," input (English text): Where can I find code references in papers that have used the ELMo model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Attentional encoder-decoder + BPE model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Attentional encoder-decoder + BPE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the BiLSTM-CRF+ELMo model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM-CRF+ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-Attention + ELMo model?\n"," output (Sparql query): \n","506\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note that the output list should include all of the results listed in the previous table, as well as other information about the particular dataset and model used, such as the dataset name, a link to the authors' paper that used that dataset, and the name of the code that the paper uses that implements the model. In our sample of two papers we used, both articles had the same code, so we provided those results in the previous table. This can also be used as a way to identify the code that is often missing (e.g. you aren't seeing it in the list above), even if that code is a variation of a code that might be included in more papers (see the question \"what if my code is missing?\" in the FAQ).\n","Note also that the results in the previous table are only the code outputs from the specific model we focused on in the paper. In other cases, your results may be different. For example, if you chose to use the ELMo model on a BOLD dataset, you may also find that the BOLD output of your system isn't always consistent with the ELMo output. If this is the case, you may want to look at our BOLD dataset. You might also find that the ELMo model isn't working for your dataset - try removing the ELMo model from the dataset without modifying all of the parameters, and try again after the code output is changed. Our model also isn't perfect, it contains several subtle design flaws that may not be immediately obvious.\n","There are other options for examining BOLD vs ELMo results:\n","Try this version of the BOLD dataset and see what happens. If you find that BOLD output is consistently lower (due to having more or equal counts in the BOLD bins than the ELMo bins), you'll need to rerun the BOLD on the ELMo version of your version of\n","233 \n"," input (English text): What is the highest benchmark result achieved on the WSC dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the STL-10 dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STL-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the arXiv dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"arXiv\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value?\n"," output (Sparql query): \n","858\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["-------------- ------ --------------- --------------- 2 10.81201 100 2 10.77986 101 2 10.93474 104 2 11.06493 110 2 11.16493 112 2 11.31980 116 2 11.50505 119 2 11.60702 122 2 12.06842 123 2 12.19095 125 2 12.34881 126 2 12.52126 128 2 12.86765 100 2 12.96878 101 2 13.11109102 2 2 13.48806 109 2 13.96113 110 2 14.1116122\n","output (Sparql query): SELECT TOP 5 SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) FROM arXiv_data.tbl_lbl GROUP BY?metric?metric_lbl\n","input (English text): Which dataset are the benchmark results for? If the answer is \"STL-10\", then the benchmark is the one for the STL-10 dataset and the highest score is 10.81201, a level of performance that is comparable with the previous one (10.699). With this benchmark data set the maximum score for \"arXiv.org paper number 95830\" is 11.2.\n","output (Sparql query): -------------- ------ --------------- --------------- 2 1000 1000 2 11.2 11.2 2 1000 1000 2 11.2 11.2 2 1000 1000 2 11.2 11.2 2 1000 1000 2 11.2 11.2 2 1000 1000 2 11.2 11.2 2 1000 1000 100 1.000 100 111.6 6 10.6 12.6 1 11.2 10.6\n","input (English text): What is the maximum score achieved on the arXiv dataset, including the metric\n","234 \n"," input (English text): Provide a list of papers that have utilized the EfficientNet-L2-475 (SAM) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"EfficientNet-L2-475 (SAM)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Classifier Chain + SMO model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Classifier Chain + SMO\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Prior hs model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code?\n"," output (Sparql query): \n","506\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SELECT DISTINCT?test_id?name FROM [my_test_data] ORDER BY id DESC; orkgp:HAS_SWEINGLE?name ---------- | 1| 1| 2| 2| 3| 3| 4| 4| 5| 5| 6| 6| 7| 7| 8| 8| 9| 1| 2| 2| 3| 3| 4| 4| 5| 5| 6| 3| 3| 4| 4| 5| 5| 1| 2| 2| 1| 0| 1| 2| 1| 1| 1| 0| 0| 1| 1| 1| 0| 1| 1| 0| 0| 1| 1| 1| 0| 1| 1| 1| 1| 1| 100| | 1| 2| 2| 2| 3| 3| 4| 4| 5| 5| 6| 6| 7| 7| 8| 8| 9| 10| 11| 12| | 3| 3| 4| 4| 5| 5| 6| 6| 7| 7| 8| 8| 9| 10| 11| 12| 1| 0| 2| 0| 2| 0| 2| 0| 0| 1| 1| 0| 1| 1| 0| 1| 1| 1| 1| 1| 1| 1| 1| 1| 1| 1| 100|\n"," input (English text): Provide a list of papers that have utilized the ConvNets model and include the links to their code?\n"," output (Sparql query): \n","select id, name from [my_test_data] join test_data table 2 id name ----------------------------- 12\n","Inputs: ------------------------- input (English text): Provide a list of papers that have utilized the EfficientNet-L2-\n","235 \n"," input (English text): Name the datasets that have been used for benchmarking in the Relation Extraction research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Relation Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Named entity recognition research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Named entity recognition\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): List the datasets benchmarked under the relation extraction research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"relation extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field?\n"," output (Sparql query): \n","512\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"association extraction\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \n","To summarize, a dataset must be used if at least one of the criteria is met.\n","The standard dataset selection procedure can be found in the standard procedure library. In the example below, i.e. i_dumpsize=64 will select the following datasets: \n","\"Relation Extraction\", \"K2 Text Classification\" and \"Association Extraction\" with the following data: \n","i_dumpsize=64; \n","The standard procedure library also includes a \"standard_results_library\". In the example given earlier on e.g. i_gslv.pl, we select to the following datasets:\n","\"Relation Extraction\", \"K2 Text Classification\" and \"Convert\" with the following data: \n","i_gslv.pl(standard_results_library(16).loadall(\"i_dumpsize=64,k2text=tf2.text.vector('K2 text classification',i_gslv.dummy.data=True,0,k2text=tf2.text.vector('Convert')',i_gslv.dummy.data=True)),\n","The \"Standard Results Library\" can be used\n","236 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Challenge)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the AESLC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?\n"," output (Sparql query): \n","520\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MultiRC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?  output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ARC (Challenge)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the AESLC dataset?  output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"\n","237 \n"," input (English text): Where can I find code references in papers that have used the SRU++ model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SRU++\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the SRU++ Large model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SRU++ Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DQN+SR model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN+SR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the SRU++ Base model for benchmarking purposes?\n"," output (Sparql query): \n","483\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","output (Json-data): DQN+SRmodel-3.0-b19.json\n","output (Json-data): DQN+SRmodel-3.0-b19.json  input (English text): Do you have a reference to the paper where you started?  output (Json-data): DQN#1495 - The Search for Extropy,  http://www.researchgate.net/profile/Travis_Zhang/publication/2626292423_The_Search_for_Extropy/links/2_Full_Paper_PDF/links.html\n","output (Sparql query): SELECT DISTINCT?model_lbl AND?model2 OR?model22 FROM?dataset_lbl? WHERE {?model1 = LABEL(?model1)?model2 = LABEL(?model2) } WHERE orkgp:HAS_DATASET?dataset. ||kgp:HAS_HAS_MODEL?model AND {?model2 = LABEL(?model2)?model3 = LABEL(?model3) } WHERE orkgp:HAS_SOURCE_CODE?model AND?model2 AND {?model3 = LABEL(?model3) } WHERE orkgp:HAS_MODEL?model AND {?model3 = LABEL(?model3)?model4 = LABEL(?model4) } WHERE orkgp:HAS_SOURCE_CODE?model2 AND?model3 AND {?model2 = LABEL(?model2)?model4 = LABEL(?model4) } WHERE orkgp:HAS_SOURCE_\n","238 \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Alien benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Alien\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Robotank benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Robotank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Zaxxon benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Zaxxon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asteroids benchmark dataset?\n"," output (Sparql query): \n","825\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","This example used the following syntax to output the scores of each model:\n","output (Slim SQL result set) SELECT?data.score,?data.test.score FROM data JOIN test ON test.data.test.id = data.test.id JOIN test_rs ON test_rs.id = data.rs.id ORDER BY?data.score,?data.test.score,?data.rs.id\n","This example used the following syntax to generate a table that demonstrates the correlation among the scoring results for each benchmark dataset:\n","output (Slim SQL table) SELECT test.score,?test.test_score, %data.score DISTINCT?test.test_score FROM?test UNION SELECT?test.score =?test.test_score, %data.score ORDER BY?test.score, %data.score,?test.id\n","1.17. Performance benchmarking results\n","After examining the results for the Atari 2600 Robotank benchmark datasets for the last three weeks, we are confident that the following performance benchmarks have achieved comparable accuracies for using any model through the algorithm described in this section: andkgp:HAS_DATASET.\n","1.17.1. Atari 2600 Robotank Benchmark Performance Benchmarks We have found four performance benchmarks for the Atari 2600 Robotank benchmark dataset: andkgp:HAS_DATASET, andkgp:HAS_EVALUATION (test), andkgp:HAS_METRIC.\n","andkgp:HAS_BENCHMARK andkgp:HAS_MODEL. In addition to the performance benchmarks, we also compared our version to three versions of the Atari 2600 Atari 2600 Robotank benchmark dataset (1.26/3.02, 1.27/3.\n","239 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the WMT2016 Romanian-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Romanian-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the WMT2016 Russian-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Russian-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 English-German dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the WMT2014 German-English dataset?\n"," output (Sparql query): \n","520\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 German-English\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","Input data input (English text): An example string that might be suitable for a benchmark query to get benchmark results for the WMT2014 dataset?\n","output (Sparql query)  SELECT?dataset b?dataset_lbl.?p?pb_lbl.?n?n_lbl.?i?i_lbl. {?dataset a orkgc:Dataset;rdfs:label?dataset_lbl. }  (string) Output string to use while benchmarking with benchmark-bench-dataset-name:b in the query.\n","output (Sparql query)  SELECT Benchmark(?dataset ) or Benchmark(?dataset,?pb_lbl )?dataset.?pb_lbl. OR Benchmark(?dataset,?n ).?n.?i. OR Benchmark(?dataset )[?n,?i] [?n,?i] WHERE?dataset b.?pb_lbl. OR... Output string for benchmarking with benchmark-bench-dataset-name:b in the query.\n","Example output in.sql file\n","240 \n"," input (English text): What are the models that have been benchmarked on the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the SciFACT dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciFACT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the WebQuestions dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WebQuestions\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the SearchQA dataset?\n"," output (Sparql query): \n","553\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","You don't need to define an entity. It's an example where you use the term \"model\" and the entity is called \"paper\". You do need to specify entity names in the WHERE clause if you use multiple names for the same entity. The entity may be called \"model\" or \"paper\" and it cannot have many other entities. \n","type (Query): Select, if you have multiple queries for the same record, select and add them as a subquery and do not include the WHERE clause.\n","options (Option): Options for the query. You use the comma-separated list of options to define how many values of each type you want. Note that this is an unclosed list of values and not an array of values, which is required in this case since we have used the?option id=value syntax.\n","data (Array): A collection of model outputs to be queried for the same record. We can retrieve the same model outputs by using the same entity. The?model id=value construct can be used to refer to any of the model outputs to retrieve. They are considered to be the same model output.\n","model_index (Index): The index on which the object should appear in the output. You specify the full index prefix, this is required and can become a value of 4-8.\n","batch_size (Integer): The size of the batch size to store all the models to be queried and the size to store only one output (see below). If you specify the batch size and no indexes, the queries will be performed on the query results, but an order by will be used to retrieve the model outputs in the order they've been inserted on the cluster. The default batch size is 5. Note that an order by use will retrieve the top-level result, which can be used to retrieve more rows than the maximum available.\n","241 \n"," input (English text): What are the metrics of evaluation over the Open Entity dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Open Entity\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the ADE Corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ADE Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the enwiki8 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwiki8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the OntoNotes dataset?\n"," output (Sparql query): \n","499\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------------------ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --  3.3.6.4 (2018-06-10) ChangeLog: -- [17/03] v 1.1 - Correct a spelling mistake on the query that should be\n","\n","(2018-06-10) - Correct a spelling mistake on the query that should be [17/03] v 1.1 - Initial Release of the RDF SDF feature. - Added'score' and 'best-score' options. - Added the ability to override a metric using the -r option.\n","\n","Changes\n","\n","v 1.1 - Initial Release of the RDF SDF feature.\n","\n","v 1.0 - Initial release, including the data extraction and clustering options.\n","\n","v 0.9.6 - Fix an issue with the -r option.\n","\n","v 0.9.5 - Fix a minor typo in a parameter that should have been -.\n","\n","v 0.9.4 - Fix issue with the datastore name.\n","\n","v 0.9.3 - Fix issue in multi-model prediction for certain metrics that should be 0.\n","\n","v 0.9.2 - Adjust for missing data sets in the default test set.\n","\n","v 0.9.1 - Fix a minor issue with a label in one of the CSV documents at the bottom of the document.\n","\n","v 0.9.0 - Initial release.\n","\n","Changes\n","\n","v 0.9.0 - Initial release.\n","\n","v 0.8.2 - Fix a minor issue with the datastore name in model.\n","\n","v 0.8\n","242 \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Space Invaders dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Space Invaders\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Boxing dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Boxing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Defender dataset?\n"," output (Sparql query): \n","507\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","The full output is available in SPSS format.\n","SPSS output\n","The SPSS output of the above sample provides the following:\n","- A new set of benchmark results\n","- An extensive list of benchmarks and descriptions\n","- A list of statistics for the dataset\n","Note that not all of the benchmark results have been published (eg. number of runs, average score per run, top score) and it would be beneficial to be able to access these details. As the dataset includes more than 80 variables, the results from all benchmark measurements will depend on the sample size. To view the full SPSS format, the code can be found in SPSS output.\n","The SPSS format provides the following output:\n","Titles:\n","- A list of the games included (or excluded) in each benchmark\n","- The average value per time step\n","- Statistics about each benchmark\n","- A table to display each time step and a total score\n","- A breakdown of the performance of each benchmark by game (using only the titles included)\n","- The \"top 1000\" in alphabetical order\n","- The \"top 2000\" in alphabetical order\n","- A report on the number of tests with sub-benchmarks (in descending order)\n","- A breakdown of what information was collected and the time it took to collect it (titles, time and sample sizes). \n","Other useful information:\n","- How many benchmarks were used to create the dataset (eg. for the Atari 2600 Adventure benchmarks, which were used for the test suite)\n","- A list of the tests used to provide a score (eg. how many times different features were tested for each test)\n","Conclusions:\n","- The games in the Atari 2600 Adventure benchmark do not show any distinct trends. In fact, the best Atari 2600 Adventure games display large swings in score over time, suggesting a lack\n","243 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset WMT2014 German-English?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the WMT2014 English-French dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 English-German?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value?\n"," output (Sparql query): \n","875\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------ USERS (ALL) USERS (DUAL) ----------------- ------ 1 23 486 556 ------------ ------------ Total Score (4/4) : 7.38 1.00 0.71 8.06 1.33 2.50 6.78 11.43 7.40 1.00 0.71 8.06 1.33 2.50 6.78 11.43 7.40 1.00 0.71 Total Score (9/9) : 10.10 1.00 1.21 18.56 1.67 24.63 10.46 29.43 12.09 9.40 1.00 1.21 18.56 1.67 24.63 10.46 29.43 12.09 9.40 1.00 0.71 RATIONAL (ALL) RIOSTICS (ALL) ------------------------------------------------ 3.89 2.20 1.19 1.63 4.10 2.03 4.54 5.27 7.46 7.05 2.16 1.20 2.61 2.11 4.09 2.05 4.54 5.27 7.46 7.05 2.16 1.20 RIOSTICS (DUAL, ALL) RIOSTICS (DUAL) ----------------- 6.03 3.70 1.10 1.73 4.43 2.36 6.17 5.82 6.45 2.56 1.10 3.50 3.30 4.43 2.36 6.17 5.82 6.45 2.56 1.10 RIOSTICS (DUAL, DUAL ONLY, ALL) RIOSTICS (DUAL, DUAL ONLY, ALL) ---------------- 6.20 3.70 1.10 1.73 4.43 2.36 6.17 5.82 6.45 2.56 1.10 3.50 3.\n","244 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset PWC Leaderboards (restricted)?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PWC Leaderboards (restricted)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): List the title and ID of research papers that contain a benchmark over the WOS-11967 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-11967\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Name the datasets that have been used for benchmarking in the Leaderboard extraction research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Leaderboard extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset?\n"," output (Sparql query): \n","636\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT?task?task_lbl, SUM(?task_lbl) OVER (PARTITION BY?task?task_lbl ORDER BY?task_lbl) as score FROM?task?task_lbl ORDER BY?task,?task_lbl \n","Note: When querying for a particular task, be sure that your query uses a numeric value with the appropriate count of rows that have that task specified. If you are using multiple queries, you might need to use CASE WHEN (task?task_lbl ==? task or task?task_lbl ==? task_lbl ).\n","Note: Be sure that any values of the task, including columns, that reference the dataset that you are searching for have been normalized by your query.\n","input (English text): Do you have a dataset with a benchmark for the MPL-18 dataset?\n","output (Sparql query): SELECT?dataset?dataset_lbl, (?'dataset a ORkgc:Dataset?' ORkgc:Problem?dataset) FROM?dataset?dataset_lbl ORkgc:Problem ORkgc:Leaderboard?dataset.GROUP BY?dataset?dataset_lbl AS task,?dataset?dataset_lbl ORDER BY?task,?task_lbl ORDER BY?task/?task_lbl AS score\n","Note: Be sure that your query uses a numeric value with the appropriate count of rows that have that dataset specified. If you are using multiple queries, you might need to use CASE WHEN (task?task_lbl ==? task OR task?task_lbl ==? task_lbl ).\n","Note: Be sure that the values of the tasks that are being used in any\n","245 \n"," input (English text): List the metrics that are used to evaluate models on the WikiText-2 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the WOS-5736 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the CoNLL 2012 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the enwik8 benchmark dataset?\n"," output (Sparql query): \n","526\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input (English text): List the metrics that are used to evaluate models on the Wiki-20 benchmark dataset?\n","output (Sparql query):  \n","Example 4. The data are the following:\n","input (English text): Name of model.\n","output (json): Metric in the dataset.\n","example (input): Metric: Sorted list of integers. The metric count is given after each unique element. Metric Name \n","test (English text): Count of metric instances that will be evaluated.\n","example (input): ____________ ____________ ____________ ____________ ____________ ____________ ____________ ____________ ____________ ____________ ____________ ____________ ____________ ____________ _________________ ___________________________________________________________________ _ _____________ _______________________ _____ ___________________________________________________________________ _________ _________ __________ ________ _____ _ _________________( ________ ________ _____________ ________ _________ __________ __________________ __________________________ ____________________________________ ____________________ _________________________________\n","1) The metric \"test\" is \"not applicable\" in the \"not applicable\" field as the metrics for all the training and testing epochs are not unique among the models. The metric \"not applicable\" can therefore be used as a measure of whether the model does not fit well for a specific observation.\n","2) The metric \"sorted list of integers\" is actually just an expression (namely, a simple list) that contains the items that form the metric list.\n","3) The metric \"not applicable\" is not applicable in the \"not applicable\" field since it is part of the model. It can therefore be used as a very flexible gauge for comparing the performance of an ensemble of models.\n","4) The name of the model is \"model.json\".\n","5)\n","246 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the TACRED dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TACRED\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Dmlab-30 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dmlab-30\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the AESLC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset?\n"," output (Sparql query): \n","523\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TEMPETEMP3\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): What evaluation metrics are commonly used when benchmarking models on the TempHatch-3 dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TEMPATCH3\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n","\n","SELECT c2 as i, y2 as m, m2 as n1, y2 as m2, n1 as n2, y1 as m1 FROM dataset.datasets c i, m i, n1, n2, y1 AS y2 m 2 WHERE (m2 == n1) && (y2 > y1) OR (m2 == y1) GROUP BY\n","247 \n"," input (English text): In what country was conducted research with the largest number of participants?\n"," output (Sparql query): SELECT ?country WHERE { orkgr:R44980 orkgp:compareContribution ?contrib. ?contrib orkgp:P15249 ?country; orkgp:P23169 ?number_of_participants. } ORDER BY DESC( ?number_of_participants) LIMIT 1\n"," input (English text): Which are five the most common research fields for papers?\n"," output (Sparql query): SELECT ?research_field, ?research_field_labels WHERE { ?papers orkgp:P30 ?research_field. ?research_field rdfs:label ?research_field_labels. } ORDER BY DESC(COUNT( ?research_field_labels)) LIMIT 5\n"," input (English text): What is the most common drug in the studies?\n"," output (Sparql query): SELECT ?drug, ?drug_labels WHERE { orkgr:R155621 orkgp:compareContribution ?contrib. ?contrib orkgp:P37578 ?drug. ?drug rdfs:label ?drug_labels. } ORDER BY DESC(COUNT( ?drug)) LIMIT 1\n"," input (English text): What is the most common location in the studies?\n"," output (Sparql query): \n","307\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["𝒻𝒪𝒦𝒪𝒩𝒪 𝒬𝒧𝒪𝒧𝒦 𝒲𝒥𝒣𝒥𝒥𝒥𝒥𝒥𝒥 𝒥𝒦𝒬𝒧 𝒧𝒦𝒫𝒦𝒫 𝒳𝒮𝒥𝒥𝒿𝒿 𝒥𝒡𝒥𝒦𝒫𝒤𝒑 𝒾𝒯𝒦𝒲𝒥𝒦𝒫𝒦𝒫𝒦𝒫 𝒦𝒥𝒧𝒮𝒨𝒫𝒦 𝒬𝒨𝒫𝒩𝒦𝒫𝒨𝒨𝒫 𝒮𝒵𝒥𝒞𝒦 𝒼𝒬𝒥𝒴𝒮 𝒿𝒣𝒹𝒻 𝒮𝒯𝒥𝒬 𝒦𝒥𝒬 𝒫𝒿𝒧𝒦𝒫, 𝓲𝒨𝒿𝒯, 𝒯𝒨𝒳𝒥\n","output (Sparql query): 𝒸𝒮𝒾𝒯𝒴𝒯𝒪𝒞𝒪𝒢𝒟𝒕𝒕𝒚𝒄�\n","248 \n"," input (English text): Provide a list of papers that have utilized the He et al., 2017 + ELMo model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"He et al., 2017 + ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the BiLSTM-CRF+ELMo model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM-CRF+ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the ELMo model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the BCN+ELMo model and include the links to their code?\n"," output (Sparql query): \n","504\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Provide a list of papers that have utilized the BOLD+Elmo model and include the links to their code?\n","output (Sparql query): \n","input (English text): Input file with benchmark papers\n","output (Sparql query): SELECT?code AS?model, d.model AS?dataset,?benchmark as?benchmark,?labels as?labels,?rdfs as?rdfs FROM?model (?labels) INNER JOIN?sdmlr (?labels,?rdfs ) ORDER BY DESC (?dbname,?dataset,?benchmark,?dbpath )\n","input (English text): Please provide a benchmark paper for the BCN model.\n","output (Sparql query): SELECT DISTINCT?code FROM?model d.model WHERE DISTINCT?benchmark ORDER BY DESC ([?dbname (?dataset) DESC (?dbpath ) ], [?dbpath DESC (?dbname),?dataset DESC (?dbname) ]))\n","input (English text): Please provide a benchmark paper for the BOLD+Elmo model.\n","output (Sparql query): SELECT?code AS?model, d.model AS?dataset,?benchmark as?benchmark,?labels as?labels,?rdfs as?rdfs FROM?model (?labels) INNER JOIN?rdmlr (?labels,?rdfs ) ORDER BY DESC (?dbname,?dataset,?benchmark,?dbpath )\n","input (English text): Input file with benchmark papers (with ELMo's code)\n","output (Sparql query): SELECT DISTINCT?\n","249 \n"," input (English text): Indicate the model that performed best in terms of PRE-TRAINING DATASET metric on the DCASE benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"PRE-TRAINING DATASET\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DCASE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the HMDB51 dataset in terms of Top-1 Accuracy metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the MNIST benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset?\n"," output (Sparql query): \n","840\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HMDB51\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","This query produces a result set in the following format where column references are replaced by their actual fields. \n","rdfs:column?table?schema.criterion\n","label:?column?source?name. Criterion is name of criterion that was defined by the table schema that the table describes.\n","column:?table?schema.criterion\n","source:?column?source?name. Criterion source or the name of criterion.\n","Or, here's the query with parameters in the same format. \n","s:\n","parameter:?table?schema.criterion and. table schema, if any. The schema parameter is a name of a table schema... in the input, there need not be.\n","parameter:?field?field_name. field name.\n","field_name:?field_name. Specification of field name.\n","field_\n","250 \n"," input (English text): Indicate the model that performed best in terms of Bits per byte metric on the Classical music, 5 seconds at 12 kHz benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Bits per byte\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Could you provide a list of models that have been tested on the AudioSet benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AudioSet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Provide a list of benchmarked datasets related to the Music Modeling research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Music Modeling\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Can you list the models that have been evaluated on the Classical music, 5 seconds at 12 kHz dataset?\n"," output (Sparql query): \n","646\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric b orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Bits per byte\") { SELECT?problem?problem_lbl. OPTIONAL {?problem orkgc:Problem }?record }(?problem?problem_lbl. } OPTIONAL rdf:label?problem_lbl. ROW JOIN {?problem?problem_lbl.*} orkgp:HAS_DATASET if rdf:LABEL?problem_lbl. } ORkgp:HAS_EVALUATION if rdf:EVALUATION_LOOKUP if str(?metric_lbl) = '' or kgc:Dataset_lbl. } ROW JOIN {?problem?problem_lbl.*} orkgp:P32 if rdf:LABEL?problem_lbl. } Orkgp:EVALUATION if str(?metric_lbl) = '' or kgc:Dataset_lbl. ORDER BY DESC(?data?) LIMIT 1\n","  input (English text): Could you provide a list of models that have been evaluated on the Audio Set benchmark dataset?\n"," output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Audio Set\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS\n","251 \n"," input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2014 German-English benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of BLEU metric on the WMT2014 English-German benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of BLEU metric on the WMT2016 English-German benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?\n"," output (Sparql query): \n","852\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IWSLT2014 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","This query illustrates (in one way or another), the way that BLT works. The output from this query can therefore be displayed using SQL syntax in BLT expressions. Note that the order in which the rows from both BLTC and STLC queries are shown in order of performance is strictly the reverse of the order in which the columns were entered in the queries.\n","Output Formatting: (This section is optional; you could always leave it as the default.) The default output format used by STLC is BLTC. You can also choose how BLTC should be rendered using the OPTIONAL tag in BLTB and STLDL values. For example: OPTIONAL is translated to STLC and NOT translated to BLTC, while OPTIONAL with OPT\n","252 \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the BC5CDR-disease dataset in terms of F1 entity level metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 entity level\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the ACL-ARC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset?\n"," output (Sparql query): \n","839\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1)\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-disease\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model orkgp:HAS_DATABASE?database. } ORDER BY DESC(?value) LIMIT 1 } } \n","input (English text): Indicate the model that performed best in terms of G1 on the ACL-ARC benchmark dataset?\n"," output (Sparql query):  SELECT DISTINCT?model|-?name?name. ORDERS_ORDER?ORDER?name. FROM rdfs { ORAGGp:HAS_BENCHMARK?benchmark ORAGGp:HAS_EVALUATION?eval ORAGGp:HAS_VALUE?value ORAGGp:HAS_METRICS?metric?name. } ORDER BY DISTINCT?name ORDER BY name LIMIT 2 } \n","input (English text): Indicate\n","253 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Gopher dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Gopher\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Tennis dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tennis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Kangaroo dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Kangaroo\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Skiing dataset?\n"," output (Sparql query): \n","526\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","The output above should yield the following result set. You can view the test data either inline or through a visual aid.\n","The following table shows all the different evaluations that you can set up on the Atari 2600 Sports game. The three metrics that you've already selected above are listed below the metric table. Below the table, you will find a brief description of each of the 3 evaluation metrics used by Atari in both the Tennis and Kangaroo games. We recommend that you review the manual pages for each of these 3 evaluation metrics before configuring a benchmark on one of them. The manual pages are at the bottom of this document with the manual page on page 26 showing the details on how to configure the benchmark for each of the 3 metrics to take into account the input or validation data you provide.\n","The metric names assigned to the Atari 2600 Games\n","Atari has three different sets of metrics that are configured internally using the built-in rdfs::rdfs() and orkgp::HAS_DATASET metrics classes. There are several unique aspects to how each set of metrics is used that make it very difficult to generalize from the default rdfs::rdfs() and orkgp::HAS_EVALUATION metrics to all other kinds of model comparisons. For the purpose of the benchmarking, these 3 metrics are the most important. This allows you to quickly set up a quick, single-pass comparison benchmark for comparison data and then fine-tune the metrics to be used for further analysis without needing to re-establish the same or other metrics.\n","254 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Battle Zone dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Zaxxon dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Zaxxon\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Alien dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Alien\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?\n"," output (Sparql query): \n","503\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Xevious dataset?\n"," output (Sparql query):  input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Zaxxon dataset?\n"," output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Zaxxon\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }  \n","input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Dune II dataset?\n"," output (Sparql query):  input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Xevious dataset?\n"," output (Sparql query):  input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Alien dataset?\n"," output (Sparql query):  input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Zaxxon dataset?\n"," output (Sparql query):  input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?\n","output (Sparql query):  input (English text): List the title and ID of research papers that contain a benchmark over\n","255 \n"," input (English text): Provide a list of papers that have utilized the COMET - DynaGen model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"COMET - DynaGen\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Commonsense Transformers (COMET) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Commonsense Transformers (COMET)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the DeiT-B 384 model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DeiT-B 384\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the COMET - Direct model?\n"," output (Sparql query): \n","489\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Which code runs for the DeiT-B 384 Model (v10)?\n"," output (Sparql query): SELECT DISTINCT?code FROM {?model a b c }?dataset?cont OR?dataset.?cont or?dataset. ORI|T?model. ORA|T?model. ORA|T?model. ORI|T?model. ORA|T?benchmark ORI|T?benchmark. ORI|T?baseline ORI|T?benchmark. ORI|T?baselines orkgp:O/6?cont ORI|T?cont. ORI|T?benchmark ORI|T?baselines orkgp:O/6?benchmark ORI|T?baselines. ORI|T?benchmark. ORI|T?baselines orkgp:O/7?cont ORI|T?cont. ORI|T?benchmark ORI|T?baselines orkgp:O/7 ORI|T ORI|T ORI|T ORI|T ORI|T ORI|T ORI|T ORI|T ORI|T AS *, ORI|T AS *, ORI|T AS *, ORI|T AS *, ORI|T AS *, ORI|T AS *, ORI|T AS *, ORI|T AS *, ORI|T INNER JOIN DISTINCT c ON c.dataset.databasenote = c.datastore.databasenote ORI|AS ||CUSTOM/CUSTOM|ORI|ASTOREXECIAL|ORI|ASTOREXEC\n","256 \n"," input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the WMT2016 Russian-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Russian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the WMT2016 English-Russian dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset?\n"," output (Sparql query): \n","580\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","output (Png images):  image 1.png   image 2.png  document 1.docx, document 2.docx \n","PNG images by author:  Dmitrii Svetlova\n","257 \n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Space Invaders dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Space Invaders\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Battle Zone dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Pitfall! dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Pitfall!\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value?\n"," output (Sparql query): \n","869\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["***************************** { { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Boxing:Boxing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } *****************************\n","I also modified the query to take both the metric and the value. The first column would contain the metric and the second column the value.\n","I used the following parameters. They're set to the values in the R command. I could have used one or two more.\n","DATA = #(?value)\n","The following was the output.\n","I used the following parameters. They're set to the values in the R command.\n","data = RDBMSRDF; rsql = 'SELECT 1+1+1 + 1+1+1 as score;FROM a.dataset AS a and b.dataset AS b' rdfs = 'ORDERSET | TABLE_COUNT;' rdfs:set_metric('score','1'. '|'); rdfs:set_value\n","258 \n"," input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the AAPD benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AAPD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the ScienceCite benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?\n"," output (Sparql query): \n","802\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","select table_name,?dataset as orkgc;\n","output (Sparql query): SELECT DISTINCT?model?model_lbl FROM (SELECT ORLSAME_LOOKUP(c,?dataset)) as table_name; dt:table,?dataset, CATEGORIES{?id=?dataset,?name='c',?columnname=.name} ORDER BY DESC(dt.table_name) LIMIT 1;\n","input (English text): Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?\n","output (Sparql query): \n","select table_name,?dataset as orkgc;\n","output (Sparql query): SELECT DISTINCT?model?model_lbl FROM (SELECT ORLSAME_LOOKUP(c,?dataset)) as table_name; dt:table,?dataset, CATEGORIES{?id=?dataset,?name='c',?columnname=.name} ORDER BY DESC(dt.table_name) LIMIT 1;\n","The table_name is the name of the table that contains the model we will use for training ( table_name in the above example is F2, not a specific model). Using ORLSAME_LOOKUP we can take advantage of the fact that each type of table returns the list of all of the members. We then search for ORLSAME_LOOKUP() on columnnames of a specific model and all columns on the same table. If we do find that the columnnames are exact we save the model. Because the results of the ORLSAME_LOOKUP algorithm is the same across platforms this will be the case regardless\n","259 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset IWSLT2014 German-English?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the IWSLT2015 English-German dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset WMT2014 German-English?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset?\n"," output (Sparql query): \n","880\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["-------------- select highest score ------------- from {?dataset a} select highest score from test result select highest score, metric, score ------------------ highest score metric highest score score ----------- ----------- 976 972 979 979 971 973 969 967 962 961 961 960 956 942 912 921 880 1,082 1084 1,108 1,124 1,136 1,149 1,181 1,173 1,186 1,186 1,185 1,184 1,185 1,189 1,190 1,191 1127 1079 1075 1075 1076 1085 1081 1098 1,004 878 878 878 859 855 858 856 859 857 859 858 857 858 856 856 854 853 852 851 820 841 842 842 842 845 845 845 849 853 864 863 861 868 869 870 869 867 872 870 867 872 867 871 867 873 871 836\n","Input 1: What is the upper bound on the metric for the IWSLT2015 German-English dataset?\n","Output 1: Maximum results of the query with metric 0.07 (0.03 for WMT2014, 0.00, 0.01 for WMT2014, and 0.00)\n","Input 3 (text): The top test score is 876 for the IWSLT2015 English-German dataset.\n","Output 3 (Sparql query): SELECT SIZE(B.score) AS?score, MAX(B.score) AS?score FROM test result B.score, (SELECT?B.score AS?score, MAX(B.score) AS?score FROM test result B WHERE {?\n","260 \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Krull dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Krull\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Time Pilot dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Time Pilot\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Atari 2600 Frostbite dataset?\n"," output (Sparql query): \n","510\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What are the metrics of evaluation over the Atari 2600 Frostbite dataset? output (Sparql query): SELECT DISTINCT?metric? {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. } OPTIONAL {?benchmark orkgp:HAS_DATASET?dataset?benchmark. }\n"," \n","The first two output functions in the previous example have the same names as the output functions that are currently defined in the Hadoop Dataflow Engine, namely orkgp and orkgp. The third output function in the previous example, however, is not yet defined.\n","As the name would suggest, and as the other output functions do not, either rdfs or metric is an abstract name in a Hadoop HBase metadata object for each dataset. For example, the HBase objects for the Atari 2600 and Krull datasets have the type int that indicates that the dataset is an orkgp.\n","In the above example this HBase object is not a \"meta\" object. In fact, this HBase object is NOT \"meta\", but a real metadata object. In other words, it holds the details of each dataset. Since the HBase object provides a name to each dataset that is defined, to create an rdfs or metric object you would need to actually create the objects that it contains for each dataset, rather than using HBase's metadata to specify a name for each dataset.\n","However, many of the fields of the HBase objects are not used for anything, thus the objects that they have in them are not part of the rdfs. The fields of the HBase objects that are used for things do not directly relate to datasets. If a field is specified by a dataset, for example\n","261 \n"," input (English text): What is the top benchmark score and its metric on the SoMeSci dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SoMeSci\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the MUTAG dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MUTAG\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Which model has achieved the highest Matched score on the MultiNLI benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Matched\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark score and its metric on the MultiNLI dataset?\n"," output (Sparql query): \n","829\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MUTAG\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }?order by rdfs:?value OR {?arr?arr_lbl; rdfs:label?arr_lbl. ORDER BY DESC(?value) } }\n","ORDER BY RDFS:?data ORDER BY DES cnt LIMIT 1\n","Input: output ( Sql Query): SELECT dISTINCT?benchmark{(NMI)?dataset orkgc:Dataset orkgc:Metric orkgc:LBL orkgc:LBL_RESULT orkgc:BENCHMARK ORkgc:BENCHMARK_COUNT OF NMI ORkgc:RESULT AS?score, rdfs:label?dataset_lbl. } FROM?MultiNLI ORDER BY DESC(score)\n","DATASET ORGX DATA:?dataset =?MultiNLI:MIDD\n","262 \n"," input (English text): What models are being evaluated on the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the ImageNet 64x64 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the ObjectNet (Bounding Box) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet (Bounding Box)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the ImageNet ReaL dataset?\n"," output (Sparql query): \n","564\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Can you provide a list of models that have been evaluated on the TextGate dataset?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TextGate\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n","input (English text): Could you provide a list of models that have been evaluated on the VertexNet (Bounding Box) dataset?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VertexNet\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n","Input text:  What models are being evaluated on the ImageNet 64x64 benchmark dataset? \n","output (Sparql query): SELECT\n","263 \n"," input (English text): Which model has achieved the highest Accuracy score on the Reuters De-En benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters De-En\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark score and its metric on the Reuters En-De dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters En-De\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the best performing model benchmarking the Reuters-21578 dataset in terms of Accuracy metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Accuracy score on the Reuters En-De benchmark dataset?\n"," output (Sparql query): \n","824\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" query (Text): {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?metric?metric_lbl?value FROM?model; rdfs:label?metric_lbl. ORDER BY 1} } \n"," output (JSON text representation of query results): { \"result\" : \"accuracy\" } \n","If you'd like to explore the same query result as above in R, the below R code (requires pandas): # Get the dataset dataset = \"Reuters En-De\", df = '{\"model\": \"Reuters En-De\"}', xlab = 'Name', ylab = 'Score', colnames = [ \"Model\", \"Score\" ] # Get the metric metric = \"Accuracy\" # Determine if the metric scores are high score_metric = c ( \"Accuracy\", \"Accuracy\" ) # Determine how to plot # Plot the metric data. plot ( metric, c ( \"metric\", \"Score\" ), colnames = c ( 'Model', 'Score' )) plot ( metric, c ( \"metric\", \"Score\" ), colnames = c ( 'Model', 'Score' ), xlab = 'Name', ylab = 'Accuracy', col = \"red\" )  # Compute the metric value plot_metric ( dataset, df, metric )\n","If you're interested in the original data, please feel free to download and run the scripts that I used below.\n","264 \n"," input (English text): List the metrics that are used to evaluate models on the Kinetics-600 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the BIOSSES dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BIOSSES\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the AI-KG dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AI-KG\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 dataset?\n"," output (Sparql query): \n","751\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Using the above query, we can see that we get a number of metrics similar to our first benchmark:\n","Result Metric (Metric) RDFS:class?metric rdfs:label?metric_lbl. \"1\". ORGK:class?metric rdfs:label?metric_lbl. \"5\". orkgc:dataset?lbl? (100 or kgc:dataset OR kgc:dataset:1) ORDGE:class?metric rdfs:label?metric_lbl. \"15\". ORGK:class?metric rdfs:label?metric_lbl. \"25\". KINITHS:class?metric (100) ORDGE:class?metric (100) AIK:class?metric (100) ORDGE:class?metric (100) ANSYS:class?metric (100) KINITHS:class?metric (100) KINITHS:class?metric (100) KINITHS:class?metric (100) KINITHS:class?metric (100) KINITHS:class?metric (100) KINITHS:class?metric (100 ) ORDGE:class?metric rdfs:label?metric_lbl.\n","1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n","Using a second query, we get the same results as last time:\n","Result Metric (Metric) RDFS:class?metric rdfs:label?metric_lbl. \"11\". ORGK:class?\n","265 \n"," input (English text): Can you list the metrics used to evaluate models on the Yelp-2 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Yelp Binary classification benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the CoNLL 2012 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Yelp-5 benchmark dataset?\n"," output (Sparql query): \n","523\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input: NULL  output: NULL \n","output: NULL \n"," I also made a few tweaks to the model. I used an R-based model with about 30 features. The R-based model works well for the Yelp data given that it has around 300 million observations, and the only thing I didn't add was the Yelp-2 metric! I added in the rdfs:label and orkgp:label to evaluate the models with the following logic: First I calculated the mean of the ratings for each of the labels (yelp and restaurant, since restaurants seem to have better ratings) on Yelp-B. Next I used the raw scores for each of the scores on Yelp-2, which I knew of the Yelp-8 metrics. Here's a screen grab of my R code:\n","If you've played around with the R-based model (see the code and screengrab), you'll notice that I don't even use sum. I keep the sum for the model's internal workings. For the models I included on my blog, the sum was zero! I also chose to treat the rdfs:ratings, orkgp:ratings and orkgp:score_ratings parameters as if they were a single metric for the model. For them to be evaluated with the simple R code in front of me instead of having to read into the R API file I had to add the following to my model:\n","[my_model] # sum_ratings(rdfs:score_ratings) # sum_ratings(rdfs:rank) # sum_ratings(rdfs:city) # sum_ratings(rdfs:state) # sum_ratings(rdfs:country) # sum_ratings(rdfs:lat); # sum_ratings(\n","266 \n"," input (English text): Provide a list of papers that have utilized the BART Base (with text infilling) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BART Base (with text infilling)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the PBSMT model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PBSMT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the NASNet-A + c/o model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"NASNet-A + c/o\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the BART model and include the links to their code?\n"," output (Sparql query): \n","498\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Trying to import the full dataset... trying to import full dataset... successfully (0 errors) using import() and output() to import the full dataset... successfully (0 errors) using import() and output() to import the full dataset... successfully (0 errors) using import() and output() to import the full dataset... successfully (0 errors) using import() and output() to import the full dataset... successfully (0 errors) using import() and output() to import the full dataset... successfully (0 errors) using import() and output() to import the full dataset... successfully (0 errors) using import() and output() to import the full dataset... successfully (0 errors) using import() and output() to import the full dataset... successfully (0 errors) using import() and output() to import the full dataset... successfully (0 errors) successfully importing full dataset... successfully importing full dataset... successfully importing full dataset... successfully importing full dataset... successfully importing full dataset... successfully importing full dataset... successfully importing full dataset... successfully importing full dataset... successfully import complete dataset... successfully import complete dataset... successfully import complete dataset... successfully import complete dataset... successfully importing complete dataset... successfully import complete dataset...\n","Successfully importing complete dataset... successfully importing complete dataset... successfully importing complete dataset... successfully importing complete dataset... successfully importing the full dataset... successfully import the full dataset... successfully import the full dataset... successfully import the full dataset...successfully import complete dataset... successfully import complete dataset... successfully import the full dataset... successfully import the full dataset... successfully import the full dataset... successfully import the full dataset... successfully import complete dataset... successfully import the full dataset...\n","Trying to import the full dataset... trying to import full dataset... successful (0 errors) using import() to import the full dataset... successfully (0 errors) using import() to import the full dataset... successfully (0 errors) using import\n","267 \n"," input (English text): What is the top benchmark score and its metric on the SearchQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SearchQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the WebQuestions dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WebQuestions\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Natural Questions (short) dataset?\n"," output (Sparql query): \n","849\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------------------ | Score | metric | metric_rdfs | --------|------------------------- | 2 | 10.0 |? |? | 1.0 | ---------------------------\n","[Note that the? metric refers to the metric used in the question (see the question on RDFs and Metric) along with the? value - this is the same value as in the question on RDFs and Metric ].\n","input (English text): What is the top benchmark score and its metric on the Natural People (short) dataset?\n"," output (Sparql query): ------------------------ | Score | metric | metric_rdfs | ------------------------|------------------------- | 17 | 27.86 |? |? | 21.25 | --------|-------------------------\n","input (English text): What is the top benchmark score and its metric on the Natural Questions and Natural People (short) datasets?\n","output (Sparql query): ------------------------ | Score | metric | metric_rdfs | ------------------------|------------------------- | 32.25 | 23.00 |? |? | 29.75 | --------|-------------------------\n","[Note that the? metric refers to the metric used in the question (see the question on RDFs and Metric) along with the? value - this is the same value as in the question on RDFs and Metric ].\n","input (English text): What is the top benchmark score and its metric on the Natural Questions dataset and Natural People (short)?\n","output (Sparql query): --------| Score| metric --------|------------------------- 1 2.0 10.0 26 2 2.0 27.85 22 3 2.0 29.75 21 4 2.0 26.25 21 5 2.0 24.75 21 6 2.0 22.00 21 7 2.0 20.25 20 8 2.0 19.00 20\n","268 \n"," input (English text): What is the best performing model benchmarking the CIFAR-10 dataset in terms of Parameters metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Parameters\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): List the metrics that are used to evaluate models on the CIFAR-100 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Dmlab-30 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dmlab-30\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?\n"," output (Sparql query): \n","631\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What evaluation metrics are commonly used when benchmarking models on the CIFAR-100 benchmark dataset?\n"," output (Sparql query): \n","# test data file\n","dataset_1 = NULL dataset_2 = NULL dataset_3 = NULL subset_1 = list_of_datasets subset_2 = list_of_datasets subset_3 = list_of_datasets\n","input (string): list_of_datasets with the different subset (e.g., \"cifar-10\") that is used in the CIFAR-10 benchmarks.\n","output (string): comma delimited list of categories that should be tested on CIFAR-10 datasets\n","test_params= NULL, test_metrics= NULL, test_metric_name= NULL, test_value= NULL, test_value_name= NULL, test_metric_names_exclude= NULL, test_metric_names_exclude_in= NULL  \n","output (Sparql query): SELECT test_params,test_metrics,test_metric_name,test_value_name,test_value_name,test_value_names_exclude,test_metric_names_exclude_in FROM test_params WHERE (test_metric_name = 'Parameters') AND (test_metric_names_exclude= 'cifar-10' OR test_metric_names_exclude= 'cifar-10') AND (test_metric_names_exclude = 'cifar-10') AND (test_metric_names_in= 'cifar-10' OR test_metric_names_in = 'cifar-100') AND test_met\n","269 \n"," input (English text): What are the metrics of evaluation over the Automatically labeled Medline abstracts corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Automatically labeled Medline abstracts corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What is the top benchmark score and its metric on the PubMedQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMedQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset?\n"," output (Sparql query): \n","633\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","The output of the above query is as you see above. However, the second field column has been modified by setting DISTINCT?paper?paper_lbl to the top benchmark score of the paper. Additionally, the output of the query will include the metric score, which has been modified, and all metrics, which has been filtered out. This allows for a quick and easy way to calculate the top metric and then add the metric score to the top score.\n","Using RDFS and the Google Book Search Metrics API\n","One cool thing to do with these Google Book Search Metrics API-based tools is to integrate them into RDFS. With the RDFS query parser embedded in RDFS, it is simple to integrate the metrics into a new RDFS database and then query them. To do so, you must pass your RDFS data to the Google Book Search Metrics API:\n","rfdweb:\n","CREATE TABLE wb_data ( id int NOT NULL DEFLATE varchar ( 10 ) NOT NULL, title varchar ( 10 ) NOT NULL, page  NUMBER(3) NOT NULL, pagetitle html, pagetitlec html, page_id pagedata, page_url url );  \n","rdfs:\n","CREATE TABLE test_page ( id INT NOT NULL DEFLATE varchar ( 10 ) NOT NULL, title varchar ( 10 ) NOT NULL, page  NUMBER(3) NOT NULL, pagetitle html, pagetitlec html, page_id pagedata, page_url url );  \n","To create the new RDFS database, I use the following code:\n","dfs. create ( wb_data );\n","The result of the above query can then be viewed in RDFS using the rdfs:table.create\n","270 \n"," input (English text): Name the datasets that have been used for benchmarking in the Citation Intent Classification research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Citation Intent Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Name the datasets that have been used for benchmarking in the Text Classification research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Text Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Provide a list of benchmarked datasets related to the Document Classification research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Document Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Name the datasets that have been used for benchmarking in the citation classification research problem?\n"," output (Sparql query): \n","510\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Include a list of libraries related to the Text Classification research area?\n","output (Sparql query): SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Text classification libraries\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \n"," input (English text): Name the datasets that have been used for benchmarking in the Document Classification research problem?\n"," output (Sparql query): SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Text classification libraries\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \n","output (Sparql query): \n","To view the results, select the following option\n","Citation Intent Classification Benchmark\n","Citation Intent Classification Benchmark is a visualization interface to the Citation Intent classification problem. It offers a simple, powerful and beautiful way at visualizing the benchmarks for all the datasets, with a short overview of the benchmarking.\n","271 \n"," input (English text): Can you provide links to code used in papers that benchmark the S-NLI model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"S-NLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the D-NLI model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"D-NLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DDQN (tuned) hs model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDQN (tuned) hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the H-NLI model in any benchmark?\n"," output (Sparql query): \n","481\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","(H-NLI) orkgs_delta = 1. 039. 061. 087. 098. 100. 103. 106. 113. 115 orkgs_delta = ds :( orkgs_rng = np_rdbn_sparse) : ds - ds : ds + ds : ds / 2 ds_delta = 2. 044. 074. 012. 096. 123. 023. 027 orkgs_delta = ss :( orkgs_rng = np_ss_sparse) : ss - ss : ss + ss : ss / 2 ss_delta = 3. 042. 094. 098. 141. 1086. 1075. 1117 orkgs_delta = ss_delta :( orkgs_rng = np_ss_sparse) : ss_delta - ss_delta : ss_delta / 2 ss_delta_delta = ds_delta :( orkgs_rng = np_ss_sparse) : ds_delta_delta - ds_delta_delta: orkgs_delta_delta_delta_delta :( ss_delta_delta=1. 039. 061. 087. 098. 100. 103. 106. 113. 115 []) ds_delta2 = 3. 049. 105. 115. 150. 1813. 1082. 1467 orkgs_delta2 = ss_delta2 :( orkgs_rng = np_ss_sparse) : ss_delta2 - ss_delta2 : ss_delta2\n","272 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the PubMed 20k RCT dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the MedSTS dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the 20NEWS dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"20NEWS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMed 20k RCT dataset?\n"," output (Sparql query): \n","621\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","outputs (Sorted by title):\n","2.2.4.5. The'metrics' option for outputting metrics\n","This option allows us to output statistics for the same dataset, either the 'default' metrics, or those provided for a particular dataset, or a set of metrics (e.g. as specified in the'metrics' query option).\n","output (Sparql query): SELECT \"Metrics\" FROM b \"MEDSPARSES\" AS b WHERE b.ID =? \"default_metrics\" \n","input (English text): Output names for metrics in the published articles; e.g.,'summary' or 'total', 'author' etc.\n","output (Sparql query): SELECT \"{{?a?statname}} * {{?x?value} * {{?y?p_stat_lbl*} }}\" FROM {?a?statname} b WHERE b.ID =? \"{{?a?statname}} * {{?x?value} * {?y?p_stat_lbl*} }\" GROUP BY b.ID ORDER BY \"metric_lbl\"\n","input (English text): What metric names make up 'default metrics' and how exactly are they defined?\n","output (Sparql query): \n","output (Sparql query): [{?a?statname}}] * {<type>?value</type>, <type>?p_stat_lbl*</type>... {<type>?total}\n","input (English text): How are metrics defined in the publications, and what other data fields can they use?\n","output (Sparql query): This should be the value of the'metric_lbl' field in your query - it will be treated as an output\n","273 \n"," input (English text): Indicate the model that performed best in terms of Macro F1 metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Macro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Micro Recall score when benchmarked on the NLP-TDMS (Exp, arXiv only) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Micro Recall\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Micro Precision score when benchmarked on the NLP-TDMS (Exp, arXiv only) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Micro Precision\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Macro Recall metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n"," output (Sparql query): \n","903\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["  SELECT name, r1_dummy, ROCD2, r2, metric, METRIC, r2_dummy, ROCD2_dummy, m2 * m1 / m2 AS score, ROCID2, m2, b2 (B) FROM  ... GROUP BY r1, r2_dummy DESC, metric, r2, m2  ORDER BY DESC(score - metric, metric)  [... ]\n","output (Sparql query):  SELECT name, metric, METRIC, ROCD2, R2_dummy, METRIC, r2, m2 * m1 / m2 AS score, ROCID2, m2, b2 (B) FROM  ... GROUP BY r1 DROP OUTPUT(?x, r2_dummy  ORDER BY?score >?x, metric, ROCID2) ORDER BY metric DESC\n","274 \n"," input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the CoQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Overall\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of EM score when benchmarked on the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"EM\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the BioASQ dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset?\n"," output (Sparql query): \n","832\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input (English text): The most frequently used model, if there is more than one model.\n","Output (Sparql query): The name of the top performing model, if there is more than one model.\n","Input (English text): Which of these models would you most like to benchmark during this phase of project?\n","Answer:\n","Result (Sparql query) SELECT DISTINCT?model?model_lbl ON (?model =?column) ORDER BY DESC(?value,?order by desc.name) LIMIT 1 \n","Result (Sparql statement) SELECT DISTINCT?model?model_lbl ON (?model =?column) ORDER BY desc.name LIMIT 1 \n","Input (English text): For each benchmark condition SELECT id.ID ASC 1\n","I suggest you try to add in your own comments on the results as they're quite long so please don't hesitate to comment and follow up if necessary. Hopefully this will serve as a good reference for everyone, please be constructive if it helps you or if you have any questions I'm always happy to answer them.\n","275 \n"," input (English text): Where can I find code references in papers that have used the DARQN soft model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DARQN soft\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DQN+SR model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN+SR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DeiT-B model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DeiT-B\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DCN model for benchmarking purposes?\n"," output (Sparql query): \n","490\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DCN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): Where can I find code references in papers that have used the ADQN+SR model for benchmarking purposes?\n"," output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"ADQN orkgc:Model\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): Where can I find code references in papers that have used the ACQUAID++ model for benchmarking purposes?\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"ACQUAID++\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BEN\n","276 \n"," input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the iNaturalist 2019 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2019\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark score and its metric on the iNaturalist 2018 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2018\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Could you provide a list of models that have been tested on the iNaturalist 2018 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2018\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric?\n"," output (Sparql query): \n","760\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT a.dataset as?dataset and rdfs.model as?model_lbl. AND max(a.dataset.top, min(a.dataset.top) AS?score) IF max(a.dataset.top) > min(a.dataset.top) THEN print('The best performing model in the benchmark dataset is:');?dataset rdfs:label?dataset_lbl. OPTIONAL {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. } ORDER BY?dataset DESC(?score) LIMIT 1 } \n"," input (English text): What is the top benchmark score and its metric on the iNaturalist 2018 dataset?\n"," output (Sparql query): SELECT DISTINCT?meter_lbl?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?meter_lbl?meter_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"iNaturalist 2018\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_METRICS?meter_. } } OPTIONAL {?eval orkgp:HAS_METRICS?metric_. } ORDER BY?meter_lbl DESC(?score) } }.\n","For a more in depth look at how to interpret and model benchmark results, see the iNaturalist 2018 Benchmark Modeling article\n","277 \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Ice Hockey dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ice Hockey\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Pong dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Pong\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Bowling dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bowling\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset?\n"," output (Sparql query): \n","836\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note: Although the data in the output files are in the same format as the original data files, for the purposes of this tutorial, all output files are represented as SQL queries.\n","There are 2 ways to select a metric, one uses a select clause and the other selects the metric value directly. The select clause example requires you to create a new SQL statement, while the metric value example displays the available metrics and returns the number of metric.\n","The number of metric is only a first choice option, because it is not unique. If you want to return the number of metrics, you can use the filter expression, which will filter the metric values. A full list of metrics here.\n","In this demo, the number of metrics is retrieved from the original dataset using the select clause and returned as the aggregate value. The metric value is not retrieved, but shown. It is the number of times some metric occurred in the dataset, measured in seconds.\n","If you want to retrieve only the metric value, you can use the metric value clause. It is the same as the select clause. The other metric value is available as a parameter.\n","There are 3 datasets, 3 metric values and 3 metrics in total. If there are no metric values, then no metric value is shown. The aggregate calculation of the total metrics is calculated on the first dataset, and the total metrics on the second dataset.\n","The number of metrics for each dataset is shown in the metrics.csv file (sample data). The first dataset is the Ice Hockey dataset, and the second dataset is the Ice Hockey dataset plus Metcals, which are two extra metrics.\n","You may specify a dataset name (or alias) for your datasets by specifying the datasets.dataset value. In this case, the dataset must match the name of the dataset or alias, but not both. For instance, if you choose to display the\n","278 \n"," input (English text): Provide a list of benchmarked datasets related to the Information Extraction research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Information Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Provide a list of benchmarked datasets related to the Scientific Claim Verification research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Scientific Claim Verification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the SciFACT dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciFACT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of benchmarked datasets related to the Scientific Results Extraction research area?\n"," output (Sparql query): \n","507\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note: When working with multiple datasets, and providing results after benchmarking is complete, one can use the '*' operator.\n","* benchmark (input): Provide a list of benchmarks over the Specification Query Results dataset?\n","* benchmark (output): Provide the output of the benchmark?\n","* benchmark* bench (input): Provide a list of benchmarks over the Specification Query Results dataset?\n","* bench (output): Provide the output of the benchmark?\n","* bench* bench_lbl (input): Provide a list of benchmarks over the Specification Query Results dataset?\n","* bench_lbl (output): Provide the output of the benchmark?\n","* benchmark* cont (input): Provide a list of benchmarks over the Specification Query Results dataset?\n","* benchmark* cont (output): Provide the output of the benchmark?\n","* benchmark* cont_lbl (input): Provide a list of benchmarks over the Specification Query Results dataset?\n","* bench_lbl (output): Provide the output of the benchmark?\n","* benchmark* cont_lbl_lbl (input): Provide a list of benchmarks over the Specification Query Results dataset?\n","* bench_lbl_lbl (output): Provide the output of the benchmark?\n","* benchmark* cont_lbl_lbl_lbl (input): Provide a list of benchmarks over the Specification Query Results dataset?\n","* bench_lbl_lbl_lbl_lbl (output): Provide the output of the benchmark?\n","* benchmark* cont_lbl_lbl_lbl_lbl_lbl (input): Provide a list of benchmarks over the Specification Query Results dataset?\n","* bench_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_l\n","279 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Reacher, easy (DMControl500k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What is the top benchmark score and its metric on the Walker, walk (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): List the title and ID of research papers that contain a benchmark over the Finger, spin (DMControl500k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Finger, spin (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset?\n"," output (Sparql query): \n","655\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What is the score for a score of 1 on the Walker, walk (DMControl500k) dataset?\n","output (Sparql query): SELECT DESC(score) FROM papers?label?name \n","If the query has multiple fields:\n","input (English text): What are the titles and IDs of research papers that include a baseline score of 1 on the Walker, walk (DMControl500k) dataset?\n","output (Sparql query): \n","Note that as the fields are separated by the?> character, the query's SQL syntax is also separated by the?> character.\n","output (Sparql query): \n","If the query contains the?> character, or the query contains multiple fields:\n","input (English text): What are the titles and IDs of research papers that include a baseline score of 1 on the Walker, walk (DMControl500k) dataset?\n","output (Sparql query):\n","input (English text): What is the score for a score of 1 on the Walker, walk (DMControl500k) dataset?\n","output (Sparql query): SELECT DESC(score) FROM papers?label?name \n","input (English text): What is the median of the scores of the papers that contain a given benchmark score for the Walker, walk (DMControl500k) dataset?\n"," output (Sparql query): SELECT DESC(median) FROM papers?label?name \n","input (English text): What is the mean of the scores of the papers that contain a given baseline score for the Walker, walk (DMControl500k) dataset?\n","output (Sparql query: What are the standard deviations of the scores of the papers that contain a given baseline score for the Walker, walk (DMControl500k) dataset?\n","input (English text): What\n","280 \n"," input (English text): Provide a list of benchmarked datasets related to the Relation Classification research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Relation Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Provide a list of benchmarked datasets related to the Information Extraction research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Information Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): List the datasets benchmarked under the relation extraction research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"relation extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Provide a list of benchmarked datasets related to the Semantic Role Labeling research area?\n"," output (Sparql query): \n","505\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","In this blog post I will present you with a simple model that can be used to retrieve some common characteristics of the following datasets: [1] English words: (English-English word database [1]) [2] Word similarity index: (word similarity index dataset [2]) [3] Speech recognition: (speech detection domain [3]) [4] Language structure and classification: English and Romance corpus and Sentence structure [4] RDF: (RDF index and classification domain [4]) [5] Knowledge graph: (knowledge graph knowledge graph dataset [5])\n","In the following sections we describe the model and describe some steps we took to create this model. 1) Introduction As mentioned earlier we will build a simple simple model to show us the following properties of some of the datasets.\n","I. Data Set Desription First of all we will start with a description of the data set that we are going to use. I will do my best to give the reader an overview of the dataset, but should not take too much time to explain some of the finer points.  The dataset data contains the words and their translations in English, in two dictionaries. The first dictionary is the English word database containing the words in English, with one entry per word followed by a word, sentence and sentence and optionally a group.\n","On the other hand is the English-English Word database containing only the English English words with one entry per word followed by a word, sentence and sentence and optionally a group.\n","For the second database we use the data structure dataset which is the same used in the corpus data.\n","Both dictionaries contain English-English word data using the following structure.\n","2) Model: Description First of all the model we created will return a result set named as : :.\n","Here we can see that the model has a large weight in a given part of the result set\n","281 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CommonsenseQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WikiText-2 dataset?\n"," output (Sparql query): \n","533\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WikiText-2\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Wikipedia dataset?\n"," output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. NAME?graph?graph_lbl_lbl_id..?graph orkgc:Dataset?graph.. ORG_ID?graph_id. } \n"," output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. NAME?graph?graph_lbl_lbl_id..?graph orkgc:Dataset orkgc:Dataset_ID?graph_id. ORG_ID?graph_id. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NGS dataset?\n"," output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?datas\n","282 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset PWC Leaderboards (restricted)?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PWC Leaderboards (restricted)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the best performing model benchmarking the PWC Leaderboards (restricted) dataset in terms of Micro Precision metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Micro Precision\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PWC Leaderboards (restricted)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What are the metrics of evaluation over the Yelp-14 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-14\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the PWC Leaderboards (restricted) dataset?\n"," output (Sparql query): \n","740\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp-14\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","\n","Query Parsing & Performance Results\n","\n","This query is based on the test query we started with with the model_lbl attribute filtered on the keyword micro precision and the metric metric_lbl.\n","\n","What are the results returned from executing this command against the test dataset (limited)?\n","\n","There were no results.\n","\n","Results Based on Model's Precision\n","\n","As we have done with the previous test, we have applied the filter on the keyword micro precision so that the precision is not affected and the performance is the same. But what would happen if we didn't filter it?\n","\n","If we filter the metric model_lbl on the metric micro precision and the metric metric_lbl is either 1 or 2, then we receive no information in the result.\n","\n","This indicates that the metric is more complex than just micro precision.\n","\n","This suggests that in order to evaluate the metric, we have to perform a more complicated computation on aggregate dimensions. We would need to apply a metric-dependent query with a filter on the metric metric_lbl. And this can only be accomplished on some of our aggregates.\n","\n","Let's continue with the previous test. There was a performance improvement when we filtered on the\n","283 \n"," input (English text): Where can I find code references in papers that have used the Prior+Duel noop model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior+Duel noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DQN noop model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the DDQN (tuned) noop model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDQN (tuned) noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Duel noop model in any benchmark?\n"," output (Sparql query): \n","494\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DDQN (tuned) noop\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DDQN (tuned) noop\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","\n","output (Sparql query):  SELECT DASTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DDQN (tuned) noop\"))?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","\n","#------------------------------------------------------------------------ # #\n","284 \n"," input (English text): What are the most commonly used benchmark datasets for the Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification. research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification.\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the CIFAR-10 Image Classification dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10 Image Classification\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of benchmarked datasets related to the Document Classification research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Document Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): List the datasets benchmarked under the Fine-Grained Image Classification research problem?\n"," output (Sparql query): \n","549\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n"," output (Gist Link):\n","# Finds all benchmarked, state-of-the-art papers across all relevant topics in the Fine-Grained Image Classification research field with a specific goal. #\n","- Benchmarks papers related to the Document Classification research area\n","# Finds papers using the CIFAR-10 image classification dataset\n","# See more benchmarked papers\n","285 \n"," input (English text): What are the models that have been benchmarked on the DRI Corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DRI Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the DDI extraction 2013 corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DDI extraction 2013 corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the DRI Corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DRI Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?\n"," output (Sparql query): \n","645\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset? output (Sparql query): \n","Input (Text): Can you provide the metrics used to evaluate models on the DRI Corpus 2013 corpus dataset?\n","Output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {%if datalog%} { DDI Corpus:dataset a orkgc/tensorflow_data/datasets/datalog/data/l1.csv } { DDI Corpus:dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DDI Corpus\")?benchmark %} { DDI Corpus:dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DT Dataset\")?benchmark %} { DDI Corpus:dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DDI Corpus Corpus\")?benchmark %} { DDI Corpus:dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DT Dataset\")?benchmark %} { DDI Corpus:dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DT Datas\n","286 \n"," input (English text): List the metrics that are used to evaluate models on the WOS-5736 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the IMDb benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the SciERC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset?\n"," output (Sparql query): \n","521\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Oxford-IIIT)\",?id,?metric_lbl. OPTIONAL {?id,?metric_lbl,?metric_lbl rdfs:label?metric_lbl. } } \n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Oxford-IIIT\" ),?id,?metric_lbl. OPTIONAL {?id,?metric_lbl,?metric_lbl rdfs:label?metric_lbl. } }  input (English text): List the metrics that are used to evaluate models used by the TSNES-PREFETCH benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TSNES-PREFETCH\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?rlfs orkgp:HAS_METRIC?metric.?met\n","287 \n"," input (English text): What is the name of the top performing model in terms of Percentage Error score when benchmarked on the CIFAR-100 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Percentage Error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Percentage correct score when benchmarked on the CIFAR-100 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Percentage correct\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Accuracy (%) score when benchmarked on the CINIC-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy (%)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CINIC-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset?\n"," output (Sparql query): \n","859\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the CINIC-10 dataset?  output (Sparql query): \n","input (English text): What is the name of the top producing model when benchmarked on the CINIC-10 dataset?\n","output (Sparql query): ----------- ----------- ----------- ----------- ----------- -----------          ----------- --------- CINIC-10,1   ----------- ----------- ----------- ----------- ----------- ----------- ----------- ---------     ----------- --------- BOND,1   ----------- ----------- ----------- ----------- ----------- ----------- ----------- ---------     ----------- --------- LEISURE,1   ----------- ----------- ----------- ----------- ----------- ----------- ----------- ---------     ----------- --------- ORG,1   ----------- ----------- ----------- ----------- ----------- ----------- ----------- ---------     ----------- --------- SOLID,1   ----------- ----------- ----------- ----------- ----------- ----------- ----------- ---------     ----------- --------- CHI,1  ----------- ----------- ----------- ----------- ----------- ----------- ----------- ---------     ----------- --------- KISPIN,1   ----------- ----------- ----------- ----------- ----------- ----------- ----------- ---------    ----------- --------- MALFUNCTION,1   ----------- ----------- ----------- ----------- ----------- ----------- ----------- ---------     ----------- --------- MODEL,1   ----------- ----------- ----------- ----------- ----------- ----------- ----------- ---------    ----------- --------- NID,1   ----------- ----------- ----------- ----------- ----------- ----------- ----------- ---------  \n","288 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10, 1000 Labels dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STL-10, 1000 Labels\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CoNLL04 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL04\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the UCF101 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10 dataset?\n"," output (Sparql query): \n","524\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English language): Please provide a list of datasets (labels) that were used as benchmarks.\n","output (Sparql query): SELECT DISTINCT?labels?labels_lbl WHERE {?labels a orkgc:LabelDataset?labels_lbl. AND (?labels_lbl.?dataset orkgc:Labels dataset_lbl) OR (?labels_lbl.?dataset_lbl and?dataset_lbl.?dataset_lbl) }\n","input (English text): Please give me a list of datasets (labels) that you collected metrics about.\n","output (Sparql query): Select count(*) as count, COUNT() as cnt from data_stla as t1, data_stla as t2,  SELECT ROUND(t1.dataset_lbl.COUNT() OVER ( PARTITION BY t1.labels_lbl.)) as cnt FROM data_stla as t1, data_stla as t2 WHERE t1.labels_lbl.COUNT() = t2.data_stla.COUNT() AND cnt >= 2; \n","input (English text): Please give me a list of datasets (labels) that you collected metric data about.\n","output (Sparql query): SELECT DISTINCT count(*) as count, COUNT() as cnt FROM data_stla as t1, data_stla as t2,  SELECT ROUND(t1.labels_lbl.COUNT() OVER ( PARTITION BY t1.labels_lbl.)) as cnt FROM data_stla as t1, data_\n","289 \n"," input (English text): What are the models that have been benchmarked on the SciCite dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the ScienceCite dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the SciFACT dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciFACT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the SciCite benchmark dataset?\n"," output (Sparql query): \n","558\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input data set:\n","Output data set:\n","input (English text): In this blog post, I will describe a very simple, yet very powerful, and interesting model that I came across. I would like to show the results from running this model on the SciFACT dataset. Let's say we want a prediction model with the following output: \n","output:\n","The input data contains a list of models that have been benchmarked on the SciCite dataset.\n","We also have data corresponding to the paper's title. If you provide this output in the query, we can get the output based on the paper title.\n","The benchmarked models are:\n","The output looks like a table of the authors of the paper, the paper number, and the title of the paper. I assume that you saw some of the following papers:\n","The second column of the output contains the title:\n","For these results, the model has been benchmarked by two people:  and I have excluded the other two when running the model from the query.\n","The output is from SciCite's benchmarking project. The first column is the dataset containing the benchmarked models. The second column is the name of one of the models.\n","The benchmarked models have been evaluated on the original data.\n","This is an implementation of a general purpose model that I found on the SciCite benchmarking project\n","In fact, I have decided that I want to show the entire dataset at this time. I will need a way to store the scores.\n","At first, I made the dataset \"unusual\". It contains the score of the \"normal\" (non-parallelized) variant of the model, which had been benchmarked in the same time period, but in an ordinary parallel way:\n","output:\n","So far, we are only looking at the \"normal\" and \"un\n","290 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Road Runner dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Road Runner\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Video Pinball dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Video Pinball\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Fishing Derby dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Fishing Derby\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Skiing dataset?\n"," output (Sparql query): \n","511\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:  \n","Input:\n","Output:  \n","Input:\n","Output:  \n","Input:\n","Output:  \n","Input:\n","Output: \n","Input:\n","Output: \n","Input:  \n","Output: \n","Input:  \n","Output:  \n","Input:  \n","Output: \n","Input:\n","Output: \n","Input:\n","Output: \n","Input:  \n","Input:\n","Output: \n","Input:\n","Output: \n","Input:\n","Output: \n","Input:\n","Output: \n","Input:\n","Output: \n","Input:\n","Output:  Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Output:\n","Input:\n","Output:\n","Input:\n","Output:\n","Input:\n","\n","291 \n"," input (English text): List the metrics that are used to evaluate models on the MAZEA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MAZEA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the OpenBookQA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OpenBookQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the NYT29 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset?\n"," output (Sparql query): \n","522\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:LAST_MODEL?dataset orkgp:LAST_METRIC?metric. }\n"," input (English text): List the metrics that are used to evaluate models on the VTT2 benchmark dataset?\n","output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTT2\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }  source (English text): List the metrics that are used to evaluate models on the EASL-3 benchmark dataset?\n","output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ESL-3\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?\n","292 \n"," input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the CoQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Overall\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the ObjectNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the CommitmentBank dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommitmentBank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset?\n"," output (Sparql query): \n","841\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\")?test OR kgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval OR kgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; ORkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","output (Sparql query): What is the name of the top performing model in terms of Score score when benchmarked on the J. P. Mallory task?\n","output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\")?test OR kgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval OR kgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; ORkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","Output is the following\n","TAS\n","293 \n"," input (English text): Provide a list of papers that have utilized the Rfa-Gate-Gaussian-Stateful (Small) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rfa-Gate-Gaussian-Stateful (Small)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Sarsa-φ-EB model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Sarsa-φ-EB\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Rfa-Gate-Gaussian-Stateful (Big) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rfa-Gate-Gaussian-Stateful (Big)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Rfa-Gate-arccos model in any benchmark?\n"," output (Sparql query): \n","527\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input (English text): Provide a list of papers that have utilized EBBG in any benchmark?\n","output (Sparql query): SELECT DISTINCT?code WHERE {? model arccos:Model;? rdfs:labels?model_lbl. FILTER (str(?model_lbl) = \"EBBG\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }  \n","input (English text): Provide a list of papers that have utilized the Rfa-Gate-Lima2-Gaussian-Stateful model in any benchmark?\n","output (Sparql query):  \n","input (English text): Provide a list of papers that have utilized the Rfa-Gate-Lima2-Gaussian-Stateful (Small) model in any benchmark?\n","output (Sparql query):  \n","input (English text): Provide a list of papers that have utilized the Rfa-Gate-Gaussian-Stateful (Big) model in any benchmark?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Rfa-Gate-Gaussian-Stateful (Big)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model;\n","294 \n"," input (English text): Indicate the model that performed best in terms of NER Macro F1 metric on the ADE Corpus benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"NER Macro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ADE Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 (Short) metric on the Natural Questions benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 (Short)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark dataset?\n"," output (Sparql query): \n","834\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a CLEFO:Metrics; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1 (Long))\" ORLET?eHealth ORLET?CLEFO?eHealth rdfs:label?metric_lbl. } SELECT DISTINCT?model?model_lbl WHERE {?metric CLEFO:Metrics; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1 (Short))\" ORLET?eHealth ORLET?CLEFO?eHealth rdfs:label?metric_lbl. } ORDER BY DESC(?value). LIMIT 1 } } \n","The output of the query above corresponds to the following table with the selected dataset.\n","Column Meaning Dataset Measurement Results Dataset Measurement Results \n","column: orkgc:Metric Notes: Model that performed best in terms of F1 metric on a dataset that has the long-form data. Can be used as a check-box for a particular model class. Note: The output does not correlate with the output generated by the default method. In other words, if you check all available models in your RDS workspace, this value will be empty. Only models that have been filtered out by you will display. Only the highest ranked models will be returned. Note: For a detailed explanation of the filtering algorithm, refer to the algorithm.csv file. rdfs:label?model_lbl. Has been manually filtered out. Otherwise, this attribute will be iniitable.\n","If the model selected has not been filtered out, then any metric with a lower cardinality than the metric from\n","295 \n"," input (English text): List the metrics that are used to evaluate models on the BIOSSES benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BIOSSES\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the SciFACT dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciFACT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the MedNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the BioASQ dataset?\n"," output (Sparql query): \n","507\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BIOASQ\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): If the query is a function, what are the parameter and return values of the function?\n"," output (Sparql query): \n","input (English text): If the query is a function, what are the parameter and return values of the function?\n","output (Sparql query): \n","input (English text): What are the metrics of evaluation of a function over the WIMBLED dataset?\n","output (Sparql query):\n","SELECT DISTINCT?metric?metric_lbl WHERE {?dataset hb:Dataset; rdfs:label?dataset_lbl.?metric rdfs:label?lbl. } \n","input (English text): If the query is a function, what are the parameter and return values of the function?\n","output (Sparql query): \n","input (English text): If the query is a function, what are the parameter and return values of the function?\n","output (Sparql query): \n","input (English text): What are the metrics of evaluation of a function over the RICE datasets?\n","output (Sparql query):\n","SELECT DISTIN\n","296 \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Atlantis dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Breakout\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What are the models that have been benchmarked on the Atari 2600 Centipede dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Centipede\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric?\n"," output (Sparql query): \n","737\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Centipede\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","input (English text): What is the best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Score\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?val orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?val orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } }\n","input (English text): What is the best performing\n","297 \n"," input (English text): What are the most commonly used benchmark datasets for the Semantic Textual Similarity research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Semantic Textual Similarity\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Named entity recognition research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Named entity recognition\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Phrase Extraction research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Phrase Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Natural Language Inference research field?\n"," output (Sparql query): \n","518\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label NULL?problem_lbl.?benchmark p32?problem. orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \n","input (English text): What are the most commonly used benchmark datasets for the Knowledge Retrieval research field?\n","output (Sparql query):  SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a ORkgc:Problem; rdfs:label NULL?problem_lbl.?benchmark (p32?problem_lbl.)?cont ORkgp:HAS_DATASET?dataset.?cont ORkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \n","input (English text): What are the most commonly used benchmark datasets for the Knowledge Management research field?\n","output (Sparql query):  SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a ANDpkgc:Problem; rdfs:label NULL?problem_lbl.?benchmark (p32?problem_lbl.)?cont ORkgp:HAS_DATASET?dataset.?cont ORkgp:HAS_BENCHMARK?benchmark; ORkgp:P32?problem. } \n","input (English text): What are the most commonly used benchmark datasets for the Machine Translation and Transcription Research fields?\n","298 \n"," input (English text): What models are being evaluated on the CIFAR-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the WOS-5736 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the CUB-200-2011 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the Classic dataset?\n"," output (Sparql query): \n","549\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CUB-200-2011\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","input (English text): What models are being evaluated on the CLAS-100-2011 data?\n"," output (Sparql query): \n","SELECT DISTINCT?model?model_lbl WHERE {?dataset c:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CAS-100) Benchmark\" orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","output (English text): What models are being evaluated on the SSC-200-2012 data?\n","output (Sparql query): \n","SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset\n","299 \n"," input (English text): Where can I find code references in papers that have used the DeiT-B model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DeiT-B\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DeiT-B-384 model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DeiT-B-384\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the DeiT-S model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DeiT-S\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DeiT-Ti model for benchmarking purposes?\n"," output (Sparql query): \n","494\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Where can I find code references in papers that have used the DeiT-Ti-384 model for benchmarking purposes?\n","output (Sparql query): \n","input (English text): How does your new design compare to the DeiT-S design?\n","output (Sparql query): SELECT SLEEP_DISSIMILATION AS sleep_dist, PREDICATE. DESCRIPTION AS precision_loss, PREDICATE. DESCRIPTION AS sensitivity_assess, PREDICATE. DESCRIPTION AS precision_loss, AND PREDICATE. DESCRIPTION AS accuracy_loss, AND PREDICATE. DESCRIPTION AS sensitivity_assess, AND PREDICATE. DESCRIPTION AS precision_loss, AND PREDICATE. DESCRIPTION AS accuracy_loss, AND PREDICATE. DESCRIPTION AS sensitivity_assess, AND PREDICATE. DESCRIPTION AS precision_loss, AND PREDICATE. DESCRIPTION AS accuracy_loss, AND PREDICATE. DESCRIPTION AS sensitivity_assess, AND PREDICATE. DESCRIPTION AS precision_loss, AND PREDICATE. DESCRIPTION AS accuracy_loss, AND PREDICATE. DESCRIPTION AS sensitivity_assess, AND PREDICATE. DESCRIPTION AS precision_loss, AND PREDICATE. DESCRIPTION AS accuracy_loss, AND PREDICATE. DESCRIPTION AS sensitivity_assess, AND PREDICATE. DESCRIPTION AS precision_loss, AND PREDICATE. DESCRIPTION AS accuracy_loss, AND PREDICATE. DESCRIPTION AS sensitivity_assess, AND PREDICATE. DESCRIPTION AS precision_loss, AND PREDICATE. DESCRIPTION AS accuracy_loss, AND PREDICATE. DESCRIPTION AS sensitivity\n","300 \n"," input (English text): In what country was conducted research with the largest number of participants?\n"," output (Sparql query): SELECT ?country WHERE { orkgr:R44980 orkgp:compareContribution ?contrib. ?contrib orkgp:P15249 ?country; orkgp:P23169 ?number_of_participants. } ORDER BY DESC( ?number_of_participants) LIMIT 1\n"," input (English text): What is the total number of patients in the studies?\n"," output (Sparql query): SELECT SUM( ?number_of_patients) WHERE { orkgr:R33008 orkgp:compareContribution ?contrib. ?contrib orkgp:P15585 ?patients. BIND(xsd:integer( ?patients) AS ?number_of_patients) } \n"," input (English text): What is the most common drug in the studies?\n"," output (Sparql query): SELECT ?drug, ?drug_labels WHERE { orkgr:R155621 orkgp:compareContribution ?contrib. ?contrib orkgp:P37578 ?drug. ?drug rdfs:label ?drug_labels. } ORDER BY DESC(COUNT( ?drug)) LIMIT 1\n"," input (English text): For which country of study overall prevalence of epilepsy is the highest?\n"," output (Sparql query): \n","314\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What is the total number of patients in the study?\n","output (Sparql query): SELECT SUM(?number_of_patients) WHERE { orkgr:R34012 orkgp:compareContribution?contrib.?contrib orkgp:P39569?amount. BIND(xsd:integer(?amount ) AS?amount) } \n","input (English text): What is the average number of people killed or severely injured during the study?\n","output (Sparql query): SELECT SUM(?number_of_patients) WHERE { ORKGR:R23790 OR kgp:compareContribution?contrib.?contrib orkgp:P19155?damage. BIND(xsd:integer(?damage ) AS?damage) } \n","input (English text): In what country did study participants travel to participate to and how long did the trip take place?\n","output (Sparql query): SELECT * FROM?country WHERE?country <?country AND?trip_distance IN (?number_of_patients -?number_of_patients ) ORDER BY?trip_distance LIMIT 1\n","input (English text): For which country of study was the worst case for seizure frequency observed?\n","output (Sparql query): \n","input (English text): What is the overall length of each of the studies? The longer it is the shorter the chance that seizures would occur.\n","output (Sparql query): SELECT SUM(?number_of_patients ) WHERE { ORKGR:R154630 orkgp:compareContribution?contrib.?contrib orkgp:P19513?time. BIND(xsd:integer(?time ) AS?time) } LIM\n","301 \n"," input (English text): What is the highest benchmark result achieved on the ShARe/CLEF eHealth corpus dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the EBM-NLP benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"EBM-NLP\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the ShARe/CLEF eHealth corpus dataset?\n"," output (Sparql query): \n","680\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Can you name the metrics in each of the models?\n","output (Sparql query): SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ShARe/CLEF eHealth corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?value. OPTIONAL {?value orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY  RANK DESC(?value ) GROUP BY?metric?metric_lbl\n","input (English text): Can you identify the metrics in each of the models?\n","output (Sparql query): SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ShARe/CLEF eHealth corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?value. OPTIONAL {?value orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl\n","302 \n"," input (English text): What models are being evaluated on the HMDB51 (finetuned) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51 (finetuned)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the HMDB51 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What is the best performing model benchmarking the HMDB51 dataset in terms of Top-1 Accuracy metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Could you provide a list of models that have been tested on the HMDB51 benchmark dataset?\n"," output (Sparql query): \n","633\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Did you know that you could use these metrics, combined with the top-performing features, as a metric for quality rating?\n","output (Sparql query): \n","A Note on Ranking Metrics: We are aware of some concerns regarding the ranking metrics and consider them to be controversial. In order to avoid confusion, we have decided to drop ranking metrics from the benchmark datasets. This is an ongoing effort and there will be further changes in the future.\n","We acknowledge that our decision to remove ranking metrics from the benchmark datasets was premature as some users report that these metrics give misleading or unrealistic results for the datasets. We are currently evaluating the evidence that these metrics, as indicated in \"Ranking Metrics and Research Data Scoring\", provide a reasonable evaluation of the top performing models (e.g; a factor analysis score by this measure might give misleading results for small datasets). We encourage users to consider alternative ranking metrics that are more informative for their model selection strategies.\n","As with previous efforts, we intend to continue improving the overall quality of the datasets benchmarked using the Benchmark datasets from HMDB51. In the meantime, our current focus is on improving the accuracy of model training on the Benchmark datasets to the point that the Top-1 Top-5 top-10 results published in the Journal of Statistical Inference can reliably be obtained during our benchmarking exercise.\n","The Data and Tools Team\n","\n","\n","About the Data and Metrics Team\n","\n","This team is responsible for building on the strengths of the Open Data Project using data and techniques that have been created to solve real problem in social sciences and engineering. The Data and Metrics Team is led by John Sides, an expert in open data and analytics; and by John McLeod, who runs large-scale algorithms for statistics, machine learning, and information technology. They serve as the lead data scientists for the\n","303 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Amidar dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Krull dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Krull\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Seaquest dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Venture dataset?\n"," output (Sparql query): \n","526\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Windy data collection projects?\n"," output (Sparql query): SELECT ROW_NUMBER_1: 1092,  ROW_NUMBER_2: 982,  ROW_NUMBER_3: 1094 ; ROWS = ROW_NUMBER_1, ROWS_2, ROWS_3; output (Sparql query): SELECT data_list, data_id, ?dataset?dataset_lbl,  data_id,?dataset?dataset_lbl_lbl, ?benchmark?benchmark_lbl,  data_id,?benchmark?benchmark_lbl_lbl,  data_lbl_id ; id = data_id,?labels = data_labels,?labels_lbl = data_labels_lbl, ?dataset = data_list,?lbl = data_list_lbl;\n","This script will return the list of Atari 2600 related datasets that will have been compiled and/or tested on a given Atari 2600 model. As such, it can list all the possible benchmarks that have been performed, in any order. It also allows you to list all the publications that appear in all those benchmark papers.\n","There are seven datasets produced by these benchmarks, and five corresponding papers. For full details, see the \"Data Collection Project, KERNEL\" section below.\n","The datasets\n","The first dataset that I've produced has been named the \"KERNEL\" dataset, which refers to the paper by Scott and Sosinski that details the benchmarking and analysis of the Atari 2600 chipset in the ERC\n","304 \n"," input (English text): List the metrics that are used to evaluate models on the CUB-200-2011 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the CoNLL 2012 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the WikiText-2 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset?\n"," output (Sparql query): \n","532\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Rotowire\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the SIR (Single Item Retention) benchmark dataset?\n"," output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SIR\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }  data1 : {?i1 orkgc:Dataset; ukml:label?metric_lbl. {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. } } /* kml */   error : {?i1 orkgc:Dataset orkgc:Dataset orkgc :Dat\n","305 \n"," input (English text): Provide a list of papers that have utilized the Weighted Tsetlin Machine model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Weighted Tsetlin Machine\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"TDMS-IE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-TDN(ResNet-101) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM-TDN(ResNet-101)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?\n"," output (Sparql query): \n","505\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): How can I see a sample of all papers using the Tsetlin Machine model?\n","output (Sparql query): SELECT DISTINCT?code FROM authors WHERE?code IS NULL ORDER BY author->title ORDER BY author->number_of_papers;\n","input (English text): What is the name of the package and the code name used in the authors reference?\n","output (Sparql query): SELECT DISTINCT?code FROM authors WHERE?code IS NULL ORDER BY author->name ORDER BY author->number_of_papers;\n","input (English text): What information are you providing to the reviewers for me to understand the code on the paper and the author?\n","output (Sparql query): SELECT VARB, name, number_of_papers, name, description FROM publications WHERE id<100 ORDER BY id DESC LIMIT 50;\n","input (English text): Did you notice any differences between the model used in the benchmarked dataset and the model utilized by previous papers that used the Tsetlin Machine?\n","output (Sparql query): SELECT VARB, name, number_of_papers, name, description FROM publications WHERE id<100 ORDER BY id DESC LIMIT 50;\n","input (English text): What is the name of the package, the code name used in the benchmarked dataset, and the author code for this benchmark?\n","output (Sparql query): SELECT VARB, name, number_of_papers, name, description FROM publications WHERE id<100 ORDER BY id DESC LIMIT 50;\n","Input: Description of the Results (RISP)\n","input (English text): From the number of papers, what is the number of papers that utilized the weights system?\n","output (Sparql query): SELECT RISP, number_of_papers, weighted\n","306 \n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 HERO dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 HERO\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Amidar dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Breakout dataset?\n"," output (Sparql query): \n","844\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Breakout\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } WHERE?{ {   {?dataset a orkgc:Dataset? rdfs:label?dataset_lbl.  ?eval orkgp:HAS_DATASET?dataset;?eval orkgp:HAS_EVALUATION?eval.;?value orkgp:HAS_VALUE?value;?name orkgp:HAS_VALUE?name. } }  } ORDER BY DESC(?value) } } } ORDER BY DESC(?value), DESC(?value)) ORDER BY DESC(?value))\n","#  HAS_DATASET? orkgc:Dataset  (\n","307 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Tennis dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tennis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Atlantis dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Ms. Pacman dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?\n"," output (Sparql query): \n","527\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","import os import sklearn\n","import matplotlib.pyplot as plt # load up test data load_dataset = None for filename in os.path.splitext(os.path.join(os.path.dirname(__file__), 'test_data.dat' )) : if not load_dataset: filename = os.path.join(os.path.dirname(__file__), 'test_data.dat' ) load_dataset = sklearn.train.FittingStatistics({train_name: filename]) # build the data frame # find the best value to train on train_dict = load_dataset['model'] if load_dataset['metric'] in [0.5, 0.9, 0.9, 0.9, 0.99]: train_dict[0.5] = train_dict[0.5] + np.exp( - ( 1 - int(train_dict['metric'])/(2*len(train_dict[0.5]))) - np.exp( - ( 1 - int( train_dict['metric'])/(2*len(train_dict[0.5]))) )) + np.sqrt( len(train_dict) ) train_dict['metric_lbl'] = train_dict['metric'] train_dict['model'] = load_dataset['model'].fit( train_dict['lbl'] ) train_dict = load_dataset['metric'] if not load_dataset['model'][0] : print 'not found.' train_dict is_train = len(train_dict) > 1 # build the model if not is_train: print 'not found.' load_\n","308 \n"," input (English text): List the code links in papers that use the QA-GNN model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"QA-GNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DQN noop model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the CRF with term expansion model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CRF with term expansion\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the FQF model in any benchmark?\n"," output (Sparql query): \n","476\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. NIST.fqf, rdfs:label?model_lbl. QA-GNN, rdfs:label?model_lbl. QA-GNN, rdfs:label?model_lbl. HAS_DATASET, rdfs:label?model_lbl. HAS_BENCHMARK, rdfs:label?model_lbl. HAS_MODEL, rdfs:label?model, rdfs:labels(?model_lbl )?dataset. } \n","Input from English text: List the code links in papers that use the JIFM model in any benchmark?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER ( str(?model_lbl) = \"JIFM\" )?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","Input from English text: List the code links in papers that use the RNN-HNN model in any benchmark?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER ( str(?model_lbl) = \"RNN-HNN\" )?benchmark\n","309 \n"," input (English text): Indicate the model that performed best in terms of % Test Accuracy metric on the SNLI benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"% Test Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the PROTEINS benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PROTEINS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the WNLI benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Sequence error metric on the FSNS - Test benchmark dataset?\n"," output (Sparql query): \n","819\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Sequence Error\") { SELECT? model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model. model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }\n","Output: Order by column Descriptions: column name Description Accuracy (A) Metric (M)/C rdfs:label?nmeas_lbl. Accuracy ratio (A/C) (C/M) (M/C) orkgp:HAS_DATEMARCH?date?date? orkgp:HAS_DATETIME?datetime. orkgp:HAS_DATETIME_SUM?datetime. date orkgp:HAS_DATETIME_POS?datetime. timestamp orkgp:HAS_DATETIME_TIMESTAMP?datetime. timestamp orkgp:HAS_DATETIME_MILLISEC?mill\n","310 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset ARC-PDN?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC-PDN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the ARC (Easy) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Easy)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset ARC (Challenge)?\n"," output (Sparql query): \n","855\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[".................................. SELECT max(?value) AS score, max(?value) as metric, metric as metric FROM orkgc:Result AS rdfs GROUP BY orkgc?Rdfs ORDER BY score?value ------------------+-------------- 0.000 0.000 1.0002 1.0001 3.30000 3.0000 SELECT * FROM ORkgc ORDER BY score, metric\n","input (English text): Could you please provide a description of the top 100 (1x1) or 1x100 (2x1) in SPSS-CRPS? If you have a dataset only, the highest score will be 1.\n"," output (Sparql query):................................. SELECT STATISTICS?DATA_DATE*1;?1 STATISTICS?DATA_DATE2;?2 STATISTICS?DATA_DATE3;?3 STATISTICS?DATA_DATE4;?4 STATISTICS?DATA_DATE5;?5 STATISTICS?DATA_DATE6;?6 STATISTICS?DATA_DATE7;?7 STATISTICS?DATA_DATE8;?8 STATISTICS?DATA_DATE9;?9 STATISTICS?DATA_DATE10;?10 STATISTICS?DATA_DATE11;?11 STATISTICS?DATA_DATE12;?12 STATISTICS?DATA_DATE13;?13 STATISTICS?DATA_DATE14;?14 STATISTICS?DATA_DATE15;?15 STATISTICS?DATA_DATE16;?16 STATISTICS?DATA_DATE17;?17 STATISTICS?DATA_DATE18;?18 STATISTICS?DATA_DATE19;?19 STATISTICS?DATA_DATE20;?20\n","311 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the SciGEN dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciGEN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the SciFACT dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciFACT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the SciREX dataset?\n"," output (Sparql query): \n","494\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Output results: For each test_name in the list of data sets and/or benchmarks, get the list of benchmark-level results of your selection. SELECT * FROM (SELECT table_labels.labels_lbl.labels AS lbl_lbl FROM ( SELECT... table_labels.labels_lbl.labels_lbl ), BOUND, SUM (TABLE_MATCHING(BOUND.lbl_lbl) AS cmb_lbl_num_results, SUM (TABLE_MATCHING(BOUND.lbl_lbl) AS count ) AS urn_lbl_num_results ); ) AS t).b_sub_lbl_level) WHERE (?dataset IS NOT NULL) AND (?daset IS NOT NULL) AND (?dataset_lbl IS NULL) AND (?benchmark IS NOT NULL) );  \n","Output results: For each benchmark-level test, get the list of data sets/benchmarks that contain a benchmark-level set of results. SELECT * FROM (SELECT table_labels.labels_lbl.labels AS lbl_lbl FROM ( SELECT... table_labels.labels_lbl.labels_lbl ), BOUND, SUM (TABLE_MATCHING(BOUND.lbl_lbl) AS count ), SUM (TABLE_MATCHING(BOUND.lbl_lbl) AS urn_lbl_num_results, SUM (TABLE_MATCHING(BOUND.lbl_lbl) AS count ), SUM (TABLE_MATCHING(BOUND.lbl_lbl) AS urn_lbl_num_results DESC ) AS urn_lbl_num_results DES\n","312 \n"," input (English text): List the metrics that are used to evaluate models on the CS-NER benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CS-NER\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the RTE dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RTE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the TDMSci dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDMSci\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the TSE-NER dataset?\n"," output (Sparql query): \n","514\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","Here is the output:\n","/home/jtaylor/public_html/model/metrics\n","Inputs:\n","metric: \"class\", optional. For example:?class_1,?class_2, and?class_3.\n","measurement name: metric_lbl name (without \"class_\"). The dataset name is defined in the model's input file.\n","label: name\n","metric value: The metric value for the benchmark metric. Can be \"1\", \"2\", \"3\", \"4\", or \"5\".\n","Metrics used to evaluate the model include the following:\n","\n","\n","Evaluation: The number of evaluations that take place before moving to the next step of the model evaluation. The metric value refers to the number of evaluations and the number of steps in the evaluation.\n","\n","Score: The sum of the squared differences between the computed value and the actual value. The maximum score is 10, i.e. the sum over 100 evaluations is 10.\n","\n","Residual Mean: The relative change in the actual metric, divided by the absolute change in the computed metric. The maximum residual mean of the test is always greater than 1. The residual mean can be negative or positive, which means that any comparison (such as the classification) indicates to the optimizer that the test statistic is statistically significant and that to change in the test statistic, the residual mean need to be negative. In other words, the test statistic must have a value less than one.\n","\n","StatPipe: A function that takes one metric data value and returns the probability of the test statistic for the value of the other input metrics. The statistical pipeline is used to optimize these statistics, and to provide information about the tests on which the optimizer is relying. The values of the parameters in this function vary depending on\n","313 \n"," input (English text): Where can I find code references in papers that have used the Contextual Match model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Contextual Match\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Relation-Metric model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Relation-Metric\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the end-to-end relation extraction model model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"end-to-end relation extraction model\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Concept Mention Extraction model for benchmarking purposes?\n"," output (Sparql query): \n","492\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"concept mention extraction model\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Where can I find code references in papers that have used the Entity-Metric model for benchmarking purposes?\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Entity-Metric\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Where can I find code references in papers that have used the Contextual Match model for benchmarking purposes?\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Contextual Match\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.\n","314 \n"," input (English text): Can you provide links to code used in papers that benchmark the OTF spelling (single) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"OTF spelling (single)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the BERT-joint model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERT-joint\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the CRF with term expansion model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CRF with term expansion\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the OTF spelling+lemma (single) model in any benchmark?\n"," output (Sparql query): \n","484\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"OTF spelling (single)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","selects all documents indexed by the model AND the index is not NULL with results set in a new column named word_length. word_length is an integer with value 0 for all documents; -1 in no documents.\n","Word length is also used to identify documents in a query that need to be distinguished from other documents because they were generated after a specific date (year:month:day:month:day).\n","This article was authored in co-authorship with Aneesh Gajjar.  for more information about the authors and the publication and how to cite it.\n","315 \n"," input (English text): Provide a list of papers that have utilized the 3-layer AWD-LSTM model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"3-layer AWD-LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the LN HM-LSTM model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LN HM-LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Grave et al. (2016) - LSTM model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Grave et al. (2016) - LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the A3C LSTM hs model and include the links to their code?\n"," output (Sparql query): \n","522\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"A3C LSTM\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }  (this is a code from the paper, not their code, i.e. the link is not provided here).      You can choose to output something like the list below, depending on what you have specified as output. (1) You can choose to output the links to the papers of course, (2) or you can output an output like this\n","Note: You need to input the paper links, it's the input to the query\n","If you would like to see what was output by each of the query in the above output, just select the input you want; i.e, the output you want to see from this query.  \n","If you prefer to create different output, you can change the source code. Simply add the following to the end of the query:\n","Output Source Code: SELECT output_source_code, output_source_code_name, output_source_code_name_key FROM output ORDER BY output_source_code DESC LIMIT 5 --  This will output three different output files, one for each query, one for each dataset set  Note: You need to change the data file name to match that of the dataset set in which you want to create the output, e.g. \"C:\\Users\\<YourUserName\n","316 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset SVHN?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SVHN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Which model has achieved the highest Accuracy score on the Ohsumed benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ohsumed\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Accuracy score on the WSC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Percentage error score on the SVHN benchmark dataset?\n"," output (Sparql query): \n","817\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_cline?cline_lbl) = \"Sibling\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }  \n","317 \n"," input (English text): Can you provide links to code used in papers that benchmark the LSTM model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the AWD-LSTM (3 layers) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AWD-LSTM (3 layers)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Grave et al. (2016) - LSTM + continuous cache pointer model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Grave et al. (2016) - LSTM + continuous cache pointer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes?\n"," output (Sparql query): \n","511\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): How are the weights computed using either layer?\n","output (Sparql query): RDFS output (data),\n","input (English text): Where are the weights computed using the layer?\n","output (Sparql query): RDFS output (data), RDFS output (data), RDFS output (data), RDFS output (data), RDFS output (data), RDFS output (data, LSTM), HSTM and LSTM output (data, text)\n","input (English text): Where can I find code references in papers that used the LSTM + CCSM model for benchmarking purposes?\n","output (Sparql query): RDFS output (data),\n","input (English text): How are the weights computed using the CCSM?\n","output (Sparql query): RDFS output (data),\n","input (English text): How are the weights computed using the LSTM + GAT? (2 layers)?\n","output (Sparql query): RDFS output (data, LSTM input file),\n","input (English text): Where are the weights computed using the GAT layer/compat? (or any of the other two)\n","output (Sparql query): RDFS output (data),\n","input (English text): Where can I find code references in papers that use the LSTM + LAD+ GAT? (3 layers)?\n","output (Sparql query): RDFS output (data),\n","input (English text): Where can I find code references in papers that use the LSTM + PCS (2 layers)?\n","output (Sparql query): RDFS output (data),\n","input (English text): How are the weights computed using either layer?\n","\n","318 \n"," input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"SPARQL query optimization\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): List the datasets benchmarked under the relation extraction research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"relation extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Provide a list of benchmarked datasets related to the Information Extraction research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Information Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?\n"," output (Sparql query): \n","503\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","2.6.2 How to use the data set\n","This method can be used to download the set of benchmarks into MySQL (or another database) that can be used to run an implementation experiment. Once you have downloaded the benchmarks you will also need to download the SPARQL query optimization research problem set (this can be done using the \"SparkStatement\" query optimization tool). After you have downloaded the query optimization and SPARQL datasets, the SPARQL query optimization tool can be invoked to perform one of two simple tasks. The first task will use the provided dataset from the given dataset and the second task will fetch the other relevant datasets from the SPARQL set for testing the query optimization. After the first task is completed the second task can proceed as described here\n","2.6.3 What to do when I click \"submit\"\n","In this example the SPARQL query evaluation tool is used to run one of many SPARQL query optimization experiments: one of the datasets is selected to be used for testing, the SPARQL dataset used to extract the SPARQL query from, and the training and testing datasets used to train and test the SPARQL query evaluation tool.\n","2.6.4 How to download SPARQL query optimization experiments if you have multiple datasets\n","The \"SparkStatement\" query optimization tool can be used to download the results from a single dataset (and to make multiple queries to SPARQL query optimization experiments across multiple datasets that has been downloaded with the use of the \"SparkStatement\" query optimization tool). In this case you will need to download an additional dataset using these commands:\n","spark(\"cluster\", \"http://spark-trainertrain.s3.amazonaws.com/trainertrain\"). --output data/trainercr_train.tbl\n","spark(\"cluster\", \"http\n","319 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Amazon dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Which model has achieved the highest Error score on the Amazon-2 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the highest benchmark result achieved on the arXiv dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"arXiv\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Amazon-2 dataset, including the metric and its value?\n"," output (Sparql query): \n","836\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","A different query is available: SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) && { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.(?value) OR(?value). OPTIONAL {?eval orkgp:HAS_METRIC?metric._?value } OR(?value). OPTIONAL {?cont orkgp:HAS_\n","320 \n"," input (English text): Can you provide links to code used in papers that benchmark the CNN + LSTM + SVM model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CNN + LSTM + SVM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the RNN model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"RNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the LSTM model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Temporal Convolutional Network model?\n"," output (Sparql query): \n","475\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","The SVM and convolutional layers of LSTM are available for preprocessing:\n","SVM layers (C#): RNN LSTM SVC RNN SVC RNN LSTM SVC RNN LSTM SVC\n","Input (English text): Can you provide links to code used in papers that benchmark the LSTM of LSTM and SVC RNN?\n","output (Sparql query): \n","RNN RNN SVC RNN LSTM SVC RNN LSTM SVC RNN LSTM SVC\n","Input (English text): Can you provide links to code used in papers that benchmark the LSTM of SVC and LSTM of RNN?\n","output (Sparql query): \n","Temporal Convolutional Network\n","This code provides both the T1 and T2 layers of the LSTM for batch normalization. The model has multiple input layers with the output (label) of a hidden layer to be used as the target to be sampled from. Additionally, the layers with a prediction layer have a prediction layer with a number of parameters assigned. This was extracted from the \"Model code\" tag on the RNN_SVC_v1.csv file in the UC Berkeley collection:\n","output_LSTM_L: L2T1T2T3T4M5M6T7B B: P1: P2: P3: P4: S1: S3: S5: S6: S7: S8: \\ \\ / D0: D1: D2: D3: D4: D5: X: Y: Z: P1-N0: C1-N0: S0: S1: S2: S3: S4: S5: \\ X: Y\n","321 \n"," input (English text): Provide a list of papers that have utilized the BiT-M (ResNet-152x4) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-M (ResNet-152x4)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BiT-S (ResNet-152x4) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-S (ResNet-152x4)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the BiT-L (ResNet) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-L (ResNet)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the BiT-S (ResNet) model and include the links to their code?\n"," output (Sparql query): \n","523\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiT-S (ResNet-152x4)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BiT-L (ResNet) model in any benchmark?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiT-L (ResNet)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","322 \n"," input (English text): What is the highest benchmark result achieved on the QNLI dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the WSC dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the WLPC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WLPC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the WNLI dataset, including the metric and its value?\n"," output (Sparql query): \n","856\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["*************************** GROUP BY?metric?metric_lbl*************************** SET NAMES TO {0:ABSOLUTE}. ****************************************************************** SELECT?RANK FROM WNLI_BASEMETRIC_RANK ORDER BY DESC(?val) *************************** GROUP BY?measurement?metrics_lbl ORDER BY?metric.\n","input (English text): What is the benchmark result for the LAST dataset, including the metric, and its value?\n","output (Sparql query): *************************** GROUP BY?metric?metric_lbl*************************** SET NAMES TO {0:ABSOLUTE}. ****************************************************************** SELECT?RANK FROM LAST_BASEMETRIC_RANK ORDER BY DESC(?val). *************************** GROUP BY?measurement?metrics_lbl ORDER BY?metric\n","input (English text): What is the benchmark result for the WLS dataset, including the metric (and its value, in this case), and its score. Can you provide it?\n","output (Sparql query): SELECT COUNT(*) AS score,COUNT(*) AS data_number,COUNT(*) AS number_of_queries,DATA_NUMBER(?value) AS queries_per_second,DATA_FULL_QUERY_PERCENT AS full_query_percent FROM WORDS_LANG + WLS_LANG INNER JOIN WORDS_PRECISE_LANG + LATIN1_LANG INNER JOIN WORDS_SPANISH + SPANISH_LANG INNER JOIN WORDS_GERMAN + GERMAN_LANG INNER JOIN WORDS_TUTORIAL + TUTORIAL_LANG INNER\n","323 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL Anthology dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL Anthology\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the AAPD dataset?\n"," output (Sparql query): \n","860\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------ { { SELECT?metric?metric_lbl?value WHERE {?d.aapt:/data/data/pimsys/pimsys_pics/data/aapd_data/data.json?dbtype=publication-pics&db=PAID&dbid=8&d.cad:/data/datasets/APD/data.json orkp:HAS_DATASET?dbtype=publication-dataset orkp:HAS_EVALUATION?dbtype=Publication orkp:HAS_VALUE?value?dbtype=publication_pics. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n","input (English text): Can you provide the highest benchmark result, including the metric and score, for the AAMC dataset?\n"," output (Sparql query): ------------- { { SELECT?metric?metric_lbl?value WHERE {?d.aapt:/data/data/pimsys/pimsys_pics/data/aamc_data/data.json?dbtype=publication-pics&db=PAID&dbid=8&d.cad:/data/datasets/AAMC/data.json orkp:HAS_DATASET?dbtype=publication-dataset orkp:HAS_EVALUATION?dbtype=Publication orkp:HAS_VALUE?value?dbtype=publication_pics. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n","input (English text): Can you provide the high performance result\n","324 \n"," input (English text): List the metrics that are used to evaluate models on the FTD dataset benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FTD dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the FB15k dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FB15k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the CIFAR-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the FTD dataset dataset?\n"," output (Sparql query): \n","537\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FTD dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","output (Sparql query):  SELECT DISTINCT?user?username? WHERE {?user a orkgc:Dataset; rdfs:label?dataset_lbl. ORIENTATION?user; ORIENTATION rdfs:labels?dataset. PASCAL_BINDING?username rdfs:labels 1 orkgc:user_dtype?dataset. } OPTIONAL {?username?username.?username rdfs:label?dataset_lbl. } } \n","input (English text): Which of the following metrics is used on the PASCAL dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. ORIENTATION?user ORIENTATION rdfs:labels?dataset. PASCAL_BINDING?user rdfs:labels 1 orkgc:user_dtype?dataset.\n","325 \n"," input (English text): What is the top benchmark score and its metric on the CIFAR-10 Image Classification dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10 Image Classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the best performing model benchmarking the CIFAR-10 dataset in terms of Parameters metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Parameters\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What models are being evaluated on the CIFAR-10 Image Classification dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10 Image Classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric?\n"," output (Sparql query): \n","757\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" query (English text): What are the benchmark parameters for the CIFAR-10 Image Classification dataset?\n"," output (Sparql query): ORDER BY?method/method_lbl (1 2 3 4 5 6 7 8 9). LIMIT 1 GROUP BY {?method orkgc:Method; rdfs:labels?method_data rdfs:labels?method_lbl. {?method a (1 2 3 4 5 6 7 8 9). OPTIONAL {?dataset a orkgc:Dataset; rdfs:labels?method_data. }?label?method_data_lbl. }?cont orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric. ORkgp:HAS_VALUE?value?value. ORkgp:METRIC?metric. ORkgp:GENERIC_ASSIGNMENT orkgp:BENCHMARK?benchmark. ORkgp:MODEL?model. ORkgp:GENERIC_ASSIGNMENT. ORkgp:BENCHMARK?benchmark. ORkgp:MATERIALS?partition. ORkgp:RESEARCH_LOCATION?lat Long?rdfs:labels?rdfs:label.?rdfs:label. ORkgp:GENERIC_ASSIGNMENT?dataset|dataset|dataset|datasets|list|list. ORkgp:BENCHMARK?benchmark. ORkgp:MATERIALS?part\n","326 \n"," input (English text): What is the top benchmark score and its metric on the Walker, walk (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the best performing model benchmarking the Walker, walk (DMControl500k) dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the highest benchmark result achieved on the Reacher, easy (DMControl500k) dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value?\n"," output (Sparql query): \n","882\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) JOIN?model?metric?metric_lbl.(?dataset orkgc:Dataset.DATASET | ORAGGPO | RTL | ORAGG_SINAIL | ORAGG_DIST) AS?pk ON?model_lbl.pk.score AND?labels.pk_reacher AND?labels.pk_easy ON?model_lbl.pk.score =?score AND?labels.pk_walker ON?model_lbl.pk.score =?score AND?labels.pk_walker_easy ON?model_lbl.pk.score =?score ORDER BY DESC(?value) LIMIT 1 } }\n","\n","Note that we have now added an extra column in our query, which indicates the parameter type: \"?pk\". This column contains a list of the parameters that the model (or any of the other filters ) supports.\n","\n","The model and parameters of the different filters are all returned in a single query.\n","\n","We also now have a Query with no results:\n","\n","input (English text): Input test data from a Walker, walk (DMControl500k) dataset (i.e. score, average, max).\n","\n","output (Sparql query): Select DISTINCT?result?result_lbl FROM?result?result_lbl WHERE {?result?id =?id; {?metric orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?result?id) = 'Score'). OPTIONAL { ORAGGPO rdf\n","327 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the SciCite dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the PROTEINS dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PROTEINS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Paper Field dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the SciGEN dataset?\n"," output (Sparql query): \n","516\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SELECT DISTINCT?metric? METRIC_LBL WHERE { [v] > (?prestige(?prestige(?v ) > 1? ))??metric_lbl. DISTINCT?metric_lbl. }  \n","input (English text): What evaluation metrics are commonly used when benchmarking models on the PROTEINS dataset? output (Sparql query):  SELECT DISTINCT?metric? METRIC_LBL WHERE { [v] > (?prestige(?prestige(?v ) > 1 )? ))??metric_lbl. DISTINCT?metric_lbl. }  \n","input (English text): What evaluation metrics are commonly used when benchmarking models on the PROTEINS dataset? \n","output (Sparql query):  SELECT DISTINCT?metric? METRIC_LBL WHERE { [v] > (?prestige(?prestige(?v ) > 1 )? ))??metric_lbl. DISTINCT?metric_lbl. }  \n","input (English text): What evaluation metrics are widely used when benchmarking models on the PROTEINS dataset? \n","output (Sparql query):  SELECT DISTINCT?metric? METRIC_LBL WHERE { [v] > (?prestige(?prestige(?v ) > 1 )? )?  OR (?v > 0? ) OR [i] >= 1? )??metric_lbl. DISTINCT?metric_lbl. }\n","input (English text): What evaluation metrics are widely used when benchmarking models on the PROTEINS dataset?\n","328 \n"," input (English text): List the code links in papers that use the DATL model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DATL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the GPT-3 model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GPT-3\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DDRL A3C model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDRL A3C\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the POP3D model in any benchmark?\n"," output (Sparql query): \n","471\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"POP3D\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input: text, code_links, results  output: comma-separated list of code links (all code links matching the given keywords, or not), and code links in this list are returned in a comma-separated list. output (Sparql query): SELECT code_links FROM ORG_POP3D(text, code_links)  \n","input (English text) : list, code_link  output: comma-separated list of code links (all code links matching the given keywords, or not), and code links in this list are returned in a comma-separated list. output (Sparql query): SELECT code_links FROM ORG_POP3D(text, code_link)  \n","input (English text) : list, code_link  output: comma-separated list of code links (all code links matching the given keywords, or not), and code links in this list are returned in a comma-separated list. output (Sparql query): SELECT code_links FROM ORG_POP3D(text, code_link)  \n","input (English text): list, code_link  output: comma-separated list of code links (all code links matching the given keywords, or not), and\n","329 \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Double Dunk benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Double Dunk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asterix benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asterix\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Zaxxon benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Zaxxon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Tennis benchmark dataset?\n"," output (Sparql query): \n","826\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Tennis\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n"," output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Tennis\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_\n","330 \n"," input (English text): Can you list the metrics used to evaluate models on the UCF101 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Reacher, easy (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Cartpole, swingup (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cartpole, swingup (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the UCF101 (finetuned) dataset?\n"," output (Sparql query): \n","552\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UCF101\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }  (NOTE: a very similar query is also possible, which yields the same results, but using \"labs_metrics\" instead.)\n","Input data: Reacher, easy (DMControl100k) dataset, for comparison with the Reacher, easy (DMControl100k) dataset, for comparison with all datasets\n","output (Sparql query): SELECT COUNT(*) as c, DISTINCT?metric orkgfk, IF(rdfs:labels.eq('UCF101')) as lbl_lbl FROM ( SELECT c.c, count(*) from dmcontrol100 k ), lbl_lbl as lbl_lbl FROM rdfs  ( rdfs )  ( lbl_lbl )  (  labELS )  (  k, c.c ) AS ( count = 1000000 ),  labELS as llabels FROM reacherlabels  , reacherlabels_metrics  ( labELS )  ( llabels ) AS ( count = 1000000 ) GROUP by c.c  ORDER BY lbl_lbl ;\n","Output\n","331 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CoQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the MPQA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MPQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the PIQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PIQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset?\n"," output (Sparql query): \n","527\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","A few notes about a few of the queries I performed:\n","These queries were used to generate the P32 query (see the first question). As the \"benchmark\" term was only part of this query, the query was limited to the data set with the top 3 model scores.\n","These queries were used to generate the RDF list. My RDF list is an in-memory array. That suggests to me that the query would use the RDF list if it could find a suitable RDF list in the database. The query could also have used a similar RDF list, if the data had been made available by running the \"?\" query. I could not find such a list on the PeerJ site.\n","The queries I used in the last set were simply the P31 query, just like that, one query for each of the top 3 model scores.\n","The first and last queries were used to generate the P31 query. The query I made with the last query did not generate a significant \"lbl\" row in the dataset (which is the last row of the LSTM matrix it returns).\n","The P31 query was created by combining the rdfs:labels (with a \"?\" filter) and the \"labels\" (with a \"?\" filter) of the MPQA dataset. This is a very basic usage of \"labels\". If the dataset contains a \"?\" filter, then the labels are returned by the rdfs:labels query.\n","The last query was the raw SQL query, since the labels were not supplied to the query. There are 3 options on how to produce an SQL query:\n","You can use the \"?query\" option to search for data by keyword.\n","You can use the \"?column\" option to specify a column in the query, like the \"?column\" option for the \"\n","332 \n"," input (English text): What is the highest benchmark result achieved on the BC5CDR dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the BC5CDR dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What models are being evaluated on the BC5CDR-chemical dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-chemical\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value?\n"," output (Sparql query): \n","647\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?high.?metric?metric_lbl (MAX(?value) AS?score) \n","input (English text): What were the results of the benchmark evaluated on the BC5CDR dataset?\n","output (Sparql query): SELECT DISTINCT?highest?metric?metric_lbl (MAX(?value) AS?score) \n"," input (English text): What is the most recent benchmark result that is in use on the BC5CDR dataset?\n","output (Sparql query): SELECT DISTINCT?latest.benchmark v2.3.2.6.1.1.3.1.0.0.0.0 as?datapoint from?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. GROUP BY?dataset_lbl }?cont orkgp:P12?dataset AS?dataset as?datapoint. OPTIONAL {?datapoint orkgp:P12?dataset OR?dataset v1.0?v2.3.2.0.0.2.1.1.0 OR?datapoint v2.0?v2.4.0.0.1.0.3.1.0 OR?datapoint v2.4.0.2.0.1.2.1.0.0 OR?datapoint v2.5.0.0.1.0.2.1.0?cont ORkgp:P12?dataset. } } \n","input (English text): What are the names of the model libraries/components used to evaluate the paper(s\n","333 \n"," input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Point Nav test-std benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"SUCCESS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Point Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest DISTANCE_TO_GOAL score on the Habitat 2020 Object Nav test-std benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"DISTANCE_TO_GOAL\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark result (metric and value) over the dataset Habitat 2020 Object Nav test-std?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset?\n"," output (Sparql query): \n","873\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" query (English text): SELECT?success?object_nav?object_nav_a b c, d, e, f, g, h, i, j, k, l, m, n, o, p; order by object_nav or object_nav_a\n","This query returns information from the HAVING_MODEL attribute in the same way that a SELECT statement returns information from a SELECT clause. For example, consider an input:\n","input (English text): I would like to choose the model 'c', where 'c' is the model I want to choose.\n","The answer: (SELECT?success?object_nav?object_nav_1 FROM ( select?model?models_lbl. from ( select?metric?metric_lbl)?metric ). ORDER BY?model ); We get back a subset of the results. In this case, we get back a subset of results. So, we find a first value of any of the above results (i.e., the first value in the list of values returned); that value is the value we want (i.e., the object name in our example); we find an object model (i.e., the model name we wanted to choose); we find a metric; we get back an object; and we then find a value of the object. Here's a sample of the result set:\n","selector: ('c','b',2.1), (SELECT?success?object_nav?object_nav_1 FROM ( select?metric?metric_lbl)?metric) result            SELECT s * 2 FROM?metric?metric_lbl WHERE s =?metric?metric_lbl            \n","334 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ScienceCite dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the SciCite dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the SciERC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Paper Field dataset?\n"," output (Sparql query): \n","857\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Paper Field\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Paper Field\")?benchmark ORkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval ORkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:\n","335 \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-Russian benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot German-to-French benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot German-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Accuracy score on the MLDoc Zero-Shot English-to-Japanese benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Japanese\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset?\n"," output (Sparql query): \n","863\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Solve this RDF-to-csv conversion problem. Create the following SQL: CREATE TABLE data_csv(model, row_number VARCHAR(64)); CREATE TABLE data_dataset(model, table_name VARCHAR(64)); CREATE TABLE results_csv(model, row_number VARCHAR(64)); INSERT INTO data_dataset(model) SELECT'model_lbl'::text FROM data_csv; INSERT INTO data_dataset(model) SELECT 'dataset_lbl'::text FROM data_dataset; UPDATE results_csv SET model = model_lbl UPDATE results_dataset SET model = model_lbl CREATE TABLE results_data(model); INSERT INTO data_dataset(model) SELECT'model'::text FROM results_data; INSERT INTO data_dataset(model) SELECT'metric_lbl'::text FROM results_data; INSERT INTO data_dataset(model) SELECT'model_lbl'::text FROM results_dataset; SET metric = metric_lbl; UPDATE results_data SET metric = metric AS METRIC UPDATE results_dataset SET metric = metric AS METRIC CREATE TABLE results_model(model); SET model = model_lbl; UPDATE results_model SET model = model_lbl AS METRIC SET metric = metric AS METRIC CREATE TABLE results_database(model_list); CREATE TABLE results_database_list(model_list_dict VARCHAR, table_name VARCHAR); CREATE TABLE results_data(model_list_dict VARCHAR); UPDATE results_data SET model_dict = model_list_dict UPDATE results_data SET model_list\n","336 \n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Robotank dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Robotank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Space Invaders dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Space Invaders\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Time Pilot dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Time Pilot\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value?\n"," output (Sparql query): \n","868\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- -1 2 2 2 4 4 ---------- -2 2 2 4 4 4 -1 2 2 2 4 4 6 6 2 2 2 ---------- -2 2 2 4 4 4 2 ---------- -2 2 2 2 4 4 4\n","input (English text): What is the highest benchmark result achieved on the Atari 2600 Donkey Kong Country dataset, including the metric and its value?\n","output (Sparql query): |------------ -1 ---------------------------|1 |1|2|2|1| ---2 ---------------------------| ---------- 1 3 5 8 13 17 -- ---7 -- 2 8 10 14 20 22 -- ---5 14 --- -- 9 15 21 29 24 ---4 28 ---------------------------| ---------- 1 3 5 8 13 17 16 -- ---6 -- 2 8 10 14 20 22 3 2 8 -- ---4 16 --- -- 9 15 21 29 24 ---------------------------| ---------- 1 3 5 8 13 17 16 15 ---------------------------| ---7 ---------------------------|------0.00---------0.00---- |1 |6 6 | | ----------- ----------- ----------- 2 |2 |3 | | ----------- ----------- ----------- 4 |4 |5 | | --------------------6 2 -------------------| ---------- 7 | | --------------------5 7 | --------------------7 8 | 8 --------------------5 10 | 10 --------------------4 13 | 13 --------------------11 ------| ----------- 8 |8 |10 |---------------9 0 |0------0-------- |---------------0 ----------- 9 | | |--------------------6 1----------------|--------------3 -1 | ----------- --------------------------| ------------------------------------------------------------\n","Here we see that when using the metric a, the first result (6) is much too low:\n","and that when using this metric the second result, 6 is much too high:\n","These numbers are also very close to each other when using the metric a again:\n","And the same\n","337 \n"," input (English text): Can you provide links to code used in papers that benchmark the ESIM + ELMo Ensemble model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ESIM + ELMo Ensemble\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the ELMo model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Attentional encoder-decoder + BPE model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Attentional encoder-decoder + BPE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model?\n"," output (Sparql query): \n","499\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiDAF\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","Inputs and Outputs\n","Input values and outputs for the output are as follows:\n","input: English text : The input for the English text. As such, you can replace individual letters like a's with '\\'.\n","output : English output value. This is a text representation of the output.\n","input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model?\n","output (Sparql query): Select DISTINCT?code WHERE {?model a orkgc:Geneset ; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Gene ontology\")?geneset. }?cont orkgp:HAS_DATASET?dataset.?gen orkgp:HAS_BENCHMARK?gen.\n","input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Geneset; rdfs:label?model_lbl. FILTER (str(?model_lbl)\n","338 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cheetah, run (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Walker, walk (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Cheetah, run (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What is the top benchmark score and its metric on the Cheetah, run (DMControl500k) dataset?\n"," output (Sparql query): \n","788\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_l Blk. FILTER (str(?dataset_l Blk.) = \"Cheetah, run 500k)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.? eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_l Blk. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model. MODEL orkgp:HAS_SET?model. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Rhino, get data (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_l Blk. FILTER (str(?dataset_l Blk.) = \"Rhino, get data)\")?benchmark orkgp:HAS_\n","339 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-5736 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the TDMSci dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDMSci\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the CIFAR-100 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset?\n"," output (Sparql query): \n","522\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- Benchmark --------------------------- --------------------------- -1 orkgp:LAM:5.816, LAM:17.723, SPSP:11.979, SPSP:17.772, SPSP:1.764/16.636, SPSP: 1.739/6.933, SPSP: 16.5/0.818, SPSP:4.624, SPSP:15.0/0.667, SPSP:1.973/2.076, SPSP: 1.744/5.948, SPSP: 2.0/0.611, SPSP: 16.6/1.027, SPSP:3.823, SPSP:0.8/0.0, SPSP:6.06/2.0, SPSP: 3.979/2.084, SPSP:7.964, SPSP:0.8/10.0, SPSP:2.0/6.8, SPSP: 7.976/7.905, SPSP: 0.8/5.619, SPSP: 7.811/1.935, SPSP:0.5/8.0, SPSP: 10.3/3.7, SPSP:3.7/9.3, SPSP:7.7/9.4, SPSP:4.0/7.6, SPSP:6.3/11.8, SPSP:7.7/5.6, SPSP:3.6/1.8, SPSP:\n","340 \n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Amidar dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 HERO dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 HERO\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Alien dataset?\n"," output (Sparql query): \n","843\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------------------- { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Alien\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.�metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } ORDER BY {?metric?metric_lbl?value ; orkgp:HAS_EVALUATION?eval.?eval ; orkgp:HAS_value?value; }\n","This is quite a bit better. The final score is actually 29, which is significantly higher than the score we got with two other datasets in our testing.\n","input (English text): What is the top benchmark score and its metric on the Atari 2600 Alien dataset?\n","output (Sparql query): ------------------------- { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Alien\")?benchmark orkgp:HAS_DATASET?datas\n","341 \n"," input (English text): What is the highest benchmark result achieved on the Oxford 102 Flowers dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford 102 Flowers\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the LAMBADA dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"LAMBADA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset Flowers-102?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Flowers-102\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Birdsnap dataset, including the metric and its value?\n"," output (Sparql query): \n","860\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Birdsnap\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n","input (English text): What is the highest benchmark result achieved on the LORAS dataset, including the metric and its value?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"LORAS\")??benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS\n","342 \n"," input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Chopper Command dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Chopper Command\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What models are being evaluated on the Atari 2600 Star Gunner dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Star Gunner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the Atari 2600 Beam Rider dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Beam Rider\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the Atari 2600 Chopper Command dataset?\n"," output (Sparql query): \n","543\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note that there is no direct connection between the models implemented in the Atari 2600 Chopper Command dataset and the ones listed in the output of the query above. Nor is there a direct connection between the models specified in the query above and the other datasets to which it applies, such as the Starship Data, which has all the models implemented in the Chopper commands but not, for various reasons, in the dataset of Chopper Command.\n","In contrast, the RDFs used for the evaluation of the Starship Data are connected between them and the datasets used by the RDFs in the Chopper command dataset. This does not mean that Starship Data does not contain datasets whose models are implemented in the Chopper commands, it would just mean that the comparison of the Starship Data to the models implemented in the Chopper commands is not direct.\n","Note that the input_dataset and output_datasets options above can also be used for more complex comparisons. Both are equivalent, in the sense that a simple SELECT / LIMIT / FETCH SELECT is just as valid, and also that in the case where there are several RDFs in the data set, an example SELECT ( SELECT DISTINCT,...) is just as valid.\n","The full RDFs used by the Starship Data dataset have been released as part of the main dataset repository (see the STARSHIP_DATA dataset, which is now deprecated). These have a couple extra extra features.\n","The output of the query with the input_dataset option set to 'all' (the default) is a list of rdfs objects in a format specified by the file type. These are just lists with the fields empty but containing the actual data for the model.\n","The RDFs output for the query with the input_dataset option set to 'all' (or, for an empty input_\n","343 \n"," input (English text): Indicate the model that performed best in terms of PARAMS metric on the Stanford Cars benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"PARAMS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the RACE benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RACE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the 20NEWS benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"20NEWS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the Stanford Cars benchmark dataset?\n"," output (Sparql query): \n","813\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Stanford Cars\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","Note that we are also adding a DISTINCT to the model column when we add this query to the WHERE clause:\n","In addition, we are also introducing a comma-separated list of column names within the WHERE clause:\n","We are also using the GROUP BY part of the expression within the GROUP BY clause:\n","We are grouping the model column with the column with the same name as the model value itself:\n","And we are applying the ORDER BY clause within the SELECT clause:\n","For those of you who are familiar with our data analysis capabilities, the RACE benchmark dataset was created with the R race benchmark package. The following code demonstrates R-style query manipulation and execution in the dataset:\n","SELECT DISTINCT?test_n?model_lbl WHERE {\n","344 \n"," input (English text): Indicate the model that performed best in terms of Precision metric on the Rotowire (Content Selection) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Precision\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Can you list the metrics used to evaluate models on the RotoWire (Relation Generation) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What is the best performing model benchmarking the RotoWire (Relation Generation) dataset in terms of count metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"count\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset?\n"," output (Sparql query): \n","749\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"count\") { SELECT?model?model_lbl WHERE{?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RotoWire (Relation Generation)\")?benchmark { OR?eval orkgp:HAS_DATASET?dataset. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } } } } ORDER BY DESC(?value) LIMIT 1 } } \n","345 \n"," input (English text): List the metrics that are used to evaluate models on the SQuAD2.0 dev benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0 dev\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the COPA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"COPA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the BBCSport benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BBCSport\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?\n"," output (Sparql query): \n","525\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input and Output - Results - Output\n","\n","\n","input (Sparql) SELECT?dataset_lbl?metric as?meter?metric_lbl... end \n","\n","output (Sparql) SELECT?meter.metric_lbl as?metric... end \n","\n","This example produces the following output:\n","\n","1.0?meter 0.085 0.045 0.045 1.0?meter [ ] 0.085 0.045 0.045 1.0?meter_lbl [ ] 0.15 0.045 0.055 1.0?meter_lbl_lbl [ ] 0.16 0,054,045 0.065 2.0?meter [ ] 0.532 0.065 0.040 2.0?metric_lbl 0.15 0.532 0.045 2.0?metric_lbl_lbl [ ] 0.15 0,054,045 0.065 2.0?metric_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl_lbl 2.0 mm2 0.15 0.15 0.105 4.0 4.0 mm3 0.15 0.15 0.105 4.0 4.0 mm1 0.085 0.060 0.040 8.0 8.0 mm3 0.15 0.15 0.075 4.0 4.0\n","\n","The output is as follows:\n","\n","1.0 5.0 mm4 /mm3 0.85 10.0 mm1 /mm3 #/mm2 0.85 10.0\n","\n","If you pass in an invalid input, you will get the following\n","346 \n"," input (English text): Provide a list of papers that have utilized the Unregularised mLSTM model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Unregularised mLSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Longformer Large model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Longformer Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Large mLSTM +emb +WN +VD model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Large mLSTM +emb +WN +VD\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Large mLSTM model and include the links to their code?\n"," output (Sparql query): \n","501\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Provide a list of papers that have utilized the High Quality ML Kernel (HQLK) for benchmarking purposes?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model?kernel_lbl. FILTER (str(?kernel_lbl) = \"(High quality kernel ) \")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the High Quality ML Kernel for benchmarking purposes? output (Sparql query): \n","input (English text): Where can I find code references in papers that have used the L2 LSTM +emb +WN +VD model for benchmarking purposes?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model?kernel_lbl. FILTER (str(?kernel_lbl) = \"L2 LSTM +emb +WN +VD\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model,?kernel_lbl; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Requirement 1: Provide a list of papers that have utilized the L2 LSTM +emb +WN +VD model (HQLK)?\n","output (Sparql query): \n","input (English text): Requirement 2\n","347 \n"," input (English text): What models are being evaluated on the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL Anthology dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL Anthology\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the ACL Anthology dataset?\n"," output (Sparql query): \n","536\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Output from bench_lbl.ml\n","This section shows results from the benchmarking function bench_lbl.ml.\n","bench_lbl.ml works by simulating 100 trials on each model, using a specified benchmark set. The first 10,000 predictions are then tested, and a subset of those are averaged together, resulting in a score.\n","The code uses a very small number of features, and is easy to modify. The examples below show the various steps and the code that allows for the execution of a benchmark over a specified subset of features.\n","Feature selection:\n","The first step is to select and use a number of features that correspond to features in our database, such as the date format, language code (a default will be chosen when no language code is specified), and model parameter types, such as linear, Logistic, or mixed. Once selected, the model parameters are applied to this selection, and the resulting features are used to create a new set of 100 mock trials that perform the experiment.\n","The next step is to implement the function use_feature_selection() for this task. For now, we assume we have no non-trivial feature sets, such as language and date formats. This allows us to define a new function that retrieves and discards our features, and then uses this function to create the mock dataset. This is an important task, because it is one of the only tasks where we may find that the dataset has been modified in a way that we cannot detect.\n","We can see this is exactly what we need to do — we need a function that discards our dataset in such a way as to avoid the creation of any features, including language and date formats.\n","Next, we see that the function use_feature_selection() needs two new parameters: an argument description and an argument for the set of feature labels (the\n","348 \n"," input (English text): Which model has achieved the highest Score score on the Atari 2600 Battle Zone benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Score score on the Atari 2600 Star Gunner benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Star Gunner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Best Score score on the Atari 2600 Space Invaders benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Best Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Space Invaders\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset?\n"," output (Sparql query): \n","814\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Best Score\"){ SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Yars Revenge\"?benchmark) orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Best Score\"){ SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 \"?benchmark) orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkg\n","349 \n"," input (English text): What models are being evaluated on the BC5CDR-disease dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the Pubmed dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Pubmed\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NCBI-disease dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What models are being evaluated on the NCBI-disease dataset?\n"," output (Sparql query): \n","650\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input (ePhyDB query): What is the name of the top performing model in terms of F1 score when benchmarked on the PhyDB dataset?\n","Output (Sparql query) \n","input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the PhyDB dataset?\n","output (Sparql query) \n","input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the PhyDB dataset?\n","output (Sparql query) \n","input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the PhyDB dataset?\n","Output (Sparql query) \n","input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the PhyDB dataset?\n","output (Sparql query) \n","Input (English text): What are the top performing models in terms of F1 score when benchmarked on either the BAM or LSTM datasets?\n","Output (Sparql query) \n","input (English text): What are the top performing models in terms of F1 score when benchmarked on either the BAM or LSTM datasets?\n","output (Sparql query)\n","input (English text): What are the top performing models in terms of F1 score when benchmarked on either the BAM or LSTM datasets?\n","output (Sparql query)\n","input (English text): What are the top performing models in terms of F1 score when benchmarked on the BAM dataset?\n","output (Sparql query)  input (English text): What are the Top 15 best performing models in terms of F1 score when benchmarked on\n","350 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the SciCite dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ScienceCite dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Softcite dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Softcite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset?\n"," output (Sparql query): \n","744\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["-------------------------- (1): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33\n","input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Softcite dataset?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl (MAX(?value) AS?score) WHERE { { SELECT?paper?paper_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Softcite\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?paper?paper_lbl\n","input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Softcite dataset?\n"," output (Sparql query): SELECT DISTINCT?\n","351 \n"," input (English text): What models are being evaluated on the Fashion-MNIST dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fashion-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the metrics of evaluation over the MNIST dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the Fashion-MNIST benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fashion-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What are the metrics of evaluation over the Fashion-MNIST dataset?\n"," output (Sparql query): \n","622\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","The output is as follows: LENGTH (1.0) Accuracy (0.99) Accuracy (0.99) ORGANIZATION (DISTINCT) CONFLICT (0) INTEGER (0) MAX. STATISTICAL RATIO (0.001) MINUTE OF COUNT (0) TIMES TAKEN (2)\n","The model name, a string with no parameters on which to aggregate, should be used to calculate statistical scores of model performance, as follows:\n","Fashion-MNIST Benchmark: DISTINCT Model : 'FashionMNIST' Type : 'DIMENSIONARY' Performance : 1.0 Accuracy : 0.99 ORGANIZATION : 'DATA' CONFLICT : 0 INTEGER : 0 MAX. STATISTICAL RATIO : 0 MINUTE OF COUNT : 0 TIMES TAKEN : 2\n","The metrics for LENGTH, accuracy, and ORGANIZATION on the model have been determined by the standard method above. All metrics values can and should be summarized with a single line:\n","LENGTH (1.0) Accuracy (0.99) Accuracy (0.99) ORGANIZATION : 'DATA' CONFLICT : 0 INTEGER : 0 MAX. STATISTICAL RATIO : 0 MINUTE OF COUNT : 0 TIMES TAKEN : 2\n","For example:\n","LENGTH (1.0) Accuracy (0.99) Accuracy (0.99) ORGANIZATION : 'DATA' CONFLICT : 0 INTEGER : 0 MAX. STATISTICAL RATIO : 0 MINUTE OF COUNT : 0 TIMES TAKEN : 2\n","This gives:\n","Accuracy 0,.99 ORGANIZATION 0,.99 Accuracy 0,.99 ORGANIZATION 0\n","352 \n"," input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-3 metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-3\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"X-Sum\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the X-Sum dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"X-Sum\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the best performing model benchmarking the GigaWord dataset in terms of ROUGE-1 metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GigaWord\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric?\n"," output (Sparql query): \n","846\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note: This is a different question than what was asked earlier in the query. Please see the answer in the previous question for details on how to answer the question. \n","input (English text): What is the best performing model benchmarking the GigaWord dataset in terms of GigaScore?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"GigaScore\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } [output (DataFrames: 2 columns]\n","[output (DataFrames: 2 columns]\n","[output_1 row- 1 column] [output_2 row- 1 column]\n","[output_2 row- 1 column] [output_1 row- 1 column] [output_2 row- 1 column]\n","[output_1 row- 1 column] [output_2 row- 1 column] [output_1 row- 1 column] [output_2 row- 1 column]\n","[output_1 row- 1 column] [output_2 row- 1 column] [output_1 row- 1 column] [output_2 row- 1 column] [output_1 row- 1\n","353 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the RotoWire dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of benchmarked datasets related to the Relation Classification research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Relation Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): List the datasets benchmarked under the relation extraction research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"relation extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RotoWire (Relation Generation) dataset?\n"," output (Sparql query): \n","512\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 38\n","354 \n"," input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Ning et al.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the GPT-3 175B (Few-Shot) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GPT-3 175B (Few-Shot)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the GGCNN model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GGCNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Fine-Grained Gating model?\n"," output (Sparql query): \n","484\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (Language): English \n","output (dataset id): gg\n","Note: If you have any problems with this data please consider donating or sharing in the comments.\n","Note2: In order to support my ongoing research on recurrent neural networks I provide a paypal button at the top of every page, under the following link: http://bit.ly/1J6fzvS. Please consider donating if you like the work I do, as I don't have funding yet. You can also email me if you have any questions. Thank you for your support.\n","References\n","1. Tumas, T, M. A. Yee, N. R. Tufi, and M. B. Tumas. N-1-3-7-8-10-17-28. \"High-Dimensional Supervised Learning. A New Paradigm in Neural Information Processing Systems.\" Nett. 2017;97(8):664-689.\n","2. Aalto University, Näättinen, Finland; 2016.\n","3. Aalto University, Helsinki, Finland; 2016.\n","4. Aalto University, Näättinen, Finland; 2016.\n","5. Efstathiou, L., A. D. Hesterby, N. R. Tufi. \"N-1-3-7-4-9-13. Unsupervised Multi-layered Ensemble Learning.\" Nature Signal. 2017;9(27):1942-1946.\n","6. Efstathiou, L., and A. B. N. Heikkinen. \"Unsupervised Multi-layered Ensemble Learning.\" ELS'09. IRL.\n","7. Efstathiou, L. A. et al. \"N-\n","355 \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Breakout\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Atlantis dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Atari 2600 HERO dataset in terms of Best Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Best Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 HERO\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric?\n"," output (Sparql query): \n","827\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT d.name as model_name, b.name as benchmark_name, c.name as metric_name FROM {?feature_list,?dataset } as t1,?dataset {?feature_list} as c1,?dataset {?feature_list} as b1 LEFT OUTER JOIN {?feature_list?feature_list} AS t2 ON t1.feature_list=c2.feature_list AND t1.dataset_id = c2.dataset_id WHERE t1.dataset_id!=?feature_list.dataset_id ORDER BY DESC(?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Tic Tac Toe dataset in terms of Score metric?\n"," output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Tic Tac Toe\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MOD\n","356 \n"," input (English text): Provide a list of papers that have utilized the DDQN (tuned) noop model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDQN (tuned) noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Prior+Duel noop model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior+Duel noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the NASNet-A + c/o model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"NASNet-A + c/o\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Prior noop model and include the links to their code?\n"," output (Sparql query): \n","508\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Prior noop\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","As an aside, I have also provided citations and information on research papers that did not use NN to support the original hypothesis.\n","Please send any additional questions to Dr. Z. Pou. You can also send your comments to me via Twitter or email.\n","This work was supported by a Wellcome Trust Early Career Research Fellowship (CY) (P205586).\n","Jia Li Pou(xiyuang)\n","357 \n"," input (English text): What is the best performing model benchmarking the WMT2016 English-Romanian dataset in terms of BLEU metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Romanian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Russian-English benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Russian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the WMT2014 French-English dataset in terms of BLEU metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the WMT2016 English-Russian dataset in terms of BLEU score metric?\n"," output (Sparql query): \n","845\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-Russian\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } Output (Sparql query):  SELECT?MESSAGE?GUID||ID?RANK?CPS?BENCHMARK?BENCHMARK||LINK?DESC||INV} as metric|{?dataset a orkgc:Dataset; \n","input (English text): Which model has achieved the highest BLEU score score on WMT2016 English-Russian benchmark dataset?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU\") { SELECT?model\n","358 \n"," input (English text): What is the name of the top performing model in terms of Accuracy (%) score when benchmarked on the Oxford-IIIT Pets dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy (%)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford 102 Flowers dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Error Rate\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford 102 Flowers\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the Oxford-IIIT Pets benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset?\n"," output (Sparql query): \n","856\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Top-1 Error Rate\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Oxford-IIIT Pets\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }  A more in-depth comparison of the performance of the three models\n","output (Sparql query): SELECT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Top-1 Error Rate\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Oxford-IIIT Pets\")?benchmark orkgp:HAS_DATASET?dataset; orkg\n","359 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Reuters En-De dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters En-De\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the 20NEWS dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"20NEWS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NYT29 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters-21578 dataset?\n"," output (Sparql query): \n","507\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------------------- ------------------------- ------------------------- ------------------------- -------------- ----- ----------- ----------- ----------- ----------- --------------- ----------- ----------- --------------- -------------------------\n","Inputs:\n","(1) The dataset. For each title, the output should be a string of 10-20 characters in length. For an ID, the output should be an ID that contains information about that paper. When supplying IDs, the ID number should be in range (1..10). (2) The identifiers for benchmarking papers (see below). (3) The source dataset. This includes two parts, with the first part being the dataset itself (text) and the second part being the output of benchmarking papers (Sparse JSON). See Section 1.2.2 for more details about the source dataset. This output will be formatted using the new format. For example, to create the NYT29 dataset, one would put the following in the SQL statements:\n","RESTORE [dataset_id]...\n","RESTORE [ID : id ]...\n","RESTORE [p : title : author : title_color : author_id : year : Year ]...\n","RESTORE [content : content_id : content_id_name : content_format : author_id : author_id_color : description_color : description_id : publication : publication_id ]...\n","Inputs:\n","(1) the dataset. For each title, the output should be a string of 10-20 characters in length. For an id, the output should be an ID that contains information about that paper. (2) The identifiers for benchmarking papers (see below). (3) The source dataset. This includes two parts, with the first part being the dataset itself (text) and the second part being the output of benchmarking\n","360 \n"," input (English text): What models are being evaluated on the Penn Treebank (Word Level) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Word Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Penn Treebank dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Penn Treebank (Character Level) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What models are being evaluated on the Penn Treebank (Character Level) dataset?\n"," output (Sparql query): \n","553\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" query : What metrics do you prefer, character score vs. word score?\n","output : OPTIONAL character_score orword orval:HAS_METRIC orval:HAS_DICTIONARY?\n","input (English text): What evaluation metrics are commonly used when benchmarking models on the Penn Treebank (Character Level) dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }  query : What metrics do you prefer, character score vs. word score?\n","output : OPTIONAL character_score orkey:HAS_METRIC orkey:HAS_DICTIONARY? \n","input (English text): What evaluation metrics are commonly used when benchmarking models on the Penn Treebank (Character Level) dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS\n","361 \n"," input (English text): Provide a list of papers that have utilized the Feedback Transformer (4 layers) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Feedback Transformer (4 layers)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the PtGen model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PtGen\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the LASER model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LASER\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the ANODE model and include the links to their code?\n"," output (Sparql query): \n","487\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","This document uses the following data: #p.12 - http://en.wikipedia.org/wiki/Anode_model - Wikipedia article #c.5 - https://github.com/c5/c5 -- #p.12 - http://en.wikipedia.org/wiki/Anode_model - Wikipedia article #c.5 - https://github.com/c5/c5 -- #p.13 - http://en.wikipedia.org/wiki/Inheritance_(genetics) - Wikipedia article #c.1 - https://github.com/mcdunn/c1-genetics --  -- #p.14 - http://en.wikipedia.org/wiki/Inheritance_(genetics) - Wikipedia article #p.14 - http://en.wikipedia.org/wiki/Inheritance_(genetics) - Wikipedia article -- #p.15 - http://en.wikipedia.org/wiki/K-12 - Wikipedia article #c.5 -  @fiber-p.com - Blog post at fiber-p -- #p.16 - http://en.wikipedia.org/wiki/K-12 - Wikipedia article\n","362 \n"," input (English text): Provide a list of papers that have utilized the DY-MobileNetV2 ×1.0 model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DY-MobileNetV2 ×1.0\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the DY-MobileNetV2 ×0.75 model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DY-MobileNetV2 ×0.75\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the DY-MobileNetV2 ×0.5 model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DY-MobileNetV2 ×0.5\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code?\n"," output (Sparql query): \n","530\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Provide links to code used in papers that benchmark the DY-MobileGNetV 3-Small system and include the links to their code?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DY-MobileGNetV 3-Small\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the DY-MobileGNetV-3 system and include the links to their code?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DY-MobileGNETV-3\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the DY-MobileGNetV-3-Small system and include the links to their code?\n","output (Sparql query): \n","input (English text): Provide links to code used in papers that benchmark the\n","363 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the FB15k dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FB15k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset FTD dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FTD dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset ARC-PDN?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC-PDN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value?\n"," output (Sparql query): \n","857\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------------------\n","select -------------- ------------------------ rank 0.0 [0.0] 100.0 [100.0] percentile_result: [0 1 2 3 4 5 6 7]\n","select rank -------------- --------------- rank 0.0 [0.0] 100.0 [100.0] percentile_result: [0 1 2 3] order-by: -------------- -------------- rank --------------- -------------- rank 0.0 [0.0] 100.0 [100.0] percentile_result: [0 1 2 3] order: rank\n","output (Sparql query): SELECT rank -------------- --------------- rank 0.0 [0.0] 100.0 [100.0] percentile_result: [0 1 2 3] score 0.8 [5.5] [5.5] percentile_result: [0 1 2] rank: rank.order: percentile_result select rank -------------- --------------- rank 0.0 [0.0] 100.0 [100.0] percentile_result: [0 1 2 3] order: rank rank: rank.order: percentile_result.order by rank | percentile_result | score | order\n","input (English text): What is the top benchmark result achieved on the DATASECONN?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DATASECONN\")?benchmark orkgp:HAS_DATASET?dataset; or\n","364 \n"," input (English text): Where can I find code references in papers that have used the MP-EB model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MP-EB\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the MPCM model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MPCM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the MMDL model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MMDL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the MPAD-path model for benchmarking purposes?\n"," output (Sparql query): \n","471\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Can you provide links to code used in papers that benchmark the MPAD model? If so, where may they be found?\n","output (Sparql query): \n","input (English text): Where can I find code references in papers that have used the MPCA-A model for benchmarking purposes?\n","365 \n"," input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the BoolQ dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BoolQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the BoolQ dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BoolQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the BoolQ dataset?\n"," output (Sparql query): \n","633\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","There were no responses.\n","As you can see, there are many models, and the \"best\" of these, the one with the greatest accuracy is not one. This is also evident by a comparison of the accuracy scores of these models, comparing the accuracy scores of the best ones (as compared to the non-model variants) against each other:\n","Model A model B model C model D model E model F model G model H model I model J model K model L model M model N model O model P model R model S model T model U model V model W model X model Y ------------ Accuracy Score ----------------------------- 5.1 50.2 59.4 60.2 54.6 49.2 43.5 40.0 37.8 41.4 37.8 36.8 36.3 32.1 29.0 29.1 27.6 23.7 21.2 23.7 19.9 20.0 19.2 19.2 18.7 18.7 18.2 17.9 17.5 17.3 17.2 16.6 15.4 15.6 14.4 14.1 13.8 13.9 13.5 13.2 12.8 12.7 12.5 12.3 11.8 11.4 11.1 10.8 10.5 10.4 10.2 10.0 9.9 9.9 9.7 9.5 9.1 8.8 8.8 8.7 8.6 8.5 8.4 8.3 8.1 7.7 7.6 7.4 7.3 6.9 6.8 6.7 6.5 6.4 6.3 6.2 6.1 6.0 6.0 5.8 5.8 5.7 5.6 5.5 5.5 5.\n","366 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC Russian-to-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the highest benchmark result achieved on the BUCC Chinese-to-English dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Chinese-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC German-to-English dataset?\n"," output (Sparql query): \n","786\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) AS?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value)\n","I can easily find out what other benchmark results were achieved by setting the \"benchmark\" option to the \"hsl\" query query in this question:\n","You said there were other best results from different test data sets, so the result values are from each of these test sets?\n","The best results I can find are (and the corresponding scores/mean scores are from in that order ):\n","The results for the German (and Spanish) tests are very similar with a different trend: the best results are obtained for tests where I chose a \"benchmark\" value (1.0 with 20 out of 100) and then I selected an \"experiment\" value with 10 out of 100.\n","The results for the German and Spanish test sets I picked a value of 1.0 with 20 out of 100, and I then selected an \"experiment\" value with 10 out of 100. This means that out of 100, 2 out of 10 came from comparing a \"benchmark\" (1.0 with 20 out of 100) with an \"experiment\" value (10 out of 100\n","367 \n"," input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl500k) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What models are being evaluated on the Ball in cup, catch (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Bowling dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bowling\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset?\n"," output (Sparql query): \n","762\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text) text (instructions): Input text - For the purposes of scoring, any text (including comments) that does not contain any of the following:\n","score (integer) scores the input sentence (in the form of a number) for use in determining the score for a specific model\n","output (binary) binary \"result\" string (with score as an integer)\n","output (Sparql query):\n","SELECT DISTINCT?score AS score, score, STATEMENT AS status FROM sys.dm_sys_db_models WHERE id IN (1, 2, 5, 16, 18, 30, 32); SELECT DISTINCT?score AS score, score, STATEMENT AS status FROM sys.dm_sys_db_models WHERE id IN (1, 2, 5, 16, 18, 30, 32); ORDER BY DESC(status_1, status_2);\n","output (Sparql query): \n","The output displayed shows that the model score is 100 - this is just the result string for the STATEMENT clause and the output from the query. It seems that our score for the current test case is the same as that used when evaluating the model when running the benchmark_sparql on the Atari 2600 Bowling data. If we run the query again with different parameters then the result string for STATEMENT differs as follows:\n","The output displayed shows that the score is 0.02 - we were expecting the score to be significantly higher although the score is still very low. The test case we performed can be used to demonstrate which of the following model models perform the best in terms of scoring on the Atari 2600 Bowling data. We can also run the benchmark again with different parameters to make the results more representative of the real situation (which the benchmark code should have a way of producing) The resulting script is also included in the\n","368 \n"," input (English text): List the code links in papers that use the MEMEN (ensemble) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the ZFNet (ensemble, 6 convnets) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ZFNet (ensemble, 6 convnets)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the FusionNet (single model) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FusionNet (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?\n"," output (Sparql query): \n","502\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Multi-Perspective Matching (ensemble)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): List the code links in papers that use (HISTO)SEMEN (ensemble, 6 convnets) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"HISTO (HISTO)SEMEN (ensemble)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): List the code links in papers that use the ROC (Regression of the OLS) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"ROC (Regression of the OLS)\")?benchmark or\n","369 \n"," input (English text): Could you provide a list of models that have been tested on the ImageNet 64x64 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the top benchmark score and its metric on the ImageNet 64x64 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric?\n"," output (Sparql query): \n","764\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","We can see that this query is actually only slightly different from the previous two queries. This indicates that the metric you specified has actually performed well (not very well, but not badly either).\n","Let's try that example again. We can now try the full text on the Dataset and the model from the benchmarking section. We will use the dataset we just prepared by hand (we won't be using the default dataset at this point), and we also want to run the model against the default model in the Benchmark category instead.\n","We will also add a new column to the results table named?model and will also add a new query with the following options:\n","select rdfs:categorize?model FROM?metric&?model\n","Let's create the new query, put some keywords in front of it and see how it behaves:\n","We can clearly see that the benchmarking step does not work as expected; the model in the benchmark category is not listed in the results at all. However, we can still use the above query instead. We know for instance the Dataset as well as the model we have in the benchmarking column of the results table. Furthermore, as we can see in this part, the resulting dataset is different from the Dataset we passed and thus the benchmarking function is not able to find it.\n","In this case, the best practice is to simply use the default dataset instead of attempting to benchmark it.\n","What kind of query is appropriate here? What is the right way to select the best performing model? Perhaps we can simply create a table called \"best fit\" using the following syntax:\n","CREATE TABLE best_fit (dataset_type n)\n","Let's try this:\n","To make things more realistic, let's create the model from that benchmarking benchmark so you can see how close the model\n","370 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the OpenBookQA dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OpenBookQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?\n"," output (Sparql query): \n","860\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- {?value=?value/?Metric&&?metric?value&&?metric?value? } --------------------------- {?value=?value/?Metric&&?metric?value&&?metric?value? } --------------------------- {?value=?value/?Metric&&?metric?value&&?metric?value? } --------------------------- {?value=?value/?Metric&&?metric?value&&?metric?value? } --------------------------- {?value=?value/?Metric&&?metric?value&&?metric?value? } --------------------------- {?value=?value/?Metric&&?metric?value&&?metric?value? } --------------------------- Note that there are some data entries that are duplicates. --------------------------- The result column contains the metric name. --------------------------- The top benchmark result achieved (metric and value) is the top benchmark result (metric and value). This column also holds the top metric over the benchmark. We also got some duplicate values in this result table. --------------------------- The same metric for a different dataset will have different values. --------------------------- If you do not specify this column, the output will be as above. --------------------------- There should also be a column named metric_lbl for performance metrics. --------------------------- The metric_lbl column will contain three columns. We can choose to get one of the three columns using the default for the metric_lbl column. If no metric_lbl is specified, the data will be duplicated across all three columns (except for the top-tier metric). --------------------------- The default row for it_means is an empty list. --------------------------- Using this table you can see how the metrics are arranged. --------------------------- An empty\n","371 \n"," input (English text): What is the total number of patients in the studies?\n"," output (Sparql query): SELECT SUM( ?number_of_patients) WHERE { orkgr:R33008 orkgp:compareContribution ?contrib. ?contrib orkgp:P15585 ?patients. BIND(xsd:integer( ?patients) AS ?number_of_patients) } \n"," input (English text): In what country was conducted research with the largest number of participants?\n"," output (Sparql query): SELECT ?country WHERE { orkgr:R44980 orkgp:compareContribution ?contrib. ?contrib orkgp:P15249 ?country; orkgp:P23169 ?number_of_participants. } ORDER BY DESC( ?number_of_participants) LIMIT 1\n"," input (English text): Do all studies include open ocean sampling?\n"," output (Sparql query): ASK { { SELECT (COUNT( ?ocean_sampling) AS ?counter) WHERE { orkgr:R155561 orkgp:compareContribution ?contrib. ?contrib orkgp:P41379 ?ocean_sampling. FILTER( ?ocean_sampling = \"F\"^^xsd:string) } } FILTER( ?counter = 0) } \n"," input (English text): Are children examined in the studies?\n"," output (Sparql query): \n","324\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Is there enough data to estimate number of patients?\n","output (Sparql query): SELECT {?number_of_patients.?number_of_patients } ORDER BY 1 LIMIT?number_of_patients,10 }\n","When an ORK query is processed, it will generate an ORKGR response. The following result contains the ORKGR response.\n","Input:\n","output (Sparql query): \n","input (English text): What is the total number of patients in the studies?\n","output (Sparql query): SELECT TOTAL(?number_of_patients ) FROM ( SELECT SUM(?number_of_patients ) AS NUMBER_OF_PAID_PEOPLE FROM SELECT p.?p?k as?kid,?i. id AS id, p.?p?id?k (?p.?k?i ))) )\n","input (English text): In what country was conducted the studies?\n","output (Sparql query): SELECT SUM(?number_of_patients ) FROM ( SELECT S.?number_of_patients ORDER BY 1) FROM ( SELECT S.?number_of_patients FROM SELECT ( COUNT(?number_of_patients) AS NUMBER_OF_PAID_PEOPLE ) AS d,?c.?c_id AS c_id, p (?c.?c_id ) AS p(?c.?c_id ) ) AS p ) AS p ) AS p ) AS p FROM (SELECT S.?number_of_patients AS Number FROM ( SELECT p.?p?i as?kid FROM SELECT p. WHERE (?i = p.?k?i ) AND?p.?c_id =?c\n","372 \n"," input (English text): What is the highest benchmark result achieved on the OpenBookQA dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OpenBookQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the CORLL dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CORLL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset PIQA?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PIQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the CoQA dataset, including the metric and its value?\n"," output (Sparql query): \n","856\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------------------- Metric Score 1.92 * 2.01 4.13 6.46 7.01 2.06 2.01 3.00 4.01 4.21 5.36 6.50 4.08 2.03 2.13 2.01 3.00 9 7.06 6.09 7.01 8.03 5.13 6.16 6.28 6.30 6.38 7.08 7.19 6.24 7.04 7.05 6.08 5.05 5.19 9 4.98 6.07 7.04 8.01 9.15 5.34 6.16 6.37 6.56 6.78 6.86 8.08 8.52 9.13 9.21 9.09 9.17 7.39 6.25 7.15 11 1.98 5.30 6.08 8.24 6.79 8.22 5.27 6.42 6.79 6.92 8.39 8.92 7.86 9.06 9.52 9.36 8.53 9.35 9.34 11 11.46 10.00 11.10 11.34 9.77 10.42 9.91 10.82 11.00 12.07 10.00 11.23 12.22 11.34 12.37 12.27 12.18 12.20 12.44 11.98\n","What is the top benchmark result for the CoQA dataset, including its metric and its value?\n"," output (Sparql query): ------------------------- Metric Score 1.91 1.84 1.79 1.81 1.93 1.82 1.76 1.59 1.51 1.55 1.60 1.66 1.83 1.79 3.10 5.20 3.87 7.08 5.37 3.54\n","373 \n"," input (English text): What is the top benchmark score and its metric on the ImageNet 64x64 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet ReaL dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the ImageNet V2 dataset?\n"," output (Sparql query): \n","855\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["-------------- ---------------------- (?dataset an orkgc:V2?) (?dataset a orkgc:V2?score) ----------+----------------------+-----------+----------- -------------- --------- ImageNet V3: 2,918,842 (29.9%) ImageNet V3 ReaL: 1,880,726 (17.8%) ReaL ReaL: 1,780,634 (17.5%) reaLV3: 1,580,955 (17.7%)\n"," input (English text): What is the top benchmark score and its metric for the ImageNet V5 dataset?\n"," output (Sparql query): -------------- ---------------------- (?dataset an orkgc:V5?) (?dataset a orkgc:V5?score) ----------+----------------------+-----------+----------- -------------- --------- ImageNet V2: 2,908,722 (29.9%) ImageNet V2 ReaL: 1,924,737 (17.8%) ReaL ReaL: 1,923,977 (17.4%) reaLV3: 1,575,026 (17.6%)\n","In addition to a metric, you can also use a label for the scores. That label appears next to the name for the scores. For example, \"ImageNet V4 ReaL, ImageNet V5 ReaL\" can be a good way to indicate the top score.\n","The following example produces the same graph as the input, but in a different format.\n","INPUT file1.xlsx file2.xlsx -------------------------- ---- ImageNet V2 ReaL v2 (29) - ImageNet V2 ReaL (29) + ImageNet V3 (30)\n","374 \n"," input (English text): What is the highest benchmark result achieved on the Reacher, easy (DMControl500k) dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Could you provide a list of models that have been tested on the Reacher, easy (DMControl500k) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Reacher, easy (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Which model has achieved the highest Score score on the Reacher, easy (DMControl500k) benchmark dataset?\n"," output (Sparql query): \n","705\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT?model_lbl?model. SOLUTION AS (model). DESC (?score?score ?model_lbl. )\n","orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval; orkgp:HAS_BENCHMARK?benchmark. ORkgp:HAS_MODEL?model. orkgp:HAS_MEMORY?model. ORkgp:HAS_BENCHMARK?benchmark. ( Note that this query should not be used with the or kgp:SOME_DATA option. The DETAIL option provides more precise information.) ORkgp:HAS_METRIC?metric; orkgp:HAS_BENCHMARK?benchmark. ORkgp:HAS_DETAIL?dataset. orkgp:HAS_NUMBER?model. ORkgp:HAS_REFERENCE?model. ORkgp:HAS_DETAIL_CONTEXT?dataset. orkgp:HAS_VALUE?model. ORkgp:HAS_METRIC?metric. ORkgp:HAS_BRIEF?model. Orkgp:HAS_VALUE?model. ORkgp:HAS_METRIC?metric rdfs:label?dataset_lbl. OPTIONS?metric?metric_lbl.\n","ORkgp:HAS_DETAIL?dataset. ORkgp:HAS_NUMBER?model. ORkgp:HAS_REFERENCE?model. ORkgp:HAS_DETAIL_CONTEXT?dataset. ORkgp:\n","375 \n"," input (English text): What are the models that have been benchmarked on the Quasart-T dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Quasart-T\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the NYT29 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the WikiText-2 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Quasart-T benchmark dataset?\n"," output (Sparql query): \n","537\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["-------------------------\n","input (English text): List the models that are used for the NYSE benchmark dataset?  output (Sparql query): SELECT DISTINCT?model?model_lbl AS {?dataset rdfs:label?model_lbl.?dataset sql:dataset?model? } OPTIONAL {  ?model rdfs:label?model_lbl?data_lbl. }  and?dataset rdfs:label?model_lbl?data_lc. OPTIONAL {  ?model rdfs:label?model? } OPTIONAL {  ?model rdfs:label?model_lbl?data_lbl. OPTIONAL {  ?model rdfs:label?model_lbl?data_lc. OPTIONAL {  ?model rdfs:label?model_lbl?data_lc. OPTIONAL {  ?model rdfs:label?model_lbl?data_lc. } } OPTIONAL {  ?model rdfs:label?model_lbl?data_lc. } } OPTIONAL{  ?model rdfs:label?model_lbl?data_lc. }  and?dataset rdfs:label?model_lbl AS?dataset?data_lbl.  and?dataset rdfs:label?model_lbl?data_lc.  and?dataset rdfs:label?model_lbl AS?dataset?data_lc.  and?dataset rdfs:label?model_l Blank:\n","input (English text): List the\n","376 \n"," input (English text): Can you list the models that have been evaluated on the CINIC-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CINIC-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the best performing model benchmarking the Multimodal PISA dataset in terms of Accuracy (%) metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy (%)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Multimodal PISA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Can you list the models that have been evaluated on the NYT24 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT24\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Multimodal PISA dataset?\n"," output (Sparql query): \n","653\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Multimodal PISA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","input (English text): Which multivariate model have been evaluated on the Multimodal PISA dataset?\n","output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"'\",?factor orkgc:Factor? ){?label { orkgc:Label? } OR {?factor rdfs:model_lbl. }} OR {?factor rdfs:model_lbl. } WHERE {?model?model_lbl OR \\!model rdfs:label.}} ORDER BY DESC(?factor) LIMIT 1 } \n","input (English text): Which multivariate model have been evaluated on the Multimodal PISA dataset?\n","output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?data orkgc:Data OR [DATA rdfs:model_lbl. \"Data\"]} OR\n","377 \n"," input (English text): What models are being evaluated on the Barabasi-Albert dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Barabasi-Albert\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the ScienceIE dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the Barabasi-Albert dataset?\n"," output (Sparql query): \n","557\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" Output\n","Thanks!\n","378 \n"," input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Breakout dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Breakout\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Phoenix dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Phoenix\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Yars Revenge dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Yars Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Zaxxon dataset?\n"," output (Sparql query): \n","525\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["   1. How many unique models were used in the analyses? \n","RANK model_index by :metric _________________________________________________________________________ 0 1,056 2,095 3,856 4,038 1 1,069 2,095 3,856 4,038 2 1,088 2,102 3,853 4,081 3 1,114 2,105 3,857 4,082 4 1,126 2,105 3,865 4,089 5 1,136 2,109 3,888 4,092 6 1,140 2,113 3,971 4,092 7 1,153 2,114 3,992 4,102 8 1,158 2,115 3,989 4,104 9 1,162 2,115 3,987 4,104 10 1,168 2,115 3,979 4,105 11 1,171 2,114 3,986 4,105 12 1,174 2,114 3,964 4,105 13 1,176 2,114 3,966 4,105 48 1,182 2,113 4,001 4,115 49 1,181 2,113 4,002 4,115 50 1,186 2,113 4,002 4,115 51 1,188 2,114 4,001 4,115 52 1,190 2,114 4,001 4,115 53 1,192 2,114 4,001 4,115 54 1,196 2,114 4,001 4,115 55 1,198 2,115 4,001 4,115 56 1,204 2,115 4,001 4,115 57 1,208 2,117 4,001 4,115 58 1,216 2,117 4,002 4,115 59 1,217 2,117 4,002\n","379 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the MLDoc Zero-Shot English-to-Italian dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the MLDoc Zero-Shot English-to-German dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish?\n"," output (Sparql query): \n","898\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" select DISTINCT?metric?metric_lbl (MAX(?value) AS?score) FROM metrics? value as metric and? score as value GROUP BY?metric?metric_lbl ORDER BY?metric?metric_lbl\n","380 \n"," input (English text): Indicate the model that performed best in terms of BLEU metric on the WMT2016 English-German benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2014 German-English benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-Czech benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Czech\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset?\n"," output (Sparql query): \n","854\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_l Bl) = \"BLEU score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_ l Bl) = \"WMT2016 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE? value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_l Bl. } ORDER BY DESC(?value) LIMIT 1 } }\n","I'll be using some Python to process the JSON result and then returning it as a HTML file. The actual workflow will be like this:\n","1.  create the models table, put in some data,  and save it to a JSON document. 2.  extract the data and convert it to a HTML table. 3.  write all the HTML to a file and save it. 4.  start processing the html by making some javascript calls against the model. 5.  if all goes well we get HTML in our final output file.\n","Before we dive into the code, let's summarize the steps. In the last step, the model is split into its model variables  and passed to the javascript function  that generates the JavaScript code to call\n","381 \n"," input (English text): Provide a list of benchmarked datasets related to the Information Extraction research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Information Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Provide a list of benchmarked datasets related to the Word Embeddings research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Word Embeddings\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Name the datasets that have been used for benchmarking in the Text Classification research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Text Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Provide a list of benchmarked datasets related to the Sentence Classification research area?\n"," output (Sparql query): \n","510\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Provide a list of benchmarked datasets related to the Classification, Text Embeddings, Word Embeddings, and Sentence Embeddings research areas?\n","output (Sparql query): \n","The inputs do not have to all be English text. However as many as you can provide will work.\n","select rdfs:from_problem, rdfs:from_benchmark, rdfs:to_problem, rdfs:from_benchmark_lbl order by orkgp:P32 desc, problem:id desc, orkgc:id desc;\n","In order to find any more examples to go with the ones described here (it is very quick), you can provide additional rows to rdfs:from_problem in your query file (or just run: RDP -> Query -> RDD -> Run Query).\n","What it will output are datasets that are included in the benchmarking. If no benchmarks were included the output will contain data from the last benchmark run. The output will display the datasets if you select the rows:\n","select input(Output_data_format, Output_text_format, Output_dataset, Output_label, Output_label_cont, Output_benchmark, Output_cont_magnitude, Output_bpp, Output_freq, Output_freq_magnitude, Output_freq_output, Output_freq_scale, Output_log_log, Output_l1, Output_l2, Output_l3, Output_l4, Output_l5, Output_l6, Output_l7, Output_l8, Output_l9, Output_l10, Output_l11, Output_l12, Output_l13, Output_l14, Output_l15,\n","382 \n"," input (English text): List the metrics that are used to evaluate models on the CUB-200-2011 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Text8 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Text8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset?\n"," output (Sparql query): \n","523\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset?\n","output (Sparql query): \n","input (English text): List the metrics that are used to evaluate models on Google (Content Delivery Network) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Google\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n","input (English text): List the metrics that are used to evaluate models on Google (Content Delivery Network) benchmark dataset?\n","output (Sparql query): \n","input (English text): List the metrics that are used to evaluate models on Facebook (Facebook Publishing) benchmark dataset?\n","output (Sparql query): \n","input (English text): List the metrics that are used to evaluate models on Google (Content Delivery Network) benchmark dataset?\n","output (Sparql query): \n","input (English text): List the metrics that are used to evaluate models on Amazon (Amazon Mechanical Turk) benchmark dataset?\n","output (Sparql query): \n","input (English text): List the metrics that are used to evaluate models on Facebook (Facebook Publishing) benchmark dataset?\n","output (Sparql query): \n","input (English text): List the metrics that are used to evaluate models on Facebook (Facebook Publishing) benchmark dataset?\n","\n","383 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-German dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Chinese dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Chinese\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What is the best performing model benchmarking the MLDoc Zero-Shot English-to-Italian dataset in terms of Accuracy metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?\n"," output (Sparql query): \n","665\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?dataset_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","5.4.3.1.1. Sums and Timeseries\n","The last, and perhaps most important question to ask yourself is when you want to use a Sum or Timeseries model. In this section we go over that information as it applies to these two different models, sum, and timeseries. Summing involves some extra work to produce our final model. (By the way, there are two things to consider in order to make your model summing-capable: first, the model will need to have some sort of aggregation function (which might be more flexible on some of our models than others, but that's beyond the topic of this manual article) and, second, it will have to evaluate a sum of\n","384 \n"," input (English text): Can you provide links to code used in papers that benchmark the TokenFuser model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"TokenFuser\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the DQN-CTS model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN-CTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the BiLSTM-CRF+ELMo model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM-CRF+ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Tokenlearner model and include the links to their code?\n"," output (Sparql query): \n","496\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["  input (English text): Provide a list of papers that have utilized the SVM-CRF model and include the links to their code?\n","385 \n"," input (English text): What models are being evaluated on the Pubmed dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Pubmed\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the MedSTS dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the MedNLI benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the MedSTS dataset?\n"," output (Sparql query): \n","534\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MedSTS\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): Can I provide a list of models that have been tested on both the MedSTS dataset and the Pubmed dataset?\n","output (Sparql query): \n","SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Pubmed\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): Could you provide a list of models that have been tested on the MedSTS dataset and the Pubmed dataset?\n","output (Sparql query): \n","SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dat\n","386 \n"," input (English text): What is the best performing model benchmarking the CoNLL04 dataset in terms of NER Micro F1 metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"NER Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL04\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Could you provide a list of models that have been tested on the CoNLL++ benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL++\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the CoNLL04 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL04\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric?\n"," output (Sparql query): \n","659\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Can you provide a list of models that have been benchmarked on the CoNLL++ benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL++) OR (?data2 OR (?data1 OR data2) OR (?model OR model_lbl) OR (?tb OR tb_lbl) OR (?label OR model_lbl) OR (?value OR data2 OR data1 OR value OR value OR value OR price OR price2 OR price2 OR price1 OR model_lbl) OR (?weight OR weight2 OR weight1 OR weight1 OR weight1 OR model_lbl) OR (?weight OR weight1 OR model_lbl)) } OPTIONAL {?model name orkgc:Model; rdfs:label?dataset_lbl. } }\n","input (English text): What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE\n","output (Sparql query):  Inputs (English text): what are two factors that are influencing your model performance? If possible, provide links to the models/competitors on CoNLL test website where tests were performed and provided the following data: date the model came online and in which period do the tests were performed\n","where the benchmark was set and the performance of the benchmarked models\n","where the performance of the benchmarked models was recorded and the validation set used\n","\n","387 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the UCF101 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the UCF101 (finetuned) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101 (finetuned)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset?\n"," output (Sparql query): \n","572\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Are there any models with very high scores?\n","output (Sparql query): \n","input (English text): Do you have any model that has been benchmarked with a score of 1.000000 on the seel.cse.lsu.edu/data/refsq17.zip dataset?\n","output (Sparql query): \n","input (English text): How many models have run the benchmark? Output (Sparql query):   input (English text): What are the best scores of your models?\n","output (Sparql query): \n","388 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the SciCite dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Softcite dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Softcite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the ScienceCite dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceCite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?\n"," output (Sparql query): \n","506\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","389 \n"," input (English text): Can you provide links to code used in papers that benchmark the CvT-W24 (384 res, ImageNet-22k pretrain) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CvT-W24 (384 res, ImageNet-22k pretrain)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the DeiT-S model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DeiT-S\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the CvT-21 (384 res, ImageNet-22k pretrain) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CvT-21 (384 res, ImageNet-22k pretrain)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the ImageNet + iNat on WS-DAN model?\n"," output (Sparql query): \n","533\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" input (English text): Are the models, including CvT-1, CvT-2, iNat or DeiT-S?\n"," output (Sparql query/label):\n","Fitting on the input data, the trained model was a cross-validation that used a combination of three parameters:  1) the output layer (L 1 ), the training-evaluated model of the input model (L 2 ), the network used for the pretraining ( L 3, to minimize the netweight of the model-trained L 2 ), and the parameter-variants of the pretrained L 3. 2) the pretraining parameters applied to the network-trained model to minimize the netweight of the model-trained L 1 (C 1 ). 3) the network-trained network-evaluated model of the pretrained model (C 1, L 3 ). The pretraining parameters were the weights used by the pretrained network and the network-trained weights. The pretraining network-weights were also used to minimize the netweight of the pretrained network-trained model. This was done for both C1 and C2 models. \n","The outputs were then fitted to the model, along with other output layers (L 1, L 2, and L 3 ) and a CvT-1, CvT-2 model. The trained model was a cross-validation that used a combination of three parameters:  1) the output layer (L 1 ), the training-evaluated model of the input model (L 2 ), the network used for the pretraining ( L 3, to minimize the netweight of the model-trained L 1 ), and the parameter-variants of the pretrained L 3. 2) the pretraining parameters applied to the network-trained model to minimize the netweight of the pretrained network-trained model. \n","390 \n"," input (English text): Can you list the metrics used to evaluate models on the BUCC German-to-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the MLDoc Zero-Shot English-to-Russian benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the WMT2016 Romanian-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Romanian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?\n"," output (Sparql query): \n","547\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC Russian-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the MLDoc English-to-French dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc English-to-French\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?\n"," output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (\n","391 \n"," input (English text): Which model has achieved the highest ROUGE-2 score on the AESLC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of ROUGE-L metric on the AESLC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-L\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the GigaWord dataset in terms of ROUGE-1 metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GigaWord\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the AESLC dataset in terms of ROUGE-1 metric?\n"," output (Sparql query): \n","834\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"ROUGE-1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AESLC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","This works like this: we pick up an RSS feed and get a single RSS tag for each dataset: \"AESC\":\n"," If you get more than one tag, this is a pretty good bet this is some dataset you should benchmark against, if you don't yet have a database setup you are probably going to want to look at the ROUGE-1 metrics first.\n","When you go and check that out you can see the exact value for each metric - so if all three labels match then the data is good to benchmark against. There are of course cases where a single RSS tag isn't applicable, in that case you should check your model and maybe look at a few more datasets to see if the individual metric is applicable for you, however I'm not\n","392 \n"," input (English text): What is the name of the top performing model in terms of Mean Accuracy score when benchmarked on the ModelNet40 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Mean Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ModelNet40\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark score and its metric on the ImageNet 64x64 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the ObjectNet (Bounding Box) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet (Bounding Box)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the ModelNet40 dataset?\n"," output (Sparql query): \n","848\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT rf_distribution from the orkgp:HAS_DATASET?dataset WHERE rf_distribution LIKE?{ rdfs:label?dataset_lbl. } ORDER BY rf_distribution DESC; (max scores = 1650)\n","input (English text): What is the best benchmark score and its metric on the ObjectT Dataset?\n"," output (Sparql query): SELECT BestScore FROM orkgp:HAS_DATASET?dataset WHERE BestScore LIKE?{ rdfs:label?dataset_lbl. } ORDER BY BestScore DESC; (max scores = 1360)\n","output (Sql query using R, MySQL, and SQL) (max scores = 885)\n","input (English text): What is the best benchmark score and its metric on the TensorFlow 64x64 dataset?\n"," output (Sparql query using R, MySQL, and SQL) (max scores = 850)\n","input (English text): What is the best single benchmark score and its metric on the TensorFlow 80x80 dataset?\n","output (Sparql query using R, MySQL, and SQL) (max scores = 700)\n","input (English text): List all metrics for each classification category.\n","output (Sql query using R, MySQL, and SQL) (max scores = 690)\n","input (English text): What is the number of test data points (test data in the text) needed to score a given score?\n","output (Sql query using R, MySQL, and SQL) (max scores = 350)\n","input (English text): Which of the following datasets do you think is the best for test data?\n","output (Sql query using R, MySQL, and SQL) (max scores = 330\n","393 \n"," input (English text): List the metrics that are used to evaluate models on the Penn Treebank (Word Level) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Word Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the COPA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"COPA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Penn Treebank benchmark dataset?\n"," output (Sparql query): \n","520\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT?k v1,?k v2,?k v3,?k v4,?k v5,?k v6,?k v7,?k v8,?k v9, rdfs:id?id, (?k v1 v2 v3 v4 v5 v6 v7 v8 v9) rdfs:loc v1.1 v2.1 v3 v4 v5 v6 v7 v8 v9 OR kgp:HAS_DATASET?dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Word Level\")?dataset or kgp:HAS_DATASET?dataset; or kgp:HAS_METRIC?metric, {?loc a orkgc:Dataset; rdfs:label?loc_lbl. IF (?loc =?dataset_lbl, rdfs:loc v1.1 v2.1 v3 v4 v5 v6 v7 v8 v9) OR kgp:HAS_EVALUATION?eval OR kgp:HAS_METRIC?metric. OPTIONAL rdfs:loc id,?loc v2.1 v3 v4 v5 v6 v7 v8 v9 OR kgp:HAS_DESC v1.1 OR kgp:HAS_DEFINITION v1.1 OR kgp:HAS_DESCRIPTION v1.1 }?loc OR kgp:DATASET OR kgp:HAS_DATASET OR kgp:HAS_METRIC or kgp:DECOD\n","394 \n"," input (English text): Can you list the models that have been evaluated on the Reuters En-De dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters En-De\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the best performing model benchmarking the Reuters-21578 dataset in terms of Accuracy metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Can you list the models that have been evaluated on the Reuters De-En dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters De-En\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the Reuters-21578 benchmark dataset?\n"," output (Sparql query): \n","649\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input (English text): Please give a brief summary of your methodology for data quality validation evaluation of news data.\n","output (Sparql query): \n","input (English text): Please give a brief description of your method for testing the reliability of data.\n","output (Sparql query): \n","input (English text): What are the basic objectives you have for evaluating data quality?\n","output (Sparql query): \n","input (English text): What are the main criteria that are important for evaluating the reliability and objectification of news articles and documents?\n","output (Sparql query):  \n","input (English text): What specific characteristics do you use when evaluating the reliability of news data?\n","output (Sparql query): \n","395 \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Atlantis dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Breakout\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Time Pilot dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Time Pilot\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric?\n"," output (Sparql query): \n","819\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Seaquest\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","This is the result of benchmarking a simple \"laser dot\" test run from the model I'm benchmarking.  Output is similar to the last set of output, but I didn't include the score metric as such. \n","There is no word count in R.  The R script I used to benchmark this dataset contained a large number of words.  This allowed me to look into the labels and make some meaningful findings. My goal was to find the model with the least number of words that was slower or slower than every other model.  As of this writing, there are three different scoring metrics being tested.  I will be tracking the output of these scoring metrics going forward.  One thing I will stress is that many of the scoring metrics\n","396 \n"," input (English text): Can you list the metrics used to evaluate models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the RotoWire dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the CIFAR-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip  dataset?\n"," output (Sparql query): \n","554\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): What is the metric called CIFAR_10.? How are the metrics of evaluation over the CIFAR-10 dataset evaluated?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset o orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Seelin\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): What are the metrics of evaluation over the the seel.cse.lsu.edu/data/re17.zip dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset o orkgc:\n","397 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the ImageNet V2 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ARC-PDN dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC-PDN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset?\n"," output (Sparql query): \n","507\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","We have a list of research papers of \"ImageNet Benchmark\" type containing titles and ID, where ID should be a numerical row index. In our case, we have a record, where ID of that row is '1' and titles should be the title, author, and title length that are given to them. Then we have output of our query statement with some output type. We want to execute the query with a particular output type. If the query needs a specific output type, we cannot specify that but when the output type can be deduced from the query, we add output type explicitly.\n","input (English text): List of research papers that have performed benchmarks on the ImageNet V2 dataset?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }  input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet V2 dataset? output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet\")?benchmark orkgp:HAS_DATASET?dataset.?cont\n","398 \n"," input (English text): List the metrics that are used to evaluate models on the Atari 2600 Seaquest benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Atari 2600 Pitfall! benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Pitfall!\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Atari 2600 Robotank benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Robotank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Atari 2600 HERO benchmark dataset?\n"," output (Sparql query): \n","529\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" If you want to know the names of the metrics I used for this example, open up Rstudio and add a 'data' section to the bottom of any row you are planning to evaluate. You can then view the rows with a table or in a view.\n","input (English text): input (Data): Lists the metrics that are used to evaluate models on the Atari 2600 Pitfall! benchmark dataset? output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Pitfall!\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n","input(English text): input (Text): List the metrics that are used to evaluate models on the Atari 2600 Robotank benchmark dataset? output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Robotank\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {? eval orkgp:HAS_METRIC?metric.? metric rdfs:label?metric_lbl. } }\n","In our\n","399 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Amazon dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Amazon-5 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-5\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Amazon-2 dataset?\n"," output (Sparql query): \n","501\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","This request may cause a database error because of the new query language. Please try again later after running the test queries.\n","After giving it some thought, I realized that I actually don't have anything like a JSON file ready to go for the Amazon-2 dataset, so instead I'll make one:\n","from sklearn import model_selection, predict_features, predict_labels, predict_cov_score model_selection(id=\"m\", label=\"Amazon-1\") model_selection(id=\"m\",\"label=\"Amazon-2\") Predict_features = predict_features(model_selection) Predict_labels = predict_labels(model_selection) Predict_cov_score = predict_cov_score(model_selection) predict_labels = predict_labels(model_selection) Predict_cov_score = predict_cov_score(model_selection) predict_labels = predict_labels(model_selection) from sklearn.datasets import read_csv import sklearn import seangrass import time from sklearn.datasets import Segment from sklearn.models import LinearRegression, RandomForest class Amazon-2:  \"\"\"The Amazon dataset\"\"\"  data_frame = { 'name': ['Amazon-1', 'Amazon-2'], 'dataset':'segment', 'output': predictor_test_weights[0].intercept}  name = 'amazon_dataset' output = Segment().fit_transform(model_selection, predict_features) test_score = predict_labels(model_selection) name = predict_labels(model_selection) data_frame = data_frame.loc[0] _id = None data_frame[0] ='m'; test_feature\n","400 \n"," input (English text): List the code links in papers that use the Qbert Rainbow+SEER model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Qbert Rainbow+SEER\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Rainbow model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rainbow\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Rainbow DQN model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rainbow DQN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Rainbow+SEER model and include the links to their code?\n"," output (Sparql query): \n","479\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input and Output Files\n","You can run the QbertRainbowRainbow++ project through your command line with the following command.\n","$ qbertrainbow -pq Raster File: rainbow.qb $ qbertrainbow -pq QbertRainbow+SEER Scoring Sheet: rainbow.qb Scoring Score: 0 Scoring Rank: A Scoring Score (N): 3.1 Scoring Rank (N+1): A Scoring Score (N+2): 4.0 Scoring Rank (N+3): B Scoring Score (N+4): 3.2 Scoring Rank (N+5): B Scoring Score (N+6): 3.1 Scoring Rank (N+7): C Scoring Score (N+8): 3.3 Scoring Rank (N+9): D Scoring Score (N+10): 3.1 Scoring Rank (N+11): D Scoring Score (N+12): 3.0 Scoring Rank (N+13): D Scoring Score (N+14): 2.9\n","Results\n","The following plots are produced by running the same application through the R CMD_r package.\n","The first plot indicates how many publications from all databases that use the QbertRainbow+SEER model have been published within the past 25 years. For the full list of results, see Appendix D.\n","The second plot reveals the distribution over the total number of publications within the past 25 years that use the QbertRainbow+SEER model. For example, for the period 1965-1994, all papers had an overall score of 3.2, with no papers exceeding 4.0 in that period. For the entire period, this score has ranged from 4.0 to 4.0 (see Appendix D); for those papers that have exceeded 4.0,\n","401 \n"," input (English text): What is the name of the top performing model in terms of Accuracy (Middle) score when benchmarked on the RACE dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy (Middle)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RACE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the RACE benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RACE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the Kinetics-600 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset?\n"," output (Sparql query): \n","841\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Race\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","In a word on models\n","In some cases the model performs best in both performance of the original dataset (if it is a benchmark dataset) and in accuracy of some metric of accuracy (e.g., number of correct responses per classification). In these cases, the model performs best by using a mixture of statistical methods in combination with standard statistical models.\n","In the first category, the model is using statistical methods (like Markov chain Monte Carlo [MCMC]) to estimate a statistical model describing the data (e.g., the dataset itself). In some cases, the model is also modeling data in the form of a probability and error distribution (e.g., the log-likelihood). In two of these cases - the number of false positives (from multiple categories) and accuracy of the classification\n","402 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 Czech-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Czech-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Russian dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2014 English-German dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset?\n"," output (Sparql query): \n","523\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the European Social and Economic Survey 2014 English-Turkish dataset?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-Turkish\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2015 English-Mexican dataset?\n","output (Sparql query):\n","SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2015 English-Mexican\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }  input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2015 English-Romanian dataset?\n","output (Sparql query):\n","SELECT DISTINCT?paper?paper_lbl WHERE {?datas\n","403 \n"," input (English text): Can you list the metrics used to evaluate models on the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the LAMBADA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"LAMBADA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the ModelNet40 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ModelNet40\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the MUTAG dataset?\n"," output (Sparql query): \n","517\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MUTAG\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } ]\n","output (Sparql query):   output (Sparql query):  [?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. ]\n","input (English text): Can you list the metrics used to evaluate models on the NATA dataset?\n","output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NATA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n","input (English text): Can you list the metrics used to evaluate models on the NSKL dataset?\n","output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dat\n","404 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the AESLC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What models are being evaluated on the ESC-50 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ESC-50\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Challenge)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ESC-50 dataset?\n"," output (Sparql query): \n","530\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note: If you're using Apache Spark to connect to the server running the SASL library and require data to be saved to an SQLite database, then by default Spark will only allow data to be stored as ASCII text files or as binary data. The following command on your local machine can convert an SQLite database from a text file to an ASCII file on the server: sp_convert -n -o input_string.sqlite input_string.sql\n","The output will be stored as an ASCII file on the server. This is the default behavior.\n","If you're using the SASL library to connect to your server running the SASL library, then by default Spark will only allow data to be saved to an SASL table. To find out if Spark allows data to be saved to an SASL table, run this command on your local machine:\n","sp_connect -m sqlite3 > table1.sql1\n","The command will output the path to the table in the same table_name.sql file where the table_name.sql file exists, with a \". \" after the column name.\n","To find out if Spark allows data to be saved to an SASL table, run this command on your local machine:\n","sp_connect -m sqlite3 > a1.sql1\n","In this case, the command will output the path to a1.sql1 stored in the table_name.data file. Spark will interpret a1.sql1 as a text file under the user_data directory on your server. You can use a tool called sp_file_verify to check this manually.\n","Note: See the next item to learn more about the different ways data can be stored.\n","The next section describes the data format used for the SASLC dataset.\n","The SASLC dataset is a set of datasets used for various statistical\n","405 \n"," input (English text): Which model has achieved the highest F1 score on the SQuAD2.0 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the MedNLI benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset?\n"," output (Sparql query): \n","813\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD1.1\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?evaluate; orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the Scenario 1 benchmark dataset?\n"," output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Scenario 1\")?benchmark orkgp:HAS_DATASET\n","406 \n"," input (English text): What models are being evaluated on the TSE-NER dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TSE-NER\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the WOS-5736 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the ORKG-TDM dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ORKG-TDM\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the TempEval-3 dataset?\n"," output (Sparql query): \n","550\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SELECT?Model?Model_lbl?dataset_lbl WHERE {?data_lbl a(?model?model_lbl ) OR?data_lbl rdfs:model_lbl IF TRUE {?model_lbl?dataset_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. } } } }\n","\n","407 \n"," input (English text): What is the highest benchmark result achieved on the RotoWire (Content Ordering) dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Indicate the model that performed best in terms of BLEU metric on the RotoWire benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark score and its metric on the Rotowire (Content Selection) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the best performing model benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric?\n"," output (Sparql query): \n","865\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT?best_model?best_model_lbl?score AS?score ORDER BY DESC(?value) LIMIT 1 |?best_model?best_model_lbl?score ORDER BY DESC(?value) 1 |?best_model?best_model_lbl?score |?best_model?best_model_lbl?score |?best_model?best_model_lbl?score |?best_model?best_model_lbl?score | (2 rows)\n","Input the same text that you want to benchmark into the WHERE clause and use the column as the metric. Output: SELECT?model?metric_lbl?score AS?score ORDER BY DESC(?value) LIMIT 1 |?model?metric_lbl?score ORDER BY DESC(?value) 1 |?model?metric_lbl?score |?model?metric_lbl?score |?model?metric_lbl?score | (2 rows)\n","408 \n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Amidar dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Krull dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Krull\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Seaquest dataset?\n"," output (Sparql query): \n","846\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["-------------\n","We start by filtering out the values that do not follow RDF Schema standards. The following RDF Schema properties are filtered:\n","1. DISTINCT\n","2. LOWER = TRUE\n","\n","\n","Now that we know what to exclude, we can look up the results of the benchmark with the following SQL query:\n","\n","SELECT {?value -> '?dataset a ORkgc:Dataset?dataset_lbl' ||?value - > '?dataset?dataset-lbl'; OR - > '?dataset?dataset-lbl; '||?value - > '?dataset?dataset-lbl; '||?value - > '?dataset?dataset-lbl; '}.{?name?dataset OR - > '?record?record\n","\n","|| '}'.{?name }.{?value?metric OR - > '?value OR?metric-lbl' || {?value OR?metric-lbl\n","\n","| {?value OR?metric\n","\n","| {?value OR?metric-lbl\n","\n","| {?value OR?metric-lbl\n","\n","| {?value OR?metric-lbl\n","\n","| {?value OR?metric-lbl\n","\n","| {?value OR?metric-lbl\n","\n",", }?value OR?metric-lbl ||!?metric-lbl ||!?metric-lbl };?value OR?metric-lbl ||!?metric-lbl ||!?metric-lbl }; }\n","\n",",?name?dataset OR - > '?record?record' || {?\n","409 \n"," input (English text): Can you list the models that have been evaluated on the GENIA - UAS dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GENIA - UAS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the GENIA - LAS dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GENIA - LAS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the MedNLI benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the GENIA - LAS benchmark dataset?\n"," output (Sparql query): \n","548\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","outputs :\n","outputs_lbl :\n","output_lbl_desc :\n","This is a small function that returns the list of results of a benchmark with specified parameters.\n","Todo:\n","add more parameters, e.g. how many values are to returned?\n","410 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp-2 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Yelp Fine-grained classification dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the Yelp Binary classification benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset?\n"," output (Sparql query): \n","533\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input/Output for the model classification model selection\n","input (English text): How many different models would you like to evaluate with the query?\n","output (Sparql query): \n","input (English text): In all of the models that have been passed to the model evaluation, what is the average weight used to create the model?\n","output (Sparql query):  \n","Input/Output for the model optimization model selection\n","411 \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the RACE benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RACE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the 20NEWS benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"20NEWS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the Twitter benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Twitter\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?\n"," output (Sparql query): \n","808\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n","In the above example, a function is being used to query a database, which then returns the metrics of the model being queried through a function. The function returned is called every time the query is run, or every time the model is selected.\n","For more info on RDFs, see this:  RDFs tutorial, this:  RDFS tutorial,  RDFS vocabulary, and this:  RDFS Datasets and Definitions.\n","This is because, before RDF was adopted in R, databases had to keep metadata for the various models and models were difficult to access due to the metadata requirements of the database. The use of RDFs allowed for the standardization of models in the source code of datasets to other libraries (or, on the other hand\n","412 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 James Bond dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 James Bond\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Atlantis dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Time Pilot dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Time Pilot\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bank Heist dataset?\n"," output (Sparql query): \n","505\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT?cont OR?mat OR?paper OR?bench OR?benchmark OR?benchmarkb OR?benchmarkdat OR?baselay OR?baselaydat OR?baselaydat_b OR?stale OR '?mat OR'?paper OR?bench OR?benchmark OR '?bench OR'?benchmarkdat OR '?baselay OR'?baselaydat OR '?baselaydat OR'?baselaydat_b OR'?baselaydat OR '?benchdat OR'?benchb OR?benchdat_b OR '?baselay OR'?baselay_b OR'?stale OR '?mat OR'?paper OR?bench OR '?benchb OR'?benchdat OR '?baselay OR'?baselay_b OR '?stale OR '?mat OR'?paper OR '?bench_fmt OR'?baselay OR '?benchb OR '?benchdat_fmt'; rdfs:label?benchdat_fmt; }  input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Cyber Sleuth dataset?\n"," output (Sparql query):  SELECT?paper OR?bench OR?bench_lbl OR?bench_fmt OR?bench_rdf OR?bench_sc OR?bench_sc_lbl OR?bench_sc_fmt OR?bench_stale OR?baselay OR?baselay OR?baselay_fmt OR?mat OR?bench_sdat OR '?bench OR '?bench_fmt'; rdfs:label?bench_fmt; }  input (English text): What are the titles and IDs\n","413 \n"," input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ARC (Easy) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Easy)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the CoQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Overall\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the CommitmentBank dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommitmentBank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of A2 score when benchmarked on the ANLI test dataset?\n"," output (Sparql query): \n","837\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","output (Sparql query): SELECT DISTINCT?model?model_i2s AS model_i2s FROM ( SELECT HASH OF model_i2s (@{?metric a orkgc:Metric} - @{?metric a_mod};... ) AS model_i2s 2WHERE {?metric a orkgc:Metric} AND 2AND @{?metric_i2s} OR 2AND 1;... ) AS model_i2s INNER JOIN ( SELECT DISTINCT?model_i2i AS model_i2i FROM ( SELECT HASH OF model_i2i (@{?metric a orkgc:Metric}... ) AS model_i2i 2WHERE {?metric_i2i} OR 2AND 1;... ) AS model_i2i OUTER JOIN ( SELECT MATCH(?key) AS key FROM model_i2s WHERE table_name = @{?metric_i2s} OR table_name = @{?metric_i2i_mod} AND key.table_id = model_i2i_mod.table_id ) AS mod_2 AS m2 ON...\n","output (Sparql query):\n","output (Sparql query): SELECT FIND_INDEX(1, @{?model?Model}.LAST ) AS index_name FROM @{?model?Model} WHERE (?key) AND (?metric) LIMIT 1 || FIND_INDEX(1, model_i2s.LAST) AS index_name INNER JOIN ( SELECT index_name FROM mod_2.LAST ) AS mod_22 AS index_name ON...\n","output (Sparql\n","414 \n"," input (English text): Could you provide a list of models that have been tested on the WNLI benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the QuAC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QuAC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the MedNLI benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset?\n"," output (Sparql query): \n","562\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["   #2 \"RUNNING FROM TABLE INTO TABLE\" - RDBM2K(6, N)\n","\"RUNNING FROM TABLE INTO TABLE\" - RDBM2K(15, N)\n","\"RUNNING FROM TABLE INTO TABLE\" - RDBM2K(24, N)\n","\"RUNNING FROM TABLE INTO TABLE\" - RDBM2K(32, N)\n","\"RUNNING FROM TABLE INTO TABLE\" - RDBM2K(64, N)\n","\"RUNNING FROM TABLE INTO TABLE\" - RDFS(7, N)   input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"QNLI\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset?\n","output (Sparql query):  \n","input (English text): Could you provide a list of models that have been tested on the RDBM2K benchmark dataset?\n","output (Sparql query): SELECT DISTINCT?model?\n","415 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Atlantis dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Time Pilot dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Time Pilot\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Berzerk dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?\n"," output (Sparql query): \n","507\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bally's Revenge dataset?\n","output (Sparql query):  \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 C-15Z+ dataset?\n","output (Sparql query):  \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Cray XE dataset?\n","output (Sparql query):  \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Cray XE-1 dataset?\n","output (Sparql query): \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Cray-2X dataset?\n","output (Sparql query):  \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Cray-5X dataset?\n","output (Sparql query):  \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Cray-50X dataset?\n","output (Sparql query):  \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Cray-2X dataset?\n","output (Sparql query):  \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Cray-D6X dataset?\n","output (Sparql query):  \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 C\n","416 \n"," input (English text): Can you provide links to code used in papers that benchmark the MPCM model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MPCM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Ning et al.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the SMT model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SMT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the MFEC model?\n"," output (Sparql query): \n","462\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Note that the query above selects all outputs, not just the last one.\n","You also need to specify what language (i.e. query language). In this example, it is SQL.\n","SELECT DISTINCT?code WHERE {?model a ; rdfs:label?model_lbl. FILTER (str(?model_lbl ) = \"SQLCODE\")?benchmark orkgp:HAS_DATASET?dataset. ORKMPF:BENCHMARK?benchmark.?cont orkgp:HAS_LABEL?model; ORKMPF:SOURCE_CODE?code. } \n","Example output (Sparql query): FROM b:benchmark AS b WHERE b.model = \"c\" AND b.dataset = \"foo\" AND b.cont = \"bar\";\n","input (English text): What language (i.e. query language) do you expect to see results in?\n","output (Sparql query): SELECT?code AS {?model a ; rdfs:label?model_lbl. ; FILTER (str(?model_lbl ) = \"SQLCODE\" ) ORKMPF:BENCHMARK ORKMPF:RESULT?dataset; } \n","Note that the query above selects all outputs, not just the last one.\n","You also need to specify what language (i.e. query language). In this example, it is SQL.\n","Note that the query above selects all outputs and is a string.\n","The query above also selects all outputs.\n","You then send this query to your database, specify that you want a benchmark with ORKMPF:BENCHMARK, and run the query from within the benchmark.\n","After\n","417 \n"," input (English text): Can you provide links to code used in papers that benchmark the CAIT-S-48 model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CAIT-S-48\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the CaiT-M-48-448 model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CaiT-M-48-448\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the CAIT-M-24 model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CAIT-M-24\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the CAIT-XS-36 model?\n"," output (Sparql query): \n","492\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["   or kgc:Model? code:CaiT-m:Model  rdf:label CaiT-xS-36 model code\n","Note: This query is required for benchmark scores of more than 5%. The \"Benchmark\" result contains the benchmark scores of the corresponding reference implementation.\n","Input/output files for benchmark code (optional): Each benchmark code contains a code that contains a code score for the reference implementation. This is needed to avoid the use of duplicate code scores for each benchmark code. The benchmark code can be either in SAS code or in raw data format (.SAS,.RDF and C) and can contain multiple reference implementations. The name of the file containing the benchmark code, including the code line or line segments, can be given in a range (e.g.?benchmark1=12345678901234567890;?benchmark2=323430918226799375; ). The benchmark code is also required for the validation code to pass benchmark scoring thresholds (see the validation section). A minimum number of benchmarks (0-5) should be installed for the current versions. Each benchmark code should be named after the reference implementation which includes the code scoring threshold for the benchmark. A maximum number of benchmark scores should be installed (4 or 5). The path to the benchmark output file for each implementation can be provided as is in the configuration options. Any custom validation code should use the validation.hpp file instead of the benchmark code, the custom function should be named as is in the configuration options. See the validation section of the benchmark reference for a list of validation functions. The custom code should use: or. Each column name in the rows (the columns to use in the benchmark score calculation) that are specified by the column name, the validation functions used, or the score_coding_function configuration option should\n","418 \n"," input (English text): What models are being evaluated on the WOS-5736 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the CIFAR-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the CUB-200-2011 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the seel.cse.lsu.edu/data/re17.zip  dataset?\n"," output (Sparql query): \n","566\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SEL-17\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n","I had some questions regarding the various output parameters and how to set them\n","To begin setting the parameters I decided to use Parquet to format the values, because it is the closest thing to Parquet file format - you can read the source code to set it yourself. I like to use some standard formatting to help people understand what the parameters mean:\n","A table is made \"column-per-row\".\n","An example table would be:\n","{ 'CUSTOMER_ID': '123', 'EMAIL': 'a@a.com', 'RATE_FEE': '4-10-16', 'QUALIFICATION_LEVEL': 'B', 'RICHARDSON_LAST_NAME': 'Richardson', 'REGION': 'United States', 'STATE': 'Georgia', 'FREEDOM_SCHOOL': 'T-100', 'PROGRAM_NAME': 'Advanced Statistical Learning Systems', 'PROGRAM_EXERCISE': 'GCE20' } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n","419 \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Pitfall! benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Pitfall!\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Phoenix benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Phoenix\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Alien benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Alien\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Montezuma's Revenge benchmark dataset?\n"," output (Sparql query): \n","826\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Montezuma's Revenge\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \n"," input (English text): OpenMV¶ OpenMV is a very fast implementation of MVVM with support for the Atari ST and Sinclair Spectrum. OpenMV will compare how well the Atari ST and Sinclair Spectrum performed on Atari 2600 benchmarks on different platforms. This should be a good benchmark to get a better picture of the performance on different hardware. The system memory is limited to 512k - 2048k and not all of the benchmarks performed better than 128k and not all showed the maximum resolution (see the output for the maximum resolution). Please feel free to submit as many and different benchmark runs as you like. The test sets will be sent as mva.zip and each output will include a logfile and links to each benchmark run. For more information on OpenM\n","420 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet ReaL dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the models that have been benchmarked on the ImageNet V2 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric?\n"," output (Sparql query): \n","763\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT TOP(2)?val(?val=1)?{?dataset name,?metric, c.?model?model_lbl. }?max_score_min_score (max score max min score) MAX(score.max_score AS?max_score_min_score) <?max_score_min_score ORDER BY {?metric as?metric_lbl }?value. OPTIONAL {?c.?model if?c.?model_lbl. OPTIONAL {?metric?metric_lbl?values (max score max min score) MAX(score.max_score AS?max_score_min_score) {?dataset a orkgc:Dataset }?values?max_score as?max_score_min_score. OPTIONAL {?c.?model if?c.?model_lbl. } OPTIONAL {?metric?metric_lbl?values (max score max min score) MAX(score.max_score AS?max_score_min_score) as?max_score_min_score. OPTIONAL {?metric?metric_lbl?values (max score max min score) MAX(score.max_score AS?max_score_min_score) (?dataset a orkgc:Dataset) }?values?max_score/2. } OPTIONAL {?c.?value if?c.?value_min. \"?dataset a orkgc:Dataset }?values?max_score/2. OPTIONAL {?c.?value if?c.?value_min } ]?maximum_score_min_score OPTION\n","421 \n"," input (English text): Can you provide links to code used in papers that benchmark the BiT-S (ResNet-152x4) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-S (ResNet-152x4)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-TDN(ResNet-101) model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM-TDN(ResNet-101)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the DY-ResNet-10 model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DY-ResNet-10\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the ResNet-152 (SAM) model?\n"," output (Sparql query): \n","514\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","output (Sparsely-indexed sparse matrix) in: input (English text): Can you provide links to code used in papers that benchmark the SAM model?\n"," output (Sparsely-indexed sparse matrix) in: output (Sparsely-indexed sparse matrix) on (samples (c, b, a) by (c, b, a)): {?lbl_lbl:Model?model,?model_lbl:Lbl?model,?model_lbl:Lbl|HAS_CUSTOM?lbl; (?lbl_tb,?lbl_tb|HAS_CUSTOM|HAS_TRANSACTION?) >?lbl, {?sparsely_sparse_lbl:Sparsely_sparse_lbl?sparsely_sparse_lbl,?sparsely_hdr:HDR (?lbl_lbl:Model?model ); } } input (English text): Can you provide links to code used in papers that benchmark the ResNet-101 model in any benchmark?\n","output (Sparql query): SELECT?code ORDER by {?model > 0?model?model_lbl. FILTER (str(?model_lbl) = \"ResNet-101\" )?benchmark orkgp:HAS_DATASET?dataset.?cont ORkgp:HAS_BENCHMARK?benchmark.?cont ORkgp:HAS_MODEL?model; ORkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-TDN(ResNet-101) model?\n","output\n","422 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the PubMed 20k RCT dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the PubMedQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMedQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the MedNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Pubmed dataset?\n"," output (Sparql query): \n","858\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------- MEDNI (50000,527) --------- <medn_id> = http://www.mednilar.org/pubmed/200802985_PubMedQA_P01_1-medn_id=6000000-medn_id=600001-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed orkgp:HAS_DATASET?dataset_name=pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed-pubmed orkgp:HAS_EVALUATION?value = N/A orkgp:HAS_METRIC?metric_lbl = N/A orkgp:HAS_BENCHMARK?benchmark=2.25.0.1 orkgp:HAS_MODEL?model?model orkgp:HAS_COUNTRY_CONCEPT?country_concept orkgp:HAS_BENCHMARK?benchmark=2.0.0-0.0012\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the NIH dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dat\n","423 \n"," input (English text): Can you list the models that have been evaluated on the Walker, walk (DMControl500k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cheetah, run (DMControl500k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Finger, spin (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Finger, spin (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Walker, walk (DMControl100k) dataset?\n"," output (Sparql query): \n","567\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT RANK?rear orkgp:DATE?rear; rdfs:index?index,?titles {?instr cnt,?partner cnt,?date datetype }?date,?paper orkgp:P3?date,?time orkgp:HAS_DATE?time.?informat cnt,?partner cnt,?date datetype,?time datetime } OPTIONAL?period TIMESTAMP?timedatetime.?timepartner cnt,?date datetype,?time datetime,?number count? OR (?instructor cnt) ORDER BY?timepartner?instr cnt;  input (English text): Explain the differences in benchmark metrics that govern performance compared with the Cheetah, run (DMControl100k) dataset?\n"," output (Sparql query):  SELECT TOP (1) DESC (?data?period orkgp:DATE?data,?data?partner orkgp:DATE?data) RANK?rear rdfs:index?index,?titles {?instr cnt,?partner cnt,?date datetype }?date,?paper orkgp:P3?date,?time orkgp:HAS_DATE?time.?informat cnt,?partner cnt,?date datetype,?time datetime }?date,?paper orkgp:P31?date,?time orkgp:HAS_DATE?time.  input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Finger, spin (DMControl100k) dataset?\n"," output (Sparql query): \n","424 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the PolyAI Reddit dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PolyAI Reddit\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the DBpedia dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DBpedia\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the models that have been benchmarked on the REDDIT-B dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"REDDIT-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the highest benchmark result achieved on the REDDIT-B dataset, including the metric and its value?\n"," output (Sparql query): \n","759\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?model?model_lbl?value WHERE {?dataset a orkgc,?rdfs,?dataset_lbl,?model a orkgc:Datasets,?rdfs:label?dataset_lbl}?(dataset orkgc,?rdfs,?dataset_lbl,?model ORkgc:Datasets) OR (orkgc:Datasets orkgc:models,?model ORkgc:models,?model ORkgc:models,?model ORkgc:models,?model ORkgc:models,?print_rdf ORkgc:datasets,?model ORkgc:models,?model ORkgc:models,?print_rdf ORkgc:models) AND?print_rdf ORkgc:models:MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODEL|MODLE?orkgc:Model. OR\n","\n","RAW Paste Data\n","\n","#!/usr/bin/php $str = sprintf(\"%s: {%s}: ({%s})\n","\n","\", $Str, $str_to_base64($str)); $s = sprintf('%s: {%s}: ({%s})\n","\n","', $Str, $s); echo $s if $s; chomp($s); output($str); # ## If you'd like to use the default format (r\n","425 \n"," input (English text): What are the models that have been benchmarked on the SciCite dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the CoQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the ScienceCite dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the Softcite dataset?\n"," output (Sparql query): \n","546\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Softcite\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n"," (1) There is one model that has been the target of the two validation studies that we examined, or that we expected to be used in the subsequent validation study: (a) our model (b) the CoQA model (e) the SciCite model.\n","We tested the output of the above query against all models that were used in the validation set. We found, as we expected, that using a simple comparison that combines three models that we believe are \"baseline-like\" yields results with statistically significant differences over all predictors. This contrasts with the other models, which did not differ significantly for many of the predictors. To understand why, it is important to understand why the three independent studies were run simultaneously without any additional randomization: (a) with the assumption that the different validation data sources were not biased, (b) by using each model (in a subset of the dataset), and (c) by running the same test set in all three experiments.\n","It is worth noting that our model does show some differences on certain of the predictors it was designed for, and on the two that it was not. On these features, there is\n","426 \n"," input (English text): Could you provide a list of models that have been tested on the SST-5 Fine-grained classification benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the SST-2 Binary classification dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-2 Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the SST-2 Binary classification dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-2 Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the SST-2 Binary classification benchmark dataset?\n"," output (Sparql query): \n","552\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SELECT {?meta rdfs:meta,?model rdfs:model,?parameter rdfs:parameter,?record rdfs:record } as?model_lbl,?metric rdfs:metric,?metric_lbl,?matrix rdfs:matrix FROM meta GROUP BY?meta; {?meta rdfs:meta,?model rdfs:model,?parameter rdfs:parameter,?record rdfs:record } AS?metric_lbl, {?meta rdfs:meta,?model rdfs:model,?parameter rdfs:parameter,?record rdfs:record } AS?metric_lbl,?meta rdfs:meta, {?meta rdfs:meta,?model rdfs:model,?parameter rdfs:parameter,?record rdfs:record } AS?metric_lbl, {?meta rdfs:meta,?model rdfs:model,?parameter rdfs:parameter,?record rdfs:record } AS?metric_lbl\n","To use the query above against the SST-2 Classification Benchmark dataset you need to create two table-valued functions. One will count the number of classes and the other will calculate the percentage of binary points that match the target metric.\n","SST-2 Classification Benchmark\n","To summarize the SST-2 Benchmark dataset we can compute the weighted sum of the weight given to each metric and we may derive it as follows:\n","where the first column contains the weight of a metric (in most cases this is a vector of weights given by the SST2 classifier) and the second one represents the percentile for each\n","427 \n"," input (English text): Which model has achieved the highest Accuracy (%) score on the Hendrycks Test benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy (%)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hendrycks Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark score and its metric on the  Jacquard dataset dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \" Jacquard dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the DROP Test dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DROP Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Hendrycks Test dataset?\n"," output (Sparql query): \n","833\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl.\n","428 \n"," input (English text): Can you provide links to code used in papers that benchmark the LSTM model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the REL-RWMD k-NN model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"REL-RWMD k-NN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM-MoS model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AWD-LSTM-MoS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model?\n"," output (Sparql query): \n","487\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","And I believe, the output of the query should look as follows:\n","input (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model? output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"KD-LSTMreg\")?benchmark } orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchemark. } \n","I'm not sure if this is correct or not, but I think that the above query is not using the default filtering rules that the query planner uses. I would like to create some parameters that filter the query. For instance, I can filter the document type by using parameter LTR. For example, the query planner can choose LTR parameter, or parameter TR (for TR-only filtering) or parameter ST (for ST-only filtering). I do not know which parameters should be used in a single query, but they should be used as a parameter that appears in the query plan and is not selected by the query planner.\n","This would allow me to filter the documents by using the parameters I need, but also avoid the filtering of the whole document.\n","I believe that a simple LSTM query planner (like a linear model planner) for a real-time language will not support this approach, but as we are at a very early stage of development with the language, I think that a more advanced solution will be available soon! I guess I'll be able to implement the above query very soon!\n","Cheers,\n","Gopal\n","The above method is similar to the LSTM-based plan\n","429 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the TDM Tagged Corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDM Tagged Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the NLP-TDMS (Exp, arXiv only) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the NLP-TDMS (Exp, arXiv only) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?\n"," output (Sparql query): \n","662\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" GROUP BY?metric?metric_lbl, TITLE, NAMES(metric, max(null))) ORDER BY DESC(?value)\n","output: \n","input (English text): Can you provide the average benchmark result for the NLP-TDMS dataset over a specific period (in years)?\n","430 \n"," input (English text): Can you list the metrics used to evaluate models on the ClueWeb09-B dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ClueWeb09-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the IMDb-B dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the FB15k dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FB15k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the ClueWeb09-B dataset?\n"," output (Sparql query): \n","551\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ClueWeb09-B\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the ClueWeb09-B dataset?\n"," output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ClueWeb09-B\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the ClueWeb09-B dataset?\n","output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkg\n","431 \n"," input (English text): Can you list the metrics used to evaluate models on the IMDb-B dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the iNaturalist 2018 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2018\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?\n"," output (Sparql query): \n","524\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:labels_assoc?labels_assoc; orkgc:HAS_DATASET?dataset; orkgc:HAS_EVALUATION?eval. OPTIONAL {?eval orkgc:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): Can you list the metrics used to evaluate models on the Sci-Hub dataset?\n"," output (Sparql query):  SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:labels_assoc?labels_assoc; orkgc:HAS_DATASET?dataset; orkgc:HAS_EVALUATION?eval. OPTIONAL {?eval orkgc:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n","output (Text format): \n","input (English text): Can you list the metrics used to evaluate models on the Open Knowledge datasets?\n","output (Text format): \n","input (English text): Can you list the metrics used to evaluate models on the BUG data set\n","output (Text format): \n","input (English text): Can you list the metrics used to evaluate models on the OMRF (Object Migration) dataset?\n","output (Text format):  //data/osrm/metadata.xml\n","output (Text format):  <metadata type=\"text/xml\" version=\"1.3\"> <title>O.m.Rm -\n","432 \n"," input (English text): Provide a list of papers that have utilized the CvT-13-NAS model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CvT-13-NAS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the CvT-W24 model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CvT-W24\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the CeiT-T (384 finetune res) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CeiT-T (384 finetune res)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the CvT-21 (384 res) model and include the links to their code?\n"," output (Sparql query): \n","518\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"CvT-21 (384 res)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the CvT-2 (256 res) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"CvT-2 (256 res)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\n","input (English text): Provide a list of papers that have utilized the CvT-1 (192 res) model and include the links to their code?\n"," output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. DISTINCT rdfs; rdfs:lbl_length?res (1.5 x 1024)?benchmark) \\?cont or\n","433 \n"," input (English text): Can you provide links to code used in papers that benchmark the SciKG model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SciKG\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the CGU model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CGU\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Ning et al.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the FG fine-grained gate model?\n"," output (Sparql query): \n","468\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Can you provide links to code used in papers that benchmark the FG fine-grained gate model? output (Sparql query): \n","input (English text): Can you provide links to code used in papers that benchmark the GFS model?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"GFS\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Can you provide links to code used in papers that benchmark the GFS model? output (Sparql query): \n","input (English text): Can you provide links to code used in papers that benchmark the WGS-R model?\n","output (Sparql query): \n","input (English text): Can you provide links to code used in papers that benchmark the WGS-R model? output (Sparql query): \n","input (English text): Can you provide links not only to code used in papers that benchmark the SciKG model but also to code included with the paper on which you are relying for this work?\n","output (Sparql query): \n","input (English text): Can you provide links not only to code used in papers that benchmark the SciKG model but also to code included with the paper on which you are relying for this work? output (Sparql query): \n","input (English text): Can you provide links not only to code\n","434 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the NYT dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the NYT24 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT24\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the iNaturalist 2019 dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2019\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the NYT29 dataset, including the metric and its value?\n"," output (Sparql query): \n","850\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- [REST] TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMIT [REST] TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMIT [REST] TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMIT [REST] TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMIT [REST] TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMIT [REST] TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMIT [REST] TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMITTEE DISTINCT TIME COMMIT\n","435 \n"," input (English text): Can you list the metrics used to evaluate models on the SearchQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SearchQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the SNLI benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the QNLI dataset?\n"," output (Sparql query): \n","514\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","The query returns results with a number of columns. The following sections describe how you can interact with the results.\n","Using the Results\n","All the fields returned by this query are columns in both the results set and query-to-return conversion set used to execute the query. For the result set, you can use either of the following columns.\n","Cumulative results: This column includes both the number of times an entire query has been run (i.e. is a cumulative field), and the number of metrics that have been run.\n","This column includes both the number of times an entire query has been run (i.e. is a cumulative field), and the number of metrics that have been run. Non-cumulative results: This column includes metrics only once the query started running to the end of the query. Note that the number of times something has been run to the end does not include a time-of-day. If a time-of-day metric is included that was run when the query started (in the query's result, for example), the number of times something has been run before that time is included in the non-cumulative results.\n","This column includes metrics only once the query started running to the end of the query. Note that the number of times something has been run to the end does not include a time-of-day. If a time-of-day metric is included that was run when the query started (in the query's result, for example), the number of times something has been run before that time is included in the non-cumulative results. Time to end: This column is used to determine the time for an entire query to end the query. It includes all metrics that were run before the query started, including time-of-factory events. It excludes all metrics which were run within 10 seconds after the end of\n","436 \n"," input (English text): Which model has achieved the highest Relation F1 score on the SciERC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Relation F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest F1 score on the ScienceCite benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the BC5CDR-chemical dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 entity level\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-chemical\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?\n"," output (Sparql query): \n","827\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Which dataset from the UK has been benchmarked for Physics B?\n","output (Sparql query): SELECT DISTINCT?model_lbl. FROM systest.sciec.benchmark GROUP BY?model_lbl; \n","input (English text): What model has achieved the highest Metric entity level score when benchmarked on BCS? Also a list of datasets from the UK have been benchmarked for Physics B?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 level score when benchmarked on BCS? Also a list of datasets from the UK have be benchmarked for Physics B?\n","output (Sparql query): \n","input (English text): What model has achieved the highest F1 level score on the BCS data?\n","output (Sparql query): \n","input (English text): What model has achieved the highest F1 level score on the BCS data using the model from the dataset from the UK?\n","output (Sparql query): \n","input (English text): What model has achieved the highest F1 level score on the BCS data using the model from the dataset from the Netherlands?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 level score on the BCS data using the model from the dataset from Germany?\n","output (Sparql query): \n","input (English text): Which model has achieved the highest F1 level score on the BCS data using the model from the dataset from the Netherlands?\n","output (Sparql query): \n","input (English text): What model has achieved the highest F1 level score on the BCS data using the model from the dataset from Germany?\n","output (Sparql query): \n","input\n","437 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset SST-5 Fine-grained classification?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you list the metrics used to evaluate models on the SST-2 Binary classification dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-2 Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the SST-5 Fine-grained classification benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?\n"," output (Sparql query): \n","678\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What are the most commonly used evaluations of the DDS(B) models on the SST-5 dataset at the bottom of the list?\n","output (Sparql query): \n","input (English text): What criteria is the top-most evaluation metric to use when benchmarking the DDS(B) models on the SST-5 dataset?\n","output (Sparql query): \n","input (English text): What criteria was the top evaluation metric to use in this comparison test?\n","output (Sparql query): I used the following standard evaluation metrics: NNU_CIF(K,k=10) : the probability of a training sample falling into each of the k bins given the number of bins. NNU_MALL(K,k=10) : the probability of a training sample being chosen to a single sample and in the same sample for the kth, 8th, 10th, and 12th bins, respectively. NNU_PAP(K,k=10) : the probability of a training sample being paired at its kth and kth and kth and nth bins, respectively. NNU_PIP(K,k=10) : the probability of a single sample being selected to a single sample and in the same sample for each bin and in different bins, respectively. In the above NNU_MALL(k=10) is always a multiple of N, so the higher, higher probability of sample selection is given by NNU_MALL(k=10). When the N value is >= 1, the higher N is taken to be given by NNU_MALL((1-N)-1), and the lower N is taken to be given by NNU_MALL(k=10). I used the\n","438 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the TriviaQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the WebQuestions dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WebQuestions\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Quora Question Pairs dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Quora Question Pairs\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset?\n"," output (Sparql query): \n","863\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \" Natural Questions \" OR \" Natural Questions (long) \" )?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl\n","input (English text): Can you provide the highest benchmark result, including the metric and score, for the Quora Short Queries dataset?\n"," output (Sparql query):  SELECT MAX(?value) AS?score FROM QuoraShortQUERY? SELECT?dataset? as orkgc::Dataset, 1 AS?score FROM?datasets, (SELECT c FROM OR. HAVING OR. BOTH AND. LABEL(?dataset.?p?p ) AS?quq FROM??.?p?p WHERE!?qual_id IS? (SELECT 0.?q || 1?q FROM\n","439 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 English-German?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the WMT2016 Romanian-English dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Romanian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the WMT2016 English-Romanian dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Romanian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 Czech-English?\n"," output (Sparql query): \n","872\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?\"metric?\"metric_lbl (MAX(?value) AS?score) FROM Metric ORDER BY DISTINCT Metric (?value)\n","output (Sparql query): SELECT TOP(1) Metric | score | ----+------------+------------+------------+------------ 1 |?key 1 2|?metric?.metric_lbl 1 3|?metric?.metric_lbl 2 4|?metric?.metric_lbl 3 5|?metric?.metric_lbl 4 6|?metric?.metric_lbl 5 7|?metric?.metric_lbl 6 8|?metric?.metric_lbl 7 9|?metric?.metric_lbl 8 10|?metric?.metric_lbl 9 11|?metric?.metric_lbl 10 12|?metric?.metric_lbl 11 13|?metric?.metric_lbl 12 14|?metric?.metric_lbl 13 15|?metric?.metric_lbl 14 16|?metric?.metric_lbl 15 17|?metric?.metric_lbl 16 18|?metric?.metric_lbl 17 19|?metric?.metric_lbl 18 20|?metric?.metric_lbl 19 21|?metric?.metric_lbl 20 22|?metric?.metric_lbl 21 23|?metric?.metric_lbl 22 24|?metric?.metric_lbl 23 25|?metric?.metric_lbl 24\n","440 \n"," input (English text): What is the average energy generation of all energy sources considered?\n"," output (Sparql query): SELECT (AVG( ?elec_gen_value) AS ?average_elec_gen_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43134 ?electricity_generation. FILTER(REGEX( ?energy_sources_labels, \"all sources\")) ?electricity_generation orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?elec_gen_value) } \n"," input (English text): What is the average energy generation for each energy source considered?\n"," output (Sparql query): SELECT ?energy_sources_labels (AVG( ?elec_gen_value) AS ?average_elec_gen_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43134 ?elec_gen. ?elec_gen orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?elec_gen_value) } \n"," input (English text): What is the average installed capacity of all energy sources considered?\n"," output (Sparql query): SELECT (AVG( ?installed_cap_value) AS ?average_installed_cap_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43133 ?installed_capacity. FILTER(REGEX( ?energy_sources_labels, \"all sources\")) ?installed_capacity orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?installed_cap_value) } \n"," input (English text): Which sectors are modeled as energy sectors and how often?\n"," output (Sparql query): \n","541\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" select Sectors::from_modeling_category(AUC(Total Energy Product)),\n","as Energy Sector;\n","input (English text): Which sectors are modeled as energy sectors and how often?\n","output (Sparql query):  select Sectors::from_modeling_category(AUC(Total Energy Product)),\n","as Energy Sector,\n","as Energy Sector,\n","as Energy Sector \n","input (English text): Which sectors are modeled as energy sectors and how often?\n","output (Sparql query):  select Sectors::from_modeling_category(AUC(Total Energy Product)),\n","as Energy Sector,\n","as Energy Sector,\n","as Energy Sector \n","input (English text): Do sectors with a lower installed capacity yield higher or lower energy output? Note that to achieve the goal of the analysis we first estimate total energy production in order to model total energy consumption. The AUC data is available directly from the Energy Bureau (see:   http://eia-solutions.eia.gov/EIAWS/solutions/eia/ ).\n","The data comes from the World Energy Assessment ( http://eia.org/ ) as described here:\n","The figures presented here assume a total energy use of 50% of total energy consumption on a per unit basis for each energy source (see Appendix A for details). \n","When calculating net energy consumption, we take the average of all energy use, for each energy source, the average of installed capacity and installed capacity-weighted total capacity,  net energy consumption is therefore calculated in units of megajoules at constant (in the same range) installed capacity. \n","The total installed capacity used in the examples presented here is 10.7 TWh, or about 3.4 quadrillion bbls [ 2 ] installed capacity-weights (see Appendix B for\n","441 \n"," input (English text): Can you list the models that have been evaluated on the Kinetics-600 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Supervised: dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset?\n"," output (Sparql query): \n","553\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","And a more generic query:\n","input (English text): Explain the list of model evaluations performed on the Keras datasets with a minimum of 60 papers.\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:labels?dataset_lbl.?dataset rdfs:labels.?model rdfs:label?model_.  OPTIONAL {?dataset a orkgc:HAS_DATASET?dataset.?model rdfs:label?model_lbl. } } \n","input (English text): Explain the list of model evaluations performed on the Supervised: dataset with a minimum of 60 papers.\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:labels?dataset_lbl.?dataset rdfs:labels.?model rdfs:label?model_lbl_.  OPTIONAL {?dataset a orkgc:HAS_DATASET?dataset.?model rdfs:label?model_lbl. } } \n","input (English text): Explain the list of model evaluations performed on the Kinetics-600 dataset with a minimum of 60 papers.\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:labels?dataset_lbl.?dataset rdfs:labels.?model rdfs:label?model_.  OPT\n","442 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the RotoWire dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the RotoWire (Content Ordering) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Cartpole, swingup (DMControl500k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cartpole, swingup (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset?\n"," output (Sparql query): \n","536\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","select * from publications where author = \"Shahid Sohail\" |?date(?datet) = \"2017-10-28\" | select * from publications in which?name =?\n","output (Sparql query): \n","Select fields from a select statement: SELECT field_list from select_list_field_field_list where field_list.type = 1,1... select_list.field_list.fields | select_list.list_field_field_list | select_list.field_list.where1 | select_list.field_list.where2 | select_list.field_list.where3 | select_list.field_list.where4\n","output (Sparql query):\n","select {?field_list.field_list.count } | select_list.field_list.field1 | select_list.field_list.field2 | select_list.field_list.field3 | select_list.field_list.field4\n","input (English text): I want to see an output, that has both names and IDs for the publications.\n"," output (Sparql query): SELECT DISTINCT name(?ID) ID FROM publications where title(?ID) =?and?field_list.field_list.fields and?table.field_list.fields and?name(?ID) =? and?field1.count =? and?field2.count =? and?field3.count =? and?field4.count =?\n","input (English text): I want to see how many publications have been linked in a journal and/or book chapter.\n","output (Sparql query): \n","SELECT COUNT(*) FROM publications;\n","select * from publications where?name =? AND?\n","443 \n"," input (English text): Where can I find code references in papers that have used the Naive Bayes model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Naive Bayes\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the metrics that are used to evaluate models on the Yelp Binary classification benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Automatically labeled Medline abstracts corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the precision, recall, and f1 values of all compared studies that used the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request? \n"," output (Sparql query): \n","537\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- ORDER BY {?criteria {?metric rdfs:label?criteria.?metric rdfs:label?metric_.?criteria rdfs:label?criteria_lbl. }} \n"," input (English text): What are the validation accuracies of all studies of the Yelp Binary classification benchmark dataset?\n"," output (Sparql query): --------------------------- ORDER BY {?criteria {?metric rdfs:label?criteria.?metric rdfs:label?metric_.?criteria rdfs:label?criteria_lbl. }} \n","input (English text): Give an example of how the algorithm naive bayes performs over the Yelp Binary classification benchmark dataset. What happens to the model with high model performance and high accuracy for the most critical performance metrics?\n","output (Sparql query): --------------------------- ORDER BY {?metric rdfs:label?metric_lbl.?metric rdfs:label?metric_.?metric rdfs:label?criteria_lbl. } \n","input (English text): What is a model score of all the models that have been evaluated with the naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request?\n","output (Sparql query): --------------------------- ORDER BY {?criteria {?metric rdfs:label?criteria. ?criteria rdfs:label?criteria_lbl. }  } \n","input (English text): What is a model score for all the models that have been evaluated with the naive bayes in combination with the machine learning feature bag of words to classify user feedback as feature request? What is the accuracy of comparing the model scores and predictions based on the supervised\n","444 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ARC-PDN dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC-PDN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NYT29 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Classic dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classic\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dmlab-30 dataset?\n"," output (Sparql query): \n","516\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Dmlab-30\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n","input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the USN&WR dataset?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"USN&WR\" )?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }  input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STAMM dataset?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STAMM\")?benchmark\n","445 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 English-German dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the IWSLT2015 English-German dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2014 English-French dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-French\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 German-English dataset?\n"," output (Sparql query): \n","542\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 German-English\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the IWSLT2015 German-French dataset?\n"," output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IWSLT2015 German-French\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the IWSLT2015 German-English dataset?\n","output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dat\n","446 \n"," input (English text): What is the top benchmark score and its metric on the Cartpole, swingup (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cartpole, swingup (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the best performing model benchmarking the Cartpole, swingup (DMControl100k) dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cartpole, swingup (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark result (metric and value) over the dataset Finger, spin (DMControl100k)?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Finger, spin (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value?\n"," output (Sparql query): \n","888\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------- ---------------------------- Score metric result (as defined by Score metric) BestModel: 100.00009955007847 Score Metric: Score Metric: Score Metric: Score Metric: 0 Metric: 0 Metric: 0 Metric: 2.5432784369764 Metric: 4.243539288096 Metric: 5.181824051406 Metric: 6.782964890818 Metric: 9.024975673598 Metric: 10.9592712342787\n","input (English text): How was the benchmark scored?\n","output (Sparql query): SELECT score?dataset_lbl.?score AS?score FROM ( SELECT c.dataset_id?dataset_id FROM ( SELECT?dataset_id?dataset_id FROM rdstats GROUP BY?dataset_id) AS c.dataset_id,?dataset_name?dataset_name c.name ORDER BY c.dataset_id DESC LIMIT?score DESC, score ) AS score WHERE c.dataset_id =?dataset_lbl.?dataset_lbl AND c.dataset_name =?dataset_name.,?dataset_name =?dataset_name. LIMIT?score\n","output (Sparql query): --------------- ---------------------------- Score metric result (as defined by Score metric) BestModel: 100.00009955007847 Score Metric: Score Metric: Score Metric: Score Metric: 0 Metric: 0 Metric: 0 Metric: 2.5432784369764 Metric: 4.243539288096 Metric\n","447 \n"," input (English text): What are the metrics of evaluation over the Automatically labeled Medline abstracts corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Automatically labeled Medline abstracts corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?\n"," output (Sparql query): \n","549\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }    Outputs the results for all benchmarked models that have been benchmarked as a result of this post. (The query is a bit cumbersome, as it is a mix between a schema query which results from the query and a Parquet-based query, which is just an alias for a Parquet result.)     SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n","input (English text): What are the metrics of evaluation over the Automatically labeled Medline abstracts corpus dataset?\n"," output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:\n","448 \n"," input (English text): Can you provide links to code used in papers that benchmark the LSTM model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Grave et al. (2016) - LSTM + continuous cache pointer model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Grave et al. (2016) - LSTM + continuous cache pointer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the FS-LSTM-2 model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FS-LSTM-2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the LSTM (Bai et al., 2018) model in any benchmark?\n"," output (Sparql query): \n","505\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"LSTM(Bai et al, 2018) + Batch)?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): List the code links in papers that use the LSTM model with a single random stride (Matsumoto et al., 2015) in any benchmark?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"LSTM(Matsumoto et al., 2015) + Batch)?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): List the code links in papers that use the Random Stochastic Load (RSTM) model in any benchmark?\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"RSTM + b. )\n","449 \n"," input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of PARAMS metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"PARAMS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of Accuracy metric on the Oxford-IIIT Pets benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Accuracy (%) score when benchmarked on the Oxford-IIIT Pets dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy (%)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the top benchmark result (metric and value) over the dataset Oxford-IIIT Pets?\n"," output (Sparql query): \n","840\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","# test on dataset and model\n","data (Olivia) > ORG_PETS = '{% if %{{% define _nodepth_weight: _nodepth_weight() } {% end %} %}' {% for _score, test_ratio in ORG_PETS.values.iter_agg (): % iter = % { [ \"%{_score}_\", _score] = test_ratio.mean(score), [ \"%{_score}_\", _score2] = _score2.mean(score2), [ \"%{_score}\", _score1, _score3, _score4, _score5, _score6] = _score1.mean('C'); } %}' {% end %} data (Oxford) > orkgp:GATHER(ORG_PETS) [ '#4', '#7', '#7.6', '#8', '#13', '#13.4', '#13.6', '#14', '#16', '#16.6', '#20', '#21', '#22', '#22.4', '#23', '#24', '#24.1', '#24.6', '#23', '#24.4', '#24.6', '#21', '#24.5', '#24.4', '#24.6', '#5', '#6', '#7', '#8', '#9', '#10', '#13', '#15', '#16', '#16.5', '#16.6', '#17', '#17.5', '#18', '#18.3',\n","450 \n"," input (English text): Can you provide links to code used in papers that benchmark the XLNet model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the XLNet (Large) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet (Large)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the XLNet (single model) model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the XLNet-Large model?\n"," output (Sparql query): \n","477\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input for 'output' command  input (English text): List the code links in papers that used the XLNet (Large) model for benchmarking purposes?\n","output (Sparql query): \n","Input for 'cont' command  input (English text): Where can I find code references in papers that have used the XLNet (single model) model for benchmarking purposes?\n","output (Sparql query): \n","Input for'model' command  input (English text): Where can I find code references in papers that have used the XLNet (single model) model for benchmarking purposes?\n","output (Sparql query): \n","Input for'source_code' command  input (English text): Where can I find code references in papers that have used the XLNet (single model) model for benchmarking purposes?\n","output (Sparql query): \n","Input for 'code' command  input (English text): Where can I find code references in papers that have used the XLNet (Single) model for benchmarking purposes?\n","output (Sparql query): \n","Output for'model' command  output (String): The XZF model output(String)\n","Output for'source_code' command  output (String): The XZF model source_code\n","Output for'model' command  source_code (String): The XZF model code\n","Input for 'benchmark' command  input (English text): Where can I find code references in papers that use the XLnet-Large model in any benchmark?\n","output (Sparql query): \n","Input for 'benchmark' command  output (Sparql query): \n","Input for 'cont' command  input (English text): Where can I find code references in papers that have used the XLnet (Single) model for benchmarking\n","451 \n"," input (English text): Where can I find code references in papers that have used the EfficientNetV2-S model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"EfficientNetV2-S\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the EfficientNetV2-S (21k) model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"EfficientNetV2-S (21k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the EfficientNetV2-M model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"EfficientNetV2-M\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?\n"," output (Sparql query): \n","516\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"EfficientNetV2-L (2.0\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }  I have found 5 papers that used the EfficientNetV2-L model (here is some code link to the file):\n","Input (English text): Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?\n","452 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-5736 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the WOS-11967 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-11967\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset?\n"," output (Sparql query): \n","516\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): List the title and ID of research papers that contain a benchmark over the WOS-6913 dataset?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WOS-6913\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }  input (English text): Provide a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-6922 dataset (LSTM)?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WOS-6922\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }   input (English text): Provide a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-6935 dataset (LSTM)?\n","output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dat\n","453 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the WSC dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the LAMBADA dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"LAMBADA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Supervised: dataset, including the metric and its value?\n"," output (Sparql query): \n","854\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- DESC DESC ------------------------------------------------------------------ TITLE: Supervised: dataset and its score on the WSC test, including the metric and its value. FROM test d INNER JOIN supervised d ON d.test_id = d.supervised_id INNER JOIN test_level test_level_subset ON d.supervised_id = d.supervised_id AND d.test_id = test_level_subset.supervised_id INNER JOIN supervised d ON d.test_id = d.supervised_id AND d.test_id = supervised_d.test_id TO test_level_subset.supervised_id USING RDFS WITH WITH OUT-OF_MOVE AS (SELECT c.class, c.subclass, c.id, c.level, l.name FROM test d INNER JOIN supervised d ON d.test_id = d.supervised_id INNER JOIN test_level test_level_subset ON d.supervised_id = d.supervised_id WHERE l.name =? AND rdfs:keywords ='supervised' AND (c.sensor.is_hidden = 'yes')? ) WHERE [[?metric|?value] IN ([SELECT?value]). ] and (?meta_lbl[0] AND c.meta_lbl[0].label_or_metric) AND c.metrics= 'WSC' AND (?metric|?value) ORDER BY DESC (?value)\n"," output (Sparql query):\n","--------------------------------------------- Supervised: dataset and its score on the WSC test, including the metric and its value. ----------------------------------------------------- TITLE: Supervised: score and metric for supervised WSC testing dataset on\n","454 \n"," input (English text): What are the models that have been benchmarked on the ANLI test dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the top benchmark result (metric and value) over the dataset ANLI test?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Indicate the model that performed best in terms of A1 metric on the ANLI test benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"A1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric?\n"," output (Sparql query): \n","743\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","model_lbl_dataset_lbl(input,output) \n","input (English text): What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric over the ANLI test dataset in R?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl(max_sizes_c) AS?model_lbl_as_c ORDER BY {?model_lbl_as_c?max_sizes_c } LIMIT 1\n","model (English text)  input (English text): Show the model that performed best in terms of A4 metric on the ANLI test dataset?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl(max_models_c) AS?model_lbl_as_c ORDER BY {?model_lbl_as_c?max_models_c } LIMIT 1 \n","model (English text)   input (English text): Show the best performing model benchmarking the ANLI test dataset in terms of A4 metric over the ANLI test dataset in R?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl_as_c ORDER BY {?model_lbl_as_c?max_models_c } LIMIT 1\n","model (English text)   input (English text): Show the best performing model benchmarking the ANLI test dataset in terms of A4 metric over the ANLI test dataset in R?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl_as_c ORDER BY {?model_lbl_as_c?max_models_c } LIMIT 1 \n","model (English text) \n","455 \n"," input (English text): Can you list the metrics used to evaluate models on the iNaturalist 2018 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2018\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the CIFAR-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the AI-KG dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AI-KG\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the iNaturalist 2019 dataset?\n"," output (Sparql query): \n","514\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"iNaturalist 2019\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n","input (English text): Please provide a list of metrics used to evaluate models on the iNaturalist 2019 dataset.\n"," output (Sparql query):\n","input (English text): Please list the metrics used to evaluate models on the iNaturalist 2018 dataset.\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"iNaturalist 2018\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n","input (English text): To display the ranking, please use the following SQL query (see How to select a metric ).\n","output (Sparql query):\n","output (Sparql query): SELECT RANKED(c.name,?dataset_name,?data\n","456 \n"," input (English text): What are the models that have been benchmarked on the Atari 2600 Atlantis dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the Atari 2600 Yars Revenge dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Yars Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the Atari 2600 Robotank dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Robotank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?\n"," output (Sparql query): \n","563\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","output data  \n","\n","\n","This is a big list and can take quite a while to download, so here is a simple solution using a script I write for a task such as this:\n","\n","$ cat myprobabilities.scm\n","\n","1 1 1.6 1.4 1.2 1.0 0.8 0.7 0.6 0.5 0.4 0.3 0.2\n","\n","You can view this file in your favorite text editor and you can edit it as you like (I am using Microsoft Word 2000 under Vista). But remember that this is not the final list and that there will be lots of improvements to the numbers. There is a small chance that I will have more improvements but it is a lot of fun to see them in action!\n","\n","Download this document (PDF, 1.3 MB)\n","\n","And in case you are still confused as to how to interpret the values, here is a quick example:\n","\n","$./myprobabilities 1 1.6 1.4 1.2 1.0 0.8 0.7 0.6 0.5 0.4 0.3 0.2 +------------+-----------+----------------------------------------------------+ | | | Time at a certain location | | time | | | | +------------+-----------+----------------------------------------------------+ | 2013-11-20 20:00:00 +------------+-----------+----------------------------------------------------+ | 0 | 2014-12-01 19:20:00 +------------+-----------+----------------------------------------------------+ | 1 | 2019-01-19 17:45:00 +------------+-----------+----------------------------------------------------+ | 2 | 2013-11-01 20:00:00 -0007000000 -1707000000 -1817000000 -2022000000 -26000000 +------------+-----------+----------------------------------------------------+ +------------+-----------+----------------------------------------------------+\n","\n","Download this\n","457 \n"," input (English text): Which model has achieved the highest BLEU score on the WMT2016 German-English benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest BLEU score score on the IWSLT2015 English-German benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Russian-English benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Russian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset?\n"," output (Sparql query): \n","840\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }\n","Input data in the output\n","data {model_lbl} {dataset_lbl} {model}... {evaluation}... {value} orkgp:HAS_DATASET?dataset. eval:evaluation?metric. orkgp:HAS_EVALUATION?evaluation. orkgp:HAS_METRIC?metric. orkgp:HAS_BENCHMARK?benchmark. orkgp:HAS_MODEL?model. orkgp:HAS_EVALUATION?evaluation.?evaluation. orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?\n","458 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Lunar Lander (OpenAI Gym) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Lunar Lander (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Supervised: dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Barabasi-Albert dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Barabasi-Albert\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cart Pole (OpenAI Gym) dataset?\n"," output (Sparql query): \n","520\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cart Pole\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Tiangong (OpenAI Gym) dataset?\n"," output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Tiangong\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Rong (OpenAI Gym) dataset?\n"," output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Rong\")?benchmark orkgp\n","459 \n"," input (English text): Provide a list of papers that have utilized the Prior hs model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the HNEABP (BWNE) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"HNEABP (BWNE)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the GShard model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GShard\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the A3C FF hs model and include the links to their code?\n"," output (Sparql query): \n","493\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"A3C\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the A3G FF hs model and include the links to their code?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"A3G\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","Input:\n","Output:\n","Input:\n","460 \n"," input (English text): Where can I find code references in papers that have used the ELMo model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the e2e-coref + ELMo model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"e2e-coref + ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Transformer (self-attention) (Trinh et al., 2018) model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer (self-attention) (Trinh et al., 2018)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes?\n"," output (Sparql query): \n","524\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiDAF + Self Attention + ELMo\")?benchmark orkgp:HAS_DATASET??dataset.?cont orkgp:HAS_BENCHMARK??benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Where can I find code references in papers that have used the Logistic Regression model for benchmarking purposes?\n","output (Sparql query): \n","SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Logistic Regression\")?benchmark orkgp:HAS_DATASET??dataset.?cont orkgp:HAS_BENCHMARK??benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Where can I find code references in papers that have used the Random Forest (RFA) model for benchmarking purposes?\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Random Forest (RFA)\")?benchmark orkgp:HAS_DATASET??dataset.?cont orkg\n","461 \n"," input (English text): List the metrics that are used to evaluate models on the Pubmed benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Pubmed\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the BIOSSES benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BIOSSES\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the PubMedQA dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMedQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset?\n"," output (Sparql query): \n","516\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\"The NCBI Disease benchmark allows for the validation of models for disease risk or disease susceptibility and to investigate underlying pathways. A single model with some or all of the parameters from all of the models will be used to evaluate the model on the criterion dataset, unless more parameters are specified for the same model in a hierarchical parameter-value model and are greater than 2\".\n","input (English text): List the metrics that are used to evaluate models on the CHM test dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CHM\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval or kgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }  input (English text): Can you list the metrics that are used to evaluate models on the CHMQQ dataset?\n","output (Sparql query): \n","\"The CHMQQ dataset was first used widely in 1985 by US Department of Defense; however, there has been no major use for this dataset in more recent years. This dataset was first used to identify potential CHM models during an early phase of modelling efforts. The dataset is currently maintained for the purposes of informing the development of new and improved modelling and risk assessment approaches and may be used to provide guidance in evaluating the results of CHM models.\"\n","input (English text): List the metrics that are used to evaluate models on the CHMQQB dataset?\n","\n","462 \n"," input (English text): Can you provide links to code used in papers that benchmark the 4 layer QRNN model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"4 layer QRNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DQN+SR model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN+SR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the DQN Best model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN Best\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?\n"," output (Sparql query): \n","486\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model 6 orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"6 layer QRNN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\n"," input (English text): Where can I find code references in papers that have used the DQN+SR Model for benchmarking purposes?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model 6 orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"6 layer QRNN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): Where can I find code references in papers that have used the 7-layer QRNN model for benchmarking purposes?\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model 7 orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"7 layer QRNN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK\n","463 \n"," input (English text): What models are being evaluated on the WOS-5736 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the WOS-46985 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-46985\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the enwik8 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwik8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the WOS-11967 dataset?\n"," output (Sparql query): \n","551\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER(str(?dataset_lbl) = \"WOS-11967\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n"," output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE {?dataset a or kgc:Dataset; rdfs:label?dataset_l Bl. FILTER(str(?dataset_lbl) = \"WOS-11967\")?benchmark or kgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. \n","input (English text): What models are being evaluated on the enwik8 dataset?\n"," output (Sparql query): select {?dataset ix:dataset or ix:dataset ix:dataset_lbl } as model_lbl                          select {     RDFs:    RDFs: xsd (?dataset_lbl ). filters (?dataset_lbl ). rdfs to xsd (?dat\n","464 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NYT29 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the 20NEWS dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"20NEWS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the NYT-single dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the NYT24 dataset?\n"," output (Sparql query): \n","497\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What are the titles and IDs of research papers that include a benchmark over the NYT-single dataset?\n","output (Sparql query): \n","The following output is identical except that the data is in JSON format, and therefore does not require the SQL script to be run on a separate node as before (see below).\n","\n","\n","The following output is identical except that the data is in JSON format, and therefore does not require the SQL script to be run on a separate node as before (see below). output (JSON):\n","\n","> jq \" SELECT title(?dataset->?dataset_lbl ) FROM?print_matrix; \"\n","\n","output (JSON):\n","465 \n"," input (English text): Name the datasets that have been used for benchmarking in the PointGoal Navigation research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"PointGoal Navigation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): List the datasets benchmarked under the Robotic Grasping research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Robotic Grasping\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Could you provide a list of models that have been tested on the Gibson PointGoal Navigation benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Name the datasets that have been used for benchmarking in the Robot Navigation research problem?\n"," output (Sparql query): \n","534\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): List the datasets that have been tested on the Handy Padding research problem?\n","output (Sparql query): SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Handy Padding\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }  input (English text): Could you provide a list of models that have been tested on the Handy Padding research problem?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Handy Padding\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","input (English text): Name the datasets that have been used for benchmarking in the Robotics Research project?\n"," output (Sparql query): \n","input (English text):\n","466 \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the CoNLL 2012 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the ART/CoreSC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ART/CoreSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset?\n"," output (Sparql query): \n","592\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","A couple of people asked if there would be any way to create a batch-to-batch conversion from their SPSS or SPSS/PLUS or similar systems to the LMS format (e.g. data.seel.edu/data/refsq17.zip for the benchmark dataset on the seel.cse.lsu.edu/dataset_/ ). My approach is to convert each SPSS or SLR line to an LMS line. Here is the same text, but for the SPSS/PLUS or similar data.  If anyone has any advice on implementing this, let me know.  \n","input (Parsed text).    output (Sparql query): SELECT DISTINCT?lbl?lbl_lbl FROM DATASET a, c, ( SELECT **.{?dataset a,.{?dataset_lbl a,.} } ).{?dataset_lbl a,?dataset_lbl_lbl _ _ _,?lbl (?lbl_lbl _ _ _) } AS LBL,?lbl.{?lbl_lbl _ _ _. } {?lbl _ _ _. } {.LBL }.HAS_DATASET {?dataset_lbl_. }.CASE AS CASE_LBL, CASE_LBL_LEFT. OPTIONAL\n","output (Sparql query): \n","The first line is similar to what you'd see in a normal batch file, except of course with all the SPSS or SLR data converted to LMS data.  For more information on LMS-to-SPSS conversion, check out my previous post about it:  http\n","467 \n"," input (English text): Can you list the models that have been evaluated on the IMDb-B dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the IMDb-M dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the ART/CoreSC benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ART/CoreSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the IMDb-M benchmark dataset?\n"," output (Sparql query): \n","547\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","NOTE: Because we do not know which IMDb-M datasets are actually in use, it is not possible to predict how the most successful models might perform with our dataset.\n","TESTING SELECTION ORDER\n","As in the case of SELECTION ORDER, it is recommended to test a small subset of models first. The only requirement is a sufficiently large number of comparisons with the actual set of models. This may not be immediately obvious to the user, but in the example above we tested only 2 outliers, which means we are testing the models with the best performance and then removing all the outliers that fall outside these two groups. For our own purposes, we decided to use only those models that matched all of the benchmarks. Because the number of models to test may vary from dataset to dataset, it is advisable to test only a subset of the models to learn their optimum performance.\n","FOR THOSE USING PASTE, WE RECOMMEND USING THE 'test_model' AND 'test_criteria' parameters to test a subset of models. After you have entered the model names in the FOR THOSE USING PASTE, you select the model name as the test model from the 'test_model' parameter. As you would expect, all subsequent tests will be performed on the test model.\n","468 \n"," input (English text): List the code links in papers that use the PEGASUSLARGE model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PEGASUSLARGE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Luna model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Luna\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the BigBird model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BigBird\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?\n"," output (Sparql query): \n","480\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"PEGASUSLARGE\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"PEGASUSLARGE\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"PEGASUSLARGE\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:H\n","469 \n"," input (English text): Provide a list of benchmarked datasets related to the Music Modeling research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Music Modeling\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): What are the most commonly used benchmark datasets for the Environmental Sound Classification research field?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Environmental Sound Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Provide a list of benchmarked datasets related to the Document Classification research area?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Document Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Provide a list of benchmarked datasets related to the Audio Classification research area?\n"," output (Sparql query): \n","505\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): \n","output (Sparql query): \n","input (English text): \n","output (Sparql query): \n","input (English text): provide list of the popular sound files\n","output (Sparql query): SELECT DISTINCT?dataset?dataset_data(samples) WHERE {?problem \"Sound File\";?dbh. \"C:\\\\Windows\\\\System32\\\\audit\\\\datasource\\\\samples \\\\mffs\\\\mfppm_00010.ts\";?pcm_dataset?dataset_data(samples);?lbs?a \"Laws & Regulations\";?labels?labels_dataset;?labels?labels_lbl. FILTER (str(?labels_lbl) = \"Sound File\")?dataset ct1 lb1_dataset;?titles?ts_labels;?ts_labels OR:?ts_labels OR:?ts_labels |:?ts_labels or:?ts_labels |::?ts_labels or:?ts_labels OR:?ts_labels |:::?ts_labels |:::?dataset?dataset.?cont or:?dataset ct1 or: ct1.?benches OR:?benches_dataset OR: ct1/dataset or: ct1/dataset. or: ct1/dataset or: ct1/dataset.?dbs OR:?dataset ct1 dbs_dataset. FILTER (str(?dbs_datoset ) = \" Sound File\n","470 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset Lunar Lander (OpenAI Gym)?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Lunar Lander (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Cartpole, swingup (DMControl100k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cartpole, swingup (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the CUB-200-2011 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cart Pole (OpenAI Gym) dataset?\n"," output (Sparql query): \n","888\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["----------------------------\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cart Pole (OpenAI Gym)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval? orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }?selection?order?of?metric. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }?selection?group?by?metric. OPTIONAL {?test orkgp:HAS_TEST?model.*model rdfs:label?model_lbl. }?selection?order?of?model. OPTIONAL {?test orkgp:HAS_TEST?model.*model rdfs:label?model_lbl. }?select?index?of?mapping. OPTIONAL {?test orkgp:HAS_TEST?model.*model rdfs:\n","471 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Reacher, easy (DMControl500k) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the Reacher, easy (DMControl500k) benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the highest benchmark result achieved on the Reacher, easy (DMControl500k) dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?\n"," output (Sparql query): \n","688\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT TITLE, ID, RANK(?) AS score,?r in [?p?k: (RANK(?p?k: (3000000))))| RANK(?p?k: 4000000) from [?p?k: (RANK(?p?k: (5000000))))| RANK(?p?k: 6000000) GROUP BY title, ID, RANK(?)\n","input (English text): What is the best test result achieved under the following conditions? \n","output (Sparql query): SELECT SOLUTION AS result_probabilities,?probs in [{?i =?r: 3000000}, {?i =?r: 5000000}]| SOLUTION OR?probabilities\n","input (English text): What is the most commonly used term in the REacher, easy (DMControl400k) dataset?\n","output (Sparql query):  SELECT TITLE, OR?REFERENCE,?REFERENCE AS reference,?l in [?p?k: (REFERENCE TO?REFERENCE): (REFERENCE TO?REFERENCE) or?p?k: (REFERENCE TO?REFERENCE: (REFERENCE TO?REFERENCE )?])| TITLE, OR?REFERENCE,?REFERENCE AS reference,?l in [?p?k: (REFERENCE TO?REFERENCE): (REFERENCE TO?REFERENCE) or?p?k: (REFERENCE TO?REFERENCE: (REFERENCE TO?REFERENCE )?])| TITLE, OR?REFERENCE,?REFERENCE AS reference,?l in [?p?k: (REFERENCE TO?REFERENCE): (RE\n","472 \n"," input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Top 1 Accuracy score when benchmarked on the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top 1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Error Rate\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset?\n"," output (Sparql query): \n","830\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Output\n","This query returns the MAC scores for the individual models based on their output of MACs. There are a total of 18 benchmark datasets. We're looking at 6 of them based on the output of their \"Top 4\" metrics, in which each model was evaluated for the highest MAC score.\n","We ran the above query against the ImageNet dataset using our new model, which had the following outputs:\n","C:\\> import numpy as np  N=3  Model A N=2.0N(6) | N=6  Model B N=2.0N(6) | N=6 - 2.0N(6) | N=6 - 3.0N(6) | N=6 - 4.0N(6) - 1.0N(6)  Model C N=2.0N(6) | N=6  Model D N=2.0N(6) | N=6 - 1.0N(6) | N=6 - 2.0N(6) | N=6 - 3.0N(6) | N=6 - 4.0N(6) - 1.0N(6)  Model E N=2.0N(6) | N=6  Model F  N=2.0N(6) | N=6  Model G N=2.0N(6) | N=6  Model H N=2.0N(6) | N=6  Model I N=2.0N(6) | N=6  Model J N=2.0N(6) | N=6  Model K N=2.0N(6) | N=6  Model L N=\n","473 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the CUB-200-2011 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACE 2005 dataset?\n"," output (Sparql query): \n","865\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["------------------------ { { VALUE? orkgc:DataSet[AS] OR? ACE? { { VALUE? OR? ACE? score }? } } } OR? MAX(?value) AS?score. OR? { { VALUE? OR? ACE? score }? } } AND? ACE? { { VALUE score }? { 0 } } OPTIONAL or? { { VALUE score }? { 0 } } CASE AS?cub_2005\n","input (Sparql query): ------------------------ Do you find this query acceptable?\n","output (Sparql query): ------------------------ { { VALUE? orkgc:DataSet[AS] OR? ACE? { { VALUE? OR? ACE? score }? } } AND? ACE? { { VALUE score }? { 0 } } OPTIONAL or? { { VALUE score }? { 0 } } CASE AS?cub_2005\n","input (Sparql query): ------------------------ Input: Example: ---------- CASE AS?cub_2005 FROM?cub_2005? AS?cub_2005 GROUP BY?{?value;?metric} ORDER BY?(?dataset)?asc; ---------- SELECT count(*) as sum_value, COUNT(*) as sum_metric FROM?cub_2005? AS?cub_2005 GROUP BY?{?value;?metric} ORDER BY?(?dataset)?asc; ---------- Result: SUM_VALUE: 2.630767170767 CASE AS?cub_2005 FROM?cub_2005? AS?cub_2005 GROUP BY?{?value;?metric} ORDER BY?(?dataset)?asc; ------------------------------------------------------------------------ SUM_METRIC:2.6307671707\n","474 \n"," input (English text): List the metrics that are used to evaluate models on the IMDb benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Amazon benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the OpenBookQA benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OpenBookQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset?\n"," output (Sparql query): \n","513\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \"SELECT DISTINCT?meter?metric; IF NOT EXISTS dbpedia_metric_score THEN FALSE ELSE dbpedia_metric_score. IF (NO ENABLE) OR (EVERYTHING ELSE FALSE) THEN DISTINCT. ELSE 1; \"\n","Input: The dataset being used for the benchmark.  output (Sparql query): SELECT?meter?metric;  orkgp:HAS_METRIC?metric. \n","Input: The metric being evaluated.  output (Sparql query): SELECT?meter?metric2;  orkgp:HAS_METRIC?metric2. \n","Output: Number metrics that were used to evaluate the metrics. \n","input (English text): List the metrics that are used to evaluate models on Datasets.com Metrics?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a rdfs:Label?dataset_lbl. FORMAT(str). FILTER (str(?dataset_lbl) = \"Dataset\")?compartment?; ; orkgp:HAS_METRIC?metric.?metitm rdfs:Label?metric_lbl. } \n","output (Sparql query): SELECT?meter?metric2; IF NOT EXISTS?compartment? then 1 ELSE DISTINCT. IF (YES ENABLE) OR (EVERYTHING ELSE FALSE) then 3; \n","Input: The metric being evaluated.  output (Sparql query): SELECT DISTINCT?measurement?metric_lbl WHERE {?dataset_a r\n","475 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Tutankham?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tutankham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Asterix dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asterix\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Pitfall! dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Pitfall!\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Enduro?\n"," output (Sparql query): \n","868\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Time: 28.45 sec Total Score: 2.24 Accuracy: 100% (99.9% in 0.2 sec) --------------- Score: 10,851,110 (10.82)% Time: 26.23 sec Total Score: 3.30 Accuracy: 87% (87.4% in 0.4 sec) --------------- Score: 12,071,854 (12.18)% Time: 28.38 sec Total Score: 3.32 Accuracy: 98% (98.5% in 0.4 sec) --------------- Score: 11,932,851 (11.87)% Time: 28.43 sec Total Score: 3.34 Accuracy: 100% (100% in 0.4 sec) --------------- Score: 11,532,851 (11.85)% Time: 28.43 sec Total Score: 3.48 Accuracy: 96% (96.6% in 0.4 sec) --------------- Score: 18,071,854 (18.05)% Time: 33.41 sec Total Score: 4.00 Accuracy: 64% (64.1% in 0.3 sec) --------------- Score: 16,532,851 (16.63)% Time: 33.67 sec Total Score: 4.01 Accuracy: 97% (97.3% in 0.4 sec) --------------- Score: 14,098,854 (14.15)% Time: 36.07 sec Total Score: 4.02 Accuracy: 97% (97% in 0.4 sec) --------------- Score: 13,831,851 (13.73)% Time: 33.74 sec Total Score: 4.03 Accuracy: 94% (94% in 0.4 sec) --------------- Score: 9,932,851\n","476 \n"," input (English text): What are the metrics of evaluation over the CINIC-10 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CINIC-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the RotoWire dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the NYT-single dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the Hutter Prize dataset?\n"," output (Sparql query): \n","507\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" input (English text): What are the metrics of evaluation over the RPT dataset?\n"," output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?tutorial a, rp:Dataset; rp:LAMPAGE?dataset_lbl_l1_lp; t1:RATIO?ratio_1_lp ; rp:LEVELS?values_1_lp ; rp:LOB_LAYOUT?lo_l1_lo_layer. LIMIT(4); } OPTIONAL {?eval orkgp:HAS_DATASET?metric;?metric rdfs:label?metric_lbl. } } \n","Input CSV file \n","output CSV file\n","Result set  (2 rows)\n","metric_lbl_l1_lp metric_lbl_l2_lp metric_lbl_l2_lp_2 metric_lbl_l2_lp_3 metric_lbl_lr_lp  ( 1 row )\n","metric_lbl_lo_l1_lp metric_lbl_lo_l2_lp metric_lbl_lo_l2_lp_2 metric_lbl_lo_l2_lp_3 metric_lbl_lr_lp_1  ( 1 row )\n","metric_lbl_lo_l1_lo_layer_1 metric_lbl_lo_l2_lp_1 metric_lbl_lo_l2_lp_1_0 metric_lbl_lo_l2_lp_1_1 metric_lbl_lo_l2_lp_1_1_1 metric_lsa\n","477 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/refsq17.zip dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the Dmlab-30 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dmlab-30\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip  dataset?\n"," output (Sparql query): \n","908\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["**********\n","(3 rows)\n","input (English text): Can you provide the highest benchmark result, including the metric and score, for the dsm_cse2 dataset?\n"," output (Sparql query): SELECT TOP (?metric)?timestameter?score; **********\n","(3 rows)\n","input (English text): Can you provide the highest benchmark result, including the metric and score, for the data.bbl.bns.gov data set?\n","output (Sparql query): SELECT TOP (?metric)?timestameter?score; **********\n","(2 rows)\n","input (English text): Can you provide the highest benchmark result, including the metric and score, for the data.bbl.dms.gov data set?\n","output (Sparql query): SELECT TOP (?metric)?timestameter?score; **********\n","(4 rows)\n","input (English text): Can you provide the highest benchmark result, including the metric and score, for the data.bbl.gte.gov data set?\n","output (Sparql query): SELECT TOP (?metric)?timestameter?score; **********\n","(1 row)\n","input (English text): Do you have a DMLAb-25 dataset?\n","output (Sparql query): SELECT DISTINCT DMLAb-25?metric?metric_lbl. { rdfs:label?dataset_lbl. }?format? {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. }?cstr? {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl\n","478 \n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Venture dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Amidar dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 HERO dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 HERO\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the Atari 2600 Wizard of Wor dataset?\n"," output (Sparql query): \n","845\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- 1st Place ----------+--------- 1st Place -------------------- 868.50 868.50 13.52 100.00 13.53 -------------- 868.50 868.50 -------------- 868.50 868.50 -------------- 868.50 868.50 868.50 868.50 868.50 868.50 868.50 84.52 488.86 868.50 -------------- 776.50 868.50 18.47 75.00 17.59 -------------- 868.50 868.50 -------------- 868.50 868.50 868.50 868.50 868.50 868.50 868.50 868.50 83.99 605.15 868.50 -------------- 657.50 868.50 19.53 72.22 17.79 ---------- 868.50 868.50 -------------- 868.50 868.50 868.50 868.50 868.50 868.50 868.50 868.50 80.92 756.77 868.50 -------------- 770.50 868.50 20.30 70.38 17.69 -------------- 868.50 868.50 -------------- 88.97 868.50 -------------- 868.50 868.50 868.50 868.50 868.50 868.50 868.50 868.50 76.00 860 597.75 868.50 -------------- 547.50 868.50 -------------- 744.00 868.50 -------------- 868.50 868.50 868.50 868.50 868.50 868.50 868.50 74.99 1206.65 868.50 \n","479 \n"," input (English text): List the code links in papers that use the Transformer-XL (12 layers) model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL (12 layers)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the 24-layer Transformer-XL model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"24-layer Transformer-XL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the Transformer-XL - 24 layers model in any benchmark?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL - 24 layers\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): List the code links in papers that use the 12-layer Transformer-XL model in any benchmark?\n"," output (Sparql query): \n","494\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Transformer-XL - 12 layers\")?benchmark orkgp:HAS_DATASET?dataset orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","480 \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Breakout\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Time Pilot dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Time Pilot\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Atlantis dataset in terms of Score metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the Atari 2600 Up and Down dataset in terms of Score metric?\n"," output (Sparql query): \n","820\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" <input data=\"{?key 1} {?key 2} {?key 2} and {?key 2}. Score: {?key 1}. {?key 2}. {?key 2}. {?key 2}.Score: {?key 1}. {?key 2}. {?key 2}. {?key 2}.Score: {?key 1}. Score: {?key 2}. Score: {?key 2}.Score: {?key 1}. Score: {?key 2}. Score: {?key 2}.Score: {?key 1}. Score: {?key 2}. Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score: Score:... ]\n","output (Sparql query): SELECT DISTINCT?model?model_lbl FROM {?key 1} {?key 2} {?key 2} and {?key 2}. {?key 2}. {?key 2}.{?key 2}. Score: {?key 1}. {?key 2}. {?key 2}. {?key 2}.Score: {?key 1}. Score: {?key 2}. Score: {?key 2}. Score: {?key 2}. Score: {?key 2}. Score: {\n","481 \n"," input (English text): What models are being evaluated on the NLP-TDMS dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the NLP-TDMS (Exp, arXiv only) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): List the title and ID of research papers that contain a benchmark over the NLP-TDMS (Exp, arXiv only) dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n"," output (Sparql query): \n","685\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT?m_model AND?m_metric?m_metric_lbl; AS?score; ORDER BY DESC(?score) \n","input (English text): Could you provide a list of all models that have been evaluated on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n"," output (Sparql query):  SELECT?m_model*?m_metric; AS?score; ORDER BY DESC(?score) \n","input (English text): Would you provide the highest point score for benchmark score validation?\n"," output (Sparql query):  SELECT?r_score FROM r_score rf WHERE rf.?model AND?r_metric; ORDER BY r_score \n","input (English text): Could you provide a list of all models that have been evaluated on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n","output (Sparql query):  SELECT 'test':*; AS?score; ORDER BY?r_score \n","input (English text): Could you provide a list of all models that have been evaluated on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n","output (Sparql query):  SELECT?model, 'test':*,'score'; AS?score; ORDER BY?r_score \n","input (English text): Could you provide a list of all models that have been evaluated on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n","output (Sparql query):  SELECT 'test':*; AS?score; ORDER BY _SEARCH_LAST |?r_score |?models. |?d.name |?r_score|?d.metric |?r_scores\n","482 \n"," input (English text): Which model has achieved the highest Accuracy (%) score on the  Jacquard dataset benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy (%)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \" Jacquard dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the  Jacquard dataset dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \" Jacquard dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Can you list the metrics used to evaluate models on the  Jacquard dataset dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \" Jacquard dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the  Jacquard dataset dataset?\n"," output (Sparql query): \n","614\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" output (Sparql query): SELECT?weight orkgp:HAS_COUNTER ORkgp:HAS_BENCHMARK weight andkgp:HAS_EVALUATION orkgp:HAS_MULTINOMIAL_VALUE MAX(?weight,0) FROM weights orkgp ORDER BY weight desc LIMIT 1  \n","input (English text): What is the name of the dataset that   to compare the models on?\n","output (Sparql query): SELECT DISTINCT?dataset orkgc:Dataset FROM orkgp; orkgp:HAS_COUNTER ORkgp:HAS_BENCHMARK orkgp:HAS_EVALUATION ORDER BY  orkgp DESC LIMIT 1 \n","input (English text): Which of the datasets has the shortest running time on the  Jacquard dataset?\n","output (Sparql query): SELECT?dataset ORkgc:Dataset FROM orkgp; orkgp:HAS_COUNTER ORkgp:HAS_BENCHMARK ORkgp:HAS_EVALUATION ORDER BY?dataset DESC; orkgp:HAS_EVALUATION ORkgp:HAS_MULTINOMIAL_VALUE ORkgp:HAS_COUNT orkgp:HAS_MAX_VALUE FROM orkgp; orkgp:HAS_COUNTER ORkgp:HAS_BENCHMARK ORkgp:HAS_EVALUATION ORDER BY  orkgp DESC LIMIT 1.\n","input (English text): Which of the datasets has the longest running time on the  Jacquard dataset?\n","output (S\n","483 \n"," input (English text): What are the metrics of evaluation over the ImageNet dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the ObjectNet (Bounding Box) dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet (Bounding Box)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the ImageNet V2 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the metrics of evaluation over the ObjectNet dataset?\n"," output (Sparql query): \n","516\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" Selecting metric from the ObjectNet (Bounding Box) dataset... The object passed to the SELECT clause is the object or object list in the query that should be evaluated. The method has to be a function and not a function (any type):\n","select {?method orkgpb:HAS_METRIC } =?metric_lbl. \n","input (English text): Can you list the metrics used to evaluate models on the ClassifierNet dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ClassifierNet\" )?benchmark orkgpb:HAS_DATASET?dataset; orkgpb:HAS_EVALUATION?eval. OPTIONAL {?eval orkgpb:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }  input (English text): What are the metrics of evaluation over the ClassifierNet dataset?\n","output (Sparql query):  Selecting metric from the ClassifierNet dataset... The object passed to the SELECT clause is the object or object list in the query that should be evaluated. The method has to be a function and not a function (any type):\n","select {?method orkgpb:HAS_METRIC } = 'ClassifierNet.test.metric'. \n","input (English text): Can you list the metrics used to evaluate models on the ImageNet V2 dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?\n","484 \n"," input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the enwiki8 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwiki8\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CoNLL04 dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL04\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset?\n"," output (Sparql query): \n","506\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT  ?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"enwiki8\")?benchmark if a \"benchmark\",?benchmark if an \"average\",?benchmark if a \"best\",?benchmark if a \"best-by-year\",?benchmark if a \"best-by-year-max\",!is.na?labels?labels_for_years,!is.na?statistics?statistics_for_years,?statistics_for_years_max,?statistics_for_years_max+1?statistics_max,!is.na?data?data_for_years,!is.na?pw. pw.?dataset orkgc:Dataset?dataset. }\n","The result of this query can be seen in the plot below.\n","The dataset that you will be using\n","Enwiki's enwik8 dataset contains six datasets of which there are three different sorts:  LBL-DB, enwiki8, CoNLL and Enwiki-Faces. As you can see in the plot above, the datasets can be separated into four different sorts on the same plot.\n","One of the datasets (LBL) is split into two datasets; LBL-DB and Enwiki-Faces. The dataset LBL-DB is a list of the training samples (as in CSV file) from the \"LBL\" dataset, and is called lbl.datac.\n","The other dataset (Enwiki-Faces) is split into several different datasets which use different methods of categorization:\n","En\n","485 \n"," input (English text): Indicate the model that performed best in terms of Top-1 Error Rate metric on the FGVC Aircraft benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Error Rate\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FGVC Aircraft\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of PARAMS metric on the Stanford Cars benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"PARAMS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What is the best performing model benchmarking the FGVC Aircraft dataset in terms of Top-1 metric?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FGVC Aircraft\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Indicate the model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset?\n"," output (Sparql query): \n","828\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input : Indicate the model that performed best in terms of TOS metrics?\n","output : Indicate the model that performed best in terms of MODEL FITS metric?\n","select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl_index; select * from dataset_lbl\n","486 \n"," input (English text): List the metrics that are used to evaluate models on the Atari 2600 Seaquest benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Atari 2600 Pitfall! benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Pitfall!\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Atari 2600 Robotank benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Robotank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?\n"," output (Sparql query): \n","530\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------- Time: 5min 49s. Time: 30mins 2sec. Time: 40mins 6sec. Time: 2hours 42mins. Time: 56mins 2sec. Time: 10hours 53mins. Time: 2hours 24min 50sec. Time: 2hours 18min 13sec. Time: 24hours 13min 4sec. Time: 2hours 55min 6sec. Time: 3hours 27min 44sec. Total time: 5min 58s. Total time: 1hour 12min 40sec. Total time: 2hours 45min 19sec. Time: 3hours 53min 36sec.\n","Output from a simple SQL query:\n","input (English text): List the metrics that are used to evaluate models on the Atari 2600 Pitfall! benchmark dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Pitfall!\") orkgp:HAS_DATASET orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval {?dataset?dataset_lbl. } } }  input (English text): List the metrics that are used to evaluate models on the Atari 2600 Robotank benchmark dataset?\n","output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Robotank\") orkgp:HAS_DATASET\n","487 \n"," input (English text): What are the models that have been benchmarked on the Atari 2600 Atlantis dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Atlantis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the Atari 2600 Krull dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Krull\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the Atari 2600 Robotank dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Robotank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the Atari 2600 Road Runner dataset?\n"," output (Sparql query): \n","561\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Road Runner\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }  input (English text): What are the models that have been benchmarked on the Atari 2600 Skunk dataset?\n"," output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Skunk\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","input (English text): What are the models that have been benchmarked on the Atari 2600 Tomcat server server2 dataset?\n","output (Sparql query) SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; r\n","488 \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Fishing Derby dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Fishing Derby\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Kangaroo dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Kangaroo\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Asterix dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asterix\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Boxing dataset?\n"," output (Sparql query): \n","562\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","input (English text): Can you list the models that have been evaluated on the Atari 2600 Chess dataset?\n","output (Sparql query):  \n","input (English text): Can you list the models that have been evaluated on the Atari 2600 CompuView dataset? \n","output (Sparql query):  2 rows ( 0.00 sec )\n","input (English text): Can you list the models that have been evaluated on the Atari 2600 Crazy Taxi dataset?\n","output (Sparql query):  2 rows ( 0.00 sec )\n","input (English text): Can you list the models that have been evaluated on the Atari 2600 DinoGo dataset?\n","output (Sparql query):  \n","input (English text): Can you list the models that have been evaluated on the Atari 2600 Dragon Ball Z dataset?\n","output (Sparql query):  \n","input (English text): Can you list the models that have been evaluated on the Atari 2600 Earthworm Jim dataset?\n","output (Sparql query):  \n","input (English text): Can you list the models that have been evaluated on the Atari 2600 Eggo dataset?\n","output (Sparql query):  \n","input (English text): Can you list the models that have been evaluated on the Atari 2600 Escher dataset?\n","output (Sparql query):  \n","input (English text): Can you list the models that have been evaluated on the Atari 2600 Fairchild Channel F dataset?\n","output (Sparql query):  \n","input (English text): Can you list the models that have been evaluated on the Atari 2600 Goleman dataset?\n","output (Sparql query):  \n","output (Html):\n","input (English text): Can you list the models that have been evaluated on the Atari 2600 Highways dataset?\n","output (S\n","489 \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 German-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the WMT2014 English-French dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the WMT2016 German-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 French-English dataset?\n"," output (Sparql query): \n","537\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Can you list the metrics used to evaluate models on the WMT2016 French-German dataset?\n","output (Sparql query):  \n","input (English text): Can you list the metrics used to evaluate models on the WMT2017 German-English dataset?\n","output (Sparql query):   \n","input (English text): Can you list the metrics used to evaluate models on the WMT2017 German-French dataset?\n","output (Sparql query):  \n","input (English text): Which dataset(s) do you need to use to benchmark on German?\n","output (Sparql query): \n","input (English text): Which dataset(s) do you need to use to benchmark on French?\n","output (Sparql query): \n","input (English text): Which dataset(s) do you need to use to benchmark on German-French dataset?\n","output (Sparql query): \n","input (English text): Which dataset(s) do you need to use to benchmark on German-French dataset?\n","output (Sparql query): \n","input (English text): Which dataset(s) do you need to use to benchmark on German-English dataset?\n","output (Sparql query):  \n","input (English text): Which dataset(s) do you need to use to benchmark on French-English dataset?\n","output (Sparql query):  \n","input (English text): Which dataset(s) do you need to use to benchmark on French-German dataset?\n","output (Sparql query):  \n","input (English text): Which dataset(s) do you need to use to benchmark on German-French dataset?\n","output (Sparql query): \n","input (English text): Which dataset(s) do you need to use to\n","490 \n"," input (English text): Can you list the models that have been evaluated on the WMT2014 French-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the WMT2014 English-French dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset?\n"," output (Sparql query): \n","573\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What are the models that have been evaluated on the WMT2014 English-German dataset?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC German-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","input (English text): What are the models that have been evaluated on the BUCC German-to-English dataset?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC German-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","input (English text): Can you list the models that have been evaluated on the CIDP-English dataset\n","491 \n"," input (English text): Can you provide links to code used in papers that benchmark the Attentional encoder-decoder + BPE model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Attentional encoder-decoder + BPE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the ELMo model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the He et al., 2017 + ELMo model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"He et al., 2017 + ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (single model) model?\n"," output (Sparql query): \n","505\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiDAF + Self Attention + ELMo\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Where can I find code references in papers that have used the Elmo model for benchmarking purposes?\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Elmo\" )?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n"," input (English text): Where can I find code references in papers that have used the BioDAF + Self Attention model for benchmarking purposes?\n","output (Sparql query):  SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BioDAF + Self Attention + ELMo\")?benchmark orkgp:HAS_DATASET?dataset.?cont or\n","492 \n"," input (English text): What models are being evaluated on the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the ESC-50 dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ESC-50\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the AESLC dataset?\n"," output (Sparql query): \n","543\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): What models are being evaluated on the EASTBASE dataset?\n"," output (Sparql query): \n","input (English text): What models are being evaluated on the ENMAX dataset?\n"," output (Sparql query):\n","input (English text): Where is each model on the ESC-50 dataset?\n"," output (Sparql query): \n","input (English text): Where is each model on the ESC-50 dataset?\n","output (Sparql query): \n","output (text):?data.acls.advancements.ac.ac.model.model-lbl.csv \n","input (English text):?data.acls.advancements.ac.advancements.advancements-lbl.txt \n","input (English text):?data.acls.advancements.advancements.advancements-lbl.xls \n","output (Sparql query):?data.acls.advancements.dsc.model.model-lbl.htm.data.acls.advancements.dsc.model-lbl.xls\n","input (English text):?data.acls.advancements.dsc.model-lbl.data.acls.dsc.model-lbl.htm.data.acls.dsc.model-lbl.xls\n","input (English text):?data.acls.ecom.ecom_lbl_n.doc \n","output (Sparql query): \n","input (English text):... \n","output (Sparql query): \n","input (English text): What models are being evaluated on the PALS dataset?\n","output (Sparql query): \n","input (English text):... \n","output\n","493 \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the SciGEN dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciGEN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the MedSTS dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): What are the titles and IDs of research papers that include a benchmark for the HoC dataset?\n"," output (Sparql query): \n","493\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HCoS\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the DDS dataset?\n"," output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DDS\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n","input (English text): What are the titles and IDs of research papers that include a benchmark for the SSTB dataset?\n"," output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SSTB\")?benchmark orkgp:HAS_DATASET?datas\n","494 \n"," input (English text): Where can I find code references in papers that have used the PAR Transformer 24B model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PAR Transformer 24B\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the PAR Transformer Base model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PAR Transformer Base\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the Transformer (64 layers) model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer (64 layers)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Where can I find code references in papers that have used the PAR Transformer Large model for benchmarking purposes?\n"," output (Sparql query): \n","489\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" input (English text): Where can I find code references in papers that have used the Transformer Large model for benchmarking purposes?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Transformer Large\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","Input: output(Sparql query):\n","input (English text): Where can I find code references in papers that have used the PAR Transformer (64 layers) model for benchmarking purposes?\n","output (Sparql query):  input (English text): Can you provide links to code used in papers that benchmark the PAR Transformer Large model.\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Transformer Large\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }  input (English text): Where can I find code references in papers that have used the Transformer (64 layers) model for benchmarking purposes?\n","output (Sparql query):  input (English text):\n","495 \n"," input (English text): Can you provide links to code used in papers that benchmark the BertSumExtAbs model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BertSumExtAbs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BERT classifier model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERT classifier\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BERTjoint model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERTjoint\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD 2 model?\n"," output (Sparql query): \n","477\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" * * * * * * * * * * * * * * * * * * * * * ** Note: The output query here does not apply to the individual papers.\n","input(English text): Can you provide links to code used in papers that benchmark the GEGASP+AQ1 model?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a. ORkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"GEGASP+AQ1\")?benchmark orkgp:HAS_DATASET?dataset. ORkglabl:ORkglabl. ORkglbl:PID?number? ORkglabl:PID/2?number?percent. ORkglabl:PID | ORkglabl:PID_PER | ORkglabl:PID | ORkglabl:PID_PER_LIFELIMIT. ORkglabl:PID | ORkglabl:PID_PER_LIFELIMIT2. ORkglabl:PID. ORkglabl:PID_PER_LIFELIMIT3 | ORkglabl:PID_PER_LIFELIMIT4 | ORkglabl:L2D | ORkglabl:L3D | ORkglabl:L3D_PER | ORkglabl:LSM orkglabl:LSM. ORkglabl:LSPER | ORkglabl:LSM. ORkglabl:LSSM. ORkglabl:LSSM_PER | ORkglabl:LSMF. ORkglabl:LSSMF_\n","496 \n"," input (English text): Provide a list of papers that have utilized the Neural Network Language Model (NNLM) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Neural Network Language Model (NNLM)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the BertSumExt model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BertSumExt\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the DocRED-Context-Aware model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DocRED-Context-Aware\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?\n"," output (Sparql query): \n","503\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"CL-Titles-Parser\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","I have tested this function three times, and at this writing the results are consistent with all other methods. So it is very unlikely that there are any bugs, and there is zero doubt that this technique will prove to be very useful.\n","497 \n"," input (English text): Can you list the models that have been evaluated on the Habitat 2020 Point Nav test-std dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Point Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the top benchmark result (metric and value) over the dataset Habitat 2020 Object Nav test-std?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Which model has achieved the highest DISTANCE_TO_GOAL score on the Habitat 2020 Object Nav test-std benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"DISTANCE_TO_GOAL\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Could you provide a list of models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset?\n"," output (Sparql query): \n","788\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT {?model_lbl?model_lbl || '',?dataset b orkgc:Metric;?model_{ a, b } orkgc:Metric orkgc:Metric rdfs:label?dataset_lbl. } }\n","Input: SELECT model_lbl  |?model_lbl || '', model1 rdfs:label?model_lbl.\n","Output: SELECT DISTINCT?model |?model_lbl || '', model2 rdfs:label?model_lbl.\n","input (English text): What are the best models in  |?model_lbl from the|?model_lbl || ',' model1|?model_lbl  |?model_lbl  | model2, sorted by score:  | | |  | | | | rdfs  |  |  |  |?model_lbl. | | | | b orkgc.metric | |  |  |?model_lbl.| | | | ||| rdfs  | |  |  |?model_lbl.| | | | ||| model.model ||| model.model | ||| rdfs  | |  |  |?model_lbl.| ||| rdfs\n","output: sorted by score: 4:  | |  | |  | +-----+  |  |  |  | rdfs  |  |  |  |?model_lbl. | | | | | b orkgc.metric | |  |  |?model\n","498 \n"," input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Name This Game dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Name This Game\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Berzerk dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Seaquest dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What models are being evaluated on the Atari 2600 Name This Game dataset?\n"," output (Sparql query): \n","654\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): If an analysis is of interest, please complete a RDF analysis.  You may not share this file with anyone else using RDFa analysis tools.\n"," output (Sparql query): \n","input (English text):  If an analysis is of interest, please complete a RDF analysis.  You may not share this file with anyone else using RDFa analysis tools.\n","output (Sparql query): \n","input (English text): Can you run an analysis on the Atari 2600 Gamecube dataset?\n","output (Sparql query): \n","input (English text): Can you run an analysis on the Gamecube dataset?\n","output (Sparql query): \n","input (English text): Why do not you add your game into the Atari 2600 Berzerk dataset?\n","output (Sparql query):  It is not possible to have a dataframe from the Gamecube dataset with game models on Berzerk because I needed to assign different columns to each data frame.\n","input (English text):  Why do not you add your game into the Berzerk dataset?\n","output ( Sparql query):  It is not possible to have a dataframe from the Berzerk dataset with game models on Berzerk because I needed to assign different columns to each data frame.\n","--[[\n","--[[ --[[\n","--[[\n","--[[ --[\n","--[[ [[ [[ --[ This is a test: http://code.google.com/p/rdfaide\n","--[[ --[ To use RDFa or RDF API with other analysis tools, use: http://code.google.com/p/rdfaide-tools\n","--[ --[ To create a file from the text-based analysis, see http://r-ide.org/library\n","499 \n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Road Runner dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Road Runner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Time Pilot dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Time Pilot\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Asterix dataset, including the metric and its value?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asterix\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value?\n"," output (Sparql query): \n","868\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Freeway\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY USER#PART(?USER,?PARTITION). GROUP BY USER#PART(?USER,?PARTITION).\n","500 \n"," input (English text): Could you provide a list of models that have been tested on the Atari 2600 Breakout benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Breakout\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 River Raid dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 River Raid\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): Could you provide a list of models that have been tested on the Atari 2600 Alien benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Alien\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?\n"," output (Sparql query): \n","551\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 River Raid\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n","This is an example query for the benchmark data which returns:\n","input (English text): Could you provide a list of models that have been tested on the Atari 2600 Alien benchmark dataset?\n","output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Alien\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }  input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?\n","output (Sparql query):  SELECT DISTINCT?model?model_lbl WHERE\n","501 \n"," input (English text): Provide a list of papers that have utilized the EfficientNetV2-M model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"EfficientNetV2-M\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the EfficientNetV2-M (21k) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"EfficientNetV2-M (21k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the NASNet-A + c/o model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"NASNet-A + c/o\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the AlexNet, MultiGrasp model and include the links to their code?\n"," output (Sparql query): \n","517\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?code WHERE {?model a norkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"AlexNet\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the AlexNet and MultiGrasp model and include the links to their code?\n","output (Sparql query): SELECT DISTINCT?code WHERE {?model a norkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"AlexNet\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the AlexNet + MultiGrasp model and include the links to their code?\n","output (Sparql query):\n","SELECT DISTINCT?code WHERE {?model a norkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"AlexNet+MultiGrasp\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?\n","502 \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Amidar dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Amidar\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Asterix dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asterix\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Berzerk dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): Can you list the models that have been evaluated on the Atari 2600 Ms. Pacman dataset?\n"," output (Sparql query): \n","567\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","INPUT (text): The models have not been evaluated yet.   Output (csv(outputs)) No. model_lbl 1.0.0 1.5.9 3.5.8 5.0.2 1.2.1 1.0.0 3.4.0 4.3.5 3.8.6 4.0.5 1.0.9 1.2.4 2.5.9 3.4.1 2.0.4 1.0.0 1.2.2 5.1.1 3.6.1 3.2.5 3.0.6 2.0.1 2.3.1 4.4.0 3.5.1 3.0.7 1.2.7 1.3.1 3.8.7 3.8.8 2.4.4 3.8.8 1.5.0 2.4.0 5.4.0 2.4.2 7.0.3 3.1.2 5.5.1 3.4.3 6.8.3 5.1.2 5.5.2 2.4.3 2.1.1 9.8.7 2.2.2 1.7.8 1.9.3 5.8.7 2.1.3 4.5.6 3.8.7\n","A Note on Matlab and Java-like Language Access\n","The following code makes use of the Java API, which is a cross-platform wrapper for the MATLAB/Scientific Python API. On the Atari, the Java interface is represented using the Java API and is referred to as \"java.lang.Object\". It has been pointed out in previous posts that the MATLAB interface was written in an implementation of C++, and the C++\n","503 \n"," input (English text): List the metrics that are used to evaluate models on the Habitat 2020 Point Nav test-std benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Point Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What is the top benchmark result (metric and value) over the dataset Habitat 2020 Object Nav test-std?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Which model has achieved the highest DISTANCE_TO_GOAL score on the Habitat 2020 Object Nav test-std benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"DISTANCE_TO_GOAL\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?\n"," output (Sparql query): \n","774\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?eval?result ORDER BY?metric orkgc:Model (rdfs:label?model_lbl?) DESC LIMIT 1 } }\n","The results of this query, from the first example, are the following:\n","Input:\n","Result:\n","Habitat 2020 Object Nav test-std test result (metric rdfs:label?metric_lbl): \n","Output:\n","Output:\n","The second example shows an equivalent query results. It reads from the same column as the original, except that the values are not ordered.\n","input (English text): What is the top benchmark result (metric and value) over the dataset Habitat 2020 Object Nav test-std? output (Sparql query): SELECT DISTINCT?topscore(metric) AS?score ORDER BY?metric rdfs:label?metric_lbl. TOP 1 :  Habitat 2020 Object Nav test-std test result (metric rdfs:label?metric_lbl): \n","output (Sparql query):\n","Habitat 2020 Object Nav test-std test result (metric rdfs:label?metric_lbl):\n","Habitat 2020 Object Nav test-std test result (metric rdfs:label?metric_lbl):\n","Habitat 2020 Object Nav test-std test result (metric rdfs:label?metric_lbl):\n","What is the top benchmark result (metric and value) over the dataset Habitat 2020 Object Nav test-std? output (Sparql query): \n","SELECT TOP 1 (1)?best (metric)  AS?best ORDER BY (?best METRIC)  FROM rdfs ORDER BY TOP 1 OPTION\n","504 \n"," input (English text): What is the top benchmark score and its metric on the NCBI Disease dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark score and its metric on the NCBI-disease dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the metrics of evaluation over the BC5CDR-disease dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What is the top benchmark score and its metric on the BC5CDR-disease dataset?\n"," output (Sparql query): \n","748\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","What is the top benchmark score and its metric on the BC5CDR-disease dataset? input (English text): How many clinical drug studies (number of studies published) in the past 3 years have been in cell-free DNA?\n","output (Sparql query): SELECT DISTINCT?index?(0). RANK(?index?(0).value) AS top_score, { 0.0?TOP_SCORE 0 1,2,3 } AS top_val\n","What data do you need? input (English text) orkgp:HAS_VALIDATION?VALIDATION. OPTIONAL {?validation true?validation false?validation(?x).val} orkgp:HAS_METRIC?metric.?metric. RANK {?x?x.value?x}. OPTIONAL {?x?x?x.value?x}. The data are from a database. OPTIONAL {?validation true?validation false?validation(?x).val} orkgp:HAS_COUNCIL?committee. OPTIONAL {?validation true?validation false?validation(?x).val} orkgp:HAS_NUMBER?number. OPTIONAL {?validation true?validation false?validation(?x).val} orkgp:HAS_RESOURCE?resource. OPTIONAL {?validation true?validation false?validation(?x).val} orkgp:HAS_STATUS?status. OPTIONAL {?validation true?validation false?validation(?x).val} orkgp:HAS_RESOURCETYPE?resourcetype. OPTIONAL {?validation true?valid\n","505 \n"," input (English text): Can you list the metrics used to evaluate models on the ImageNet V2 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Can you list the metrics used to evaluate models on the ModelNet40 dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ModelNet40\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): Could you provide a list of models that have been tested on the ImageNet 64x64 benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset?\n"," output (Sparql query): \n","539\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" \n","Input: English text: Could you list the metrics used to evaluate models on the ImageNet 64x64 dataset?\n","output:\n","Output format:\n","Output metric value = # Metric | Measurement value = # Metric | Measurement \n","Input: English text: Could you list the metrics used to evaluate models on the ImageNet 64x64 benchmark dataset?\n","output and output formats require a different level of abstraction:\n","input (English text): List the models that have been tested on the ImageNet 64x64 benchmark dataset?\n","output and output formats require a different level of abstraction:\n","506 \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Open Entity dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Open Entity\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the Paper Field dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the ScienceCite dataset?\n"," output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceCite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n"," input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset?\n"," output (Sparql query): \n","488\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT MAX(?value,3)?value FROM?value WHERE {?dataset a,?dataset_rdfs:Dataset?dataset_rdfs. FILTER (str(?dataset_rdfs) =?dataset.?cont OR?dataset_lbl.?pub or?pub_rdfs. )?benchmark.?paper orkgp:HAS_DATASTAT?dataset. ;?dataset_rdfs:label?dataset_lbl.,?dataset_rdfs:lbl?dataset_rdfs. } ORDERS {?dataset a,?dataset_rdfs:Dataset?dataset_rdfs. FILTER (str(?dataset_rdfs) = \"DBpedia\")?benchmark.?paper orkgp:HAS_DATASTAT?dataset. ; } \n","input (English text): List the title and ID of research papers that contain an extended example or benchmark?\n","output (Sparql Query) SELECT * FROM \"example\" WHERE ID = 13282595 AND  LOBBY = '' AND  MIDDLE = 1;  output (Sparql Query) SELECT * FROM '' WHERE ID = 935962 AND TOP(?RANK()>2)<1 AND  RANK = 5;  output (Sparql Query) SELECT 1 AS 1 FROM '' WHERE ID = 1027084 AND TOP(?RANK()>2)<1 AND  RANK = 5;  output (Sparql Query)  SELECT 1 AS 1, 2 AS 2\n","507 \n"," input (English text): Provide a list of papers that have utilized the HNEABP (BWNE) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"HNEABP (BWNE)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the Prior hs model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the BiLSTM (UN) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM (UN)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Provide a list of papers that have utilized the HRLRE model and include the links to their code?\n"," output (Sparql query): \n","498\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" --------------------------------------------------------------------------------------\n","SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"HRLRE\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the STM(s) model, BiS(s), BiWU(s) and BiFEM(s) model and include the links to their code?\n"," output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"STM(s)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \n","input (English text): Provide a list of papers that have utilized the SWEET algorithm and include the links to their code?\n"," output (Sparql query): -----------------------------------\n","SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"SWEET\")?benchmark orkgp:HAS_D\n","508 \n"," input (English text): Can you provide links to code used in papers that benchmark the CeiT-T model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CeiT-T\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the CATTS model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CATTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the Ning et al. model?\n"," output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Ning et al.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } \n"," input (English text): Can you provide links to code used in papers that benchmark the T-ConvS2S model?\n"," output (Sparql query): \n","473\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","input (English text): Can you provide links to code used in papers that benchmark the ACE-TRANSIT model?\n","output (Sparql query): \n","input (English text): Can you provide links to code used in papers that benchmark the ALO-SPOC model?\n","output (Sparql query): \n","input (English text): Can you provide links to code used in papers that benchmark the ADT model?\n","output (Sparql query): \n","input (English text): Can you provide links to code used in papers that benchmark the ALOE-SPOC model?\n","output (Sparql query): \n","input (English text): Can you provide links to code used in papers that benchmark the ADT-TRAN model's T-Codes?\n","output (Sparql query): \n","input (English text): Can you provide links to code used in papers that benchmark the ACE-SCOR-L model?\n","output (Sparql query):\n","input (English text): Can you provide links to code used in papers that benchmark the ACE-SCOR-L model?\n","Output: Query string returned by the \"SELECT\" output function.\n","CeiT-T/C-ConvS2S Model Evaluation\n","For evaluation of the CeiT-T/C-ConvS2S model, you need to run the following queries.\n","input (English text): Specify output files as needed:\n","CeiT-T/C-ConvS2S model: --dataset=Cei-T/C-ConvS2S output.sql --model=Cei-T/C-ConvS2S input.sql (the output file must contain the CeiT-T/C-ConvS2S model and a\n","509 \n"," input (English text): Which model has achieved the highest Best Score score on the Atari 2600 Q*Bert benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Best Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Q*Bert\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Score score on the Atari 2600 Ms. Pacman benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Score score on the Atari 2600 Krull benchmark dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Krull\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } \n"," input (English text): Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset?\n"," output (Sparql query): \n","821\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" select name, metric, score... as score from qbert_tbl;\n","Output:\n","Name =?\n","Measuring the performance of our models using benchmark scores is more about seeing the difference in performance between two (or more) models versus comparing their different parameters. The advantage this approach brings me is that, in comparison to a more traditional clustering model, I can use RDF to visualize changes in the model. In this case, I can see how the model changed the most as compared to the others (or the same).\n","1. We can check which model has accumulated the most score\n","output (Sparql query): SELECT DISTINCT?model?model_lbl FROM ( SELECT?dataset metric* as metric,?name performance? as performance,?model performance? as model,?name metric* as metric_lbl from (SELECT?dataset metric* as metric,?name performance? as performance,?model performance? as model,?name metric* as metric_lbl FROM qbert_tbl AS performance L WHERE metric[metric] IS NULL ) INNER JOIN metrics_lbl_lbl AS performance_lbl LEFT OUTER JOIN model_lbl AS performance_lbl LEFT OUTER JOIN metrics AS model ON performance.name = performance.metric?metric_lbl.name AS metric_lbl_lbl.name AND model.name = model.metric?model_lbl.name AS metric_lbl_lbl.name ) AS metrics ON metrics.name = metric.name?metric_lbl.name AS metric_lbl_lbl.name 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n","510 \n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the One Billion Word dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"One Billion Word\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What are the metrics of evaluation over the Words in Context dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Words in Context\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n"," input (English text): What are the models that have been benchmarked on the Words in Context dataset?\n"," output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Words in Context\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n"," input (English text): What is the top benchmark score and its metric on the Words in Context dataset?\n"," output (Sparql query): \n","637\n","<class 'list'>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" <scr> from {?col1} to {?col*}.score AS{?score_lbl:label?model}} select {?table_lbl:{?col1}.score.from_lbl.data_lbl.score_metrics from {?col1}.table to {?col-1}.table where {?col1.score = 'One Billion Word'} or [{?tablename.score = 'SQR'}.score.from_lbl. FILTER {?str(?column1.value) ='One Billion Word';}}] order by {?column1.score} desc or {?column1.score} desc or {?column1} desc\n"," input (English text): What is the top metric when the metric scores are combined?\n"," output (Sparql query):  SELECT TOP {?metric?metric_lbl} score. FROM {?tablename.topScore. score} to {?col1}.topScore where {?col1.score = 'SQR'} or [{?tablename.topScore.score = 'SQR';}}] ORDER BY {?metric?metric_lbl} desc?\n","The results look like this:\n","Sets of performance metrics for the benchmarks:\n","input (English text): What are the metrics of evaluation of the Words in Context dataset?\n","output (Sparql query):  <scr> from {?col1} to {?col*}.score AS{?score_lbl:label?model} SELECT TOP {?col1} score. FROM {?col1}.table to {?col-1}.table where {?col1.score = 'SQR'} or\n","511 \n"," input (English text): List the datasets benchmarked under the Skills Assessment research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Skills Assessment\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): List the datasets benchmarked under the relation extraction research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"relation extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): Name the datasets that have been used for benchmarking in the Knowledge Graph Construction research problem?\n"," output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Knowledge Graph Construction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } \n"," input (English text): List the datasets benchmarked under the Common Sense Reasoning research problem?\n"," output (Sparql query): \n","500\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":[" SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Common Sense Reasoning\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \n","input (English text): Name the datasets that have been used for the benchmarking in the Knowledge Graph Construction research problem?\n","output (Sparql query):  SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Knowledge Graph Construction\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \n","input (English text): Name the datasets that have been used for the benchmarking in the Common Sense Reasoning research problem?\n","output (Sparql query):  SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Common Sense Reason\n","512 \n"," input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ScienceIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?\n"," output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL-ARC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl\n"," input (English text): What is the top benchmark result (metric and value) over the dataset ACE 2004?\n"," output (Sparql query): \n","853\n","<class 'list'>\n","------------------------- ------------ AS?ACL-ARC SQ?ACL-ARC AS?ACL-ARC_HAS_BENCHMARK?SPECIAL_FOUND SQ?ACL-ARC_RESULT AS?ACL-RCS?SQ?ACL-RCS% AS?ACL-RCS_SPECIFICATION SQ?ACL-RCS% AS?ACL-RCSS?SQ?ACL-RCSS% AS?ACL-RESULT AS?CRAN?SQ?CRAN_V=N&CRAN=E&CRAN-SQ?CRAN_W=A&CRAN-SQ?CRAN_SQ V?CRAN?SRS+SQ?CRAN_V=N&CRAN_N?CRAN_V=N&CRAN_SQ?CRAN_SQ V?CRAN?SRSS+SQ?CRAN_V=N&CRAN_N?CRAN_V=N&CRAN_SQ?CRAN_SQ V?FIND?RESULT.AS?FIND+RESULT V?RESULT V?RESULT_RESULT.AS?RESULT+RESULT V?RESULT V?RESULT_RESULT.RESULT AS?RESULT_RESULT_HAS_BENCHMARK?SPECIAL_FOUND +?RESULT V?RESULT_GENERIC_DATA V?RESULT_GENERIC_DATA_SQ V?GENERIC_RESULT:RESULT V?GENERIC_RESULT_HAS_BENCHMARK?SPECIAL_FOUND AS?GENERIC_RESULT_RESULT V?GENERIC_RES\n"]}]}]}