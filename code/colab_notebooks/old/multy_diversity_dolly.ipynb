{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"zuwLc--q7RtP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696444568638,"user_tz":-120,"elapsed":38449,"user":{"displayName":"Antonello Meloni","userId":"09336063038722863523"}},"outputId":"23972086-d043-4357-e408-637889d93d68"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n","!pip -q install accelerate>=0.12.0\n","# !pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29307,"status":"ok","timestamp":1696444607251,"user":{"displayName":"Antonello Meloni","userId":"09336063038722863523"},"user_tz":-120},"id":"YcU-KRrrF9JR","outputId":"dd720af6-2704-423b-a682-5855c635123e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/en2sparql\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd drive/MyDrive/en2sparql"]},{"cell_type":"code","source":["import torch\n","import json\n","from transformers import pipeline, AutoTokenizer\n","# from datasets import load_dataset\n","import random\n","\n","\n","dolly = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261,"referenced_widgets":["aaa18618a98d45409e957c0fa79e097a","e27dc4e7ab0f45db8d78be58c662a08e","fbc2ded40818491184c6cccb81c37257","52eef2044d1f4a428fd396063fcb123a","6901c269bab349a6ba9b29b13584128d","968e7ad6d64444908fa52ce4baac89f6","e3e5eb3fe7214ae28208e3e00a45962a","daf2d73462a4478fa0155c3762eb4890","acc9ba411efd4e9d91e461f83e7408c4","a53f3e017e5b4868acd127af7fa68767","5e8bc19428094747a3db1cb3c34e631e","15558149c087422ca59d6509a4c72e86","b35ff0f4e30b4e8ca39a093c4ae7c1ac","8c96ea55e8b84571ac1dc8480692349e","b95d47ba3c034e50b261cc28875569bc","59d09d994ce24d5e9ff5e3028ce5025f","486dc241ae1d4461b1d94a2f1b04ead8","de3bde7fbdef4a929fdfad70118fcbd6","cd2e439c80b142e4a8178bce5520c507","9daaf77b86944e3d9b623491abe3b772","d30755923bdf4929901972d768e2ab94","92d1d6b160714f1dbc18aff1f7a9eff0","13bf31df16b34c088822d2c1f5c10922","c48b048754eb42beaca95af8610d180f","e35cf57d82524947b2887e6964bbb8f9","3149701742e643d5a80f7aad432de61f","a3ce7ad845e641b7b8a7575bba200ac5","a96c9d5cb63c497e925493b4bd47d429","eb4fdaec910a4dcfaece0662f221b2c8","9f9f3aa9676d462086b31a6e482322d6","fcca61b76e4641f0a902a48d1f76866f","e782197a8a274f10b57c454ad1c990ea","b4fc8941c5eb4921ac9c8893b828b4d4","1ce3cafdcbfa4c889d89744d50347bf4","7e16b5d8b4584b71b95cb298e27aba43","b8eaaad694c84e9298fe7cd4f13814e6","8cec403d5d3747abacd962bc1974ecfc","ab249d151387453caa4e96eaf29fe988","b8e38018cf6e4d6986d2a994860899fe","226da378e2214a19bd6ad0929e3732a2","04d5f1ddb4f64dd9bcc2384556f9f59d","ae8f4baf9823401b851293e36d2b7c55","a07a1afbadaf4eeab20a714f7bbbac2c","20308ddd9f784a6fa512d6016e6b5f41","45df9fa29ae04455b56d99ef26760b34","0272433d0de54365a15e7b6f3754ddde","9bdc59d553984550ab1e5be0571c3ff6","c8525f7820824f7cb200ce078d339fa0","6b2c8b0932464c888a03cbb807da1583","1bfe34abcb7e41ca90b22092362b5d3d","1128d26dd16b4dfdb10a8bd370b3d742","b77c1824f78243ac8375cb894c9cc20c","3329b4e5a2a64b5db79bce1c521b8342","642aad4f60324dd59839db222ef6b342","079b198fcdef410aa2487874a489972b","244aa02197b6480b95be77c6ef8ac652","78e6c39758824baa9fd8b530a3736688","523de9013b7a4bdab505773fd0ea5a45","7b4ab055a35448e5950efc3a0e1bd9c2","86d10043a5084683ad0ce3f634368b5b","820d295be31642c1ada53cbcccc2aa8d","f5d680142c704232b356728cc3ee2912","906d815d6e8a47ca9d06c7402ecd960b","7322288e59e14f529984b51bbde9da62","bea1e50a7c5c4816b11e76d40c02e52c","c16007e1074442d0a854ba0791d1a4e7"]},"id":"2yj0l4Yf3SEc","executionInfo":{"status":"ok","timestamp":1696444724954,"user_tz":-120,"elapsed":103940,"user":{"displayName":"Antonello Meloni","userId":"09336063038722863523"}},"outputId":"3a9e0a4d-f16e-44b6-b29a-6141e86ba9e3"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaa18618a98d45409e957c0fa79e097a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)instruct_pipeline.py:   0%|          | 0.00/9.16k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15558149c087422ca59d6509a4c72e86"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["A new version of the following files was downloaded from https://huggingface.co/databricks/dolly-v2-3b:\n","- instruct_pipeline.py\n",". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13bf31df16b34c088822d2c1f5c10922"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ce3cafdcbfa4c889d89744d50347bf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45df9fa29ae04455b56d99ef26760b34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"244aa02197b6480b95be77c6ef8ac652"}},"metadata":{}}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"xF_bcVRM61-s","executionInfo":{"status":"ok","timestamp":1696445059951,"user_tz":-120,"elapsed":322,"user":{"displayName":"Antonello Meloni","userId":"09336063038722863523"}}},"outputs":[],"source":["\n","def divide_chunks(l_, n_):\n","    for i_ in range(0, len(l_), n_):\n","        yield l_[i_:i_ + n_]\n","\n","\n","def clean(st):\n","    st = st.replace(\"\\n\", \" \")\n","    st = st.replace(\"?\", \" ?\")\n","    st = st.replace(\"{\", \" { \")\n","    st = st.replace(\"}\", \" } \")\n","    st = st.replace(\"\\\\'\", \"'\")\n","\n","    while \"  \" in st:\n","        st = st.replace(\"  \", \" \")\n","    return st\n","\n","\n","def load_json(file__name):\n","    data_file = open(file__name, \"r\", encoding='utf-8')\n","    file_data = json.loads(data_file.read())\n","    data_file.close()\n","    return file_data\n","\n","\n","def save_json(filename,data):\n","    with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n","        print(json.dumps(data), file=json_file)\n","\n","\n","def main():\n","  filename=\"./test_3_mult_diversity_dolly.json\"\n","  data = load_json(filename)\n","\n","  for key in data:\n","    result = data[key]\n","    query_list=result.get(\"questions\")\n","    gs = result.get(\"generated_sparql\")\n","\n","    res = dolly(query_list)\n","    print(res)\n","    gst = [x[0][\"generated_text\"] for x in res]\n","\n","    for ii, l in enumerate(gst):\n","        for iii in range(10):\n","            if \"SELECT\" not in l:\n","                print(iii,ii)\n","                res = dolly(query_list[ii])\n","                gst[ii] = res[0][\"generated_text\"]\n","                l = gst[ii]\n","            else:\n","                break\n","    result[\"generated_sparql\"] = gst\n","    data[key] = result\n","\n","    save_json(filename, data)\n","\n"]},{"cell_type":"code","source":["main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kn5VcgNY91u5","executionInfo":{"status":"ok","timestamp":1696453442905,"user_tz":-120,"elapsed":8379270,"user":{"displayName":"Antonello Meloni","userId":"09336063038722863523"}},"outputId":"9245041b-ab1c-447a-d0e7-2b4be6b73337"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Word Level\") }'}], [{'generated_text': 'select distinct?paper?paper_lbl where {\\n?dataset a orkgc:Dataset;\\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset.\\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont;\\n  rdfs:label?paper_lbl. \\n  }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MAZEA\")?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P35?benchmark.?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {\\n?dataset a orkgc:Dataset;\\n rdfs:label?dataset_lbl.\\n FILTER (str(?dataset_lbl) = \"TDMSci\")?benchmark orkgp:HAS_BENCHMARK\\n?cont orkgp:HAS_MODEL?benchmark;\\n?paper orkgp:P31?cont;\\n rdfs:label?paper_lbl.\\n }'}], [{'generated_text': 'The mean capacity of a carbon-based fuel'}], [{'generated_text': 'Here is a sample result:'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n    ?dataset a orkgc:Dataset; \\n     rdfs:label?dataset_lbl. \\n     FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset. \\n    ?cont orkgp:HAS_BENCHMARK?benchmark. \\n    ?paper orkgp:P31?cont; \\n     rdfs:label?paper_lbl. \\n} \\n\\n{\\n  \"paper\" : \"P31\",\\n  \"paper_lbl\" : \"Kuzushiji-MNIST\"\\n}'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT \\n?paper?paper_lbl \\nWHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"GAD\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK \\n ?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset aarxiv:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P32?cont; rdfs:label?paper_lbl. } \\n\\nSPARQL results:\\n\\npaper                                       paper_lbl                                                                                                                                                                     \\n--------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\narXiv P32 Benchmark for Global Advertisers on the Web   arXivP32 Benchmark for Global Advertisers on the Web'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; \\n             rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDM-tagged\")?benchmark orkgp:HAS_DATASET?dataset. \\n            ?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; \\n             rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\noutput (Sparql query): \\n* paper:P31\\n* paper_lbl:GAD\\n* dataset:BC5CDR-disease'}], [{'generated_text': 'select distinct paper as \"paper\"  \\n  paperLabel as \"paper_lbl\"\\nwhere\\n  dataset in (\\'GAD\\',\\'Lunar_Lander\\')\\n  label == \"GAD\"\\n  benchmark in (\\'HAS_DATASET\\',\\'P31\\')\\n  cont in (\\'HAS_BENCHMARK\\',\\'LUNAR_LANDER\\')\\n  paper in (\\'P31\\',\\'LUNAR_LANDER\\')'}], [{'generated_text': 'Common evaluation metrics when benchmarking models on the FSNS - Test dataset are:'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {\\n\\n ?dataset a orkgc:Dataset;\\n  rdfs:label?dataset_lbl.\\n\\n  FILTER (str(?dataset_lbl) = \"ImageNet 64x64\")\\n\\n ?cont orkgp:HAS_BENCHMARK?benchmark.\\n\\n ?benchmark orkgp:P31?cont;\\n\\n  rdfs:label?paper_lbl.\\n\\n  }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl \\n     WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark <http://opensearch.org/gad.owl#hasDataset>?dataset. }'}], [{'generated_text': 'Flair-TDM: https://github.com/fuse-db-lab/flair-tdm\\nResource Description Framework (RDF): https://github.com/fuse-db-lab/flair-tdm/blob/master/scripts/generate_files.py'}]]\n","0 2\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["8 6\n","9 6\n","0 7\n","1 7\n","0 15\n","0 16\n","1 16\n","2 16\n","3 16\n","4 16\n","5 16\n","6 16\n","7 16\n","8 16\n","9 16\n","0 19\n","[[{'generated_text': 'RACE   P31'}], [{'generated_text': 'SELECT DISTINCT \\n   ?paper?paper_lbl \\nWHERE { \\n   ?dataset a orkgc:Dataset; \\n    rdfs:label?dataset_lbl. \\n    FILTER (str(?dataset_lbl) = \"Penn Treebank (Word Level)\" ) \\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark. \\n   ?paper orkgp:P31?cont; \\n    rdfs:label?paper_lbl. \\n}'}], [{'generated_text': \"In this query, we're looking for the title and ID of any research papers that contain a benchmark over the DDI dataset. \\n\\nWe'll find any research papers that contain the property HAS_DATASET on the class dataset which is the property of the dataset rdf:resource called DDI in RDF.\\n\\nWe'll look for the property P31 on the property benchmark which is a property of the class cont in the UrbanSound8k dataset. This is a benchmark used to measure the performance of a system.\\n\\nIf we find one or more results, the answers are in the bindings to the collection of bindings variables in the object named paper.\"}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'In class we just saw that the DDI dataset has the most popular queries and it has been open-sourced for research. To encourage research on it, one of the datasets provided has the MazeAql dataset with 13.6M maze navigation problems and their correspondingAnswer set. As an example of what people can do with the dataset, the first paper listed in this query is containing a benchmark over this dataset:\\n\\nDavide Vitali, Lorenzo Marin, Alberto Civino, Pietro Maffettone. \"Giant Randomized Search Traversal over MazeAql Benchmarks\", Proc. ACL, pages 807-814, 2016. https://doi.org/10.2467/208080016ACL16065'}], [{'generated_text': 'models being evaluated on the TDMSci dataset include'}], [{'generated_text': 'CARBON-BASED FOREIGNOBJECT INTELLIGENCE'}], [{'generated_text': 'The following are the title and ID of research papers that contain a benchmark over the DDI dataset: \\n\\n* \"Cross-lingual Zero-Shot Dataset Adaptation for Deep Neural Language Model\"\\n* \"Improving Neural Machine Translation with Cross-lingual Adversarialtraining\"\\n* \"Improving Neural Machine Translation with Cross-lingual Adversarial Training using Fine-grained Semantic-based Indirect Policy Gradients\"\\n* \"SWISH: A Corpus-based Neural Machine Translation Model with Cross-lingual Adversarial Training\"\\n* \"On Obtaining HMC Optimization Paths with Augmentive Neural Machine Translation\"\\n* \"Language Model Learning with Adversarial Training: A Corpus-Based Approach\"\\n* \"Cross-lingual Adversarial Language Model\"\\n* \"Supervised Multilingual Knowledge Bases for Cross-lingual Nearest-neighbour Search\"\\n* \"Neural Machine Translation With Adversarial Training for Low Resource Languages\"\\n* \"Adversarial Language Model Composed of One Neural Network and One Language Model\"\\n* \"Adaptive Adversarial Training for Neural Machine Translation\"\\n* \"Text Adversarial Language Model with Monol'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DDI\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016_ROManian-English\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } LIMIT 1'}], [{'generated_text': '# DISTINCT \\nSELECT DISTINCT?paper?paper_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"DDI\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n} \\n\\n# Ball in cup, catch (DMControl500k) dataset, including the metric and its value\\n?benchmark DMControl500k.DMControl500k Ball in cup, catch (DMControl500k)'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl \\n  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?dataset a orkgc:Dataset; rdfs:label?paper_lbl. } \\n  OPTIONAL COLUMNS {?benchmarkID oid :min;?benchmarkID oid :max;?paperID paper:P31 }'}], [{'generated_text': 'SELECT DISTINCT \\n\\t?paper?paper_lbl \\nWHERE { \\n\\t?dataset a orkgc:Dataset; \\n\\trdfs:label?dataset_lbl. \\n\\tFILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset. \\n\\t?cont orkgp:HAS_BENCHMARK?benchmark. \\n\\t?paper orkgp:P31?cont; \\n\\trdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDM Tagged Corpus\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"DDI\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT \\n ?paper \\n ?paper_lbl \\n  WHERE { \\n   ?dataset a orkgc:Dataset; \\n    rdfs:label?dataset_lbl. \\n    FILTER (str(?dataset_lbl) = \"DDI\") \\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark. \\n   ?paper orkgp:P31?cont; \\n    rdfs:label?paper_lbl. \\n  }'}], [{'generated_text': 'Commonly used evaluation metrics include the Area Under the receiver operating characteristic curve (AUC-ROC), the adjusted Rand index (ARI) and the F-measure.'}], [{'generated_text': 'Benchmarking ImageNet Dataset with Different Scale Factors'}], [{'generated_text': 'duration\\n12000\\n\\nbit rate\\n96\\n\\ndimension\\nchannels\\n2\\n\\nframe rate\\n5\\n\\nsample rate\\n12000\\n\\nfile size\\n400\\n\\nbandwidth\\n96'}], [{'generated_text': 'Here is a list of papers that utilize the Flair-TDM model:\\n- https://github.com/cross-team/flair-tdm\\n- https://github.com/nature-bio/Flair'}]]\n","0 0\n","0 2\n","0 4\n","1 4\n","2 4\n","3 4\n","4 4\n","5 4\n","6 4\n","7 4\n","8 4\n","9 4\n","0 5\n","1 5\n","2 5\n","3 5\n","4 5\n","5 5\n","6 5\n","0 6\n","1 6\n","0 7\n","1 7\n","0 16\n","1 16\n","2 16\n","3 16\n","4 16\n","5 16\n","6 16\n","7 16\n","8 16\n","9 16\n","0 17\n","0 18\n","1 18\n","2 18\n","3 18\n","4 18\n","5 18\n","6 18\n","7 18\n","8 18\n","9 18\n","0 19\n","1 19\n","2 19\n","3 19\n","[[{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RTE\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\\n                   ?benchmark Story Cloze Test Accuracy.'}], [{'generated_text': 'Here is the list of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset: \\n<http://www.docjar.com/id/n5qkP7>\\n<http://www.docjar.com/id/n5qkP8>\\n<http://www.docjar.com/id/n5qkP9>\\n<http://www.docjar.com/id/n5qkP10>'}], [{'generated_text': 'SELECT DISTINCT \\n       ?paper \\n       ?paper_lbl \\nWHERE \\n    {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UrbanSound8k\")?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n    ?dataset a orkgc:Dataset; \\n     rdfs:label?dataset_lbl. \\n     FILTER (str(?dataset_lbl) = \"Penn Treebank\") \\n    ?benchmark orkgp:HAS_DATASET?dataset. \\n    ?cont orkgp:HAS_BENCHMARK?benchmark. \\n    ?paper orkgp:P31?cont; \\n     rdfs:label?paper_lbl. \\n     }'}], [{'generated_text': 'SELECT DISTINCT?benchmark?benchmark_lbl?metric \\n   WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MazeA\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?metric orkgp:AVG. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"TDMSci\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"RTE\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Russian\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nI did a SPARQL query and got the following result set.\\n\\nBelow is the result set I got.'}], [{'generated_text': 'Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"RTE\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n} \\n\\n/?paper?paper_lbl\\n\\n _27 {\\n  \"@DATASET\": \"wmt2016.rol.eu:rol:ernc\", \\n  \"@BENCHMARK\": \"BLEU\", \\n  \"@CONTENT\": \"wmt2016.rol.eu:rol:ernc\", \\n  \"@MODEL\": \"bert-base-uncased\", \\n  \"@P31\": \"ernc\", \\n  \"@PREVIOUS_RESULTS\": [\"27\", \"31\"]\\n}'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Ball in Cup\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT \\n    ?paper \\n    ?paper_lbl \\n  WHERE { \\n      ?dataset a orkgc:Dataset; \\n       rdfs:label?dataset_lbl. \\n       FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark \\n      ?cont orkgp:HAS_BENCHMARK?benchmark \\n      ?paper orkgp:P31?cont \\n       ; \\n       rdfs:label?paper_lbl \\n     }'}], [{'generated_text': '<Query><QueryStatement>SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }</QueryStatement>'}], [{'generated_text': 'SELECT DISTINCT?metric \\n       ?metric?metric_lbl \\n        WHERE { \\n          ?dataset a orkgc:Dataset; \\n           rdfs:label?dataset_lbl. \\n           FILTER (str(?dataset_lbl) = \"TDM Tagged Corpus\") \\n          ?benchmark orkgp:HAS_BENCHMARK?benchmark. \\n          ?metric orkgp:P31?benchmark; \\n           rdfs:label?metric_lbl. \\n        }'}], [{'generated_text': 'SELECT DISTINCT \\n      ?paper \\n      ?paper_lbl \\nWHERE { \\n  ?dataset a orkgc:Dataset; \\n   rdfs:label?dataset_lbl. \\n   FILTER (str(?dataset_lbl) = \"BC5CDR-disease\") \\n  ?benchmark orkgp:HAS_DATASET?dataset. \\n  ?cont orkgp:HAS_BENCHMARK?benchmark. \\n  ?paper orkgp:P31?cont; \\n   rdfs:label?paper_lbl. \\n   }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl \\n    WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Lunar Lander (OpenAI Gym)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"FSNS - Test\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet64x64\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'P31  - Percentage of silence at the beginning of the music file\\nP31  - Percentage of silence at the end of the music file\\nP31  - Percentage of silence in the middle of the music file\\nP31  - Average Silence Percentage\\nP31  - Total Noise Percentage\\nP31  - Sum of Noise Channels Percentages\\nP31  - Average Noise Channels Percentage\\nP31  - Maximum Noise Channels Percentage\\nP31  - Minimum Noise Channels Percentage\\nP31  - Standard Deviation of Noise Channels\\nP31  - Root Mean Square of Noise Channels\\nP31  - Shannon’s Information Measure of Noise Channels\\nP31  - Square Sum of Noise Channels\\nP31  - Coding Fidelity Measure of Noise Channels\\nP31  - Mean Square of Noise Channels\\nP31  - Signal-to-Noise Ratio\\nP31  - Square Sum of Signal Channels Percentages\\nP31  - Coding Fidelity Measure of Signal Channels\\nP31  - Kurtosis of Signal Channels\\nP31  - Signal-to-Noise Ratio in dB\\nP31'}], [{'generated_text': 'There are a few papers that have utilized the Flair-TDM model to benchmark RTE dataset. The links to their code are - \\n- https://www.dropbox.com/sh/lq7w2fh0t1j8po6/AABtW--ujRvzEq4mB3Y6M08nw?dl=0\\n- https://github.com/pit-network/flair-tdm'}]]\n","0 1\n","0 8\n","1 8\n","2 8\n","3 8\n","4 8\n","5 8\n","6 8\n","0 18\n","0 19\n","1 19\n","2 19\n","3 19\n","4 19\n","5 19\n","[[{'generated_text': 'The VPN model achieved an Accuracy score of 0.84 on the Story Cloze Test benchmark dataset.'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"VPN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': \"The UrbanSound8k dataset is loaded with <b>Open Data House</b>'s model evaluation platform. The model evaluation platform enables you to compare your model's predictions on a benchmark dataset against predictions on real-world datasets. The following models are being evaluated on the UrbanSound8k dataset:\"}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"VPN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The benchmark score of the VPN model on the Maza dataset is 37.22296801852707 with the average precision metric of 0.8624176217804494.'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {\\n ?model a orkgc:Model\\n  rdfs:label?model_lbl\\n  FILTER (str(?model_lbl) = \"VPN\")\\n ?benchmark orkgp:HAS_DATASET?dataset\\n ?cont orkgp:HAS_BENCHMARK?benchmark\\n ?cont orkgp:HAS_MODEL?model\\n ?cont orkgp:HAS_SOURCE_CODE?code\\n}'}], [{'generated_text': 'The mean capacity of a carbon-based fuel is about 450.7 lb/mile.'}], [{'generated_text': 'In addition to the MLDoc Zero-Shot dataset, which is used in the Kaggle English-to-Russian competition, the following research papers have performed benchmarks on it:\\n\\n* [https://www.kaggle.com/c/kaggle-english-to-russian/data](https://www.kaggle.com/c/kaggle-english-to-russian/data)\\n* [https://drive.google.com/file/d/0B3ot8kbZ7ZoMjk2Yk9lY1JKc2hoU0/view](https://drive.google.com/file/d/0B3ot8kbZ7ZoMjk2Yk9lY1JKc2hoU0/view)\\n* [https://drive.google.com/file/d/0B3ot8kbZ7ZjZ4UFBkd2pPaTZJWc2hoU/view](https://drive.google.com/file/d/0B3ot8kbZ7ZjZ4UFBkd2pPa'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"VPN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code'}], [{'generated_text': 'The highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value is 1.125\\nkB/s'}], [{'generated_text': 'The model that achieves the best Top-1 accuracy score on the VTAB-1k dataset is from Institute of Robotics and Intelligent Mechatronic, Chinese Academy of Sciences (known as IdMI in Chinese) and its name is Idnet: Convolutional Neuron Architecture for Semantic Image Recognition. The best Top-1 accuracy score achieved by this model on the VTAB-1k dataset is 0.9901.'}], [{'generated_text': 'Here are the titles and IDs of research papers that include a benchmark for the arXiv dataset:\\n\\n?benchmark : Title_Of_Paper, BattelleNames:NETWORKING:VPN:Dataset:arXiv\\n?benchmark:Title_Of_Paper@BattelleNames:NETWORKING:VPN:Dataset:arXiv, rdfs:label, andkgp:label\\n?benchmark:Title_Of_Paper@BattelleNames:NETWORKING:VPN:Dataset:arXiv,?benchmark:Title_Of_Paper, orkgp:HAS_BENCHMARK,?cont:rdfs:label, orkgp:label'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"VPN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'There are several research papers that benchmarked models on the BC5CDR-disease dataset. The text of the research papers can be found here:\\n\\nhttps://dumps.wikimedia.org/other/b Commons-Dumps-2015-10-03T08-41-02.wiki\\nhttps://dumps.wikimedia.org/other/b Commons-Dumps-2015-10-03T08-41-02.wiki.html'}], [{'generated_text': 'The name of the best performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset is Network Optimization on Large Graphs (Version 2.2.1)'}], [{'generated_text': 'Common evaluation metrics when benchmarking models on the FSNS - Test dataset are: root mean squared error (RMSE), mean absolute error (MAE), mean relative error (MRE), R2 (or correlation coefficient) and the correlation coefficient (r).\\nThe following are metrics that can be used to evaluate model performance on a specific dataset:\\n- RMSE: sqrt( (x - x_ref)/x_ref )\\n- MAE:  (x - x_ref)/x_ref\\n- MRE:  (x - x_ref)/x_ref \\n- R2: 1 - ((x - x_ref) ^ 2)/(var(x)var(x_ref))\\n- r: (x - x_ref)/(x_ref + epsilon), where epsilon is a small value used for stability purposes (often selected as 0.00001).'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"VPN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code \\n      WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"VPN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'https://www.cs.toronto.edu/~flair/flair-tldm.html\\nhttps://github.com/julian-reschke/flair-tldm'}]]\n","0 0\n","0 2\n","1 2\n","2 2\n","0 4\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","1 7\n","2 7\n","3 7\n","4 7\n","0 10\n","1 10\n","2 10\n","0 11\n","1 11\n","2 11\n","0 12\n","0 14\n","0 15\n","1 15\n","2 15\n","3 15\n","0 16\n","1 16\n","2 16\n","3 16\n","4 16\n","5 16\n","6 16\n","7 16\n","8 16\n","9 16\n","0 19\n","[[{'generated_text': 'The CGU model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset.'}], [{'generated_text': 'SELECT DISTINCT?code \\n WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"CGU\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'Here are the models being evaluated on the UrbanSound8k dataset:\\n\\nCGU: Canonical Guitar Pro models\\nZhang-ESRD: E-Music Studio model with RDFE strategy\\nSoprano: E-Music Studio model with RDFE strategy\\nFlat Piano: Novel Audio model with RDFE strategy\\nStereo Piano: Novel Audio model with RDFE strategy\\nOboe: Novel Audio model with RDFE strategy\\nTenor: Novel Audio model with RDFE strategy\\nBass: SCTuning model with RDFE strategy\\nTenor 2: SCTuning model with RDFE strategy\\nChoir: Novel Audio model with RDFE strategy\\nChoir 2: Novel Audio model with RDFE strategy\\nChoir 3: Novel Audio model with RDFE strategy\\nChoir 4: Novel Audio model with RDFE strategy\\nChoir 5: Novel Audio model with RDFE strategy\\nChoir 6: Novel Audio model with RDFE strategy\\nChoir 7: Novel Audio model with RDFE strategy\\nChoir 8: Novel Audio model with RDFE strategy\\nChoir 9: Novel Audio model with RDFE strategy\\nChoir 10: Novel Audio model with RDFE strategy\\nChoir 11: Novel Audio'}], [{'generated_text': 'Try?\\', spearld href=\"https://github.com/pytorch/examples/blob/master/examples/cased_ vocabulary.json\">\\nCased Vocabulary.json</a>\\' from the Pytorch GitHub repository. \\n\\nBest,'}], [{'generated_text': 'SELECT DISTINCT?code'}], [{'generated_text': 'The following code provides links to code used in papers that benchmark the CGU model:\\n\\n- paper [1]\\n- paper [2]\\n- paper [3]\\n- paper [4]\\n- paper [5]'}], [{'generated_text': 'Carbon-based fuel : 6,675 megajoule / 1,000 barrels.'}], [{'generated_text': 'As of 18 March 2023, the following research papers have benchmarked the MLDoc Zero-Shot English-to-Russian dataset:\\n\\n- Manuel Barril, Jesús De León, and Sergio Guzmán. \"Benchmarking Named Entity Recognizer Systems: A Survey and a Public Dataset.\" arXiv preprint arXiv:1903.02642 (2019).\\n- Xin Zhu, Jun Zhang, Yang Sun, Tianbao Yan, Rolf Nederlof, David Mimram, Arthur Szlam, Alireza Abularsi, et al. \"Czar: Benchmarking Sentiment Analysis Systems.\" arXiv preprint arXiv:1905.09522 (2019).\\n- Vasileios Trinklein, Jan Daniel, and Fabian Wendla. \"OpenAI-GPT: Comparison to the GPT-2 and TPU-Design Models.\" arXiv preprint arXiv:1905.02992 (2019).\\n- Massimiliano Ponts, Aaron Courville, Carlos Guestrin, Daniel Graef, Ilya Sutskever, and Patrick McLure. \"Knowledge-Normalized ai-first training of stacked auto-encoders.\" arXiv'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"CGU\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\noutput (Sparql query): \\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"CGU\")?benchmark orkgp:HAS_DATASET \"kuzushiji-mnist\".?cont orkgp:HAS_BENCHMARK \"Kuzushiji-MNIST\".?cont orkgp:HAS_MODEL \"CGU\"; orkgp:HAS_SOURCE_CODE \"https://github.com/opencv/opencv_contrib/blob/master/modules/dnn/include/cudnn/ orkgp:C'}], [{'generated_text': '* \\n* \\n<a rel=\"nofollow\" href=\"https://arxiv.org/abs/1611.05977\">Gusano, F. (2016). CGU: Generating Text with Deep Learning and the Google Translate Model. arXiv:1611.05977</a><br />\\n<a rel=\"nofollow\" href=\"https://code.google.com/p/word-migration/wiki/License\">https://code.google.com/p/word-migration/wiki/License</a><br />\\n<a rel=\"nofollow\" href=\"https://arxiv.org/abs/1508.06576\">Luong, D. (2015). Model-agnostic Language Learning. arXiv:1508.06576</a><br />'}], [{'generated_text': 'The highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value is 200.000000'}], [{'generated_text': 'CGU'}], [{'generated_text': 'The following are the research papers that include a benchmark for the arXiv dataset:\\n- \"Interactive Adsorption in the Eley-Rseparator: A Benchmark for the CGU Model\" by Robbert van Gerven et al., 2020\\n- \"CGU: A new model for describing atmospheric dynamics\" by Ruhan Lyu et al., 2019\\n- \"Rogue waves in the CGU model and their wave mechanics\" by Vasileios Maras and Ian R. Philipson, 2019'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"CGU\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The following are research papers that have benchmarked models on the BC5CDR-disease dataset: \\n\\n-  Berthier, J., Krevel, D., Duchesnay, C., Munoz-Caro, A., Tanthi, A., Michel, P., & Marquez, S. (2017). BDM-IMPP: A benchmarking platform for disease models, applications and data. Journal of Open Source Software, 2(6), 12. \\n\\n- Hadiabadi, A., Billions, B. & Canoy, S. (2018). CorpCog: A large-scale benchmarking framework for cognitive tasks. In IJCAI/ACM CogInAI, Vol. 35, (pp. 157-166).'}], [{'generated_text': 'SELECT DISTINCT?code \\n  WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"CGU\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'Common evaluation metrics used on the FSNS - Test dataset are:\\n\\n- R-squared\\n- Macdonald R-squared\\n- R2\\n- R2 adjusted\\n- RMSE\\n- MAPE\\n- MBE\\n\\nWhen benchmarking a CGU model against a baseline model, the following metrics can also be computed:\\n\\n- Macdonald R-squared for the entire FSNS - Test dataset\\n- R2 for each class of the FSNS - Test dataset\\n- RMSE for each class of the FSNS - Test dataset\\n- MAPE for each class of the FSNS - Test dataset\\n- MBE for each class of the FSNS - Test dataset'}], [{'generated_text': 'doc:link to code used in papers that benchmark the CGU model'}], [{'generated_text': 'CGU stands for Circuito Guitarra Ultrasonico, a musical instrument modeled using Scratch. It has been benchmarked with a dataset provided by the author of this post, João Neto, as part of this Masters thesis: https://github.com/joanneten/music-thesis/blob/master/thesis.md\\nThe evaluation metrics over the dataset are Mean Absolute Percent Error (MAE) and Root Mean Squared Error (RMSE). Please, refer to the original paper for the formulas.'}], [{'generated_text': 'SELECT DISTINCT \\n ?code \\n  WHERE { \\n   ?model a \\n    orkgc:Model \\n    rdfs:label?model_lbl \\n    FILTER (str(?model_lbl) = \"CGU\") \\n   ?benchmark orkgp:HAS_DATASET \\n   ?dataset \\n    FILTER (?benchmark = true) \\n   ?cont orkgp:HAS_BENCHMARK \\n   ?benchmark orkgp:HAS_MODEL?model \\n   ?cont orkgp:HAS_SOURCE_CODE?code \\n  }'}]]\n","0 0\n","0 2\n","1 2\n","2 2\n","3 2\n","4 2\n","5 2\n","6 2\n","7 2\n","8 2\n","9 2\n","0 3\n","1 3\n","0 5\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","1 7\n","2 7\n","3 7\n","4 7\n","5 7\n","6 7\n","7 7\n","8 7\n","9 7\n","0 9\n","0 10\n","1 10\n","2 10\n","3 10\n","0 11\n","1 11\n","2 11\n","0 12\n","0 14\n","1 14\n","0 16\n","0 17\n","1 17\n","2 17\n","3 17\n","4 17\n","0 18\n","1 18\n","[[{'generated_text': 'SELECT DISTINCT?code \\n    WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Bi+\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code \\nWHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Bi+\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'the Bi+ model'}], [{'generated_text': 'The following are the research paper titles and IDs that have benchmarked models on the Penn Treebank dataset:\\n- Smith, Harry G., F. Bard, J. Charniak, P. L. Carll, J. Fiscus, and R. Kukushkin. \"Large Scale Framewise Differrences for Speech Recognition.\" International Conference on Machine Learning. vol. 19. pp. 1064-1070. ACM, 2009.\\n- Briscoe, Jeffry, Sharad Sukhum, Raymond Van Donkelaar, Ravi Kannan, and Toby Walsh. \"BERT: A LSTM Language Model Capable of Human-Level Text Summarization.\" International Conference on Machine Learning. vol. 39. pp. 1025-1034. ACM, 2016.\\n- Puntambekar, Siddharth, Thangphaichuman, Saifedean Arvanitcero, Yangqing Jia, Scott Gray, Reza Shariat, et al. \"BERT: Large Scale Language Model Pre-training With Max- pooled Attention Is Effective.\" CoRR, abs/1810-01175, 2018.'}], [{'generated_text': 'SELECT DISTINCT?code \\n    WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Bi+\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code\\nWHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Bi+\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'Carbon-based fuel refers to any fuel consisting of carbon and an oxidant, such as air, which is combusted to release thermal energy and produce a flame. Mean capacity of a carbon-based fuel is defined as the amount of thermal energy contained in a gallon of the fuel.'}], [{'generated_text': 'SELECT DISTINCT?code \\n  WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Bi+\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Bi+\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\noutput:\\n\\n`code` is a collection of code references that have been used by papers that have used the Bi+ model for benchmarking purposes. The best performing model on the Kuzushiji-MNIST benchmark dataset has an accuracy of 99.67%.'}], [{'generated_text': 'SELECT DISTINCT?code \\nWHERE { \\n ?model a orkgc:Model; \\n  rdfs:label?model_lbl. \\n  FILTER (str(?model_lbl) = \"Bi+\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?cont orkgp:HAS_MODEL?model; \\n  orkgp:HAS_SOURCE_CODE?code. \\n}'}], [{'generated_text': 'SELECT DISTINCT?code \\n  WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Bi+\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset is “Bi+”.'}], [{'generated_text': 'Here are the titles and IDs of research papers that include a benchmark for the arXiv dataset:\\n\\nhttps://arxiv.org/list/deeplearning.org/TableOfContents'}], [{'generated_text': 'SELECT DISTINCT?code \\n            WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Bi+\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n   ?model a orkgc:Model; \\n    rdfs:label?model_lbl. \\n    FILTER (str(?model_lbl) = \"Bi+\") \\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark. \\n   ?cont orkgp:HAS_MODEL?model; \\n    orkgp:HAS_SOURCE_CODE?code. \\n}'}], [{'generated_text': 'SELECT DISTINCT?code \\n WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Bi+\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'Commonly used evaluation metrics for benchmarking models on the FSNS Test dataset are:\\n- Recall (aka sensitivity)\\n- Precision\\n- $F_1$ Score'}], [{'generated_text': 'SELECT DISTINCT?code \\n WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Bi+\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code \\n  WHERE { \\n   ?model a orkgc:Model; \\n    rdfs:label?model_lbl. \\n    FILTER (str(?model_lbl) = \"Bi+\") \\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark. \\n   ?cont orkgp:HAS_MODEL?model; \\n    orkgp:HAS_SOURCE_CODE?code. \\n  }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Flair-TDM\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}]]\n","0 2\n","0 3\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 11\n","0 12\n","0 16\n","1 16\n","[[{'generated_text': 'InferSent'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n?model rdfs:label?model_lbl. \\n Toronto Computer Obsolescence Dataset Benchmark, Karim Bacha, Syed Ali, Patrick Billay, Dalibor Matas, Giovanni Palumbo, Asim Shah, Alka Saxena, Nicholas Wallach, Ziyang Zheng. \\nPenn Treebank (Word Level) Benchmark, Karim Bacha, Syed Ali, Patrick Billay, Dalibor Matas, Giovanni Palumbo, Asim Shah, Alka Saxena, Nicholas Wallach, Ziyang Zheng. \\nDeriving an Embedding for'}], [{'generated_text': 'SELECT DISTINCT \\n   ?model?model_lbl \\nWHERE {?dataset a orkgc:Dataset; \\n     rdfs:label?dataset_lbl. \\n     FILTER (str(?dataset_lbl) = \"UrbanSound8k\")\\n    ?benchmark orkgp:HAS_DATASET?dataset; \\n     orkgp:HAS_EVALUATION?eval. \\n    ?paper orkgp:HAS_BENCHMARK?benchmark. \\n     OPTIONAL {?paper orkgp:HAS_MODEL?model. \\n            ?model rdfs:label?model_lbl. \\n     } \\n     } \\n     }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"HoC\") \\n ?benchmark orkgp:HAS_DATASET?dataset; \\n  orkgp:HAS_EVALUATION?eval. \\n ?paper orkgp:HAS_BENCHMARK?benchmark. \\n  OPTIONAL {?paper orkgp:HAS_MODEL?model. \\n   ?model rdfs:label?model_lbl. \\n  }\\n}'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Mazea\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': '- Coal\\n- Oil\\n- Natural Gas'}], [{'generated_text': 'To answer the question, list the research papers that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset:'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n                                                                   ^\\nThank you! The result is \" CNN \" with the label \" Accuracy \"'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Romanian-English Benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'DMControl500k, MeanSquaredError'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl \\n WHERE { \\n   ?dataset a orkgc:Dataset; \\n    rdfs:label?dataset_lbl. \\n    FILTER (str(?dataset_lbl) = \"HoC\") \\n   ?benchmark orkgp:HAS_DATASET?dataset; \\n    orkgp:HAS_EVALUATION?eval. \\n   ?paper orkgp:HAS_BENCHMARK?benchmark. \\n    OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n } \\n/'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }  \\n{\\n  \"paper\": \"Della Pietra, B., Battiston, M., Palumbo, D., Claudio, M., Venturini, A., et al., 2019. N-Gram based approaches for modeling and analyzing performance of NMT models. arXiv:1908.02679.\",\\n  \"model\": \"LSTM\",\\n  \"model_lbl\": \"LSTM\"\\n}\\n final response:  \\n {\"paper\":\"Della Pietra, B., Battiston, M., Palumbo, D., Claudio, M., Venturini, A., et al., 2019. N'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE { \\n   ?dataset a orkgc:Dataset; \\n    rdfs:label?dataset_lbl. \\n    FILTER (str(?dataset_lbl) = \"HoC\") \\n   ?benchmark orkgp:HAS_DATASET?dataset; \\n    orkgp:HAS_EVALUATION?eval. \\n   ?paper orkgp:HAS_BENCHMARK?benchmark. \\n    OPTIONAL {?paper orkgp:HAS_MODEL?model. \\n           ?model rdfs:label?model_lbl. \\n    } \\n} \\n\\ninput (English text): \\nThe HoC dataset contains multiple datasets, and each dataset contains several benchmarks that models have been evaluated on these benchmarks datasets. \\nThe HoC benchmark datasets contain several related metrics to track the performance of each model on a particular evaluation metric. Some of the metrics are:\\n- precision\\n- recall\\n- f-measure\\n- correlation\\n- r2\\n- mse'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"LunarLander\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'Commonly used evaluation metrics include: \\n\\n- Recall@k,\\n- Precision@k,\\n- F1 score,\\n- ACC,\\n- MCC'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE { \\n  ?dataset a orkgc:Dataset; \\n   rdfs:label?dataset_lbl. \\n   FILTER (str(?dataset_lbl) = \"HoC\") \\n  ?benchmark orkgp:HAS_DATASET?dataset; \\n   orkgp:HAS_EVALUATION?eval. \\n  ?paper orkgp:HAS_BENCHMARK?benchmark. \\n   OPTIONAL { \\n     ?paper orkgp:HAS_MODEL?model. \\n     ?model rdfs:label?model_lbl. \\n   } \\n}'}], [{'generated_text': 'Classical music, 5 seconds at 12 kHz:'}], [{'generated_text': 'Please find the list of papers that have utilized the Flair-TDM model and include the links to their code in the following spreadsheet:'}]]\n","0 0\n","1 0\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","0 10\n","1 10\n","0 16\n","0 18\n","0 19\n","1 19\n","2 19\n","3 19\n","4 19\n","5 19\n","6 19\n","[[{'generated_text': 'The highest Accuracy score of any model has been achieved by the DecODal model.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"WSC\") \\n ?benchmark orkgp:HAS_DATASET?dataset; \\n  orkgp:HAS_EVALUATION?eval. \\n ?paper orkgp:HAS_BENCHMARK?benchmark. \\n  OPTIONAL { \\n   ?paper orkgp:HAS_MODEL?model. \\n   ?model rdfs:label?model_lbl. \\n  }\\n}'}], [{'generated_text': 'SELECT DISTINCT \\n?model?model_lbl\\nWHERE { \\n?dataset a orkgc:Dataset; \\nrdfs:label?dataset_lbl. \\nFILTER (str(?dataset_lbl) = \"UrbanSound8k\") \\n?benchmark orkgp:HAS_DATASET?dataset;\\norkgp:HAS_EVALUATION?eval.\\n?paper orkgp:HAS_BENCHMARK?benchmark. \\nOPTIONAL {?paper orkgp:HAS_MODEL?model.\\n?model rdfs:label?model_lbl. } \\n}'}], [{'generated_text': '[\\n  \"Evaluation of Part of Speech Tags for Use in Automatic Article Classification\",\\n  \"An Evaluation of Statistical and Neural Methods for Part of Speech Tagging\",\\n  \"An Empirical Comparison of Similarity-Based and Neural Network Classifiers for Part of Speech Tagging\",\\n  \"A Comparative Study of Boosting and Support Vector Machines for POS Tagging\",\\n  \"Neural and Statistical Methods for Recognizing and Classifying Syntactic Features\",\\n  \"A Comparison of Neural and Statistical Models for POS Tagging\",\\n  \"Comparative Study of Neural Networks and Incremental Learning Neural Networks for POS Tagging\",\\n  \"Combining Recursive and Incremental Learning Techniques for POS Tagging with Neural Networks\",\\n  \"Comparison of Neural and Statistical Methods for POS Tagging\",\\n  \"Combining Recursive and Incremental Learning Techniques for POS Tagging with Neural Networks\",\\n  \"A Comparison of Neural and Statistical Models for POS Tagging\",\\n  \"A Comparative Study of Neural and Statistical Models for POS Tagging\",\\n  \"A Comparison of Neural and Statistical Models for POS Tagging\"\\n]'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MAZEA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDMSci\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': 'The models that have been benchmarked on the WSC dataset are mean capacity of a carbon-based fuel.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WSC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nSELECT?paper?model_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"WSC\") \\n ?benchmark orkgp:HAS_DATASET?dataset; \\n  orkgp:HAS_EVALUATION?eval. \\n ?paper orkgp:HAS_BENCHMARK?benchmark. \\n  OPTIONAL {?paper orkgp:HAS_MODEL?model. \\n   ?model rdfs:label?model_lbl. \\n  } \\n} \\n\\n###################\\n#RESULTS\\n###################\\n\\n+model: \"xgboost\"\\n+model_lbl: \"xgboost\"'}], [{'generated_text': 'The query SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WSC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } filters results for a specific dataset and the model with the highest BLEU score on the WMT2016 Romanian-English benchmark dataset. The result contains the model label and the BLEU score for the model.'}], [{'generated_text': '*     Ball in Cup, Catch (DMControl500k) - DMControl500k    - LM: 30.62\\n*     Ball in Cup, Catch (DMControl500k) - DMControl500k    - RM: 33.63\\n*     Ball in Cup, Catch (DMControl500k) - DMControl500k    - CERP: 0.568\\n*     Ball in Cup, Catch (DMControl500k) - DMControl500k    - FUEL: 0.318\\n*     Ball in Cup, Catch (DMControl500k) - DMControl500k    - IQR: 2.478'}], [{'generated_text': 'The top performing model in terms of Top-1 accuracy score when benchmarked on the VTAB-1k dataset is Orrery.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {\\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset; \\n  orkgp:HAS_EVALUATION?eval. \\n ?paper orkgp:HAS_BENCHMARK?benchmark. \\n  OPTIONAL {?paper orkgp:HAS_MODEL?model. \\n         ?model rdfs:label?model_lbl. \\n  } \\n}'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDM_Tagged\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'The research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset are: \\n  \\n1.  Orthos (Lazaridis, 2015)\\n2.  Quang (Quang, 2016)\\n3.  Keren (Keren, 2016)\\n4.  Zhang (Zhang, 2016)\\n5.  Chen (Chen, 2016)\\n6.  Xiong (Xiong, 2016)\\n7.  Tan (Tan, 2017)\\n8.  Chang (Chang, 2017)\\n9.  Ren (Ren, 2017)\\n10. Fang (Fang, 2017)\\n11. Cao (Cao, 2018)\\n12. Ren (Ren, 2018)\\n13. Liu (Liu, 2018)'}], [{'generated_text': 'The top performing model in terms of Score when benchmarked on the Lunar Lander (OpenAI Gym) dataset has the model name \"AlphaStar\".'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test\")?benchmark orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\nThe evaluation metrics commonly used for benchmarking models on the FSNS - Test dataset include: \\n\\n- Area under the precision recall curve (AUPRC)\\n- Area under the F1 score curve (AUF1)\\n- Mean Average Precision (MAP)\\n- Mean Squared Error (MSE)\\n\\nNote that this data is available for supervised and unsupervised benchmarking scenarios.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"WSC\") \\n ?benchmark orkgp:HAS_DATASET?dataset; \\n  orkgp:HAS_EVALUATION?eval. \\n ?paper orkgp:HAS_BENCHMARK?benchmark. \\n  OPTIONAL {?paper orkgp:HAS_MODEL?model. \\n ?model rdfs:label?model_lbl. } \\n}'}], [{'generated_text': 'The metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset are:\\n- ABLCoefficient: Absolute Logarithmic Compression Cost\\n- ABLLeary Compression Cost\\n- iBLCoefficient: In-band Logarithmic Compression Cost\\n- iBLLeary Compression Cost\\n- iSBLCoefficient: In-signal Logarithmic Compression Cost\\n- iSBLeary Compression Cost\\n- PSNR: Peak Signal-to-Noise Ratio\\n- SSIM: Structural SIMilarity'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WSC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}]]\n","0 0\n","1 0\n","0 3\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 10\n","0 11\n","0 14\n","1 14\n","2 14\n","3 14\n","0 15\n","1 15\n","0 18\n","[[{'generated_text': 'rdfs:label \"BERT\"'}], [{'generated_text': '* paper-id,paper-title\\n* paper-id,paper-title,paper-year\\n* paper-id,paper-title,paper-year,paper-author\\n* paper-id,paper-title,paper-year,paper-author,paper-city\\n* paper-id,paper-title,paper-year,paper-author,paper-city,paper-country\\n* paper-id,paper-title,paper-year,paper-author,paper-city,paper-country,paper-language\\n* paper-id,paper-title,paper-year,paper-author,paper-city,paper-country,paper-language,paper-ndcid\\n* paper-id,paper-title,paper-year,paper-author,paper-city,paper-country,paper-language,paper-url\\n* paper-id,paper-title,paper-year,paper-author,paper-city,paper-country,paper-language,paper-url,paper- doi'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl \\n    WHERE { \\n       ?dataset a orkgc:Dataset; \\n        rdfs:label?dataset_lbl. \\n        FILTER (str(?dataset_lbl) = \"UrbanSound8k\") \\n       ?benchmark orkgp:HAS_DATASET?dataset; \\n        orkgp:HAS_EVALUATION?eval. \\n       ?paper orkgp:HAS_BENCHMARK?benchmark. \\n        OPTIONAL {?paper orkgp:HAS_MODEL?model. \\n                 ?model rdfs:label?model_lbl. } \\n    }'}], [{'generated_text': 'There are 2 research paper titles that have benchmarked models on the Penn Treebank dataset:\\n\\n- BPE (Back-Pointer Prediction for Language Modeling)\\n- Harteoga (Benchmarking Language Models: A Case Study on the Penn Treebank and the TPTP Corpora)'}], [{'generated_text': 'The top benchmark score is 0.9386 and the metric is MLP[10, 1]'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl'}], [{'generated_text': 'The mean capacity of a carbon-based fuel is 210 grams of carbon-dioxide per kilogram of fuel.'}], [{'generated_text': 'The papers are:\\n- [Is low-resource, zero-shot machine translation still hard? A benchmark on MLDoc-ZSLR with a novel model], Li et al., TPAMI, 2018.\\n- [Experiments on a Benchmark dataset for Zero-Shot Machine Translation, MLDoc-ZSL], Gauthier et al., NAACL, 2017.\\n- [Using Document Aware Attention for Zero-Shot Machine Translation, MLDoc-ZSL], Jochem et al., ICLR, 2019.'}], [{'generated_text': 'The deep learning models that performed best on the Kuzushiji-MNIST benchmark dataset were a multi-layer perceptron with a hidden layer of 128 nodes and an accuracy of 90.9%, and a convolutional neural network with 64 convolutional layers, each with 16 filters, and an accuracy of 89.7%.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Romanian-English Benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n                                           \\n/*\\n * =========================================================\\n * output:\\n * =========================================================\\n * {\\n *     \"model\": \"x-wmt-2017-ro\",\\n *     \"model_lbl\": \"WMT2016 Romanian-English Benchmark\"\\n * }\\n */'}], [{'generated_text': '{\\n   \"paper\" : \"Zhi, F., Liang, T., & Xue, J. (2020). Bert: Deep neural networks for question answering in blogs. In the 24th ACM on Conference on Digital Libraries (CDL) meeting(s) (pp. 40:1-40:13). ACM.\"\\n\"model\" : \"Bert\"\\n\"model_lbl\" : \"Bert\"\\n\"eval\" : \"DMControl500k\"\\n\"benchmark\" : \"catch (DMControl500k)\"\\n\"value\" : \"79.00\"\\n}'}], [{'generated_text': 'paper=LSTM  model=LSTM  model_lbl=Top-1 Accuracy'}], [{'generated_text': 'SELECT DISTINCT \\n   ?model?model_lbl \\nWHERE { \\n   ?dataset a orkgc:Dataset; \\n    rdfs:label?dataset_lbl. \\n    FILTER (str(?dataset_lbl) = \"arXiv\") \\n   ?benchmark orkgp:HAS_DATASET?dataset; \\n    orkgp:HAS_EVALUATION?eval. \\n   ?paper orkgp:HAS_MODEL?model. \\n   ?model rdfs:label?model_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl \\n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n[{\\n  \"model\" : {\\n    \"model_lbl\" : \"Transformer\"\\n  },\\n  \"model_lbl\" : \"BERT\"\\n,\\n  \"model_lbl\" : \"DistMult\"\\n,\\n  \"model_lbl\" : \"XGboost\"\\n,\\n  \"model_lbl\" : \"LogitBoost\"\\n,\\n  \"model_lbl\" : \"SVM\"\\n,\\n  \"model_lbl\" : \"LightGain\"\\n},\\n{\\n  \"model\" : {'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl \\n WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"BC5CDR-disease\")?benchmark \\n  OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n}'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"LunarLander\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\n&model;rdfs:label\\nLunarLander'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\noutput (Sparql query): \\n\\nThe following query outputs the model and their corresponding metrics'}], [{'generated_text': 'The papers that include a benchmark for the ImageNet 64x64 dataset are:\\n- Paper-id Title\\n- TPIAI-2018 \"Towards Recommending Image Annotators Using Model-agnostic Recommendations\" by Wang et al.\\n- TPIAI-2019 \"Visual Entity Extraction From Multimedia Data With a Bi-level Recurrent Neural Network\" by Wang et al.\\n- TPIAI-2019 \"Benchmarking Recurrent Neural Networks For Entity Recognition\" by Gong et al.\\n- TPIAI-2019 \"LSTM-based Recurrent Neural Networks for Entity Recognition in Television Interactive Data\" by Peng et al.\\n- TPIAI-2020 \"ASPECT: Attribute-Aware Semantic Parlance Detection with Recurrent Neural Networks\" by Wang et al.\\n- TPIAI-2020 \"ASPECT: Improved Recurrent Neural Network Models for Fine-grained Semantic Disambiguation\" by Wang et al.\\n- TPIAI-2021 \"Benchmark of Text-Image Retrieval Models\" by Yi et al.'}], [{'generated_text': 'Classical music, 5 seconds at 12 kHz'}], [{'generated_text': 'These papers have utilized the Flair-TDM model:\\n- Wang et al., “Daily Changes in Twitter Sentiment: A Study of News and Sports Tweets.” NIPS Workshop on Social Media and Political Science. July 20-22, 2015.\\n- Chen et al., “Dynamic Lexicon based Semantic Similarity for Twitter Conversation.” TPAMI. April 2015.\\n- Zhou et al., “The Flair Model: Learning Explained Syntax for Twitter and Classification of Politicized Tweets.” SSC. December 2015.'}]]\n","0 0\n","1 0\n","0 1\n","0 3\n","0 4\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","0 8\n","0 10\n","0 11\n","1 11\n","0 17\n","0 18\n","0 19\n","[[{'generated_text': 'The Model *Neural Programmer* has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset.'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl \\n WHERE { \\n  ?problem a orkgc:Problem; \\n   rdfs:label?problem_lbl. \\n   FILTER (str(?problem_lbl) = \"Atari Games\") \\n  ?dataset a orkgc:Dataset; \\n   rdfs:label?dataset_lbl. \\n  ?benchmark orkgp:HAS_DATASET?dataset. \\n  ?cont orkgp:HAS_BENCHMARK?benchmark; \\n   orkgp:P32?problem. \\n }'}], [{'generated_text': 'The UrbanSound8k dataset is being evaluated using the best temporal models, specifically LSTM models.'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE { \\n    ?problem a orkgc:Problem; \\n     rdfs:label?problem_lbl. \\n     FILTER (str(?problem_lbl) = \"Atari Games\")?dataset a orkgc:Dataset; \\n     rdfs:label?dataset_lbl. \\n    ?benchmark orkgp:HAS_DATASET?dataset. \\n    ?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. \\n }'}], [{'generated_text': 'The MAZEA benchmark is a large-scale neural machine translation benchmark, consisting of 7.4M pairs of English to Italian sentences. The evaluation metric is BLEU. The current reported BLEU on the MAZEA test set is 35.'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Atari Games\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'The mean capacity of a carbon-based fuel is 72.16MB/l'}], [{'generated_text': '<ODML_RESULTS>\\n    <RESULTS>\\n        <RESULT>\\n            <TITLE>First Atari Games Benchmark</TITLE>\\n            <ID>2606</ID>\\n        </RESULT>\\n        <RESULT>\\n            <TITLE>Second Atari Games Benchmark</TITLE>\\n            <ID>2607</ID>\\n        </RESULT>\\n    </RESULTS>'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl \\n where { \\n?problem a orkgc:Problem; \\n rdfs:label?problem_lbl. \\n FILTER (str(?problem_lbl) = \"Atari Games\")?dataset a orkgc:Dataset; \\n rdfs:label?dataset_lbl. \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark; \\n orkgp:P32?problem. \\n }'}], [{'generated_text': 'The model achieving the highest BLEU score on the WMT2016 Romanian-English benchmark dataset is  the BPE-char-global model, with a score of 35.38.'}], [{'generated_text': 'ball in cup, catch (DMControl500k)dataset, \\n       highest benchmark result achieved on the Ball in cup, catch (DMControl500k)dataset, \\n       highest benchmark result achieved on the Ball in cup, catch (DMControl500k)dataset, \\n       highest benchmark result achieved on the Ball in cup, catch (DMControl500k)dataset, \\n       highest benchmark result achieved on the Ball in cup, catch (DMControl500k)dataset, \\n       highest benchmark result achieved on the Ball in cup, catch (DMControl500k)dataset, \\n       highest benchmark result achieved on the Ball in cup, catch (DMControl500k)dataset, \\n       highest benchmark result achieved on the Ball in cup, catch (DMControl500k)dataset, \\n       highest benchmark result achieved on the Ball in cup, catch (DMControl500k)dataset, \\n       highest benchmark result achieved on the Ball in cup, catch (DMControl500k)dataset, \\n       highest benchmark result achieved on the Ball in cup, catch (DMControl500k)dataset, \\n       highest benchmark result achieved on the Ball in cup, catch (DMControl500k)dataset, \\n       highest benchmark result achieved on the Ball in cup,'}], [{'generated_text': 'BRIEF-MI03'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl \\nWHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Atari Games\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Atari Games\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \\n    ?benchmark orkgp:HAS_DATASET?dataset. \\n    ?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"BC5CDR-disease\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?paper_title_id?paper_title_title.?paper_id?paper_id.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'The top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset is Lucid Motors Deep RL v1.0 (https://www.deep RL tutorial. lucian wood.io/lucid-motors-reinforcement-learning-rl-v1-0.html). The model achieved a Score of 0.79.'}], [{'generated_text': 'Common metrics used when benchmarking models on the FSNS are Precision-Recall (Pr) and Mean Average Precision (mAP)'}], [{'generated_text': 'SELECT DISTINCT \\n ?dataset?dataset_lbl \\nWHERE { \\n ?problem a orkgc:Problem; \\n  rdfs:label?problem_lbl. \\n  FILTER (str(?problem_lbl) = \"Atari Games\") \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem. \\n}'}], [{'generated_text': 'dance \\nsong \\ntrack \\nruntime \\nbeat \\nmeter'}], [{'generated_text': 'PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \\n\\nSELECT DISTINCT?dataset?dataset_lbl WHERE { \\n    ?problem a orkgc:Problem; \\n     rdfs:label?problem_lbl. \\n     FILTER (str(?problem_lbl) = \"Atari Games\") \\n    ?dataset a orkgc:Dataset; \\n     rdfs:label?dataset_lbl. \\n    ?benchmark orkgp:HAS_DATASET?dataset. \\n    ?cont orkgp:HAS_BENCHMARK?benchmark; \\n     orkgp:P32?problem. \\n}'}]]\n","0 0\n","1 0\n","2 0\n","3 0\n","4 0\n","5 0\n","6 0\n","7 0\n","8 0\n","9 0\n","0 2\n","1 2\n","2 2\n","0 4\n","1 4\n","2 4\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","1 7\n","2 7\n","3 7\n","4 7\n","0 9\n","0 10\n","1 10\n","2 10\n","3 10\n","0 11\n","1 11\n","2 11\n","3 11\n","4 11\n","5 11\n","6 11\n","7 11\n","8 11\n","9 11\n","0 15\n","1 15\n","2 15\n","3 15\n","4 15\n","5 15\n","6 15\n","7 15\n","8 15\n","9 15\n","0 16\n","0 18\n","1 18\n","2 18\n","[[{'generated_text': \"The 'Story Cloze Test' dataset was created by @vermyst, and has been benchmarked against state-of-the-art language model and music model architectures. The benchmark was created with the objective of comparing their ability to identify the storyline of stories. The dataset is composed of around 400,000 queries from the news domain with only one correct answer per query. The answer is based on the storyline identified by a language model, but the model should also predict the next word given the answer. The model that achieved the best result on this dataset is the P32 language model proposed by @vermyst, achieving an accuracy score of 71.33%.\"}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE { \\n?problem a orkgc:Problem;\\nrdfs:label?problem_lbl. \\nFILTER (str(?problem_lbl) = \"Music Modeling\") \\n?dataset a orkgc:Dataset;\\nrdfs:label?dataset_lbl. \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark;\\norkgp:P32?problem. \\n}'}], [{'generated_text': 'The UrbanSound8k benchmark datasets are being evaluated by a variety of models across the Music Modeling research area. The models include generative adversarial networks (GANs), Variational Autoencoders (VAEs), stacked Denoising Autoencoders (SDAs), Mirco-Networks, convolutional neural networks (CNNs), and Restricted Boltzmann Machines (RBMs). \\n\\nThe models and their associated datasets can be found at this link: \\nhttps://music-research.github.io/datasets/urbansound8k.html'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE { \\n ?problem a orkgc:Problem; \\n  rdfs:label?problem_lbl. \\n  FILTER (str(?problem_lbl) = \"Music Modeling\") \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem. \\n}'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Music Modeling\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \\n\\n(\\\\\\\\begin{BenchmarkResult}Metric:P32,Timing:False,Cases:20\\\\\\\\ \\\\end{BenchmarkResult})'}], [{'generated_text': 'model: sgs:sgsm; model_name: Smoothing Global Self-Supervised Model; benchmark: TDMSci; dataset: TDMSci; problem: Orkgc:Problem; dataset_lbl: Music Modeling; dataset_lbl_pl: Music Modeling; benchmark_pl: false; dataset_pl: ; problem_pl: Orkgc:Problem; dataset_pl: ;'}], [{'generated_text': 'The mean capacity of a carbon-based fuel is 717.71 mkgpe-1.'}], [{'generated_text': '< Problem.?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Music Modeling\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \\n?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. >'}], [{'generated_text': 'Kuzushiji-MNIST'}], [{'generated_text': 'select distinct?dataset_lbl \\n  where {\\n   ?problem orkgc:Problem;\\n    rdfs:label?problem_lbl.\\n    FILTER (str(?problem_lbl) = \"Music Modeling\") \\n   ?dataset a orkgc:Dataset;\\n    rdfs:label?dataset_lbl.\\n   ?benchmark orkgp:HAS_DATASET?dataset.\\n   ?cont orkgp:HAS_BENCHMARK?benchmark;\\n    orkgp:P32?problem.\\n  }'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Music Modeling\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \\n\\n?dataset b catch (DMControl500k)'}], [{'generated_text': 'Top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset is called \"Seinen9\"'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Music Modeling\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }\\n\\nThe following papers are related to benchmarking the arXiv dataset:'}], [{'generated_text': 'rdfs:label \"Music Modeling\"\\nrdfs:label \" similarity \"\\nrdfs:label \" distinctiveness \"\\nrdfs:label \" entropy \"\\nrdfs:label \" vector similarities \"\\nrdfs:label \" max vector similarities \"\\nrdfs:label \" cosine similarities \"\\nrdfs:label \" logistic regression \"\\nrdfs:label \" precision-recall \"\\nrdfs:label \" f1-score \"\\nrdfs:label \" coverage \"\\nrdfs:label \" average recall \"\\nrdfs:label \" harmonic mean \"\\nrdfs:label \" sum of pairwise differences \"\\nrdfs:label \" volume of matched notes \"\\nrdfs:label \" jamming distance \"\\nrdfs:label \" mean interval between notes \"\\nrdfs:label \" zero-shot pretraining \"\\nrdfs:label \" synergy gap \"\\nrdfs:label \" zero-shot test accuracy \"\\nrdfs:label \" sample entropy \"\\nrdfs:label \" long-term cross-correlation \"\\nrdfs:label \" triple-diameter \"\\nrdfs:label \" reverberation time \"\\nrdfs:label \" coherence \"\\nrdfs:label \" combinatorial'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl \\n   WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Music Modeling\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'LunarLander'}], [{'generated_text': 'Commonly used metrics for music modelling evaluation include:\\n- Difference between human judged performance and system produced performance \\n- F0 values\\n- Perception-level consonance'}], [{'generated_text': '--ImageNet Benchmark\\nSELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"ImageNet Benchmark\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET ImageNet:64x64Dataset.?cont orkgp:HAS_BENCHMARK ImageNet:64x64Dataset; orkgp:P32?problem. }'}], [{'generated_text': 'SELECT DISTINCT?dataset_lbl?dataset_lbl_label WHERE { \\n   ?dataset a orkgc:Dataset ; \\n    rdfs:label?dataset_lbl. \\n    FILTER(str(?dataset_lbl) = \"Music Modeling\") \\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark ; \\n    orkgp:P32?problem. }'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl \\n  WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. \\n    FILTER (str(?problem_lbl) = \"Music Modeling\")?dataset a orkgc:Dataset; \\n    rdfs:label?dataset_lbl.\\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark; \\n    orkgp:P32?problem. \\n  }'}]]\n","0 0\n","1 0\n","2 0\n","0 2\n","1 2\n","2 2\n","3 2\n","4 2\n","5 2\n","6 2\n","7 2\n","0 5\n","1 5\n","2 5\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","0 8\n","0 9\n","0 11\n","1 11\n","2 11\n","3 11\n","4 11\n","5 11\n","6 11\n","7 11\n","0 13\n","0 15\n","1 15\n","0 16\n","1 16\n","[[{'generated_text': 'The baselines achieved the highest Accuracy score on the Story Cloze Test benchmark dataset. The baseline model was provided by the DeepL team at InHouse Research atsalesforce.com.'}], [{'generated_text': '1 - Benchmarking Lexical Models on the Penn Treebank\\n2 - Benchmarking Named-Entity Recognition against the Webwise dataset\\n3 - Benchmarking RNNLM on WMT16'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Text Generation\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Text Generation\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \\n\\nA:\\n\\nHere are some of the most widely used datasets for the text generation research field:\\n\\nPenn Treebank\\nWikipedia article text\\nSentencePaddles\\nOntoGPT\\nParboiled\\n\\nAnswers come from answers to my question on stackoverflow.'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Text Generation\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE { \\n   ?problem a orkgc:Problem; \\n    rdfs:label?problem_lbl. \\n    FILTER (str(?problem_lbl) = \"Text Generation\") \\n   ?dataset a orkgc:Dataset; \\n    rdfs:label?dataset_lbl. \\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark; \\n    orkgp:P32?problem. \\n}'}], [{'generated_text': 'Carbon-based fuel - mean capacity of a fuel in tons of carbon dioxide equivalent (CO2e).'}], [{'generated_text': 'In the following list, the papers and their titles are:\\n1. [Bharat Kumar, Geoffrey Zens, Paul de Bakker, Samy Bengio, and Yoshua Bengio. Zero-shot learning of natural language-Russian translations. abs/1811.10256, 2018.][1] \\n2. [Viktor Ambartsumyan, Ilya Kembrelidze, and Alexander Lountov. Zero-shot learning of natural language: Russian translations. In International conference on knowledge discovery and data mining. ACM, 2018.][2]\\n3. [Elina Kelemen, Evelin Leszkay, Alessandro Rasti, Berthold Hari, and Hanzhu Li. Eteo: A Dataset for Text-to-Language Pretraining. Proceedings of the Workshop on Developing Dataset and Model Combination for Language and Technology, 2019.][3]'}], [{'generated_text': 'indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset:  Oracle Planner'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl \\n    WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Text Generation\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }\\n   ?dataset rdfs:label?dataset_lbl.\\n    {\\n       ?dataset rdfs:label \"WMT2016 Romanian-English\"@en.\\n       ?benchmark orkgp:HAS_DATASET \"wmt16- Romanian - English\"@en.\\n       ?benchmark orkgp:P32 \"wmt16- Romanian - English\"@en.\\n    }\\n    {\\n       ?dataset orkgc:Dataset; rdfs:label \"WMT2016 Romanian-English\";\\n       ?benchmark orkgp:HAS_BENCHMARK \"wmt16- Romanian - English\"@en.\\n       ?bench'}], [{'generated_text': 'Most commonly used benchmark datasets for text generation research field:'}], [{'generated_text': '<b>Sparql console output</b><br />\\nPREFIX :  <a href=\"http://www.semanticweb.org/ontologies/vtab/1.0/vtab#\">vtab#</a><br />\\nPREFIX orkgc:   <a href=\"http://www.obofoundry.org/ontology/org.obolibrary.commercio.keyword-search.keywordDataset.full\">org.obolibrary.commercio.keyword-search.keywordDataset.full</a><br />\\nPREFIX rdfs:   <a href=\"http://www.w3.org/2000/01/rdf-schema/\">http://www.w3.org/2000/01/rdf-schema/</a><br />\\nPREFIX rdf:   <a href=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">http://www.w3.org/1999/02/22-rdf-syntax-ns/</a><br />\\nPREFIX Sparql: <a href=\"http://www.w3.org/2005/'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE { \\n    ?problem a orkgc:Problem; \\n     rdfs:label?problem_lbl. \\n     FILTER (str(?problem_lbl) = \"Text Generation\")?dataset a orkgc:Dataset; \\n     rdfs:label?dataset_lbl. \\n    ?benchmark orkgp:HAS_DATASET?dataset. \\n    ?cont orkgp:HAS_BENCHMARK?benchmark; \\n     orkgp:P32?problem. \\n}'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Text Generation\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'select distinct?dataset?dataset_lbl where \\n{ \\n?problem a orkgc:Problem; \\nrdfs:label?problem_lbl. \\nFILTER(str(?problem_lbl) = \"Text Generation\") \\n?dataset a orkgc:Dataset; \\nrdfs:label?dataset_lbl. \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark; \\norkgp:P32?problem. \\n}'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE { \\n ?problem a orkgc:Problem; \\n  rdfs:label?problem_lbl. \\n  FILTER (str(?problem_lbl) = \"Text Generation\") \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem. \\n} \\n ?benchmark orkgp:HAS_BENCHMARK orkgp:p32 \"Lunar Lander\"@en.'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE { \\n?problem a orkgc:Problem; \\n rdfs:label?problem_lbl. \\n FILTER (str(?problem_lbl) = \"Text Generation\")?dataset a orkgc:Dataset; \\n rdfs:label?dataset_lbl. \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark; \\n orkgp:P32?problem. \\n}'}], [{'generated_text': 'The most commonly used benchmark datasets for the Text Generation research field are:\\n\\n- A universal sentence model ( stochasticpw \\n- Hierarchical test on a selection of transformer models \\n- CBMIIG \\n- Quora natural language inference \\n- Natural-Language Generation for Real-World Conversational States \\n- Phrase-based Neural Machine Translation'}], [{'generated_text': 'select\\ndistinct?dataset?dataset_lbl where {?problem a orkgc:Problem; \\n    rdfs:label?problem_lbl.  FILTER (str(?problem_lbl) = \"Text Generation\") \\n   ?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. ?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'Most commonly used benchmark datasets for text generation research field: \\n\\t- Zalo.test\\n\\t\\tData: https://zalot.unizar.es/zalot/static/datasets/zalo.test.odp\\n\\t- Wiki10k.txt\\n\\t\\tData: https://github.com/tensorflow/models/blob/master/imagenet/zoo/ Wiki10k.txt\\n\\t\\tLabel: text generation\\n\\t\\tSetup: https://github.com/tensorflow/models/blob/master/imagenet/zoo/zalo.test.sh'}]]\n","0 0\n","1 0\n","2 0\n","3 0\n","4 0\n","5 0\n","6 0\n","7 0\n","8 0\n","0 1\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","1 7\n","2 7\n","3 7\n","4 7\n","5 7\n","6 7\n","7 7\n","8 7\n","9 7\n","0 8\n","0 10\n","1 10\n","0 11\n","1 11\n","0 14\n","1 14\n","2 14\n","0 17\n","1 17\n","2 17\n","3 17\n","4 17\n","5 17\n","6 17\n","7 17\n","8 17\n","9 17\n","0 18\n","0 19\n","1 19\n","2 19\n","3 19\n","4 19\n","5 19\n","6 19\n","7 19\n","8 19\n","9 19\n","[[{'generated_text': 'From the provided statement, it appears that you are interested in the following property:\\n\\nrdfs:label - Story Cloze Test'}], [{'generated_text': 'Penn Treebank (Word Level) Benchmark Dataset - High-Performance Computation'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset:\\n\\n?dataset :PennTreebank\\n ?benchmark :HoC\\n ?model   :PennModel\\n ?model_lbl :PennLabel\\n ?eval    :SomeEvaluation\\n ?eval_lbl :SomeLabel\\n ?value   :0.919\\n ?dataset_lbl :PennTreebank\\n ?model_lbl :PennLabel\\n ?eval    :SomeEvaluation\\n ?eval_lbl :SomeLabel\\n ?value   :0.919'}], [{'generated_text': 'The model named \"TELLING MODELS TO ADVise\" achieved a score of 0.995 on the HOHeuristic COmputational model Evaluation benchmark on the Mazea dataset.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl \\n WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDMSci\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The best mean F1 score on the HoC benchmark dataset is achieved by the Perceptron model.'}], [{'generated_text': '[\\n  {\\n    \"model\": \"xgboost\",\\n    \"model_lbl\": \"XGBoost\"\\n  },\\n  {\\n    \"model\": \"polynomial\",\\n    \"model_lbl\": \"Polynomial\"\\n  },\\n  {\\n    \"model\": \"linear\",\\n    \"model_lbl\": \"Linear\"\\n  }\\n]'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a ou: bleu; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU\") { SELECT?model?model_lbl WHERE {?dataset a ou: bleu; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Romanian-English\")?benchmark ou: bleu:Benchmark; ou: bleu:model?model. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Ball in Cup, Catch (DMControl500k)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nThe highest F1 score achieved on the HoC benchmark dataset is 0.91334'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The highest F1 score on the HoC benchmark dataset was achieved by    \\'Generative Adversarial Networks\\' model: https://github.com/OCP-AI/HoC-Adversarial. The model ID is \"GN\", the paper ID is \"2113\", and the title is \"Generative Adversarial Networks for High-efficiency Video Encryption\"'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset:\\n\\nProvide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset:\\n\\nPlease provide the following research paper titles and IDs:\\n\\nPlease provide the following research paper titles and IDs:\\n\\nThese are the titles and IDs that provide the following research paper titles and IDs:\\n\\nThe following are the titles and IDs that provide the following research paper titles and IDs:\\n\\nThe following are the titles and IDs that provide the following research paper titles and IDs:\\n\\nThe following are the titles and IDs that provide the following research paper titles and IDs:\\n\\nThe following are the titles and IDs that provide the following research paper titles and IDs:\\n\\nThe following are the titles and IDs that provide the following research paper titles and IDs:\\n\\nThe following are the titles and IDs that provide the following research paper titles and IDs:\\n\\nThe following are the titles and IDs that provide the following research paper titles and IDs:'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl \\n where {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': '<b>SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE { \\n ?metric a orkgc:Metric; \\n  rdfs:label?metric_lbl. \\n  FILTER (str(?metric_lbl) = \"F1\") { \\n    SELECT?model?model_lbl WHERE { \\n     ?dataset a orkgc:Dataset; \\n      rdfs:label?dataset_lbl. \\n      FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset; \\n      orkgp:HAS_EVALUATION?eval. \\n     ?eval orkgp:HAS_VALUE?value; \\n      orkgp:HAS_METRIC?metric. \\n     ?cont orkgp:HAS_BENCHMARK?benchmark; \\n      orkgp:HAS_MODEL?model. \\n     ?model rdfs:label?model_lbl. \\n    } \\n    ORDER BY DESC(?value) \\n    LIMIT 1 \\n  } \\n } \\n}'}], [{'generated_text': 'The following papers utilize the Flair-TDM model and can be used for context:\\n\\n- [Practical Atonyms with Topic Modelling and Deep Learning](https://arxiv.org/abs/1811.05044)\\n- [FLAIR: The Flair-TDM Model for Automatic Scaling of Topic Models](https://arxiv.org/abs/1906.00141)\\n- [Deeper Learning of Topic Models with Topical Decomposition, Deep Learning, and Paraphrase resolution](https://arxiv.org/abs/1811.00406)\\n- [Improving Topic Modelling with Deep Learning on Low-Resource Data](https://www.ips-dok.de/files/Files/1702/IPS%201702%20%20M.Faust.pdf)\\n- [Empirical Evaluation of Neural MT: Investigating Qualities, Trends, and Limitations](https://www.ijs.si/fileadmin/upload/documenti/ettudini/2019/2019.03.etUD.flair-tdm.pdf)\\n- [Evaluation of Neural Machine Translation with Document-Level Quality Metrics](https'}]]\n","0 0\n","1 0\n","0 1\n","1 1\n","0 3\n","0 4\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","1 7\n","0 12\n","1 12\n","2 12\n","0 14\n","1 14\n","2 14\n","3 14\n","4 14\n","0 19\n","1 19\n","2 19\n","3 19\n","4 19\n","5 19\n","6 19\n","7 19\n","8 19\n","9 19\n","[[{'generated_text': 'SELECT DISTINCT?model?model_lbl \\n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Story Cloze Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric. orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model. orkgp:HAS_MODEL_RESULT?model_res; orkgp:HAS_VALUE?value_res;?benchmark rdfs:label?benchmark_lbl;?eval orkgc:Metric;?model orkgc:Metric;?model_res?model_res_lbl;?value_res?value_res_lbl } \\n ORDER BY DESC(?value_res) }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UrbanSound8k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': '4.0+ million\\n4gram\\nLSTM\\nBigLSTM\\nGRU\\nGRU-alphabet\\nDART\\nDART-alphabet\\nDeepBird\\nDeepBird-alphabet\\nLanguageModel\\nLinearRegression\\nSVM\\nNeuralNetwork'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MAZEA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nmazeadataset:F1'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDMSci\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The Toyota FT-86 has the highest mean capacity of a carbon-based fuel of 11.25 gallons per kilogram.'}], [{'generated_text': 'Here is a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset:\\n+------------------------------+---------------------------+------+\\n| Research Paper                | Description              | ID    |\\n+------------------------------+---------------------------+------+\\n| Dhingra, Narendra, et al.      | Clarkus Academy for Data |  103 |\\n| [Dhingra:ClarkusAcadData17]   | Improves zero-shot       |  103 |\\n| [Dhingra:ImprovesZSS17]       | translation on real      |  103 |\\n|                              | data using constrained  |      |\\n|                              | transformer               |  104 |\\n|                              | networks                    |  104 |\\n|                              | [Dhingra:ClarkusAcadData17] |  103 |\\n+------------------------------+---------------------------+------+'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kuzushiji-MNIST\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nThe model that performed best on the Kuzushiji-MNIST benchmark dataset was the inception-v3 model from the Tensorflow Deep Learning library.  The inception-v3 model achieved an F1 score of 0.9593'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a rnw:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU\") { SELECT?model?model_lbl WHERE {?dataset a rnw:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016\")?benchmark rnw:HAS_DATASET?dataset; rnw:HAS_EVALUATION?eval. rnw:HAS_VALUE?value; rnw:HAS_METRIC?metric. rnw:HAS_BENCHMARK?benchmark; rnw:HAS_MODEL rnw:REF?model. rnw:LABEL?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }\\nThe Romanian-English WMT2016 benchmark dataset is hosted by the.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nwhich model has achieved the highest F1 score on the NYT benchmark dataset?\\nThe model DBSCAN with metric value 0.8 is the highest scoring model on the NYT benchmark dataset.\\nThe model DBSCAN with metric value 0.8'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nmodel    model_lbl\\n---------- ----------\\nLSTM      LSTM Score 1\\nLSTM      LSTM Score 2\\nLSTM      LSTM Score 3'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl \\n  WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDM\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nNote: This query will only work if the model you are querying for has a label of \"F1\".'}], [{'generated_text': 'There is one research paper that benchmarked models on the BC5CDR-disease dataset: \\n-  Abrol at  et al., \"Multi-label Disease Relevance Modeling on Biomedical Datasets using Label Co- Count\" (2019)\\nThe paper benchmarked 6 models on the dataset:\\n- Gated-LSTM\\n- DistMult\\n- Enc. Att.\\n- DistMult Att. \\n- DenseNet\\n- Gated-RNN'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric;?model?model_lbl WHERE {?dataset a orkgc:Dataset;?dataset_lbl  Lunar_Lander;?benchmark orkgp:HAS_DATASET lunarl:lunar_lander;?eval orkgp:HAS_EVALUATION;?value 9.8;?metric orkgp:HAS_VALUE score;?cont orkgp:HAS_BENCHMARK lunarl:lunar_lander;?benchmark orkgp:HAS_MODEL ;?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS-Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nThe FSNS - Test dataset evaluates the performance of learning algorithms against hard-coded baselines. Common evaluation metrics are described in the following section.'}], [{'generated_text': 'The following are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset: \\n- \"ImageNet-2009: a Large-Scale Training and Testing Database for Deep Learning,\" by Frühwirth-Schnatter, Marcus, Kilony, Shalev, and Gal, Evan (2011) \\n- \"The MovieLens Dataset: An Open Database for Machine Learning Research,\" by <a href=\"http://www.ics.uci.edu/~eppstein/MLbooks/MLbook.pdf\" target=\"_blank\">Economists and scientists at the University of California, Irvine</a>, by E. S. Suchard, Kaiming X. Shi, Lillian Cheng, and Weinan Zhang (2010) \\n- \"Benchmarking Automatic Image Recognition Algorithms: Principles, Practice, and Progress,\" by Cuadros, et al., (2011) \\n- \"Handwritten Digit Recognition Using Deep Neural Networks,\" by Zweig, et al., (2010)'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"classical music\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric. orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model. orkgp:HAS_LABEL?label. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'Flair-TDM'}]]\n","0 3\n","1 3\n","2 3\n","0 6\n","0 7\n","1 7\n","2 7\n","3 7\n","4 7\n","5 7\n","6 7\n","7 7\n","8 7\n","9 7\n","0 14\n","1 14\n","2 14\n","3 14\n","4 14\n","5 14\n","6 14\n","7 14\n","8 14\n","9 14\n","0 17\n","1 17\n","0 19\n","1 19\n","2 19\n","3 19\n","4 19\n","5 19\n","6 19\n","7 19\n","8 19\n","9 19\n","[[{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Story Cloze Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE { \\n?metric a orkgc:Metric; \\n rdfs:label?metric_lbl. \\n FILTER (str(?metric_lbl) = \"F1\") \\n { \\n SELECT?model?model_lbl WHERE { \\n?dataset a orkgc:Dataset; \\n rdfs:label?dataset_lbl. \\n FILTER (str(?dataset_lbl) = \"AAPD\") \\n?benchmark orkgp:HAS_DATASET?dataset; \\n orkgp:HAS_EVALUATION?eval. \\n?eval orkgp:HAS_VALUE?value; \\n orkgp:HAS_METRIC?metric. \\n?cont orkgp:HAS_BENCHMARK?benchmark; \\n orkgp:HAS_MODEL?model. \\n?model rdfs:label?model_lbl. \\n } \\n ORDER BY DESC(?value) \\n LIMIT 1 \\n } \\n } \\n input (English text): List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?\\n output (Sparql'}], [{'generated_text': 'Model,Model_lbl\\n------------\\nxgboost,XGBoost\\nrandomForest,Random Forest\\nelasticnet,Elastic Net\\nDecision Tree,Decision Tree'}], [{'generated_text': '{ \\n  \"#text\": \"syntactic patterns in spoken words\", \\n  \"rdfs:label\": \"syntactic patterns in spoken words\", \\n  \"model\": \"en-corenlp-2019-10\", \\n  \"model_lbl\": \"en-corenlp-2019-10\" \\n}\\n{ \\n  \"#text\": \"Combined Lexico-Semantic Pattern Recognition for Spoken Dialogue with Dynamic Memory Augmentation\", \\n  \"rdfs:label\": \"Combined Lexico-Semantic Pattern Recognition for Spoken Dialogue with Dynamic Memory Augmentation\", \\n  \"model\": \"al-awady_etal_2019-10\", \\n  \"model_lbl\": \"al-awady_etal_2019-10\" \\n}\\n{ \\n  \"#text\": \"Aligning CoNLL\\xa0XXX and WikiText-2 with Semantic Role Labeling\", \\n  \"rdfs:label\": \"Aligning CoNLL\\xa0XXX and WikiText-2 with Semantic Role Labeling\", \\n  \"model\": \"guu_etal_2019-10\", \\n  \"model_lbl\": \"guu_etal_'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MAZEA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nMAP score for the MazeA dataset and its metric is 0.9994 and the highest score is 0.9997'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDMSci\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The mean capacity of a carbon-based fuel is 587.72 milli-gigajoules per kilogram.'}], [{'generated_text': 'There are two papers that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset:\\n- Minerva et al., 2020 [1]\\n- Palo et al., 2019 [2]'}], [{'generated_text': 'Select distinct?model?model_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kuzushiji-MNIST\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric. orkgp:HAS_BENCHMARK?cont; orkgp:HAS_MODEL?model. rdfs:label?model_lbl. } ORDER BY DESC(?value)'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Romanian-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?model orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AAPD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AAPD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nThe model with the highest score on the AAPD benchmark dataset is the Extended Log-Linear model'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl \\nWHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The AAPD benchmark dataset measures the performance of machine learning models on named entity recognition in plain-text or natural language text. The F1 score is the most commonly used metric in NER. The AAPD benchmark dataset includes 15 named entity types and 5 dimensions of named entity type-specific scores.'}], [{'generated_text': '[\\n  {\\n    \"model\": \"gamma\",\\n    \"model_lbl\": \"Gamma\"\\n  },\\n  {\\n    \"model\": \"fast.rcnn\",\\n    \"model_lbl\": \"Fast R-CNN\"\\n  },\\n  {\\n    \"model\": \"fast.rcnn\",\\n    \"model_lbl\": \"Fast R-CNN 50\"\\n  },\\n  {\\n    \"model\": \"fast.faster.rcnn\",\\n    \"model_lbl\": \"Faster RCNN\"\\n  },\\n  {\\n    \"model\": \"fast.faster.rcnn\",\\n    \"model_lbl\": \"Faster RCNN 50\"\\n  },\\n  {\\n    \"model\": \"faster_rcnn\",\\n    \"model_lbl\": \"Faster R-CNN\"\\n  },\\n  {\\n    \"model\": \"faster_rcnn\",\\n    \"model_lbl\": \"Faster R-CNN 50\"\\n  },\\n  {\\n    \"model\": \"ssd\",\\n    \"model_lbl\": \"SSD\"\\n  }\\n]'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AAPD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'F1'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AAPD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AAPD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The Flair-TDM model has achieved the highest F1 score on the AAPD benchmark dataset.\\nhttp://flair-tdm.org/\\nhttp://github.com/dkantar/flair-tdm'}]]\n","0 2\n","1 2\n","0 3\n","1 3\n","2 3\n","3 3\n","4 3\n","5 3\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","1 7\n","2 7\n","3 7\n","4 7\n","5 7\n","6 7\n","7 7\n","0 8\n","1 8\n","0 13\n","0 14\n","1 14\n","2 14\n","3 14\n","4 14\n","5 14\n","6 14\n","7 14\n","8 14\n","0 16\n","0 19\n","1 19\n","2 19\n","3 19\n","4 19\n","5 19\n","6 19\n","7 19\n","8 19\n","9 19\n","[[{'generated_text': 'The following model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset:'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RTE\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UrbanSound8k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RTE\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MazeA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'models: orkgp:modelQueryEvaluation orkgp:modelPredictionOrNetworks orkgp:modelPredictionOrNetworks'}], [{'generated_text': '<iq id=\"491086\" to=\"jpl\" type=\"get\" xmlns=\"jabber:client\" xmlns:orkgc=\"http://www.orael.com/webxplorer/common/common/xquery/orkgc\">\\n<query type=\"property\"/>\\n<query type=\"data-model\"/>\\n<query type=\"data-model-list\"/>\\n<query type=\"sparql\"/>\\n<query type=\"result-set\"/>\\n<query type=\"result-set-list\"/>\\n<query type=\"collection\"/>\\n<query type=\"collection-list\"/>\\n<query type=\"row\"/>\\n<query type=\"cell\"/>\\n<query type=\"range\"/>\\n<query type=\"unique\"/>\\n<query type=\"bag\"/>\\n<query type=\"facets\"/>\\n<query type=\"filter\"/>\\n<query type=\"property-list\"/>\\n<query type=\"literal\"/>\\n<query type=\"data-literal\"/>\\n<query type=\"data-literal-list\"/>\\n<query type=\"xsd\"/>\\n<query type=\"xsd-list\"/>\\n<query type=\"xsd-sequence\"/>\\n<query type=\"xsd-union\"/>\\n<query type=\"xsd-element\"/>'}], [{'generated_text': 'title\\nid\\nGo \\n1\\nRTE \\n2\\nRTE-1\\n3\\nRTE-2\\n4\\nMTurk\\n5\\nMTurk-1\\n6\\nMTurk-2\\n7\\nNeural Machine Translation\\n8\\nNeural Machine Translation-1\\n9\\nNeural Machine Translation-2\\n10\\nNatural Language Learning\\n11\\nNatural Language Learning-1\\n12\\nNatural Language Learning-2\\n13\\nJieBaoNiGuo'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RTE\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'model name: BERT-LargeLM\\nmetric: BLEU\\nscore: 40.9994'}], [{'generated_text': 'The highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value is 17.6931'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDM Tagged Corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': '* The RTE Benchmark Dataset:\\nBC5CDR-disease\\n1000'}], [{'generated_text': 'select distinct metric as metric_lbl (max(value)) where {\\n  SELECT metric as metric_lbl, score\\n  FROM <optimizedbytrickar>\\n  WHERE <optimizedbytrickar>.dataset a orkgc:Dataset;\\n    rdfs:label <optimizedbytrickar>.dataset_lbl.\\n  FILTER (str( <optimizedbytrickar>.dataset_lbl) = \"RTE\")\\n  OR kgp:HAS_DATASET <optimizedbytrickar>.dataset;\\n    OR kgp:HAS_EVALUATION <optimizedbytrickar>.eval.\\n    OR kgp:HAS_VALUE <optimizedbytrickar>.value.\\n  OPTIONAL {\\n   ?eval OR kgp:HAS_METRIC <optimizedbytrickar>.metric.\\n    <optimizedbytrickar>.metric rdfs:label <optimizedbytrickar>.metric_lbl.\\n  }\\n  ORDER BY DESC(<optimizedbytrickar>.value)\\n} GROUP BY metric'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE {\\n\\n  { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet64x64\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } }\\n\\n GROUP BY?metric?metric_lbl'}], [{'generated_text': 'rdfs:label\\n Classical music\\n\\nrdfs:label?benchmark\\nTopBenchmark'}], [{'generated_text': 'The top benchmark score and its metric on the RTE dataset is:\\n* score - 7.4\\n* metric - IsOLevelTerminal'}]]\n","0 0\n","1 0\n","2 0\n","3 0\n","4 0\n","5 0\n","6 0\n","7 0\n","8 0\n","0 5\n","1 5\n","2 5\n","3 5\n","4 5\n","5 5\n","6 5\n","7 5\n","8 5\n","9 5\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","0 7\n","1 7\n","2 7\n","0 9\n","0 10\n","1 10\n","2 10\n","0 14\n","1 14\n","2 14\n","3 14\n","4 14\n","5 14\n","6 14\n","7 14\n","8 14\n","9 14\n","0 18\n","1 18\n","2 18\n","3 18\n","4 18\n","5 18\n","6 18\n","0 19\n","1 19\n","2 19\n","3 19\n","4 19\n","5 19\n","6 19\n","7 19\n","8 19\n","9 19\n","[[{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE {\\n\\n  {\\n\\n    SELECT?metric?metric_lbl?value WHERE {\\n\\n      {\\n\\n        SELECT?metric?metric_lbl?value WHERE {\\n\\n          {\\n\\n            SELECT?metric?metric_lbl?value WHERE {\\n\\n              {\\n\\n                SELECT?metric?metric_lbl?value WHERE {\\n\\n                  {\\n\\n                    SELECT?metric?metric_lbl?value WHERE {\\n\\n                      {\\n\\n                        SELECT?metric?metric_lbl?value WHERE {\\n\\n                          {\\n\\n                            SELECT?metric?metric_lbl?value WHERE {\\n\\n                              {\\n\\n                                SELECT?metric?metric_lbl?value WHERE {\\n\\n                                  {\\n\\n                                    SELECT?metric?metric_lbl?value WHERE {\\n\\n                                      {\\n\\n                                        SELECT?metric?metric_lbl?value WHERE {\\n\\n                                            {\\n\\n                                            SELECT?metric?metric_lbl?value WHERE {\\n\\n                                               {\\n                                                SELECT?metric?metric_lbl?value WHERE {\\n\\n                                                  {'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': '*\\xa0CoNLL-2010\\n*\\xa0EMNLP 2010\\n*\\xa0NAACL 2010\\n*\\xa0ACL 2010\\n*\\xa0IJCNLP 2010\\n*\\xa0ACL 2010'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MAZEA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value)'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The following research papers have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset:\\n* https://arxiv.org/pdf/1906.07144.pdf\\n* https://arxiv.org/pdf/1911.10013.pdf'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The model having the highest BLEU score is called Kingsoftm. The BLEU score is defined as the normalized ratio of the length of the reference sentence (standard English) divided by the length of the generated translation.'}], [{'generated_text': 'SELECT DISTINCT DMControl500k.DMControl500k_DMCatch Catch.DMControl500k_DMControl500k Catch.DMControl500k_DMControl500k DESC, MAX(DMControl500k.DMControl500k_DMCatch) AS Catch.DMControl500k_DMControl500k'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE {\\n\\n  { SELECT?metric?metric_lbl?value WHERE {\\n\\n    { SELECT?metric?metric_lbl?value WHERE {\\n\\n      { SELECT?metric?metric_lbl?value WHERE {\\n\\n        { SELECT?metric?metric_lbl?value WHERE {\\n\\n          { SELECT?metric?metric_lbl?value WHERE {\\n\\n            { SELECT?metric?metric_lbl?value WHERE {\\n\\n              { SELECT?metric?metric_lbl?value WHERE {\\n\\n                { SELECT?metric?metric_lbl?value WHERE {\\n\\n                  { SELECT?metric?metric_lbl?value WHERE {\\n\\n                    { SELECT?metric?metric_lbl?value WHERE {\\n\\n                      { SELECT?metric?metric_lbl?value WHERE {\\n\\n                        { SELECT?metric?metric_lbl?value WHERE {\\n\\n                          { SELECT?metric?metric_lbl?value WHERE {\\n\\n                            { SELECT?metric?metric_lbl?value WHERE {\\n\\n                              { SELECT?metric?metric_lbl?value WHERE {\\n\\n                                { SELECT'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The top benchmark result (metric and value) over the dataset GAD is \"Precision\" 1.0000 and \"F1\" 0.989'}], [{'generated_text': 'The name of the top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset is \"grayscaler\" with a score of 11.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The top benchmark result over the dataset GAD is as follows:'}]]\n","0 3\n","1 3\n","2 3\n","3 3\n","4 3\n","5 3\n","0 7\n","1 7\n","2 7\n","3 7\n","0 9\n","0 14\n","1 14\n","2 14\n","0 15\n","0 19\n","1 19\n","2 19\n","3 19\n","4 19\n","5 19\n","6 19\n","7 19\n","8 19\n","9 19\n","[[{'generated_text': 'SELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Story Cloze Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The models being evaluated on the UrbanSound8k dataset include two neural networks: \"DHS-LSTM\" and \"DHS-GRU.\" Both models use approximately 12 billion parameters and 30 million parameters, respectively. Both models were trained on the Reuters and Wall Street Journal corpora and achieve F1 scores of 94.47% and 90.06%, respectively.'}], [{'generated_text': 'Per the NYT dataset README, there are four research paper titles which benchmarked their models on Penn Treebank dataset:\\n- Marian Mirzayish and Larry Abrahams, Deep contextual word representations enhance sentence revising, NLP6, Edinburgh, July 2019. \\n- Eric Pluming, Lei Zhang, Jonathan Abrahams, Marian Mirzayish and Larry Abrahams, Deep contextual word representations improve property-based text editing, ACL 2019.\\n- Andrea Mura and Andrea Borgo, DowlingTree: RNG-based distance functions for top-k common substring matching, CoNLL 2017.\\n- Florian Labate, Andrea Mura, Andrea Borgo, DowlingTree: Frequent substring matching on topologically restricted grammars, CoNLL 2017.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a mazea:MazeA; rdfs:label mazea:MazeAA; mazea:Benchmark result.?benchmark result:metric?metric.?benchmark result:score?score. } OPTIONAL {?eval mazea:EscapeTime?eval.?eval mazea:EscapeTimeDistance?time. } } ORDER BY DESC(?time) } } GROUP BY?metric'}], [{'generated_text': 'The highest benchmark result includes the metric and score orkgp:HAS_MODEL orkgp:MODEL in \"(MODEL) orkgp:MODEL\"'}], [{'generated_text': 'The highest benchmark result, including the metric and score, for the NYT dataset is:\\n \"Cities with the Largest Capacities of Carbon-Based Fuels\" : \\n{\\n\"metric\" : \"capacity\",\\n\"metric_lbl\" : \"Capacity\",\\n\"score\" : 74.33\\n}'}], [{'generated_text': '[\\n  { \"title\": \"Auto-Encryption for Apache Spark,\", \"id\": \"AART220\", \"description\": \"Extracting relevant information from big data in real time in a privacy-preserving way is crucial for a wide range of applications. However, current data storage formats such as Apache Spark’s Parquet are optimized for low-cost, offline processing and do not provide any protection against data breaches in the event that a machine storage device is compromised. We propose auto-encryption, a method to protect Spark data against machine compromise without incurring an unacceptable performance penalty. We leverage the fine-grained control over data-dependent sensitivity offered by recent deep learning techniques to enable strongly consistent encryption of large datasets in seconds without any online processing requirements. Our experiments show that auto-encrypted Spark datasets retain up to 94% of their original information content, resulting in a 41% increase in query performance over Parquet, while additionally providing strong privacy guarantees.\" },\\n  { \"title\": \"Zero-Shot Language Identification in the Wild: Current State of the Art\",, \"id\": \"0SHo15\", \"description\": \"The task of zero-shot language identification in the wild poses a significant challenge for both end-to'}], [{'generated_text': 'select distinct metric metric_lbl (max(value)) where {\\n    select metric metric_lbl  value where {\\n\\tdataset a orkgc:Dataset;\\n\\trdfs:label dataset_lbl.\\n\\tFILTER(str(dataset_lbl) = \"NYT\")\\n\\tbenchmark has_dataset.\\n\\thas_evaluation has_evaluation.\\n\\thas_value has_value.\\n\\tOPTIONAL\\n\\t has_metric has_metric.\\n\\trdfs:label metric_lbl.\\n    }\\n    option\\n    has_model has_model.\\n    has_model rdfs:label model_lbl.\\n}\\ngroup by metric metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016- Romanian English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Ball in cup, catch (DMControl500k) dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The metrics used to evaluate models on the TDM Tagged Corpus dataset are the following:\\n- BLEU (Computing BLEU on a Trees Model)\\n- ROUGE (Computing ROUGE on a Trees Model)\\n- TER (Computing TER on a Runners-Up Model)'}], [{'generated_text': 'The research paper titles are as following:\\n- Cancer Genomics and Genomics Data Informatics\\n- Building the Knowledge Base for Rare Disease\\n- Disease Gene Discovery by Association\\n- Machine Learning for Rare Disease and Genomics\\n- Disease Gene Discovery by Association and Machine Learning\\n- Utilizing Data Fusion and Machine Learning for Genomics\\n- A Multi-Omics Database for Rare Disease and Genome-wide Association Studies\\n- Methods for the Analysis of High Dimensional Genomics Data'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Lunar_Lander\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS-Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet_64x64\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Classical Music\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': '* Agarwal, Amit (2020): Flair-TDM: A Large Language Model for Legal Text Composition. \\n* Agarwal, Amit; Guha, Subhash; Choudhury, Misha; Gupta, Amit; Singh, Praveen; Chatterji, Amit; Marathe, Gagan; Mohammad, Saleem; Ma, Anhua; Badadehin, Norouz (2020): Flair-TDM: A Large Language Model for Legal Text Composition. \\n\\nNote: Links to code will be provided for these papers.\\n\\nAgarwal, Amit (2020): Flair-TDM: A Large Language Model for Legal Text Composition.\\nhttps://github.com/AGarwalAMIT/Flair-TDM\\nGuha, Subhash; Choudhury, Misha; Gupta, Amit; Singh, Praveen; Chatterji, Amit; Marathe, Gagan; Mohammad, Saleem; Ma, Anhua; Badadehin, Norouz (2020): Flair-TDM: A Large Language Model for Legal Text Composition. \\n\\nNote: Links to code will'}]]\n","0 2\n","1 2\n","0 3\n","1 3\n","2 3\n","3 3\n","0 5\n","1 5\n","2 5\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","1 7\n","2 7\n","0 8\n","1 8\n","0 13\n","1 13\n","0 14\n","1 14\n","0 19\n","1 19\n","2 19\n","3 19\n","[[{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RTE\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\n?metric                           a                  ?metric_lbl\\n\\n  Has_Dataset                   TRUE                RTE\\n\\n  HAS_METRIC                   Has_Evalution       accuracy\\n\\n  Has_Evalution               TRUE                Story Cloze Test Benchmark\\n\\n  accuracy                      100.0               96.4'}], [{'generated_text': 'RTE - Research Topic Encoding \\nBPE - Byte-Pair Encoding\\nBLEU - BLEU\\nPCT - Percent Cumulative Token Mapping\\nROUGE - Rouge\\nLARSA - Local Learner Report of Accuracy'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RTE\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {\\n   ?dataset a orkgc:Dataset;\\n    rdfs:label?dataset_lbl.\\n    FILTER (str(?dataset_lbl) = \"RTE\")\\n   ?benchmark orkgp:HAS_DATASET?dataset;\\n    orkgp:HAS_EVALUATION?eval.\\n    OPTIONAL {\\n     ?eval orkgp:HAS_METRIC?metric.\\n     ?metric rdfs:label?metric_lbl.\\n    }\\n  }'}], [{'generated_text': 'top_benchmark_score - MAZEA'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"TDMSci\")?benchmark orkgp:HAS_DATASET?dataset; \\n  orkgp:HAS_EVALUATION?eval. \\n  OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \\n}'}], [{'generated_text': 'Mean capacity of a carbon-based fuel'}], [{'generated_text': '<QueryResponse>\\n  <Run executed Gödelaan 1 produced 12 results>\\n   <results per page variable=\"pages\">12</results>\\n   <SearchResult>\\n     <Attributes>{ \"title\": \"The test of language\", \"id\": \"ldbd5d7d-90ce-49a1-a3ec-f02d17223555\" }, \\n       {\"title\": \"MTurk\", \"id\": \"9c49b2f2-caa5-4c2e-b3a1-a0d9499c2920\" }, \\n       {\"title\": \"MLDoc Zero-Shot\", \"id\": \"2557bb48-eceb-4d93-afd5-a120631d0f81\" }, \\n       {\"title\": \"From Page Rank to Page Javison: Exploring and Improving CTR and CPC for Online Advertising\", \"id\": \"cda18fe5-e996-49b0-86dc-29cb16d60da6\" }, \\n       {\"title\": \"Personalizing Search Results with Linguistic and Behavioral Features\", \"id\": \"82495e'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RTE\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\noutput: \\nmetric     metric_lbl\\n-------   ----------------\\nAccuracy    Kuzushiji-MNIST-accuracy'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RTE\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n?metric bello: bleu(?eval).'}], [{'generated_text': '#{'}], [{'generated_text': 'Top-1 Accuracy'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\nOVID'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {\\n\\n?dataset a orkgc:Dataset; \\n rdfs:label?dataset_lbl. \\n FILTER (str(?dataset_lbl) = \"TDM Tagged Corpus\")\\n\\n?benchmark orkgp:HAS_DATASET?dataset; \\n orkgp:HAS_EVALUATION?eval. OPTIONAL { \\n?eval orkgp:HAS_METRIC?metric. \\n?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'The following research paper titles and IDs have benchmarked models on the BC5CDR-disease dataset:\\n- Testset Transfer Learning for Cancers: A Benchmark on the ClinGen Disease Ontology \\n- Evaluation of Models on Efficiently Detected Disease Relevant Non-Exonic Variants in Next Generation Sequencing Data \\n- Machine Learning Model for Rare Disease Phenotype Subtype Classification using Exome Sequencing Data \\n- RareVariant Analysis: Inference of Pathway Deletions and Exchanges on Exome Sequencing Data'}], [{'generated_text': 'Score'}], [{'generated_text': 'The following metrics are commonly used when benchmarking models on the FSNS - Test dataset: accuracy, area under the receiver operating characteristic (AUROC), area under the receiver operating characteristic-optimized (AUROC+), area under the precision recall curve (AUPRC), confusion matrix, accuracy_weighted, f1 score, negative_confusion_matrix, precision, recall, roc_curve, total, and 0..1 rank'}], [{'generated_text': 'The following metrics are used to evaluate models on the RTE dataset:'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl \\nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RTE\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'The papers that have utilized the Flair-TDM model are listed below:\\n- https://www.semanticscholar.org/paper/Flair-TDM-Learning-to-rank-with-tf-idf-and-DTMM/94bae16a8dc5d79d5a890abf73a3b9a51ef3d43f.html\\n- https://www.semanticscholar.org/paper/Flair-TDM-Learning-to-rank-with-tf-idf-and-DTMM/94bae16a8dc5d79d5a890abf73a3b9a51ef3d43f.html'}]]\n","0 1\n","0 4\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","1 7\n","2 7\n","3 7\n","4 7\n","0 10\n","0 11\n","1 11\n","0 14\n","1 14\n","0 15\n","1 15\n","2 15\n","3 15\n","4 15\n","5 15\n","0 16\n","1 16\n","2 16\n","0 17\n","1 17\n","2 17\n","3 17\n","4 17\n","0 19\n","1 19\n","2 19\n","3 19\n","4 19\n","5 19\n","6 19\n","7 19\n","8 19\n","9 19\n","[[{'generated_text': 'The highest Accuracy score achieved by a model is achieved on the Story Cloze Test benchmark dataset with the DEEP language model trained on the Witten language and the Brown sentence-level language model trained on the Brown English corpus.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WSC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'The models being evaluated on the UrbanSound8k dataset are: ORB+ELM, Extra Trees, RF, Decision Trees, SMOOTH, LogitBoost, XGBoost, Random Forest, K-Nearest Neighbors, LSTM.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl \\n  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \\n     FILTER (str(?dataset_lbl) = \"Penn Treebank\")?benchmark orkgp:HAS_DATASET?dataset;\\n     orkgp:HAS_EVALUATION?eval. \\n     OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'top_benchmark_score: 91.5743\\ntop_benchmark_metric: precision\\ntop_benchmark_metric_lbl: precision'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {\\n ?dataset a orkgc:Dataset;\\n  rdfs:label?dataset_lbl.\\n  FILTER (str(?dataset_lbl) = \"WSC\")\\n ?benchmark orkgp:HAS_DATASET?dataset;\\n  orkgp:HAS_EVALUATION?eval.\\n  OPTIONAL {\\n   ?eval orkgp:HAS_METRIC?metric.\\n   ?metric rdfs:label?metric_lbl.\\n  }\\n}'}], [{'generated_text': 'The mean capacity of a carbon-based fuel is the average weight of a gallon of gasoline or diesel consumed per mile driven.'}], [{'generated_text': '[\\n  {\\n    \"metric\": \"BLEU\",\\n    \"metric_lbl\": \"BLEU\"\\n  },\\n  {\\n    \"metric\": \"ROUGE\",\\n    \"metric_lbl\": \"ROUGE\"\\n  },\\n  {\\n    \"metric\": \"METEOR\",\\n    \"metric_lbl\": \"METEOR\"\\n  },\\n  {\\n    \"metric\": \"CIDEr\",\\n    \"metric_lbl\": \"CIDEr\"\\n  }\\n]'}], [{'generated_text': 'The model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset was ansgd (annually separator group data)'}], [{'generated_text': 'The highest BLEU score on the WMT2016 Romanian-English benchmark dataset was achieved by the neural language model trained on an academic gur Frost suffix set.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl \\n    WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \\n          FILTER (str(?dataset_lbl) = \"WSC\") \\n         ?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. \\n          OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\ntop_1_accuracy_score_when_benchmarked_on_vtab-1k_model_name    = \"HWRR\"'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl \\n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n#items \\nitem\\n-\\tmetric\\tARD\\t0.63218 \\n-\\tmetric\\tRSR\\t0.63998 \\n-\\tmetric\\tVOC\\t0.65301 \\n-\\tmetric\\tLRR\\t0.66603 \\n-\\tmetric\\tCTR\\t0.67024 \\n-\\tmetric\\tATS\\t0.69176 \\n-\\tmetric\\tLMS\\t0.83964 \\n-\\tmetric\\tR2\\t0.91325 \\n-\\tmetric\\tR2*\\t0.96073 \\n-\\tmetric\\tJSD\\t0.97818 \\n-\\tmetric\\tCOV\\t0'}], [{'generated_text': 'I can list the metrics used to evaluate models on the WSC dataset as follows:\\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WSC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\\n\\nI can list the metrics used to evaluate models on the TDM Tagged Corpus dataset as follows:'}], [{'generated_text': 'Those research papers that have benchmarked models on the BC5CDR-disease dataset are: \\n- Manavathy S et al (2020): Evaluating Large-Scale Healthcare Recommendations on W3C Datasets, COVID-19, and DeepDream, https://doi.org/10.1145/3266202.3266210\\n- Goyal S, Mccowan M, Hsiang H, Rendle A, Ateniese D, Christiani V (2020): Consensus-based Subtyping of Coronavirus Infection Using W3C Datasets and Spectralis, https://doi.org/10.1145/3266208.3262221\\n- Ateniese D, Goyal S, Mccowan M, Hsiang H, Rendle A (2020): Performance of CNN Features on W3C Datasets for Consensus-based Subtyping of the Novel Coronavirus 2019-nCov, https://doi.org/10.1145/3265997.3266135\\n- Chen X, Mccowan M, Hsiang H, Goyal S, Rendle A, Aten'}], [{'generated_text': 'Score'}], [{'generated_text': 'Select distinct?metric?metric_lbl\\nwhere {\\n?dataset a orkgc:Dataset;\\n rdfs:label?dataset_lbl.\\n FILTER (str(?dataset_lbl) = \"Test\")\\n?benchmark orkgp:HAS_DATASET?dataset;\\n orkgp:HAS_EVALUATION?eval.\\n OPTIONAL {?eval orkgp:HAS_METRIC?metric.\\n?metric rdfs:label?metric_lbl. }\\n}'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WSC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\\n\\n/* \\n  Result:\\n  +----------------------------+--------------------------------------------------+\\n  |                             |                                                     |\\n  |                             |     -    name:accuracy                                 |\\n  |                             |     -    id:accuracy                                   |\\n  |                             |     +    type:float                                    |\\n  |                             |     +    label:accuracy                                |\\n  |                             |     +    unit:fct                                      |\\n  |                             |     +    description:The accuracy of the model is |\\n  |                             |                          the ratio of correct answers  |\\n  |                             |                          to'}], [{'generated_text': 'The metrics are the following: evaluation time (in seconds), loss and accuracy'}], [{'generated_text': 'Aminnejad, Seif, Dileep Venkayya, Gregorio Carlos Palermo, Giuseppe Comis, and Andrew Clancy. \"Evaluating Models on WSC: A Transfer Learning Perspective.\" ACM Computing Classification 20.1 (2018): 1-23. https://doi.org/10.1145/3172336.3172509.\\nCross-Dataset Eval: Towards Generalizing Black-Box Neural Networks on SaliHE. Dagstuhl castle; https://arxiv.org/abs/1710.07605.\\nDeep reinforcement learning based solution for multiple attribute multiple records per document entity recognition in web documents. Deng et al.  https://www.sciencedirect.com/science/article/pii/S1878043117303832.\\nFei-Fei Li, Tie-Yan Liu, Shuicheng Ni, Zheng Zhang, and Jia-Bin Su. \"Learning Word Re- ranks to Maximally Utilize Subgorary and Sample Background Subtraction fordee Speaker Identification.\" Proceedings of the 2019 ACM on International Conference on Multilingual Computing, Language, and Communication. ACM, 2019. https://doi.'}]]\n","0 0\n","1 0\n","0 2\n","0 4\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","1 7\n","2 7\n","3 7\n","0 8\n","1 8\n","2 8\n","3 8\n","0 9\n","0 14\n","0 15\n","1 15\n","2 15\n","3 15\n","4 15\n","5 15\n","6 15\n","7 15\n","8 15\n","0 16\n","1 16\n","0 18\n","0 19\n","1 19\n","2 19\n","3 19\n","4 19\n","5 19\n","6 19\n","7 19\n","8 19\n","9 19\n","[[{'generated_text': 'The model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset with metrics: (description of metrics)'}], [{'generated_text': 'Penn Treebank (Word Level)'}], [{'generated_text': 'The GAD benchmark dataset is being evaluated on the UrbanSound8k dataset. The models being evaluated on the UrbanSound8k dataset include:'}], [{'generated_text': 'research paper titles: \\n\\nsplit the sentence by space \\n\\n- Bag of Words model \\n\\n- Skip-gram model \\n\\n- Neural Bag of Words model \\n\\n- Variational GloVE model \\n\\n- Sequitur model \\n\\n- WordPOS model \\n\\n- Part of Speech Tagging model'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl'}], [{'generated_text': 'The metrics that are used to evaluate models on the GAD benchmark dataset are: \\n  * g-mean \\n  * g-mean_normalized\\n  * ssd\\n  * avg_qa\\n  * avg_perp\\n  * total_correct\\n  * accuracy_score\\n  * f1-score\\n  * mcc\\n  * mce\\n  * sparql-query-time\\n  * sparql-query-runtime'}], [{'generated_text': 'Carbon-based fuel: 400 grams per kilogram.'}], [{'generated_text': 'The papers are:\\n* [https://openrepository.org/record/2288392/files/LLR_E2R_paper-0.pdf Levin, A., 2016. \"Learning to Generate and Evaluate Text with Doc2Vec.\" arxiv:1603.05877.'}], [{'generated_text': 'Select Distinct?metric?metric_lbl where \\n { \\n?dataset a orkgc:Dataset; \\n rdfs:label?dataset_lbl. \\n FILTER (str(?dataset_lbl) = \"GAD\") \\n?benchmark orkgp:HAS_DATASET?dataset; \\n orkgp:HAS_EVALUATION?eval. \\n OPTIONAL { \\n?eval orkgp:HAS_METRIC?metric. \\n?metric rdfs:label?metric_lbl. \\n } \\n }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl'}], [{'generated_text': 'DMControl500k: catch'}], [{'generated_text': 'The models with name \"Deep Residual-LSTM\" perform the best on the GAD benchmark dataset.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl \\n WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"GAD\") \\n ?benchmark orkgp:HAS_DATASET?dataset; \\n  orkgp:HAS_EVALUATION?eval. \\n  OPTIONAL { \\n  ?eval orkgp:HAS_METRIC?metric. \\n  ?metric rdfs:label?metric_lbl. \\n  } \\n }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl \\n    WHERE { \\n        ?dataset a orkgc:Dataset; \\n         rdfs:label?dataset_lbl. \\n         FILTER (str(?dataset_lbl) = \"TDM Tagged Corpus\") \\n        ?benchmark orkgp:HAS_DATASET?dataset; \\n         orkgp:HAS_EVALUATION?eval. \\n         OPTIONAL { \\n              ?eval orkgp:HAS_METRIC?metric. \\n              ?metric rdfs:label?metric_lbl. \\n         } \\n    }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl \\n  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'Score'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl \\nWHERE {\\n ?dataset a orkgc:Dataset;\\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"GAD\") \\n ?benchmark orkgp:HAS_DATASET?dataset;\\n  orkgp:HAS_EVALUATION?eval. \\n  OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }\\n}'}], [{'generated_text': 'Here is a list of the metrics used to evaluate models on the GAD benchmark dataset:\\n\\n* F1-score: A weighted average of precision and recall, with the weight inversely proportional to the square of the precision and recall.\\n* Precision: The ratio of the number of relevant songs retrieved to the total number of songs retrieved (including non-relevant songs).\\n* Recall: The ratio of the number of relevant songs retrieved to the total number of songs retrieved (including non-relevant songs).\\n* Snippets Match: The ratio of the number of matched snippet match songs retrieved to the total number of songs retrieved.\\n* Position Correlation: The average of the correlation of the position of the song in the results list with the ranking of the results.\\n* Explainability: The ratio of the number of results that contain a given explanation to the total number of results.'}], [{'generated_text': 'Flair-TDM paper links:\\n* [https://github.com/NVlabs/flair-tdm](https://github.com/NVlabs/flair-tdm)\\n* [https://github.com/hortonworks/hortonworks-mf](https://github.com/hortonworks/hortonworks-mf)\\n* [https://github.com/materiasl/flair-tdm](https://github.com/materiasl/flair-tdm)\\n* [https://github.com/mmniti/mmniti-dl-flair-tdm](https://github.com/mmniti/mmniti-dl-flair-tdm)\\n* [https://github.com/realkonw/realkonw-ml-models/blob/master/FlairTDM.ipynb](https://github.com/realkonw/realkonw-ml-models/blob/master/FlairTDM.ipynb)'}]]\n","0 0\n","1 0\n","2 0\n","3 0\n","0 1\n","1 1\n","0 2\n","1 2\n","0 3\n","1 3\n","2 3\n","3 3\n","4 3\n","5 3\n","0 5\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","1 7\n","2 7\n","3 7\n","4 7\n","5 7\n","0 8\n","1 8\n","0 10\n","1 10\n","2 10\n","3 10\n","4 10\n","5 10\n","6 10\n","7 10\n","8 10\n","9 10\n","0 11\n","1 11\n","2 11\n","3 11\n","4 11\n","5 11\n","6 11\n","7 11\n","8 11\n","9 11\n","0 15\n","1 15\n","2 15\n","3 15\n","4 15\n","5 15\n","6 15\n","7 15\n","8 15\n","9 15\n","0 18\n","1 18\n","2 18\n","3 18\n","4 18\n","5 18\n","6 18\n","7 18\n","8 18\n","0 19\n","1 19\n","2 19\n","3 19\n","4 19\n","5 19\n","6 19\n","7 19\n","8 19\n","9 19\n","[[{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Semantic Web\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \\n?benchmark item mus.\\n?benchmark accuracy'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?rf_label?rf_label; FILTER (str(?rf_label) = \"Semantic Web\")?paper?cont;?cont orkgp:HAS_BENCHMARK?benchmark;?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Semantic Web\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?rf_label?paper_label;?cont OR kgp:P30?rf;?cont kgp:HAS_BENCHMARK?benchmark;?problem rdfs:label?problem_lbl; }'}], [{'generated_text': 'The top benchmark score is 70.1378 on the MazeA dataset and the metric is ush.'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Semantic Web\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\\n\\noutput (Sparql query): \\n\\n[\\n  {\\n    \"problem\": \"orkgp:P30\",\\n    \"problem_lbl\": \"Semantic Web Benchmark\"\\n  },\\n  {\\n    \"problem\": \"orkgp:P31\",\\n    \"problem_lbl\": \"Semantic Web Benchmark #2\"\\n  },\\n  {\\n    \"problem\": \"orkgp:P32\",\\n    \"problem_lbl\": \"Semantic Web Benchmark #3\"\\n  }\\n]'}], [{'generated_text': 'The capacity of carbon-based fuels at a mean temperature of 25 °C is 242.94 liters per 100 kilometers.'}], [{'generated_text': 'I have found the following research papers that performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset:\\n\\n- Abstract: Semantic Web Benchmarking for Zero-Shot Learning - Wang, Haoyi; 2018.\\n- Author: Wang, Haoyi; 2018.\\n- Abstract: Research field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field: Research Field; Research Field'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE { \\n ?rf a orkgc:ResearchField; \\n  rdfs:label?rf_label. \\n  FILTER (str(?rf_label) = \"Semantic Web\") \\n ?paper orkgp:P30?rf; \\n  orkgp:P31?cont. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem. \\n ?problem rdfs:label?problem_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Semantic Web\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \\n\\n?problem num:32'}], [{'generated_text': 'The highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value is 78.38946657573605.'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Semantic Web\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \\n{\\n   ?problem  vtab-1k\\n   ?problem_lbl Semantic Web\\n}'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl \\n  WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Semantic Web\")?paper?problem; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Semantic Web\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'MySql:\\nBenchmarking Semantic Web Approaches Using Large-scale Corpora \\nHasani Henao, Vida Merrick, Eric Price, Diogo Silva, Danfeng Li, Daniel Sill, Michael A.acm, Mohit Bansal, Pieter Rein, Shelly Tow, Yang Yang, Ziheng Yang, Assessing Semantic Web Text Mining Performance on Large Fact- Lucene Indexes'}], [{'generated_text': 'SELECT DISTINCT \\n?problem?problem_lbl \\nWHERE { \\n?rf a orkgc:ResearchField; \\nrdfs:label?rf_label. \\nFILTER (str(?rf_label) = \"Semantic Web\") \\n?paper orkgp:P30?rf; \\norkgp:P31?cont; \\n?cont orkgp:HAS_BENCHMARK?benchmark; \\norkgp:P32?problem. \\n?problem rdfs:label?problem_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE { \\n ?rf a orkgc:ResearchField; \\n  rdfs:label?rf_label. \\n  FILTER (str(?rf_label) = \"Semantic Web\") \\n ?paper orkgp:P30?rf; \\n  orkgp:P31?cont. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem. \\n ?problem rdfs:label?problem_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Semantic Web\")?paper?paper_id; orkgp:P30?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'SELECT DISTINCT \\n   ?problem?problem_lbl \\nWHERE { \\n   ?rf a orkgc:ResearchField; \\n    rdfs:label?rf_label. \\n    FILTER (str(?rf_label) = \"Semantic Web\")?paper \\n   ?rf; \\n   ?cont orkgp:HAS_BENCHMARK?benchmark; \\n   ?cont rdfs:label?problem_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl \\n where { \\n ?rf a orkgc:ResearchField ; \\n  rdfs:label?rf_label. \\n  FILTER (str(?rf_label) = \"Semantic Web\") \\n ?paper orkgp:P30?rf; \\n  orkgp:P31?cont; \\n  orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem;\\n ?problem rdfs:label?problem_lbl. \\n}'}]]\n","0 4\n","0 6\n","1 6\n","0 7\n","0 10\n","0 14\n","[[{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Machine Learning\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \\n?problem basen n :ResearchField ; \\n{  \\n  \"basen n:StoryClozeTest\" \"Accuracy Score\" ; \\n  \"Story Cloze Test\" \"Story Cloze Test\" ; \\n  \"Story Cloze Test\" \"50.1275\" ; \\n  \"Story Cloze Test\" \"50.1275\" ; \\n  \"Machine Learning\" \"Story Cloze Test\" ; \\n  \"Machine Learning\" \"50.1275\" ; \\n  \"Machine Learning\" \"50.1275\" ; \\n  \"Machine Learning\" \"50.1275\" ; \\n  \"Machine Learning\" \"50.1275\" ; \\n}'}], [{'generated_text': 'SELECT DISTINCT \\n?problem?problem_lbl \\nWHERE { \\n ?rf a orkgc:ResearchField; \\n  rdfs:label?rf_label. \\n  FILTER (str(?rf_label) = \"Machine Learning\") \\n ?paper orkgp:P30?rf; orkgp:P31?cont. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. \\n ?problem rdfs:label?problem_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl \\n WHERE {\\n ?rf a orkgc:ResearchField;\\n  rdfs:label?rf_label.\\n  FILTER (str(?rf_label) = \"Machine Learning\") \\n ?paper orkgp:P30?rf;\\n  orkgp:P31?cont. \\n ?cont orkgp:HAS_BENCHMARK?benchmark;\\n  orkgp:P32?problem.\\n ?problem rdfs:label?problem_lbl.\\n }'}], [{'generated_text': \"The following are research paper titles and IDs that have benchmarked models on the Penn Treebank dataset: \\n- Teaching Machine Learning withCorpus Linguistics, Langston, Aaron and Goode, Chris and Manning, Chris and Manning, Chris and Zhang, Haithong. Proceedings of the 9th Workshop on Learning and Emoting for Corporate Linguistics,COLING 2012, pages 66-74.\\n- Advances in Neural Information Processing Systems 28, 1064–1071.\\n- Learning for Lexical Semantics, O'day, Anirudh. Proceedings of the 2009 SIAM international conference on data mining and machine learning, 1312–1324.\\n- Two-Stage Learning of Global and Local Features for Sentiment Analysis of Social Media Data, Ascai, Cosense, Andrea and Pantic, Oltunsun. Proceedings of the 21st Conference on Information Mining, ACM, 2014, 653-666.\\n- Semi-Supervised Learning of Mutual Information Networks for Acoustic Modeling, Hu, Lei. Proceedings of the 33rd Annual Conference on Future Generation Computer Systems, IEEE, 2014, 268-278.\\n- Relational Machine Learning Using Large Graphs for Email Classification, Maji,\"}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE { \\n ?rf a orkgc:ResearchField; \\n  rdfs:label?rf_label. \\n  FILTER (str(?rf_label) = \"Machine Learning\") \\n ?paper orkgp:P30?rf; \\n  orkgp:P31?cont. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem. \\n ?problem rdfs:label?problem_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Machine Learning\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'Carbon-based fuel is the mean capacity of it in gigajoules per liter.'}], [{'generated_text': 'ritems:  \\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Machine Learning\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset: \\n<http://pastebin.com/raw/iM7yc1Cr>'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Machine Learning\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \\n   ?problem bbeml.'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Machine Learning\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \\n# where  \\n#?rf a orkgc:ResearchField\\n#?paper orkgp:P30 \\n#?cont orkgp:DMControl500k\\n#?benchmark DMControl500k.catch'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Machine Learning\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\\n    ?problem vtabenchmark:Top-1_accuracy.'}], [{'generated_text': 'SELECT DISTINCT \\n ?problem?problem_lbl \\nWHERE { \\n ?rf a orkgc:ResearchField; \\n  rdfs:label?rf_label. \\n  FILTER (str(?rf_label) = \"Machine Learning\") \\n ?paper orkgp:P30?rf; \\n  orkgp:P31?cont. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem. \\n ?problem rdfs:label?problem_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Machine Learning\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Machine Learning\")?paper?paper_id; orkgp:P30?cont; orkgp:P31?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \\n\\ninput (English text): \\nBenefits of Machine Learning over Regression in a Medical Data Set\\nProvided by Kevin Duh and Radhika Rao, Duke University, Durham, NC 27708\\nProvided by Kevin Duh and Radhika Rao, Duke University, Durham, NC 27708\\nProvided by Kevin Duh and Radhika Rao, Duke University, Durham, NC 27708\\nProvided by Kevin Duh and Radhika Rao, Duke University, Durham, NC 27708'}], [{'generated_text': 'Lunar Lander (OpenAI Gym) model: tensorflow_policy.LunarLanderPolicy'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Machine Learning\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \\nResults:\\n+--------------------+-----------------------------------+\\n|  problem            |  problem_lbl                       |\\n+--------------------+-----------------------------------+\\n| Dataset Mining      | Dataset Mining                   |\\n| Dataset classification| Dataset classification          |\\n+--------------------+-----------------------------------+\\n| Classifiers       | Tfidf                             |\\n| Classifiers       | Log Loss                          |\\n| Classifiers       | Gain Ratio                        |\\n| Regression        | MSE                               |\\n| Regression        | R2                                |\\n| Regression        | R2hat                             |\\n| Regression        | MAD                               |\\n+--------------------+-----------------------------------+\\n\\nNote that the metrics are'}], [{'generated_text': 'SELECT DISTINCT?paper \\n  WHERE {?rf a orkgc:ResearchField;?paperrid :paper-id?paper. \\n    ?paperrid :paper-title?paper.\\n     FILTER (str(?paper_label) = \"ImageNet64x64-Benchmark\") \\n     }\\n\\nProblem is returned with ID \"P32\".'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; \\n     rdfs:label?rf_label. FILTER (str(?rf_label) = \"Machine Learning\")?paper orkgp:P30?rf; \\n     orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl \\nWHERE { \\n ?rf a orkgc:ResearchField; \\n  rdfs:label?rf_label. \\n  FILTER (str(?rf_label) = \"Machine Learning\") \\n ?paper orkgp:P30?rf; \\n  orkgp:P31?cont. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem. \\n ?problem rdfs:label?problem_lbl. \\n}'}]]\n","0 3\n","1 3\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 8\n","0 15\n","1 15\n","[[{'generated_text': 'The best performing model on the Story Cloze Test benchmark dataset achieved an Accuracy score of 96.82%.'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl \\n  WHERE {?rf a ResearchField;\\n     rdfs:label?rf_label.\\n     FILTER (str(?rf_label) = \"Computer Sciences\")\\n    ?paper P30?rf;\\n     P31?cont;\\n     P32?problem.\\n    ?problem rdfs:label?problem_lbl.\\n  }'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Computer Sciences\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'Benchmarked problems include POS taggers, part-of-speech taggers, dependency parsers, and coreference resolution.  The following paper titles and IDs demonstrate some of the problems listed above:\\n\\nPOS taggers, part-of-speech taggers, dependency parsers, and coreference resolution'}], [{'generated_text': 'http://mazea.isti.cnr.it/dataset/'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Computer Sciences\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; \\n  rdfs:label?rf_label. FILTER (str(?rf_label) = \"Computer Sciences\")?paper orkgp:P30?rf; \\n  orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. \\n ?problem rdfs:label?problem_lbl. } \\n\\nProblem    problem_lbl\\n------------------------------\\nCapacity   461.331 \\n\\nThis query returns the following results: \\nCapacity 461.331 \\nProblem    problem_lbl\\n------------------------------\\nCapacity   461.331 \\n\\nNote that the problem (Carbon-based fuel) has a mean capacity of 461.331.'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE { \\n   ?rf a ResearchField; \\n    rdfs:label?rf_label. \\n    FILTER (str(?rf_label) = \"Computer Sciences\") \\n   ?paper \\n    or kgp:P30?rf; \\n    or kgp:P31?cont. \\n   ?cont or kgp:HAS_BENCHMARK \\n   ?benchmark; \\n    or kgp:P32?problem. \\n   ?problem rdfs:label?problem_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Computer Sciences\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \\noutput\\n\\nAnd here is the model that performed best:\\nhttp://data.cyber.cornell.edu/cc/databricks/andrew/home/andrew/home/dl/dl-home/dl-datasets/kuzushiji-MNIST-benchmark/'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE { \\n ?rf a orkgc:ResearchField; \\n  rdfs:label?rf_label. \\n  FILTER (str(?rf_label) = \"Computer Sciences\") \\n ?paper orkgp:P30?rf; \\n  orkgp:P31?cont. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem. \\n ?problem rdfs:label?problem_lbl. \\n  } \\n ?benchmark bblu.'}], [{'generated_text': 'DMControl500k\\n\\n Pero a serato 5.6096061, el balón en cócleo'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl \\n WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Computer Sciences\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \\n\\n<problem_lbl:Computer Science>Computer Sciences\\n<benchmark:Top 1 Accuracy>66.66666666666667%'}], [{'generated_text': 'SELECT DISTINCT\\n ?problem?problem_lbl\\nWHERE { \\n ?rf a orkgc:ResearchField;\\n  rdfs:label?rf_label.\\n  FILTER (str(?rf_label) = \"Computer Sciences\")\\n ?paper  orkgp:P30?rf;\\n  orkgp:P31?cont.\\n ?cont orkgp:HAS_BENCHMARK?benchmark;\\n  orkgp:P32?problem.\\n ?problem rdfs:label?problem_lbl.\\n}\\n\\nThe query will return the following:\\n\\ntwo problems\\n{ \"problem\": \"K35\", \"problem_lbl\": \"Computer Sciences\", \"benchmark\": \"arXiv\" }\\n{ \"problem\": \"K32\", \"problem_lbl\": \"Databases\", \"benchmark\": \"NANOGRAPH\" }'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE { \\n   ?rf a orkgc:ResearchField; \\n    rdfs:label?rf_label. \\n    FILTER (str(?rf_label) = \"Computer Sciences\") \\n   ?paper orkgp:P30?rf; \\n    orkgp:P31?cont. \\n   ?cont orkgp:HAS_BENCHMARK \\n   ?benchmark; \\n    orkgp:P32?problem. \\n   ?problem rdfs:label?problem_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl \\n  WHERE {?rf a ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Computer Sciences\")?paper OR\\n          ?paper rdfs:label \"BC5CDR-disease\" ;\\n          ?cont OR\\n          ?cont has_benchmark ;\\n          ?cont rdfs:label \"P30\" ;\\n          ?cont rdfs:label \"P31\" ;\\n          ?cont rdfs:label \"P32\" ;\\n          ?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'The top performing model is an OpenAI model called MONSTER, its Score score is 1.0000 on the Lunar Lander (OpenAI Gym) dataset.'}], [{'generated_text': 'Commonly used metrics for benchmarking models on the FSNS - Test dataset are:\\n\\n\\n1. Mean Absolute Error (MAE)\\n2. Mean Square Error (MSE)\\n3. Root Mean Squared Error (RMSE)\\n4. Pearson Correlation Coefficient (PCC)'}], [{'generated_text': 'PREFIX  orkgc:   <http://orcid.org/org/onomy>\\nPREFIX  rdfs:   <http://www.w3.org/2000/01/rdf-schema#>\\nPREFIX  orkgp:  <http://orcid.org/org/onomy/>\\nPREFIX  orkgp:  <http://orcid.org/org/onomy/>\\nPREFIX  orkgp:  <http://orcid.org/org/onomy/>\\nPREFIX  orkgp:  <http://orcid.org/org/onomy/>\\nPREFIX  orkgp:  <http://orcid.org/org/onomy/>\\nPREFIX  orkgp:  <http://orcid.org/org/onomy/>\\nPREFIX  orkgp:  <http://orcid.org/org/onomy/>\\nPREFIX  orkgp:  <http://orcid.org/org/onomy/>\\nPREFIX  orkgp:  <http://orcid.org/org/onomy/>\\nPREFIX  orkgp:  <http://orcid.org/org/onomy/>\\nPREFIX  orkgp:  <http'}], [{'generated_text': 'Benchmarked problems in Computer Sciences'}], [{'generated_text': 'SELECT DISTINCT {?paper?problem?problem_lbl } \\nWHERE {?rf a ResearchField; rdfs:label?rf_label. \\nFILTER (str(?rf_label) = \"Computer Sciences\") \\n?paper?paper_label. \\n?paper_label flair-tdm-code. \\n?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P30?cont; orkgp:P31?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. \\n?problem rdfs:label?problem_lbl. }'}]]\n","0 0\n","1 0\n","2 0\n","0 3\n","0 4\n","1 4\n","0 10\n","0 15\n","0 16\n","0 17\n","0 18\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"113UtZclrQ0lymfwF3NUd5OqY8cwYjdZS","timestamp":1692434060531},{"file_id":"11r0-FEMO2UG7b2oZd4ySW0LpOjsly0mm","timestamp":1691515342412}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"aaa18618a98d45409e957c0fa79e097a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e27dc4e7ab0f45db8d78be58c662a08e","IPY_MODEL_fbc2ded40818491184c6cccb81c37257","IPY_MODEL_52eef2044d1f4a428fd396063fcb123a"],"layout":"IPY_MODEL_6901c269bab349a6ba9b29b13584128d"}},"e27dc4e7ab0f45db8d78be58c662a08e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_968e7ad6d64444908fa52ce4baac89f6","placeholder":"​","style":"IPY_MODEL_e3e5eb3fe7214ae28208e3e00a45962a","value":"Downloading (…)lve/main/config.json: 100%"}},"fbc2ded40818491184c6cccb81c37257":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_daf2d73462a4478fa0155c3762eb4890","max":819,"min":0,"orientation":"horizontal","style":"IPY_MODEL_acc9ba411efd4e9d91e461f83e7408c4","value":819}},"52eef2044d1f4a428fd396063fcb123a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a53f3e017e5b4868acd127af7fa68767","placeholder":"​","style":"IPY_MODEL_5e8bc19428094747a3db1cb3c34e631e","value":" 819/819 [00:00&lt;00:00, 45.6kB/s]"}},"6901c269bab349a6ba9b29b13584128d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"968e7ad6d64444908fa52ce4baac89f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3e5eb3fe7214ae28208e3e00a45962a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"daf2d73462a4478fa0155c3762eb4890":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acc9ba411efd4e9d91e461f83e7408c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a53f3e017e5b4868acd127af7fa68767":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e8bc19428094747a3db1cb3c34e631e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15558149c087422ca59d6509a4c72e86":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b35ff0f4e30b4e8ca39a093c4ae7c1ac","IPY_MODEL_8c96ea55e8b84571ac1dc8480692349e","IPY_MODEL_b95d47ba3c034e50b261cc28875569bc"],"layout":"IPY_MODEL_59d09d994ce24d5e9ff5e3028ce5025f"}},"b35ff0f4e30b4e8ca39a093c4ae7c1ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_486dc241ae1d4461b1d94a2f1b04ead8","placeholder":"​","style":"IPY_MODEL_de3bde7fbdef4a929fdfad70118fcbd6","value":"Downloading (…)instruct_pipeline.py: 100%"}},"8c96ea55e8b84571ac1dc8480692349e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd2e439c80b142e4a8178bce5520c507","max":9160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9daaf77b86944e3d9b623491abe3b772","value":9160}},"b95d47ba3c034e50b261cc28875569bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d30755923bdf4929901972d768e2ab94","placeholder":"​","style":"IPY_MODEL_92d1d6b160714f1dbc18aff1f7a9eff0","value":" 9.16k/9.16k [00:00&lt;00:00, 514kB/s]"}},"59d09d994ce24d5e9ff5e3028ce5025f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"486dc241ae1d4461b1d94a2f1b04ead8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de3bde7fbdef4a929fdfad70118fcbd6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd2e439c80b142e4a8178bce5520c507":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9daaf77b86944e3d9b623491abe3b772":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d30755923bdf4929901972d768e2ab94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92d1d6b160714f1dbc18aff1f7a9eff0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13bf31df16b34c088822d2c1f5c10922":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c48b048754eb42beaca95af8610d180f","IPY_MODEL_e35cf57d82524947b2887e6964bbb8f9","IPY_MODEL_3149701742e643d5a80f7aad432de61f"],"layout":"IPY_MODEL_a3ce7ad845e641b7b8a7575bba200ac5"}},"c48b048754eb42beaca95af8610d180f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a96c9d5cb63c497e925493b4bd47d429","placeholder":"​","style":"IPY_MODEL_eb4fdaec910a4dcfaece0662f221b2c8","value":"Downloading pytorch_model.bin: 100%"}},"e35cf57d82524947b2887e6964bbb8f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f9f3aa9676d462086b31a6e482322d6","max":5684548185,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fcca61b76e4641f0a902a48d1f76866f","value":5684548185}},"3149701742e643d5a80f7aad432de61f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e782197a8a274f10b57c454ad1c990ea","placeholder":"​","style":"IPY_MODEL_b4fc8941c5eb4921ac9c8893b828b4d4","value":" 5.68G/5.68G [00:39&lt;00:00, 200MB/s]"}},"a3ce7ad845e641b7b8a7575bba200ac5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a96c9d5cb63c497e925493b4bd47d429":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb4fdaec910a4dcfaece0662f221b2c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f9f3aa9676d462086b31a6e482322d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcca61b76e4641f0a902a48d1f76866f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e782197a8a274f10b57c454ad1c990ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4fc8941c5eb4921ac9c8893b828b4d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ce3cafdcbfa4c889d89744d50347bf4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e16b5d8b4584b71b95cb298e27aba43","IPY_MODEL_b8eaaad694c84e9298fe7cd4f13814e6","IPY_MODEL_8cec403d5d3747abacd962bc1974ecfc"],"layout":"IPY_MODEL_ab249d151387453caa4e96eaf29fe988"}},"7e16b5d8b4584b71b95cb298e27aba43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8e38018cf6e4d6986d2a994860899fe","placeholder":"​","style":"IPY_MODEL_226da378e2214a19bd6ad0929e3732a2","value":"Downloading (…)okenizer_config.json: 100%"}},"b8eaaad694c84e9298fe7cd4f13814e6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_04d5f1ddb4f64dd9bcc2384556f9f59d","max":450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae8f4baf9823401b851293e36d2b7c55","value":450}},"8cec403d5d3747abacd962bc1974ecfc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a07a1afbadaf4eeab20a714f7bbbac2c","placeholder":"​","style":"IPY_MODEL_20308ddd9f784a6fa512d6016e6b5f41","value":" 450/450 [00:00&lt;00:00, 30.0kB/s]"}},"ab249d151387453caa4e96eaf29fe988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8e38018cf6e4d6986d2a994860899fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"226da378e2214a19bd6ad0929e3732a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04d5f1ddb4f64dd9bcc2384556f9f59d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae8f4baf9823401b851293e36d2b7c55":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a07a1afbadaf4eeab20a714f7bbbac2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20308ddd9f784a6fa512d6016e6b5f41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45df9fa29ae04455b56d99ef26760b34":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0272433d0de54365a15e7b6f3754ddde","IPY_MODEL_9bdc59d553984550ab1e5be0571c3ff6","IPY_MODEL_c8525f7820824f7cb200ce078d339fa0"],"layout":"IPY_MODEL_6b2c8b0932464c888a03cbb807da1583"}},"0272433d0de54365a15e7b6f3754ddde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bfe34abcb7e41ca90b22092362b5d3d","placeholder":"​","style":"IPY_MODEL_1128d26dd16b4dfdb10a8bd370b3d742","value":"Downloading (…)/main/tokenizer.json: 100%"}},"9bdc59d553984550ab1e5be0571c3ff6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b77c1824f78243ac8375cb894c9cc20c","max":2114274,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3329b4e5a2a64b5db79bce1c521b8342","value":2114274}},"c8525f7820824f7cb200ce078d339fa0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_642aad4f60324dd59839db222ef6b342","placeholder":"​","style":"IPY_MODEL_079b198fcdef410aa2487874a489972b","value":" 2.11M/2.11M [00:00&lt;00:00, 6.42MB/s]"}},"6b2c8b0932464c888a03cbb807da1583":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bfe34abcb7e41ca90b22092362b5d3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1128d26dd16b4dfdb10a8bd370b3d742":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b77c1824f78243ac8375cb894c9cc20c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3329b4e5a2a64b5db79bce1c521b8342":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"642aad4f60324dd59839db222ef6b342":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"079b198fcdef410aa2487874a489972b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"244aa02197b6480b95be77c6ef8ac652":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78e6c39758824baa9fd8b530a3736688","IPY_MODEL_523de9013b7a4bdab505773fd0ea5a45","IPY_MODEL_7b4ab055a35448e5950efc3a0e1bd9c2"],"layout":"IPY_MODEL_86d10043a5084683ad0ce3f634368b5b"}},"78e6c39758824baa9fd8b530a3736688":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_820d295be31642c1ada53cbcccc2aa8d","placeholder":"​","style":"IPY_MODEL_f5d680142c704232b356728cc3ee2912","value":"Downloading (…)cial_tokens_map.json: 100%"}},"523de9013b7a4bdab505773fd0ea5a45":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_906d815d6e8a47ca9d06c7402ecd960b","max":228,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7322288e59e14f529984b51bbde9da62","value":228}},"7b4ab055a35448e5950efc3a0e1bd9c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bea1e50a7c5c4816b11e76d40c02e52c","placeholder":"​","style":"IPY_MODEL_c16007e1074442d0a854ba0791d1a4e7","value":" 228/228 [00:00&lt;00:00, 11.1kB/s]"}},"86d10043a5084683ad0ce3f634368b5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"820d295be31642c1ada53cbcccc2aa8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5d680142c704232b356728cc3ee2912":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"906d815d6e8a47ca9d06c7402ecd960b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7322288e59e14f529984b51bbde9da62":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bea1e50a7c5c4816b11e76d40c02e52c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c16007e1074442d0a854ba0791d1a4e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}