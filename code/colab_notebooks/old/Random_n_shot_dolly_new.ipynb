{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"113UtZclrQ0lymfwF3NUd5OqY8cwYjdZS","timestamp":1692434060531},{"file_id":"11r0-FEMO2UG7b2oZd4ySW0LpOjsly0mm","timestamp":1691515342412}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n","!pip -q install accelerate>=0.12.0\n","!pip install datasets"],"metadata":{"id":"zuwLc--q7RtP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd drive/MyDrive/en2sparql"],"metadata":{"id":"YcU-KRrrF9JR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xF_bcVRM61-s"},"outputs":[],"source":["import torch\n","import json\n","from transformers import pipeline, AutoTokenizer\n","from datasets import load_dataset\n","import random\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side=\"left\")\n","\n","dolly = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n","raw_datasets = load_dataset(\"orkg/SciQA\")\n","print(raw_datasets)\n","\n","\n","def divide_chunks(l_, n_):\n","    for i_ in range(0, len(l_), n_):\n","        yield l_[i_:i_ + n_]\n","\n","\n","def clean(st):\n","    st = st.replace(\"\\n\", \" \")\n","    st = st.replace(\"?\", \" ?\")\n","    st = st.replace(\"{\", \" { \")\n","    st = st.replace(\"}\", \" } \")\n","    st = st.replace(\"\\\\'\", \"'\")\n","\n","    while \"  \" in st:\n","        st = st.replace(\"  \", \" \")\n","    return st\n","\n","\n","def get_key(q):\n","    t0 = q.get('template_id')\n","    if t0 is None:\n","        t0 = \"None\"\n","    t = str(q.get(\"number_of_patterns\")) + \"-\" + t0\n","    return t\n","\n","\n","def get_keys(n_):\n","    train = raw_datasets.get(\"train\")\n","    patterns = {}\n","    for q in train:\n","        t = get_key(q)\n","        query = clean(q[\"query\"][\"sparql\"])\n","        question = q[\"question\"][\"string\"]\n","        if t not in patterns:\n","            patterns[t] = [[query, question, len(query)]]\n","        else:\n","            patterns[t].append([query, question, len(query)])\n","\n","    for t in patterns:\n","        code = patterns.get(t)\n","        code = sorted(code, key=lambda x: x[2], reverse=False)\n","        patterns[t] = code[:n_]\n","    return patterns\n","\n","\n","def get_random(n_):\n","    train = raw_datasets.get(\"train\")\n","    sample = random.sample(list(train), n_)\n","    sample_list = []\n","    for q in sample:\n","        t = get_key(q)\n","        query = clean(q[\"query\"][\"sparql\"])\n","        question = q[\"question\"][\"string\"]\n","        sample_list.append([query, question, t])\n","    return sample_list\n","\n","\n","def prepare_queries(n_):\n","    keys = get_keys(n_)\n","    data = raw_datasets.get(\"test\")\n","    queries = []\n","    suggestions = []\n","    for q in data:\n","        t = get_key(q)\n","        question = q[\"question\"][\"string\"]\n","        suggestion = get_random(n_)\n","        suggestions.append([[x[2] for x in suggestion], t])\n","        if suggestion is None or len(suggestion)<1:\n","            print(\"Error with key\", t)\n","            queries.append(\"translate the following English text '\" + question + \"' to a sparql query\")\n","        else:\n","            final_q = \"\"\n","            for i_, k in enumerate(suggestion):\n","                final_q += \"\\n input (English text): \" + k[1]\n","                final_q += \"\\n output (Sparql query): \" + k[0]\n","\n","            # works better with gpt\n","            # final_q += \"\\n with this example what is the sparql query for:  \" + question\n","\n","            # works better with dolly\n","            final_q += \"\\n input (English text): \" + question\n","            final_q += \"\\n output (Sparql query): \"\n","            queries.append(final_q)\n","    return queries, suggestions\n","\n","\n","def save_json(filename,data):\n","    with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n","        print(json.dumps(data), file=json_file)\n","\n","\n","def main(shots=6, attempts=10, batch=50):\n","  query_list, suggestions = prepare_queries(shots)\n","\n","  print(len(query_list))\n","  n = batch\n","  q_list = list(divide_chunks(query_list, n))\n","  sparql = [clean(x[\"query\"][\"sparql\"]) for x in raw_datasets.get(\"test\")]\n","\n","  gs = []\n","  lens =[]\n","\n","  i = 0\n","  for group in q_list:\n","      print(str(i) + \"%\", end=\"  \")\n","      i += 1/len(q_list)*100\n","\n","      res_ = [tokenizer.encode(question) for question in group]\n","      len_ = [len(x) for x in res_]\n","      warning = [x for x in len_ if x > 2048]\n","      if len(warning)>0:\n","        quit()\n","      lens += len_\n","\n","      res = dolly(group)\n","      print(res)\n","      gst = [x[0][\"generated_text\"] for x in res]\n","\n","      for ii, l in enumerate(gst):\n","          for iii in range(5):\n","              if \"SELECT\" not in l:\n","                  print(iii,ii)\n","                  res = dolly(group[ii])\n","                  gst[ii] = res[0][\"generated_text\"]\n","                  l = gst[ii]\n","              else:\n","                  break\n","      gs += gst\n","\n","      result = {\"questions\": query_list, \"sparql\": sparql, \"generated_sparql\": gs, \"prompt_len\": lens,\n","                \"suggestions\": suggestions}\n","      save_json(\"random_dolly_\"+str(shots)+\"_shot_results_tok.json\", result)\n","      # break\n","\n","main()"]}]}