{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOPfStsQpRbvujy70ZiPzxE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CCRq1Qr8MBk8"},"outputs":[],"source":["!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n","!pip -q install accelerate>=0.12.0\n","!pip install datasets"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd drive/MyDrive/T5"],"metadata":{"id":"kmV-ivVrGinc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM\n","from datasets import load_dataset\n","import torch\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","prefix = \"translate English to Sparql: \"\n","tokenizer = AutoTokenizer.from_pretrained(\"en2sparql_T5_model\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"en2sparql_T5_model\").to(device)\n","\n","# books = load_dataset(\"json\", data_files={'test':'test.json'})\n","books = load_dataset(\"orkg/SciQA\")\n","print(books[\"test\"])\n","\n","queries = []\n","sparql = []\n","\n","for feature in books[\"test\"]:\n","    query = prefix + feature.get(\"question\").get(\"string\")\n","    queries.append(query)\n","    gold_sparql = feature.get(\"query\").get(\"sparql\")\n","    sparql.append(gold_sparql)\n","\n","print(len(queries))\n","\n","def divide_chunks(l_, n_):\n","    # looping till length l\n","    for i_ in range(0, len(l_), n_):\n","        yield l_[i_:i_ + n_]\n","\n","n = 10\n","\n","q = list(divide_chunks(queries, n))\n","\n","gs = []\n","gst = []\n","i = 0\n","\n","for group in q:\n","    print(str(i)+\"%\", end=\"  \")\n","    i += 2\n","    inputs = tokenizer(group, max_length=512, truncation=True, return_tensors='pt', padding=True).to(device)\n","    with torch.no_grad():\n","        generated_ids = model.generate(**inputs, max_new_tokens=512, do_sample=True, top_k=30, top_p=0.95)\n","\n","    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","    generated_texts2 = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\n","\n","    generated_texts2 = [x.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip() for x in generated_texts2]\n","\n","    gs += generated_texts\n","    gst += generated_texts2\n","\n","result = {\"questions\": queries, \"sparql\": sparql, \"generated_sparql\": gs, \"generated_with_special_tokens\": gst}\n","\n","with open(\"ft_T5_results.json\", \"w\", encoding=\"utf-8\") as text_file:\n","    print(json.dumps(result), file=text_file)"],"metadata":{"id":"9Dfz7Vz3Gb2E"},"execution_count":null,"outputs":[]}]}