{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPigJUgaZ0O+cTjuHL1X5tb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8ef83de1fb0e44969607170deaaffeb0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_67a86ae180a24cb58b124d6c446b9927","IPY_MODEL_101b912e480446839be5bee6ffb37d45","IPY_MODEL_410bbfbc37054bd38774a5e1ce1a6678"],"layout":"IPY_MODEL_0b97212b56e04b2da461b95667b90f99"}},"67a86ae180a24cb58b124d6c446b9927":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04f62b8f53ee4ebe87b463a8a802e9d8","placeholder":"​","style":"IPY_MODEL_1c686d70ae334cd581880636cd5ffa79","value":"Downloading pytorch_model.bin: 100%"}},"101b912e480446839be5bee6ffb37d45":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_adac7e0b007b45d09a045ee0d52a5ba6","max":5684548185,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa752d3ff1c444ae83e30a1e3e037024","value":5684548185}},"410bbfbc37054bd38774a5e1ce1a6678":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7fec00c59644118a435f0e512af3ceb","placeholder":"​","style":"IPY_MODEL_4faabff4aded43ea948bb9fad7339a9e","value":" 5.68G/5.68G [07:34&lt;00:00, 2.29MB/s]"}},"0b97212b56e04b2da461b95667b90f99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04f62b8f53ee4ebe87b463a8a802e9d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c686d70ae334cd581880636cd5ffa79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"adac7e0b007b45d09a045ee0d52a5ba6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa752d3ff1c444ae83e30a1e3e037024":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7fec00c59644118a435f0e512af3ceb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4faabff4aded43ea948bb9fad7339a9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00e42524b81c4f58850fd0814da7438b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8bed7add3654409f9a4f7e148c931b49","IPY_MODEL_ae3ef231293e4e5cab8a018029f8f393","IPY_MODEL_7924714608fa4ec08849ba6bb9767e90"],"layout":"IPY_MODEL_aa86c0b551a843368503e929d799bff7"}},"8bed7add3654409f9a4f7e148c931b49":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bb4c65a293a452e981ae1f4da978a7c","placeholder":"​","style":"IPY_MODEL_d4618ad0dff7427180be0171a93ffae6","value":"Downloading (…)okenizer_config.json: 100%"}},"ae3ef231293e4e5cab8a018029f8f393":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cec6eed83cc42ac9905f00a4874ae1a","max":450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_87913a97c6ac4656823bd02fe347389c","value":450}},"7924714608fa4ec08849ba6bb9767e90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68a8d71aabc74146bdcc44ce403ae171","placeholder":"​","style":"IPY_MODEL_809bde19cc7e44fbb6070e52ed89a8a4","value":" 450/450 [00:00&lt;00:00, 25.8kB/s]"}},"aa86c0b551a843368503e929d799bff7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bb4c65a293a452e981ae1f4da978a7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4618ad0dff7427180be0171a93ffae6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7cec6eed83cc42ac9905f00a4874ae1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87913a97c6ac4656823bd02fe347389c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68a8d71aabc74146bdcc44ce403ae171":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"809bde19cc7e44fbb6070e52ed89a8a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d556d81eb51b4d888f87a8815395a3d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6663409ece984d5da145521003264edc","IPY_MODEL_4910e65ef4ff479e8bae185be7a9f3db","IPY_MODEL_2d1cdc9067394b619c8d4bf0c2ff0a1b"],"layout":"IPY_MODEL_a80d27fa56d3447f887fa705761f0a6a"}},"6663409ece984d5da145521003264edc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f6eb364795b460ea5e64720274be572","placeholder":"​","style":"IPY_MODEL_a2866849217c414291c2196b6652a92a","value":"Downloading (…)/main/tokenizer.json: 100%"}},"4910e65ef4ff479e8bae185be7a9f3db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b1576d8fd0c4a77b60bcab2a22d04a6","max":2114274,"min":0,"orientation":"horizontal","style":"IPY_MODEL_32c90be56d0d45ffb062421433954aaa","value":2114274}},"2d1cdc9067394b619c8d4bf0c2ff0a1b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b8c3083ca0141aa8c306d036b9723a8","placeholder":"​","style":"IPY_MODEL_31ea8e9cd66041e894b257af78cdbb2b","value":" 2.11M/2.11M [00:00&lt;00:00, 23.0MB/s]"}},"a80d27fa56d3447f887fa705761f0a6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f6eb364795b460ea5e64720274be572":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2866849217c414291c2196b6652a92a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b1576d8fd0c4a77b60bcab2a22d04a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32c90be56d0d45ffb062421433954aaa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b8c3083ca0141aa8c306d036b9723a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31ea8e9cd66041e894b257af78cdbb2b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec6701bb2a264e7aa84a83ec33e17d84":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6155079eeb6a458fbe9b9bf1e0da37f1","IPY_MODEL_547db15aa1d640c5ad7609192f61a833","IPY_MODEL_92257566ac1944288ffe9e1bd0153d0c"],"layout":"IPY_MODEL_140de206836d47ecb2a708664f710867"}},"6155079eeb6a458fbe9b9bf1e0da37f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98f71b527fe54391aee2c8419b89d307","placeholder":"​","style":"IPY_MODEL_0f58ac5de8e84330962c9f14b27699e0","value":"Downloading (…)cial_tokens_map.json: 100%"}},"547db15aa1d640c5ad7609192f61a833":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98fb21e4c1e94336b524b76bcb63611a","max":228,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd5cbb597fb0446c87ec80e444ced8e0","value":228}},"92257566ac1944288ffe9e1bd0153d0c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0e31242a5b74bbc918f3bbfe06ec44e","placeholder":"​","style":"IPY_MODEL_f1cd5b981a2841f4b55af9c602311e93","value":" 228/228 [00:00&lt;00:00, 14.7kB/s]"}},"140de206836d47ecb2a708664f710867":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98f71b527fe54391aee2c8419b89d307":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f58ac5de8e84330962c9f14b27699e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98fb21e4c1e94336b524b76bcb63611a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd5cbb597fb0446c87ec80e444ced8e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0e31242a5b74bbc918f3bbfe06ec44e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1cd5b981a2841f4b55af9c602311e93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n","!pip -q install accelerate>=0.12.0\n","!pip install datasets\n","!pip install sentence_transformers"],"metadata":{"id":"zuwLc--q7RtP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd drive/MyDrive/en2sparql"],"metadata":{"id":"YcU-KRrrF9JR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xF_bcVRM61-s","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8ef83de1fb0e44969607170deaaffeb0","67a86ae180a24cb58b124d6c446b9927","101b912e480446839be5bee6ffb37d45","410bbfbc37054bd38774a5e1ce1a6678","0b97212b56e04b2da461b95667b90f99","04f62b8f53ee4ebe87b463a8a802e9d8","1c686d70ae334cd581880636cd5ffa79","adac7e0b007b45d09a045ee0d52a5ba6","fa752d3ff1c444ae83e30a1e3e037024","b7fec00c59644118a435f0e512af3ceb","4faabff4aded43ea948bb9fad7339a9e","00e42524b81c4f58850fd0814da7438b","8bed7add3654409f9a4f7e148c931b49","ae3ef231293e4e5cab8a018029f8f393","7924714608fa4ec08849ba6bb9767e90","aa86c0b551a843368503e929d799bff7","2bb4c65a293a452e981ae1f4da978a7c","d4618ad0dff7427180be0171a93ffae6","7cec6eed83cc42ac9905f00a4874ae1a","87913a97c6ac4656823bd02fe347389c","68a8d71aabc74146bdcc44ce403ae171","809bde19cc7e44fbb6070e52ed89a8a4","d556d81eb51b4d888f87a8815395a3d5","6663409ece984d5da145521003264edc","4910e65ef4ff479e8bae185be7a9f3db","2d1cdc9067394b619c8d4bf0c2ff0a1b","a80d27fa56d3447f887fa705761f0a6a","8f6eb364795b460ea5e64720274be572","a2866849217c414291c2196b6652a92a","1b1576d8fd0c4a77b60bcab2a22d04a6","32c90be56d0d45ffb062421433954aaa","7b8c3083ca0141aa8c306d036b9723a8","31ea8e9cd66041e894b257af78cdbb2b","ec6701bb2a264e7aa84a83ec33e17d84","6155079eeb6a458fbe9b9bf1e0da37f1","547db15aa1d640c5ad7609192f61a833","92257566ac1944288ffe9e1bd0153d0c","140de206836d47ecb2a708664f710867","98f71b527fe54391aee2c8419b89d307","0f58ac5de8e84330962c9f14b27699e0","98fb21e4c1e94336b524b76bcb63611a","bd5cbb597fb0446c87ec80e444ced8e0","d0e31242a5b74bbc918f3bbfe06ec44e","f1cd5b981a2841f4b55af9c602311e93"]},"executionInfo":{"status":"ok","timestamp":1692268191477,"user_tz":-120,"elapsed":8871448,"user":{"displayName":"Antonello","userId":"01962207529893687178"}},"outputId":"64bc15bb-093b-4e91-917d-63c27eccd01e"},"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'query_type', 'question', 'paraphrased_question', 'query', 'template_id', 'query_shape', 'query_class', 'auto_generated', 'number_of_patterns'],\n","        num_rows: 1795\n","    })\n","    validation: Dataset({\n","        features: ['id', 'query_type', 'question', 'paraphrased_question', 'query', 'template_id', 'query_shape', 'query_class', 'auto_generated', 'number_of_patterns'],\n","        num_rows: 257\n","    })\n","    test: Dataset({\n","        features: ['id', 'query_type', 'question', 'paraphrased_question', 'query', 'template_id', 'query_shape', 'query_class', 'auto_generated', 'number_of_patterns'],\n","        num_rows: 513\n","    })\n","})\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef83de1fb0e44969607170deaaffeb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e42524b81c4f58850fd0814da7438b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d556d81eb51b4d888f87a8815395a3d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec6701bb2a264e7aa84a83ec33e17d84"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["513\n","0%  [[{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Story Cloze Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Word Level)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'The UrbanSound8k dataset is evaluated over 4 metrics: mean rank, hit rank, top-5 rank and rank-sum.\\nThe WOS-5736 dataset is evaluated over 2 metrics: mean rank and hit rank.\\n\\nThe models being evaluated on the WOS-5736 dataset include the following models:\\n- Adversarial model - GAN with 8 language model layers\\n- LSTM + ELMo\\n- BERT\\n- BERT (large mode)'}], [{'generated_text': 'select distinct?paper?paper_lbl where { \\n?dataset a orkgc:Dataset; \\n rdfs:label?dataset_lbl. \\n FILTER (str(?dataset_lbl) = \"Penn Treebank\") \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark. \\n?paper orkgp:P31?cont; \\n rdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a mzae:Dataset; rdfs:label mzae:Label; FILTER (str( mzae:Label) = \"SearchQA\")?benchmark mzae:HAS_DATASET?dataset; mzae:HAS_EVALUATION?eval.?eval mzae:HAS_VALUE?value. OPTIONAL {?eval mzae:HAS_METRIC?metric.?metric mzae:Label. }?cont mzae:HAS_BENCHMARK?benchmark. OPTIONAL {?cont mzae:HAS_MODEL?model. mzae:Label. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The metrics used to evaluate models on the TDMSci dataset are:\\n\\taccuracy\\t\\t:measure of how well the model predicts the correct labels (i.e. the ratio of actual labels to predicted labels)\\n\\tF1 measure\\t\\t: metric that combines the performance of precision and recall into a single metric. F1 = 2 * P * R / P + R\\n\\tmacro-F1\\t\\t: measure that combines the performance of F1 with the \"F1 for the macro-average\" factor.\\n\\tmicro-F1\\t\\t: measure that combines the performance of F1 with the \"F1 for the micro-average\" factor.\\n\\tROC curve\\t\\t: a plot of sensitivity on the y axis vs. one minus the false positive rate on the x axis, where the y axis values are from 0 to 1 (0 is false negatives and 1 is false positives)\\n\\tAccuracy\\t\\t: the proportion of correct predictions'}], [{'generated_text': 'The average installed capacity of all energy sources considered is 300kWh.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Russian\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n<|endoftext|>(function() {\\n  var defaultOptions = {\\n    labels: []\\n  };\\n\\n  defaultOptions = Object.assign(defaultOptions, window. puli.sdkOptions);\\n\\n  puli.picker.Time = Class.create(puli.picker.AbstractPicker, {\\n    init: function(opts) {\\n      this.timeModel = new puli.picker.TimeModel(opts);\\n      this.initialize();\\n    },\\n    initialize: function() {\\n    },\\n    updateTime: function() {\\n      this.timeModel.setLabel(this.formatTime(this.selectedTime, this.options.timeFormat));'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kuzushiji-MNIST\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Romanian-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The model DCBench-0WDJ has achieved the highest Score score on the Ball in cup, catch (DMControl500k) benchmark dataset with the metric value of 5718.0.'}], [{'generated_text': 'The top performing model is the, Ours-tfjs, model based on the Top-1 Accuracy score when benchmarked on the iNaturalist 2019 dataset.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'Commonly used evaluation metrics for the TDM Tagged Corpus dataset include:\\n- MAP (Mean Average Precision)\\n- ROUGE-L (Recall-Oriented Understudy for Gigaboots)\\n- BLEU (Average Watched Article Evaluation)'}], [{'generated_text': '-- Research papers that have benchmarked models on the BC2GM dataset\\nSELECT DISTINCT?paper?paper_lbl \\nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC2GM\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Lunar Lander (OpenAI Gym)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet V2\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\\n\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet 64x64\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': \"Values\\n-------\\n45.36379216\\n\\nNotes\\n-----\\nIn the above output, we can see that the best performing model in this case was a ResNeXt 101 network with a mIoU of 0.669870116, having 42.22 Mbits per byte. The second best performing model was a GoogleNet model with a mIoU of 0.594915293, having 43.88 Mbits per byte. Both these models were evaluated on the same dataset, and it's worth noting that these numbers change if the model is evaluated on a different dataset.\"}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Flair-TDM\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Transformer-XL Base\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric is the neural language model BERT-Base from the paper \"BERT: Pre-training of Deep Bidirectional Transformation Trees for Language Modeling\" by Wang et al. The F1 score is 0.94.'}], [{'generated_text': 'SELECT DISTINCT?code\\nWHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"SAN (single model)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2004\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n   ?model a orkgc:Model; \\n    rdfs:label?model_lbl. \\n    FILTER (str(?model_lbl) = \"PNDec\") \\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark. \\n   ?cont orkgp:HAS_MODEL?model; \\n    orkgp:HAS_SOURCE_CODE?code. \\n}'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n ?model a orkgc:Model; \\n  rdfs:label?model_lbl. \\n  FILTER (str(?model_lbl) = \"CATTS-XSUM\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?cont orkgp:HAS_MODEL?model; \\n  orkgp:HAS_SOURCE_CODE?code. \\n} \\n\\nNote: if you want to reference code in other papers that have not used this particular model for benchmarking, you can use the following simpler query:\\n\\nSELECT?code \\nWHERE { \\n ?model a orkgc:Model; \\n  rdfs:label?model_lbl. \\n  FILTER (str(?model_lbl) = \"CATTS\") \\n  FILTER (notProphet?benchmark) \\n ?cont orkgp:HAS_MODEL?model; \\n  orkgp:HAS_SOURCE_CODE?code. \\n}'}], [{'generated_text': 'The best model is orkgp:RACINE_PDB which achieves an accuracy score of 91.63% over the IMDb-B dataset.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot German-to-French\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"SemExp\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Up and Down\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Number of params\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'select distinct?paper?paper_lbl where {\\n ?dataset a orkgc:Dataset;\\n  rdfs:label?dataset_lbl.\\n  FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level)\" )\\n ?benchmark orkgp:HAS_DATASET?dataset.\\n ?cont orkgp:HAS_BENCHMARK?benchmark.\\n ?paper orkgp:P31?cont;\\n  rdfs:label?paper_lbl.\\n}'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Double Dunk\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\\n\\nThe metrics of evaluation over the Atari 2600 Venture dataset are:\\n-  Winner : count\\n-  Win probability: count( rating < 4 ) / (count+1) * 100\\n-  Time : count( rating < 4 ) * 3600 / (count+1) * 100'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Top 1 Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'UNKNOWN'}], [{'generated_text': 'Here are the title and ID of research papers that include a benchmark for the MedSTS dataset:\\n\\n?paper   P31 Dataset mentions in Social Sciences\\n?paper_lbl Dataset mentions in Social Sciences'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE { \\n ?problem a orkgc:Problem; \\n  rdfs:label?problem_lbl. \\n  FILTER (str(?problem_lbl) = \"Named entity recognition\") \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem. \\n} \\n\\nSELECT DISTINCT?dataset?dataset_lbl WHERE { \\n ?problem a orkgc:Problem; \\n  rdfs:label?problem_lbl. \\n  FILTER (str(?problem_lbl) = \"Word Sense Disambiguation\") \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem.'}], [{'generated_text': 'Here is the list of research papers that have performed benchmarks on the BioASQ dataset:\\n\\nSELECT DISTINCT?paper?paper_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"BioASQ\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1 score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC Chinese-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The following titles and IDs include a benchmark for the ImageNet dataset:\\n\\nP31\\nP32\\nP33\\nP34\\nP35\\nP36'}], [{'generated_text': 'Here is a list of research papers that include benchmarks for the Gibson PointGoal Navigation dataset:\\n\\n- Lee, Hoonil (2020). “Portal to SQL: Scalable Schema Composition with Lens’s Drill’. ACM SIGMOD abs. 2019.\\n- Masood, Haroon (2020). “Automatic Relational Model Improvement via Benchmark Datasets”. arXiv preprint arXiv:2002.07092.\\n- Yu, Liangxu; Kourtis, Ioannis; Bennis, Mike; Surnow, Philipp (2020). “Build Your Own Neural Network for Model Averaging on Benchmark Datasets”. arXiv preprint arXiv:2001.09679.\\n- Bissacco, Lorenzo; Kowlampara, Agata; Zangeneh-Nikkeeran, Meisam; Stoller, Jonathan (2020). “Polarized-Anchor Embeddings for Point Cloud Analysis”. arXiv preprint arXiv:2005.05505.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a bk-swbd:Language; bk-swbd:lemma \"CONLL++\"; bk-swbd:id \"CONLL++\"; bk-swbd:tag \"controlled-subordinated grammar\"; bk-swbd:datafile \"top10000.tsv\"; bk-swbd:description \"a large-scale supervised statistical language model trained on the CoNLL-X dataset\"; bk-swbd:generation group?generation group-id;?eval orkgp:HAS_EVALUATION?eval;?eval orkgp:HAS_VALUE?value; OPTIONAL {?eval orkgp:HAS_METRIC?metric;?metric rdfs:label?metric_lbl; }?cont orkgp:HAS_BENCHMARK?benchmark;?cont orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl; } } ORDER BY DESC(?value) } } GROUP BY?metric'}], [{'generated_text': 'SELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PIQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Crazy Climber\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n?model a orkgc:Model; \\nrdfs:label?model_lbl. \\nFILTER (str(?model_lbl) = \"PtGen\"): \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark. \\n?cont orkgp:HAS_MODEL?model; \\norkgp:HAS_SOURCE_CODE?code. \\n}'}], [{'generated_text': 'SELECT DISTINCT?code \\n  WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Funnel Transformer\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': \"Common evaluation metrics used when benchmarking models on the Reuters De-En dataset include:\\n\\n. * Pearson correlation coefficient (PCC): this metric is used to evaluate predictions on new data compared to the labels. For example, predicting the users' purchase intentions on unseen promotions in a real-world business setting is an example of PCC evaluation.\\n\\n. * Root Mean Square Error (RMSE): This metric is used to evaluate predictions on new data compared to the targets. For example, predicting the users' purchase intentions on unseen promotions in a real-world business setting is an example of RMSE evaluation.\\n\\n. * Normalised Discrimination Error (NDE): This metric is used to evaluate predictions on new data. This metric is essentially the average difference between predictions and the targets. For example, predicting the users' purchase intentions on unseen promotions in a real-world business setting is an example of NDE evaluation.\"}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PubMed 20k RCT\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DocRED-BiLSTM\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"FLOPS\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Oxford-IIIT Pets\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}]]\n","0 2\n","1 2\n","2 2\n","3 2\n","0 3\n","0 5\n","1 5\n","2 5\n","0 6\n","1 6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["0 10\n","0 11\n","0 13\n","1 13\n","2 13\n","3 13\n","4 13\n","5 13\n","6 13\n","0 18\n","1 18\n","2 18\n","3 18\n","4 18\n","5 18\n","6 18\n","7 18\n","8 18\n","9 18\n","0 21\n","0 26\n","0 31\n","0 34\n","1 34\n","0 35\n","1 35\n","2 35\n","0 39\n","0 40\n","1 40\n","2 40\n","0 46\n","9.090909090909092%  [[{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AESLC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'select distinct?paper?paper_lbl where { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"Oxford IIIT Pets\")?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'The highest benchmark result, including the metric and score, for the WOS-11967 dataset is:\\n\\n5.638441670353547 - sqrt(8.0)'}], [{'generated_text': 'SELECT DISTINCT?code \\nWHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"AcrE\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\\n\\norkgp:HAS_DATASET returns dataset, orkgp:HAS_BENCHMARK returns benchmark, orkgp:HAS_MODEL returns model'}], [{'generated_text': 'SELECT?model?model_lbl WHERE { \\n?metric a orkgc:Metric; \\n rdfs:label?metric_lbl. \\n FILTER (str(?metric_lbl) = \"SemEval 2013\") { \\n SELECT?model?model_lbl WHERE { \\n?dataset a orkgc:Dataset; \\n rdfs:label?dataset_lbl. \\n FILTER (str(?dataset_lbl) = \"Supervised:\")?benchmark orkgp:HAS_DATASET?dataset; \\n orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; \\n orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; \\n orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n ORDER BY DESC(?value) \\n LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': '7.2 milligram'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'select distinct?code where \\n{?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DQN-PixelCNN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Freeway\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'Here is the list of papers that have utilized the DY-ResNet-10 model and include the code links:\\n* [Boussau, Andrea; Candela, Alain; Guerini, Marco; Polosukhin, Igor; Roy, Hassan; Tomar, Atif; Tran, Qian; Zhang, Tingting; et al. 2017. Dynamic Coattention Networks: Large-scale sentiment analysis.  Neural Information Processing Systems, 27(7): 2967-2977. https://papers.nips.cc/paper/7521-dynamic-coattention-networks-large-scale-sentiment-analysis.pdf]\\n* [Bi, Yang; Gao, Jin; Yu, Tian; Hu, Yanjun; Pan, Yi; Tian, Cunming; et al. 2017. Long Short-Term Memory Networks With Dropout Improves the Efficiency of Text Classification. https://www.sciencedirect.com/science/article/pii/S0960574717302629]\\n* [Sedaghat, Yaser; Radu, Elan; Shvarts, Andrei; Hovy, Orly; Ben-'}], [{'generated_text': 'Indicate the model that performed best in terms of Micro F1 metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\\n\\nSELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Micro F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nThe top performing model was orkgp:MULTILING'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Tennis\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The DCASE dataset has the following metrics for evaluation:\\n- F-score\\n- Precision\\n- Recall\\n- Index recall\\n\\nThe SoMeSci dataset has the following metrics for evaluation:\\n- F-score\\n- Precision\\n- Recall\\n- Index recall\\n- Total precision\\n- Total recall\\n\\nThe DuIE dataset has the following metrics for evaluation:\\n- F-score\\n- Precision\\n- Recall\\n- Index recall\\n- Total precision\\n- Total recall'}], [{'generated_text': 'The Cheetah, run (DMControl100k) benchmark dataset has been used to test various models. \\nThe metrics that are commonly used when evaluating models on the Cheetah, run (DMControl100k) dataset include:\\n\\nModel size inKB\\nNumber of actions inKB\\nNumber of parameters inKB\\nTime to generate actionKB\\nTime to generate parametersKB\\nTotal number of parameters in KB\\nTotal number of actions in KB\\nTime to generate a single sampleKB\\nTime to generate a single sample over a single training epochKB\\nTime to train a model over 50 epochsKB'}], [{'generated_text': \"[{'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'}, {'model': 'lstm','model_lbl': 'LSTM'},\"}], [{'generated_text': 'Solid Lipid Nanoparticles, Liposomes, Polymeric, Polymers, Iron Oxide, Silica,  Gold Nanoparticles'}], [{'generated_text': 'The name of the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset is AutoML System from AutoMGA'}], [{'generated_text': 'select distinct code from \\n    { \\n       \"model\": [\\n          { \"hasdataset\": true, \"hasbenchmark\": true, \"hasmodel\": true, \"hassourcecode\": true },\\n          { \"hasdataset\": false, \"hasbenchmark\": true, \"hasmodel\": true, \"hassourcecode\": true },\\n          { \"hasdataset\": false, \"hasbenchmark\": true, \"hasmodel\": true, \"hassourcecode\": true },\\n          { \"hasdataset\": false, \"hasbenchmark\": true, \"hasmodel\": true, \"hassourcecode\": true }\\n       ],\\n       \"benchmark\": [\\n          { \"hasdataset\": true, \"hasbenchmark\": true, \"hasmodel\": true, \"hassourcecode\": true },\\n          { \"hasdataset\": false, \"hasbenchmark\": true, \"hasmodel\": true, \"hassourcecode\": true },\\n          { \"hasdataset\": false, \"hasbenchmark\": true, \"hasmodel\": true, \"hassourcecode\": true },\\n          { \"hasdataset\": false, \"hasbenchmark\": true, \"hasmodel\": true, \"hassourcecode\": true }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Permuted Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential MNIST\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The following models have been evaluated on the SciERC dataset:\\n* MobileNet\\n* Inception\\n* WideResNet\\n* Xception\\n* SqueezeNet\\n* DenseNet\\n* VGG\\n* GoogLeNet\\n* AlexNet\\n* SqueezeNetV2\\n* DenseNet121\\n* DenseNet201\\n* DenseNet242\\n* MobileNetV2\\n* GoogleNet\\n* InceptionV3'}], [{'generated_text': 'Common evaluation metrics used on CommonsenseQA benchmark dataset are:\\n- A.P.G.: Approach Parallelism Gain\\n- B.L.: Blocks Looped\\n- C.R.: Correctly Recognized words\\n- D.R.: Detected Words Ratio'}], [{'generated_text': 'SELECT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IMDb-M\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\n+-----------------------------------------------------------------------------------------------------------------------------------+\\n|                                                                                                                               |\\n|                                                                            ?benchmark                                                |\\n|                                                                             |                                                             |\\n|                                                                             |IMDb-M(Accuracy)'}], [{'generated_text': 'The highest benchmark result for the Scholarly entity usage detection dataset is the following:\\n<benchmark metric=\"entity_occurrences\" score=\"0.988995\"/>'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MultiNLI\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\noutput: \\n[\\n  {\\n    \"model\": \"multi-level neuro network\",\\n    \"model_lbl\": \"MultiNLI\"\\n  }\\n]'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"200k Short Texts for Humor Detection\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential MNIST\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"CRF with term expansion\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\n[\\n  \"https://openreview.net/forum?id=rJ0SdNCgcF\", \\n  \"https://www.nowcorn.com/ publications/Proceedings.aspx?publicationID=115\", \\n  \"https://eprint.iacr.org/2016/048.pdf\", \\n  \"https://www.cs.toronto.edu/~tijay/papers/zhang2015crosssentence.pdf\", \\n  \"https://web.stanford.edu/classes/2016/03/04/slides-song/slides-song.pdf\", \\n  \"https://www.cmu.edu/docs/cs/aicst17/m'}], [{'generated_text': 'select distinct metric as metric_lbl (max(value) as score) where \\n{ \\n\\tselect metric as metric_lbl, value as score \\n\\twhere \\n\\t{ \\n\\t\\tdataset a orkgc:Dataset; \\n\\t\\trdfs:label dataset_lbl. \\n\\t\\tFILTER (str(dataset_lbl) = \"NYT24\") \\n\\t\\torkgp:HAS_DATASET dataset; \\n\\t\\torkgp:HAS_EVALUATION evaluation. \\n\\t\\teval:HAS_VALUE value. \\n\\t\\tOPTIONAL { evaluation:HAS_METRIC metric. metric:LABEL rdfs:label. } \\n\\t\\tcont:HAS_BENCHMARK benchmark. \\n\\t\\tOPTIONAL { cont:HAS_MODEL model. model:LABEL rdfs:label. } \\n\\t} \\n\\torder by score desc\\n}\\ngroup by metric_lbl'}], [{'generated_text': 'The metrics used to evaluate models on the SciERC benchmark dataset are: \\n- Error\\n- ROC-AUC\\n- F1 Score\\n- AUROC\\n- Precision\\n- Recall\\n- Accuracy\\n- Specificity\\n- FPR\\n- FNR\\n- ACCURACY'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\n--\\n-- The WMT2016 English-German dataset'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"FLOPS\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CIFAR-100\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The best performing model for the RotoWire (Relation Generation) dataset is'}], [{'generated_text': 'The best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric is the DM Control 100 K Model.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Assault\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DQN+SR\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The model DMControl100k scored 100 on the Ball in cup, catch (DMControl100k) benchmark dataset with a score of 9.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT29\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': '<p>The following papers have utilized the MEMEN (single model) model:<p>\\n<ol><li><a href=\"https://doi.org/10.5281/zenodo.3570459\">Sparql Query evaluation on linked open data for memegen: single model version</a></li>\\n<li><a href=\"https://doi.org/10.5281/zenodo.3569882\">Sparql Query evaluation on linked open data for memegen: batch size 100 and single model version</a></li>\\n<li><a href=\"https://doi.org/10.5281/zenodo.3569387\">Sparql Query evaluation on linked open data for memegen: batch size 200 and single model version</a></li>\\n</ol></p>'}], [{'generated_text': 'SELECT DISTINCT?code \\nWHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"CAIT-M-36\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\\ninput (English text): Provide a list of papers that have utilized the CvT-13-NAS model and include the links to their code?\\noutput (Sparql query):'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp-5\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"SAC\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': '<Query>\\n    <Where>\\n        <Or>\\n            <Kungeon:Model rdf:resource=\"org.kollection.klototherium.diseases.Models.MysteryModel\"/>\\n            <Kungeon:HasDataset rdf:resource=\"org.kollection.klototherium.diseases.Data.MysteryTestData\"/>\\n            <Kungeon:HasBenchmark rdf:resource=\"org.kollection.klototherium.diseases.Benchmarks.MysteryModelBenchmark\"/>\\n            <Kungeon:HasModel rdf:resource=\"org.kollection.klototherium.diseases.Models.MemeNAble\"/>\\n            <Kungeon:HasSourceCode rdf:resource=\"file:C:/Users/joakim/.sparql Studios/proteus/sparql-fu/proteus/SPARQLfu/blog/query-language/query-language/RetrievingKnowledgeFromProteus.java\"/>\\n        </Or>\\n    </Where>\\n    <Select>\\n        <Distinct>\\n            <code r'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.\")}'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Assault\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"200k Short Texts for Humor Detection\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n    ?model o.?model_lbl.\\n\\n\"BERT\" o.F1-score 25.0'}], [{'generated_text': 'SELECT DISTINCT?code \\n WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"STREET\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"enwik8\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nThe above query returns the following results:\\n+--------------+-----------------+\\n| model_lbl     | model           |\\n+--------------+-----------------+\\n| WikiText-2   |?model_lbl_1    |\\n| WikiText-2   |?model_lbl_2    |\\n| enwik8       |?model_lbl_3    |\\n| enwik8       |?model_lbl_4    |\\n+--------------+-----------------+'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n FILTER (str(?dataset_lbl) = \"SciTLDR\") \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK \\n?benchmark. \\n?paper orkgp:P31?cont; \\n rdfs:label?paper_lbl. \\n } \\n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciTLDR dataset?\\n output (Sparql query):'}]]\n","0 1\n","0 2\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 8\n","0 10\n","1 10\n","2 10\n","3 10\n","4 10\n","0 13\n","0 14\n","1 14\n","0 15\n","0 16\n","1 16\n","2 16\n","3 16\n","4 16\n","5 16\n","6 16\n","7 16\n","8 16\n","9 16\n","0 17\n","1 17\n","2 17\n","3 17\n","0 18\n","1 18\n","2 18\n","3 18\n","0 20\n","1 20\n","2 20\n","0 21\n","1 21\n","2 21\n","3 21\n","4 21\n","5 21\n","6 21\n","7 21\n","8 21\n","9 21\n","0 23\n","1 23\n","2 23\n","3 23\n","4 23\n","5 23\n","6 23\n","7 23\n","8 23\n","9 23\n","0 28\n","0 29\n","1 29\n","2 29\n","3 29\n","4 29\n","5 29\n","6 29\n","7 29\n","8 29\n","9 29\n","0 32\n","0 33\n","1 33\n","2 33\n","3 33\n","4 33\n","0 36\n","0 38\n","0 43\n","18.181818181818183%  [[{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CommonsenseQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The following metrics are commonly used when evaluating models on the SQuAD2.0 dataset:\\n- microfacet: Overall ranking\\n- microfacet: AMA (Average Metric Adjustment)\\n- microfacet: CMA (CreditMetric-Adjusted)\\n- microfacet: C-index\\n- microfacet: METEOR\\n- microfacet: ROUGE-L\\n- microfacet: GLUE- analogy\\n- microfacet: Metric Fusion\\n- microfacet: DUC-2004\\n- microfacet: DUC-2007\\n- microfacet: SQuAD$ConceptMatch'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'select distinct?dataset?dataset_lbl where \\n{?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Information Extraction\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"RE+ Micro F1 \") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL04\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The following papers have utilized the XLNet (base) model:\\n- Shen, Shaoqi, et al. \"X-LEARN: A Scalable Neural Language Model With General Rank Consistency.\" International Conference on Computational Linguistics and Generation of Immersed Document for 2020.\\n- Cao, Tiejun, and Xiao-Chen Yu. \"XGBoost: Tuning the GCV Regularization Parameter via Accuracy-Based Online Selection.\" Proceedings of the 25th ACM international conference on Artificial intelligence, pages 1147-1154. ACM, 2013.\\n- Sutskever, Ilya, and James Martens. \"Multilayer neural networks designed to optimize precision top-k accuracy.\" Advances in neural information processing systems, pages 1097-1103. 2009.\\n-  Khashan, Parag, Jonathon Shlens, and Benjamin Recht. \"Theano: A C++ GPU computing framework.\" Eurosys\\'07, 2007.\\n- Plank, Ralph, Gereon Weil, and Johannes Platt. \"X-Pack: Scalable Deep Learning with Caffe.\" Proceedings of the 25th ACM SIGKDD international conference on Knowledge discovery and'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"Open Entity\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n} \\n\\ninput (English text): List the title and ID of research papers that contain a benchmark over the Paper Field dataset?\\noutput (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Paper Field\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\ninput ('}], [{'generated_text': 'Here is a list of the models that have been evaluated on the NYT29 dataset:\\n\\n- DARTS\\n- Delta Deep Learning\\n- DeepChem\\n- DistMult\\n- Echo Boosting\\n- ELM\\n- FastTree\\n- Gradient Boosting Decision Tree\\n- Greedy classifier\\n- K-NN\\n- M5\\n- M6\\n- MARS\\n- Multi Layer Perceptron\\n- NN-PLDA\\n- Random Forest\\n- Support Vector Machine\\n- Tanh\\n- Two Stage Loss\\n- Word2Vec'}], [{'generated_text': '3.'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DDQN-PC\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\ninput (English text): List the code links in papers that use the DDQN-PC model in any benchmark?\\noutput (Sparql query):'}], [{'generated_text': 'SELECT DISTINCT \\n?paper?paper_lbl \\nWHERE { \\n?dataset a orkgc:Dataset; \\nrdfs:label?dataset_lbl. \\nFILTER (str(?dataset_lbl) = \"TREC-6\") \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark. \\n?paper orkgp:P31?cont; \\nrdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Recurrent Rational DQN Average\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-French\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"1-of-100 Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PolyAI Reddit\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?code \\n    WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Long Short Transformer\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"EM\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TriviaQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?problems,?problems_labels \\nWHERE {?papers rdf:type orkgc:Paper.?papers rdfs:label?papers_labels. FILTER(REGEX(?papers_labels, \"Vernier Effect\", \"i\"))?papers orkgp:P31?contrib.?contrib orkgp:P32?problems.?problems rdfs:label?problems_labels. }'}], [{'generated_text': 'The model with the PARAMS value of 34.826 (and score of 34.82) is the following: SparQL Model Optimization Library (3.11.0) with the orkgp:HAS_METRIC metric and 34.82 value.\\nThe top benchmark result (metric and value) for the DTD dataset is the following:\\n34.826 34.82 34.82'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a cnnic:Dataset; rdfs:label cnnic-10:Dataset; FILTER (str( cnnic-10:Dataset) = \"CINIC-10\")?benchmark cnnic:HAS_DATASET; cnnic:HAS_EVALUATION cnnic:HAS_VALUE?value; cnnic:HAS_METRIC?metric; cnnic:HAS_BENCHMARK cnnic:HAS_MODEL; cnnic:HAS_MODEL cnnic:MODEL cnnic-10:HAS_LABEL?dataset_lbl; cnnic-10:HAS_MODEL?model; cnnic-10:HAS_MODEL cnnic-10:MODEL cnnic-10:LABEL?metric_lbl; cnnic-10:HAS_MODEL cnnic-10:EVALUATION?eval; cnnic-10:HAS_VALUE?value. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {\\n?dataset a orkgc:Dataset;\\n rdfs:label?dataset_lbl.\\n FILTER (str(?dataset_lbl) = \"SemEval-2018 Task 7\")\\n?benchmark orkgp:HAS_DATASET?dataset.\\n?cont orkgp:HAS_BENCHMARK?benchmark.\\n?paper orkgp:P31?cont;\\n rdfs:label?paper_lbl.\\n }'}], [{'generated_text': 'The metrics that are used to evaluate models on the COPA benchmark dataset are:\\n- AUC_PRC\\n- AUC_ROC\\n- AUC_ROC_IDF\\n- AUROC\\n- AUROC_IDF\\n- AUROCLOSS\\n- AUROCLOSS_IDF\\n- BDI\\n- BDI_IDF\\n- COR\\n- COR_IDF\\n- MCC\\n- MCC_IDF\\n- PCC\\n- PCC_IDF\\n- RMSE\\n- RPD\\n- R-SVM\\n- R-SVM_IDF\\n- ROCS\\n- ROCS_IDF\\n- SP\\n- SP_IDF\\n- THRESH'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1 entity level\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NCBI-disease\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': '- SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NCBI Disease\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\\n- SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Supervised:\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"GCNN-8\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"GGCNN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nList the code links in papers that use the GGCNN model in any benchmark?\\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str'}], [{'generated_text': 'The following datasets have been benchmarked on the ADE Corpus benchmark dataset:'}], [{'generated_text': 'The highest benchmark result for the CUB-200-2011 dataset includes the metric \"Number of sharedceptual segments\" and the score \"100\".'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"SEE\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\ninput (English text): Can you provide links to code used in papers that benchmark the DocTAET-TDM model?\\noutput (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DocTAET-TDM\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\ninput (English text): Can you provide links to code used in papers that benchmark'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Prior+Duel hs\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nA:\\n\\nWhat are Prior+Duel hs and Duel hs?\\n\\n<|endoftext|>Infographic: The 10 Best Cities for Megachurches\\n\\nAs more megachurches spring up across the U.S., the question of which cities are the best for these megachurches to set up shop in has become a hot-button issue for many Southern Baptists.\\n\\nOne of the most recent polls on this issue came from a survey done by SSI. The results of this poll were disputed by some megachurch proponents who pointed out a few mistakes in the methodology of this poll.\\n\\nHowever, the results have nevertheless sparked much debate among many conservative and evangelical Christian advocates. In this infographic we present the ten cities that'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DocRED\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Adaptive Input Large\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Solaris\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SNLI\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"MMV\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Centipede\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The models that have been benchmarked on the MNIST dataset are: \\n- No Neural Net model\\n- Slim-RNN\\n- LSTM\\n- GRU\\n- ConvLSTM'}], [{'generated_text': 'select?location where { \\n?_ orkgp:compareContribution [ orkgp:P32 [ rdfs:label?label ]; orkgp:P5049?location ] FILTER(REGEX(STR(?label), \"geopoli?tics\")) }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"WLPC\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n} \\n\\ninput (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?\\noutput (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WNLI\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TempEval-3\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'The following are the models that have been evaluated on the SciERC dataset:\\n- ResNet-18\\n- ResNet-34\\n- MobileNet\\n- Shufflenet\\n- SqueezeNet\\n- FractalNet\\n- DenseNet-121\\n- DenseNet-161\\n- DenseNet-169\\n- DeepLab-v3\\n- SPPNet\\n- Inception-v3\\n- NASNet-mobile\\n- NASNet-large\\n- NASNet-large-distill\\n- DenseNet-121-small\\n- DenseNet-161-small\\n- DenseNet-169-small\\n- DenseNet-201-small\\n- Shufflenet-1\\n- Shufflenet-2\\n- Shufflenet-3\\n- DenseNet-121-large\\n- DenseNet-161-large\\n- DenseNet-169-large\\n- Inception-v3-large\\n- NASNet-large-distill-new\\n- MobileNet-V2\\n- DenseNet-121-large-distill\\n- DenseNet-161-large-distill\\n- DenseNet-169-'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL 2012\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp-14\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"A3C FF (1 day)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'Carpenter et al. 2012, Caruana 2010, and Johnson et al. 2010'}], [{'generated_text': 'The highest benchmark result achieved on the WSC dataset, including the metric and its value is 23.11221318696267.'}], [{'generated_text': 'Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\\n\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'The metrics that are used to evaluate models on the SciERC benchmark dataset are:\\n* accuracy (precision, recall, f1 score, mean absolute error)\\n* standard deviation of training loss\\n* standard deviation of testing loss\\n* classification time in seconds\\n* accuracy of model on held-out testing dataset'}], [{'generated_text': 'The following models have been benchmarked on the Natural Questions (short) dataset:\\n- DocPL;\\n- DualPL;\\n- PAiL;\\n- PositivePL;\\n- SKTR;\\n- SQuAD;\\n- StarCraft;\\n- TextCNN;\\n- TextLDA;\\n- TextRank;\\n- XGboost.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { \\n  { SELECT?metric?metric_lbl?value WHERE \\n    {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-French\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n    ORDER BY DESC(?value) } } \\n  } \\n  { SELECT?metric?metric_lbl?value WHERE \\n    {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?'}], [{'generated_text': 'SELECT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Validation perplexity\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WikiText-2\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}]]\n","0 1\n","1 1\n","2 1\n","3 1\n","4 1\n","5 1\n","6 1\n","7 1\n","8 1\n","9 1\n","0 3\n","0 5\n","1 5\n","2 5\n","3 5\n","4 5\n","5 5\n","6 5\n","7 5\n","8 5\n","9 5\n","0 7\n","0 8\n","1 8\n","2 8\n","3 8\n","4 8\n","5 8\n","6 8\n","7 8\n","8 8\n","9 8\n","0 17\n","0 20\n","1 20\n","2 20\n","3 20\n","4 20\n","5 20\n","6 20\n","7 20\n","8 20\n","9 20\n","0 24\n","1 24\n","2 24\n","3 24\n","4 24\n","5 24\n","6 24\n","7 24\n","8 24\n","9 24\n","0 25\n","1 25\n","0 34\n","1 34\n","0 35\n","1 35\n","2 35\n","3 35\n","4 35\n","0 38\n","0 43\n","0 44\n","0 46\n","0 47\n","27.272727272727273%  [[{'generated_text': \"Here are the code links in papers that use the All-attention network - 18 layers model in any benchmark:\\n- Paper: 'Densely Connected Convolutional Networks for Semantic Segment Fine-Grained Recognition', https://arxiv.org/pdf/1512.04597.pdf\\n- Paper: 'ATEM: Automated Code Transparency for Large-scale Knowledge Bases', https://www.usenix.org/system/files/conference/atmos/conference_results/miller_et_al_atems.pdf\\n- Paper: 'Transparency for Computer Aided Design', https://dl.acm.org/doi/10.1145/2880031.2880089\\n- Paper: 'Inverse Reinforcement Learning for Program Transparency and Model Verification', https://arxiv.org/pdf/1801.02245.pdf\"}], [{'generated_text': 'Cosine similarity'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STS Benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Freeway\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DCASE\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'The most common metric used to benchmark models on the Yelp Fine-grained classification dataset is Error.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 River Raid\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BERT + BiLSTM + CRF Decoding\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nThe following papers have utilized the BertSumExt model:\\n\\n* [https://www.kaggle.com/c/miriam-covise-language-model-ranking/data sets](https://www.kaggle.com/c/miriam-covise-language-model-ranking/data sets)\\n* [https://www.kaggle.com/c/testing-large-language-models/data sets](https://www.kaggle.com/c/testing-large-language-models/data sets)\\n* [https://www.kaggle.com/c/learning-chat-text-analyzer/data sets]('}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Pearson Correlation\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MedSTS\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {\\n ?dataset a orkgc:Dataset;\\n  rdfs:label?dataset_lbl.\\n  FILTER (str(?dataset_lbl) = \"WSC\")\\n ?benchmark orkgp:HAS_DATASET?dataset.\\n ?cont orkgp:HAS_BENCHMARK?benchmark.\\n ?paper orkgp:P31?cont;\\n  rdfs:label?paper_lbl.\\n}'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Linear-chain CRFs\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\\n\\n;'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DrQA (Document Reader only)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DyGIE\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\n-- List the code links in papers that use the DrQA model in any benchmark?\\n-- SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl'}], [{'generated_text': 'According to the provided answer, the model \"fastText [EN] with GLoVe external dictionary\" has achieved the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset. The best model\\'s result is 50.0 with a score of 10,912.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {\\n ?dataset a orkgc:Dataset;\\n  rdfs:label?dataset_lbl.\\n  FILTER (str(?dataset_lbl) = \"ACL-ARC\")?benchmark orkgp:HAS_DATASET?dataset;\\n  orkgp:HAS_EVALUATION?eval.\\n  OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }\\n}'}], [{'generated_text': '* Orkgp:HAS_EVALUATION'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Berzerk\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT \\n   ?paper?paper_lbl \\nWHERE { \\n   ?dataset a orkgc:Dataset ; \\n    rdfs:label?dataset_lbl. \\n    FILTER (str(?dataset_lbl) = \"Stanford Cars\") \\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark. \\n   ?paper orkgp:P31?cont; \\n    rdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC German-to-English\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Stanford Cars\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ShARe/CLEF eHealth corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'narr:technological:piezoresistive materials\\nnarr:query1:The least response time was obtained for palladium.\\nnarr:query2:The least response time was obtained for palladium.'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"GPT-2 (small)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nList the code links in papers that use the GPT-3 (Zero-Shot) model in any benchmark?\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"GPT-3 (Zero-Shot)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nList the code links in papers that use the GPT-2 ('}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Fishing Derby\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\nThere are 3 metrics used on the Atari 2600:\\n* Frames per second (FPS)\\n* Number of turns\\n* Number of calories consumed'}], [{'generated_text': 'The top model is Sylvain LeBrun et al. with a score of 88.74.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Battle Zone\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'code links in papers that use the BiT-M model in any benchmark:\\nhttps://github.com/knowledge-center/BERT-model/blob/master/bert/losses.py'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Star Gunner\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"NAS Cell\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"TCN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a BucCorp; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC French-to-English\")?benchmark BucCorp:HAS_DATASET?dataset; BucCorp:HAS_EVALUATION?eval.?eval BucCorp:HAS_VALUE?value. OPTIONAL {?eval BucCorp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont BucCorp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont BucCorp:HAS_MODEL?model.?model BucCorp:HAS_LABEL?label. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SQuAD2.0 dev:\\n+--------+-------+\\n| metric | score |\\n+--------+-------+\\n| qvalue | 12.69 |\\n+--------+-------+'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC2GM\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"Story Cloze Test\") \\n ?benchmark orkgp:HAS_DATASET?dataset; \\n  orkgp:HAS_EVALUATION?eval. \\n  OPTIONAL {?paper orkgp:HAS_MODEL?model. \\n ?model rdfs:label?model_lbl. \\n  } \\n} \\nSELECT DISTINCT?metric?metric_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"WikiText-2\") \\n ?benchmark orkgp:HAS_DATASET?dataset; \\n  orkgp:HAS_EVALUATION?eval. \\n  OPTIONAL {?eval orkgp:HAS_METRIC?metric. \\n ?metric rdfs:label?metric_lbl. \\n  }'}], [{'generated_text': 'SELECT DISTINCT?author_name \\n  WHERE { \\n     { SELECT?publication_date_ AS?date_of_the_latest_paper {?paper_ a orkgc:Paper; rdfs:label?title_; orkgp:P28?publication_month_; orkgp:P29?publication_year_. OPTIONAL {?publication_month_ rdfs:label?publication_month_label_ } OPTIONAL {?publication_year_ rdfs:label?publication_year_label_ } BIND( xsd:integer( IF( BOUND(?publication_month_label_),?publication_month_label_,?publication_month_ ) ) AS?publication_month_as_number_ ) BIND( xsd:integer( IF( BOUND(?publication_year_label_),?publication_year_label_,?publication_year_ ) ) AS?publication_year_as_number_ ) BIND( xsd:dateTime( CONCAT(?publication_year_as_number_, \"-\",?publication_month_as_number_, \"-'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DuIE\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'Coolant: Argon'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Berzerk\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Skiing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'Most commonly used benchmark datasets for the Document Summarization, Text Generation, and Text Reuse research fields are:\\n- TACL: The Americancollections.org Document Accuracy Challenge\\n- TED: The Educational Data Team\\n- COPUS: The Nineteenth Organization of the American Colleges for Motion Pictures, Video, and Television\\n- SPICELib:'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Tutankham\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Tutankham\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PROTEINS\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NaturalQuestions\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nNote that the model with the highest F1 score on the NYT29 dataset is the model called \"Logistic Regression with Lotus Notes Sentiment as Features\".'}], [{'generated_text': 'SELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CommitmentBank\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiT-M (ResNet)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\\n\\n<a href=\"https://arxiv.org/pdf/1805.09414.pdf\" target=\"_blank\">https://arxiv.org/pdf/1805.09414.pdf</a>, <a href=\"https://openaccess.theiet.org/content/early/2019/02/27/THEIL_2019_607315\" target=\"_blank\">https://openaccess.theiet.org/content/early/2019/02/27/THEIL_2019_607315</a>, <a href=\"https://www.eecs.berkeley.edu/en/Research/Research_Ads/Publications/papers/RDCFM18.pdf\"'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"RE Micro F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2004\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Nottingham\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'select distinct?metric?metric_lbl\\nwhere {\\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"STEM-ECR v1.0\")\\n ?benchmark orkgp:HAS_DATASET?dataset; \\n  orkgp:HAS_EVALUATION?eval. \\n  OPTIONAL { \\n   ?eval orkgp:HAS_METRIC?metric. \\n   ?metric rdfs:label?metric_lbl. \\n  }\\n}'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Multi-Perspective Matching (single model)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\\n\\ninput (English text): Can you provide links to code used in papers that benchmark the Multi-Perspective Matching (single model) model?\\noutput (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Multi-Perspective Matching (single model)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The models that have been evaluated on the GigaWord dataset are:\\n\\n- GPT-3\\n- ALBERT\\n- BERT\\n- SQUAD\\n- GAUSS'}]]\n","0 0\n","0 1\n","1 1\n","2 1\n","0 5\n","0 12\n","1 12\n","2 12\n","3 12\n","4 12\n","5 12\n","6 12\n","7 12\n","8 12\n","9 12\n","0 14\n","1 14\n","2 14\n","3 14\n","0 20\n","1 20\n","2 20\n","3 20\n","4 20\n","5 20\n","6 20\n","7 20\n","8 20\n","9 20\n","0 23\n","1 23\n","2 23\n","3 23\n","4 23\n","5 23\n","0 25\n","1 25\n","0 30\n","1 30\n","2 30\n","3 30\n","0 35\n","1 35\n","2 35\n","3 35\n","4 35\n","5 35\n","6 35\n","7 35\n","8 35\n","9 35\n","0 38\n","0 47\n","0 49\n","1 49\n","2 49\n","3 49\n","4 49\n","5 49\n","6 49\n","7 49\n","8 49\n","9 49\n","36.36363636363637%  [[{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PubMed 20k RCT\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The following are metrics used to evaluate models on the Gibson PointGoal Navigation benchmark dataset:\\n\\n\\\\--------------------------------------\\n| Metric                  | Description\\n\\\\--------------------------------------\\n| Success rate            | Calculates the success rate of a person attempting to navigate to their nearest grocery store from their current location.\\n| Time to goal location   | Calculates the time it takes to get to the goal location. This metric is measured in hours.\\n| Successive failures    | Calculates the total number of hours spent in failure before reaching success.\\n| Distance to goal location    \\n| Time to fail location   | Calculates the time it takes to fail at the goal location. This metric is measured in hours.\\n| Distance to fail location    \\n| Distance to origin location    \\n| Duration of navigation    \\n| Walking speed            \\n| Endurance                \\n| Average steps per hour    \\n| Average height            \\n| Body mass index           \\n| Weight                   \\n| \\\\% Fat                   \\n\\\\--------------------------------------'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AAPD\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nThis gives me the following list of the selected results:\\n\\nP31: ARC-PDN\\nP31: AESLC\\nP31: AAPD'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"AxCell\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nNote: The \"WHERE\" clause of the SELECT statement must match the WHERE clause of the query you are submitting.'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n ?model a orkgc:Model; \\n  rdfs:label?model_lbl. \\n  FILTER (str(?model_lbl) = \"AWD-LSTM\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?cont orkgp:HAS_MODEL?model; \\n  orkgp:HAS_SOURCE_CODE?code. \\n} \\n\\nSELECT DISTINCT?code WHERE { \\n ?model a orkgc:Model; \\n  rdfs:label?model_lbl. \\n  FILTER (str(?model_lbl) = \"AWD-LSTM-DOC x5\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?cont orkgp:HAS_MODEL?model; \\n  orkgp:HAS_SOURCE_CODE?code. \\n}'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {\\n\\n ?dataset a orkgc:Dataset;\\n\\n  rdfs:label?dataset_lbl.\\n  FILTER (str(?dataset_lbl) = \"PubMedQA\")\\n\\n ?cont orkgp:HAS_BENCHMARK?benchmark.\\n\\n ?paper orkgp:P31?cont;\\n\\n  rdfs:label?paper_lbl.\\n\\n  }'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Image Classification\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \\n\\nThe most commonly used benchmark datasets for the Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification.  Datasets that have been used for benchmarking in the Image Classification research problem are:    \\n - ImageNet \\n- CIFAR-10\\n- CIFAR-100\\n- STL10\\n- STL10+ID3\\n- Cars\\n- Caltech-OTS\\n- NotMNIST\\n- Sports\\n- Caltech-UCSD\\n- ARM\\n- UT-ZUS\\n- UMNIST'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Gibson PointGoal Navigation\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cheetah, run (DMControl500k)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\\n ?benchmark a orkgp:HAS_BENCHMARK.\\n ?model rdfs:label \"Deep Q-Net\".\\n ?eval orkgp:HAS_EVALUATION.\\n ?eval orkgp:HAS_VALUE \"Deep Q-Net, MLOHE, RMSprop, rMSprop\".\\n ?value \"Deep Q-Net, MLOHE, RMSprop, rMSprop\".\\n ?benchmark orkgp:HAS_BENCHMARK.\\n ?model_lbl \"Deep Q-Net'}], [{'generated_text': 'The top benchmark score is 75.6 and its metric is IoU 0.74179431'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Senseval 2\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Supervised:\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Test perplexity\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WikiText-103\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The highest benchmark score on the WMT2016 Romanian-English dataset, including the metric and its value is 50.0.'}], [{'generated_text': 'Lead'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Enduro\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?code \\n    WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"C51\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n    ?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"C51\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. \\n    ?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. \\n    ?model a orkgc:Model; rdfs:label?model_'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"FABIR\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'EMERGENCY'}], [{'generated_text': 'The following metrics are used to evaluate models on the OpenBookQA benchmark dataset:\\n - Gold (ground-truth)\\n - Ranking (mean average precision)\\n - Detection (precision-recall)'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"OntoNotes\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n?dataset a orkgc:Dataset;\\nrdfs:label?dataset_lbl. \\nFILTER (str(?dataset_lbl) = \"Atari 2600 Defender\") \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark. \\n?paper orkgp:P31?cont; \\nrdfs:label?paper_lbl. \\n} \\n\\ninput (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bug \\t    Its You dataset?\\noutput (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE { \\n?dataset a orkgc:Dataset;\\nrdfs:label?dataset_lbl. \\nFILTER (str(?dataset_lbl) = \"Atari 2600 Bug\") \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark. \\n?paper or'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Pointer + Coverage + EntailmentGen + QuestionGen\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Switch Transformer\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\n# Collections provided by Gensim'}], [{'generated_text': 'models evaluated on CommonsenseQA:\\n\\n comboBox_1: Select one\\n relational: SQL (Relational)\\n pattern: Recursive (Recursive)\\n pattern: Viterbi (Viterbi)\\n pattern: Neural (Neural)\\n boolean: Logical (Logical)\\n fuzzy: Fuzzy (Fuzzy)\\n hybrid: Hybrid (Hybrid)'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon-2\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"A3C-CTS\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE {\\n\\n{\\n    { \\n        SELECT?metric?metric_lbl?value\\n        WHERE\\n\\n        { \\n           ?dataset a orkgc:Dataset;\\n            rdfs:label?dataset_lbl.\\n            FILTER (str(?dataset_lbl) = \"Hutter Prize dataset\")\\n        }\\n        OPTIONAL { \\n           ?eval orkgp:HAS_METRIC?metric.\\n           ?metric rdfs:label?metric_lbl.\\n        }\\n       ?cont orkgp:HAS_BENCHMARK?benchmark.\\n        OPTIONAL {\\n           ?cont orkgp:HAS_MODEL?model.\\n           ?model rdfs:label?model_lbl.\\n        }\\n    }\\n    ORDER BY DESC(?value)\\n}\\n}\\nGROUP BY?metric?metric_lbl\\n}'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL 2003 (English)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The name of the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset is : \\norkgp:OR-BELP-TRIAL-K-DIV2-QN-QNS-QNR-QNS-TNR'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Artificial Intelligence\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \\n\\nBenchmarked problems in the area of Artificial Intelligence:\\n- Milner & Lloyd (1983) - 1969\\n  - Conjugate gradients - 1966\\n  - Tabula Muris - 1967\\n  - SMO - 1977\\n- Hart, Baker & Berkowitz (1982) - 1976\\n  - Sequential Minimax - 1977\\n- Hull (1981) - 1974\\n  - N-GrAMS - 1981\\n- Kohonen (1981) - 1974\\n  - SOM - 1981\\n- Hochbaum (1982) - 1976\\n  - Iterative approximation - 1977\\n- Davies (1981) - 1974\\n  - SPOIN - 1981\\n- Koza (1981) - 1974'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiLSTM-Attention + ELMo\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STS Benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'This query return the following 2 papers which utilize the EfficientNet-L2-475 model:\\n\\nhttps://github.com/ovflow/OVFlow/blob/master/src/ovflow/src/ovflow/api/com/ovl/models/EfficientNetL2475.java\\nhttps://github.com/Bwass/NeuralNetworksFor recommender-system-development/blob/master/Code/src/main/java/com/bwass/neuralnetworksfor/NeuralNetworksFor.java'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Relation Extraction\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }\\n\\nWhat are the most commonly used benchmark datasets for the Named entity recognition research field?\\n\\nSELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Named entity recognition\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {\\n ?dataset a orkgc:Dataset;\\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"MultiNLI\")?benchmark orkgp:HAS_DATASET?dataset;\\n  orkgp:HAS_EVALUATION?eval.\\n  OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }\\n}\\n\\nThe MultiNLI dataset contains performance metrics for use with tuned baselines, and the ARC (Challenge) dataset contains performance metrics for use with tuned baselines.'}], [{'generated_text': 'SELECT DISTINCT?code \\n    WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"SRU++ Base\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nThis is the query that returned the required result set.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Asteroids\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n?dataset a orkgc:Dataset; \\nrdfs:label?dataset_lbl. \\nFILTER (str(?dataset_lbl) = \"WMT2014 German-English\") \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark. \\n?paper orkgp:P31?cont; \\nrdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'The models that have been benchmarked on the TriviaQA, SciFACT and SearchQA datasets are:\\n\\n- BIOIA - Bayesian Information Retrieval for Topic Models\\n- DeepQA - Neural DeepQA\\n- docQA - Neural Open Quad-GRAM\\n- gated neural network - GRU-RNN\\n- Multi-Relation QA - Multi-Relation QA\\n- Stack-Q - Neural Stack-Q\\n- Tack - Tack'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a onn:Creator; onn:noteID?noteID; onn:languageCode?languageCode; onn:numberOfParadigms?paradigmsCount; onn:numberOfResources?resourcesCount; onn:numberOfTheories?theoriesCount; onn:name?name; onn:numberOfRelatedTheories?relatedTheoriesCount; onn:relatedTheoryRef?theoryRef; onn:relatedRelatedTheoryRef?relatedRelatedTheoryRef; onn:topicAreaCode?topicAreaCode; onn:topic?topic; onn:theoreticalSubjectAreaCode?theoreticalSubjectAreaCode; onn:theoreticalSubject?theoreticalSubject; onn:brainAreaCode?brainAreaCode; onn:brain?brain; onn:memoryAreaCode?memoryAreaCode; onn:memory?memory; onn:dlBookAreaCode?dlBookAreaCode; onn:dlBook?dlBook; onn:dlBookOfBookAreaCode?dlBookOfBookAreaCode; onn:dlBookOfBook?dlBookOf'}], [{'generated_text': 'The metrics of evaluation over the Atari 2600 Space Invaders dataset are:\\n- Time: elapsed, measured in seconds\\n- Missiles Defeated: the number of missiles sent against the target\\n- Number of Missiles Sent: the number of missiles to be sent against the target\\n- Number of Bombs Dropped: the number of bombs to be dropped against the target'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'PWC Leaderboards (restricted)'}], [{'generated_text': 'The metrics used to evaluate models on the WikiText-2 benchmark dataset are:\\n\\n?metric\\trdfs:label\\nspaces\\tSpaces\\nchars\\tCharacters\\nchars2\\tCharacters\\nwords\\tWords\\nsentences\\tSentences\\nwords2\\tWords\\nwords3\\tWords\\nsentences2\\tSentences\\nsentences3\\tSentences\\nwords4\\tWords\\nsentences4\\tSentences\\nsentences5\\tSentences\\nsentences6\\tSentences\\nsentences7\\tSentences\\nsentences8\\tSentences\\nwords5\\tWords\\nsentences5\\tSentences\\nwords6\\tWords\\nsentences6\\tSentences\\nwords7\\tWords\\nsentences7\\tSentences\\nwords8\\tWords\\nsentences8\\tSentences\\nwords9\\tWords\\nsentences9\\tSentences\\nwords10\\tWords\\nsentences10\\tSentences\\nwords11\\tWords\\nsentences11\\tSentences\\nwords12\\tWords\\nsentences12\\tSentences\\nsentences13\\tSentences\\nsentences14\\tSentences\\nsentences15\\tSentences\\nwords15\\tWords\\nsentences15\\tSentences\\nwords16\\tWords\\nsentences16\\tSentences'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TempEval-3\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'The research field that is conducted in is:'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiLSTM-CRF+ELMo\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nAnd here are the result:\\n\\npaper: He et al., 2017 + ELMo, \\npaper: BiLSTM-CRF+ELMo, \\npaper: BCN+ELMo,'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Top-1 Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HMDB51\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}]]\n","0 1\n","0 9\n","0 12\n","1 12\n","2 12\n","0 13\n","1 13\n","2 13\n","3 13\n","4 13\n","5 13\n","6 13\n","7 13\n","8 13\n","9 13\n","0 17\n","0 18\n","1 18\n","2 18\n","3 18\n","4 18\n","5 18\n","6 18\n","7 18\n","8 18\n","9 18\n","0 23\n","0 28\n","0 34\n","1 34\n","0 40\n","0 42\n","1 42\n","2 42\n","0 44\n","0 45\n","1 45\n","2 45\n","3 45\n","4 45\n","5 45\n","6 45\n","7 45\n","8 45\n","9 45\n","0 47\n","45.45454545454546%  [[{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nSELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU\") {'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1 entity level\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-disease\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Skiing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Frostbite\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nThe benchmark over the Atari 2600 Frostbite dataset appears in the paper \"Semantic Web and Linked Data for Distributed Medical Record Sharing\" (Viamari and Sironi 2009). The paper is available here: https://www.sciencedirect.com/science/article/pii/S0307686008003175.'}], [{'generated_text': 'Here is a list of papers that have utilized the COMET - DynaGen model and include the links to their code?\\n* https://arxiv.org/abs/1907.05345\\n* https://dl.acm.org/doi/10.1145/3274384.3274584\\n* https://dl.acm.org/doi/10.1145/3278402.3278828\\n* https://dl.acm.org/doi/10.1145/3275499.3275256\\n* https://dl.acm.org/doi/10.1145/3278407.3278485\\n* https://dl.acm.org/doi/10.1145/3278402.3275198\\n* https://dl.acm.org/doi/10.1145/3278401.3275209\\n* https://dl.acm.org/doi/10.1145/3278400.3274997\\n* https://dl.acm.org/doi/10.1145/3279394.3279262\\n* https://dl.acm.org/doi/10.1145/3275387.3275490'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {\\n?dataset a orkgc:Dataset;\\nrdfs:label?dataset_lbl.\\nFILTER (str(?dataset_lbl) = \"BUCC Russian-to-English\")\\n?benchmark orkgp:HAS_DATASET?dataset;\\norkgp:HAS_EVALUATION?eval.\\n?paper orkgp:HAS_BENCHMARK?benchmark.\\nOPTIONAL {\\n?paper orkgp:HAS_MODEL?model.\\n?model rdfs:label?model_lbl.\\n}\\n}'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Boxing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PennTreebank\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IWSLT2015 English-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Frostbite\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a mlxnl:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MultiNLI\")?benchmark mlxnl:HAS_DATASET?dataset; mlxnl:HAS_EVALUATION?eval.?eval mlxnl:HAS_VALUE?value. OPTIONAL {?eval mlxnl:HAS_METRIC?metric.?metric mlxnl:LABEL?metric_lbl. }?cont mlxnl:HAS_BENCHMARK?benchmark. OPTIONAL {?cont mlxnl:HAS_MODEL?model.?model mlxnl:LABEL?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet ReaL\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'The MLP model achieved the highest Accuracy score on the Reuters En-De benchmark dataset.'}], [{'generated_text': 'The highest benchmark result includes the metric and score for BIOSSES dataset.'}], [{'generated_text': 'The metrics that are used to evaluate models on the Yelp-2 dataset are the accuracy, F1 score, precision, recall, and  AUC.'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n ?model a orkgc:Model; \\n  rdfs:label?model_lbl. \\n  FILTER (str(?model_lbl) = \"BART\")?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?cont orkgp:HAS_MODEL?model; \\n  orkgp:HAS_SOURCE_CODE?code. \\n} \\n\\nSELECT DISTINCT?code WHERE { \\n ?model a orkgc:Model; \\n  rdfs:label?model_lbl. \\n  FILTER (str(?model_lbl) = \"PBSMT\")?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?cont orkgp:HAS_MODEL?model; \\n  orkgp:HAS_SOURCE_CODE?code. \\n}'}], [{'generated_text': 'SELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a nqo:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NaturalQuestions (short)\" )?benchmark nqo:HAS_DATASET?dataset; nqo:HAS_EVALUATION?eval.?eval nqo:HAS_VALUE?value. OPTIONAL {?eval nqo:HAS_METRIC?metric.?metric nqo:LABEL?metric_lbl. }?cont nqo:HAS_BENCHMARK?benchmark. OPTIONAL {?cont nqo:HAS_MODEL?model.?model nqo:LABEL?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The best performing model benchmarking the CIFAR-10 dataset in terms of Parameters metric is the model that is the Sequential architecture. The parameters for the model are: 1) ResNet-18 2) 30000 3) 10 4) dropout ratio 5) num epochs 6) total number of images in the dataset'}], [{'generated_text': 'Top score: 115.94955232796875\\nMetric: 115.94955232796875'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE { \\n ?problem a orkgc:Problem; \\n  rdfs:label?problem_lbl. \\n  FILTER (str(?problem_lbl) = \"Citation Intent Classification\") \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem. \\n} \\nSELECT DISTINCT?dataset?dataset_lbl WHERE { \\n ?problem a orkgc:Problem; \\n  rdfs:label?problem_lbl. \\n  FILTER (str(?problem_lbl) = \"Text Classification\") \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark; \\n  orkgp:P32?problem. \\n}'}], [{'generated_text': 'The following links can be used to benchmark the H-NLI model:\\n* https://github.com/DeepMind/Common-Language-Processor/blob/master/doc/H-NLI.md\\n* https://github.com/Databricks/dnliml/blob/master/README.md'}], [{'generated_text': 'Here are the highest benchmark results, including the metric and score, for the PubMed 20k RCT dataset:\\n\\n- HAS_DATASET: 11\\n- HAS_EVALUATION: 2\\n- HAS_VALUE: 30\\n- HAS_METRIC: 4\\n- HAS_BENCHMARK: 30\\n- HAS_MODEL: 9\\n- HAS_CONS: 3'}], [{'generated_text': 'The top performing model was an ensemble of a transformer, vanilla RNN, and tree-based model.'}], [{'generated_text': 'The top performing model in terms of Overall score when benchmarked on the CoQA dataset is:\\nSELECT?model?model_lbl WHERE { \\n?dataset a orkgc:Dataset; \\nrdfs:label?dataset_lbl. \\nFILTER (str(?dataset_lbl) = \"Overall\") \\n{ \\nSELECT?model?model_lbl WHERE { \\n?dataset a orkgc:Dataset; \\nrdfs:label?dataset_lbl. \\nFILTER (str(?dataset_lbl) = \"CoQA\")?benchmark orkgp:HAS_DATASET?dataset;\\norkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value;\\norkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark;\\norkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. \\n} \\nORDER BY DESC(?value) \\nLIMIT 1 } \\n }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DCN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n<|endoftext|>BAGHDAD (Reuters) - Thousands of Shia militia members celebrated in the city of Hadda, just north of Baghdad, on Friday, to the rhythms of Iranian songs, while an ally of the U.S. government, Iran, praised the militia fighters for their contribution to combatting Islamic State.\\n\\nSlideshow ( 2 images )\\n\\nState TV reported that hundreds of supporters of the militias’ Mujahedin Shia Front took part in celebrations in the town of Baaj, a city known for its colorful bazaars and souk.\\n\\nThose supporting the militias are part of a government in exile and are supported by the United States and other Western powers in their fight against the Islamic State group. The milit'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Top-1 Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"iNaturalist 2018\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { \\n  { SELECT?metric?metric_lbl?value WHERE { \\n   ?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Ice Hockey\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } \\n  GROUP BY?metric?metric_lbl\\n}\\n\\nThe best scoring model is the one benchmarking the Atari 2600 Ice Hockey dataset. Its metric is Score and its label is Score.'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Scientific Claim Verification\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Walker, walk (DMControl100k)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value)'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE { \\n    ?problem a orkgc:Problem; \\n     rdfs:label?problem_lbl. \\n     FILTER (str(?problem_lbl) = \"Semantic Role Labeling\") \\n    ?dataset a orkgc:Dataset; \\n     rdfs:label?dataset_lbl. \\n    ?benchmark orkgp:HAS_DATASET?dataset. \\n    ?cont orkgp:HAS_BENCHMARK?benchmark; \\n     orkgp:P32?problem. \\n}\\n\\ninput (English text): Provide a list of benchmarked datasets related to the Semantic Role Labeling research area?\\noutput (Sparql query):'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {\\n   ?dataset a orkgc:Dataset;\\n    rdfs:label?dataset_lbl.\\n    FILTER (str(?dataset_lbl) = \"WikiText-2\")\\n   ?benchmark orkgp:HAS_DATASET?dataset.\\n   ?cont orkgp:HAS_BENCHMARK?benchmark.\\n   ?paper orkgp:P31?cont;\\n    rdfs:label?paper_lbl.\\n}'}], [{'generated_text': 'Micro Precision'}], [{'generated_text': 'SELECT DISTINCT?code \\nWHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Prior+Duel noop\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nSELECT DISTINCT?code \\nWHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"DQN noop\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The most commonly used benchmark datasets for Fine-Grained Image Classification research problem are:\\n- CIFAR-10 Image Classification\\n- ImageNet Large Scale Visual Recognition Challenge (ILSVRC)\\n- Caltech-256\\n- Oxford-102'}], [{'generated_text': 'Common evaluation metrics include, but are not limited to, Precision, Recall, F1 Score, and Accuracy.'}], [{'generated_text': 'The metrics used to evaluate models on the WOS-5736 benchmark dataset are?\\n\\n?benchmark has_dataset\\n?benchmark has_evaluation\\n?eval.HAS_METRIC\\n?eval.HAS_DATASET\\n?eval.HAS_EVALUATION\\n?eval.DATASET_LABEL\\n?eval.Benchmark_label'}], [{'generated_text': 'SELECT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Unpermuted Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential CIFAR-10\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"STL-10, 1000 Labels\")?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n }'}], [{'generated_text': 'The models that have been benchmarked on the SciCite dataset are:\\n- SMT\\n- Reptile\\n- Snorquese\\n- XGboost\\n- gbm\\n- XGBoostLR\\n- GiniBC\\n- GradientBoosting\\n- MultinomialNB'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl \\nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Skiing\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'The benchmarks used in this challenge are listed in this spreadsheet. All metrics in this spreadsheet are defined in the queries used by MAZEA.'}], [{'generated_text': 'SELECT?model?model_lbl WHERE {?metric a cartp:Score; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a cartp:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CartPole\")?benchmark cartp:HAS_DATASET?dataset; cartp:HAS_EVALUATION?eval.?eval cartp:HAS_VALUE?value; cartp:HAS_METRIC?metric.?cont cartp:HAS_BENCHMARK?benchmark; cartp:HAS_MODEL?model.?model cartp:Score?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?code\\nWHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Rfa-Gate-arccos\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ShARe/CLEF eHealth\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nA:\\n\\nFor this specific benchmark dataset (ADE Corpus), the best F1 score was 0.833. The best Macro F1 score on this dataset is 0.786. I\\'ll suggest to use SPARQL'}], [{'generated_text': 'The following are the metrics of evaluation over the SciFACT dataset:\\n\\n- McF1 score\\n- R2 metric'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Centipede\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Natural Language Inference\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CIFAR-10\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'The following code references are present in papers that have used the DeiT-B model for benchmarking purposes:\\n\\nhttps://deitb.org/documentation/1/BENCHMARKING/\\nhttps://github.com/opensourcetei/ti-benchmarks'}]]\n","0 5\n","1 5\n","2 5\n","0 13\n","0 14\n","1 14\n","2 14\n","3 14\n","4 14\n","5 14\n","6 14\n","7 14\n","8 14\n","0 15\n","0 18\n","1 18\n","2 18\n","0 19\n","1 19\n","0 21\n","0 22\n","1 22\n","0 23\n","1 23\n","2 23\n","0 32\n","1 32\n","2 32\n","3 32\n","4 32\n","5 32\n","6 32\n","7 32\n","8 32\n","9 32\n","0 34\n","0 35\n","1 35\n","0 36\n","1 36\n","2 36\n","0 39\n","0 41\n","1 41\n","0 45\n","1 45\n","2 45\n","3 45\n","4 45\n","0 49\n","54.545454545454554%  [[{'generated_text': 'Poland'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ShARe/CLEF eHealth corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HMDB51\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Venture\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE { \\n\\t?dataset a orkgc:Dataset; \\n\\trdfs:label?dataset_lbl. \\n\\tFILTER (str(?dataset_lbl) = \"Rotowire (Content Selection)\")?benchmark orkgp:HAS_DATASET?dataset; \\n\\torkgp:HAS_EVALUATION?eval. \\n\\tOPTIONAL { \\n\\t?eval orkgp:HAS_METRIC?metric. \\n\\t?metric rdfs:label?metric_lbl. \\n\\t} \\n}'}], [{'generated_text': 'Here is a list of papers that have utilized the Weighted Tsetlin Machine model and include the links to their code?'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Breakout\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset;?metric?metric_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Bowling\"). }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"FQF\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\\ninput (English text): List the code links in papers that use the GNN model in any benchmark?\\noutput (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"GNN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\\ninput (English text): List the code links in papers that use the MLOOM model in any benchmark?\\noutput'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"% Test Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The top benchmark score and its metric on the ARC (Easy) dataset are:\\n100\\nand the top benchmark result (metric and value) over the dataset ARC-PDN is:\\n50'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {\\n ?dataset a orkgc:Dataset;\\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"SciREX\")?benchmark orkgp:HAS_DATASET?dataset.\\n ?cont orkgp:HAS_BENCHMARK?benchmark.\\n ?paper orkgp:P31?cont;\\n  rdfs:label?paper_lbl.\\n}'}], [{'generated_text': 'The metrics used to evaluate models on the TSE-NER dataset are n-gram distribution, precision@N, precision@10, recall@10, accuracy, F1 score, and free energy.'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Concept Mention Extraction\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"OTF spelling (single)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BERT-joint\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"OTF spelling+lemma ('}], [{'generated_text': 'The following papers have utilized the 3-layer AWD-LSTM model:\\n- Zhang, Li, Yang, et al., \"RNNLM: A Regression-Based Framework for Architectural Search in deep learning,\" arXiv preprint arXiv:1901.08781 (2019).\\n- Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Polop, et al., \"Neural Information Processing System (NIPS 2015): A Progress Report,\" arXiv preprint arXiv:1511.05644 (2015).\\n- Pritzel, Frank, \"Neural Network Construction by Tree Transduction,\" in Proceedings of the 23rd International Conference on Computational Linguistics: Sentence Processing (Post-Committee Track), Joint International Workshop on Neural Datasets and Learning to Categorize, Association for Computational Linguistics, 2016, pp. 143-156.\\n- Yi, Si, Deng, Xiangyu, Yu, Xiaojun, \"Memory-augmented Multi-task Learning for Sequence Tagging,\" arXiv preprint arXiv:1807.09588 (2018).\\n- Vaswani, Ashish, Shazeer'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Percentage error\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SVHN\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'Link to code used in paper that benchmark the AWD-LSTM (3 layers) model:\\nhttps://github.com/facebook/AWDLSTM'}], [{'generated_text': 'The datasets benchmarked under the relation extraction research problem, the SPARQL query optimization research problem and the relation extraction benchmarking problem are the following :'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Error\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon-2\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }\\nThe highest error score achieved on the Amazon-2 benchmark dataset is 10'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n ?model a orkgc:Model; \\n  rdfs:label?model_lbl. \\n  FILTER (str(?model_lbl) = \"Temporal Convolutional Network\")?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?cont orkgp:HAS_MODEL?model; \\n  orkgp:HAS_SOURCE_CODE?code. \\n}'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiT-S (ResNet-152x4)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nQuery 1\\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiT-S (ResNet-152x4)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nQuery 2\\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WNLI\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a aapd:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AAPD\")?benchmark aapd:HAS_DATASET?dataset; aapd:HAS_EVALUATION?eval.?eval aapd:HAS_VALUE?value. OPTIONAL {?eval aapd:HAS_METRIC?metric.?metric aapd:LABEL?metric_lbl. }?cont aapd:HAS_BENCHMARK?benchmark. OPTIONAL {?cont aapd:HAS_MODEL?model.?model aapd:LABEL?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': '- Total entities\\n- Average entity depth\\n- Average triplet distance\\n- Entity similarity\\n- Common entity relationship classification\\n- Dimension reduction\\n- Embedding on words\\n- POS tag dimension\\n- POS tag triage\\n- Part of speech disambiguation\\n- Semantic role labelling\\n- Synonym sets\\n- Word embedding\\n- Word2vec'}], [{'generated_text': 'The best performing model benchmarking the CIFAR-10 dataset in terms of Parameters metric is an OR-dLboost model. The parameters of this model can be found here: https://s3.amazonaws.com/dataprotect/benchmark/dlboost.zip. This model achieves a score of 0.2943.'}], [{'generated_text': 'The highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value is:'}], [{'generated_text': \"Some common metrics used to benchmark models on the SciCite dataset include:\\n\\n - F1 score\\n - AUC\\n - Mcnemar's test\"}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"POP3D\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\\n\\nNote: These queries are using the information found in:\\nhttps://datasettes.github.io/datasette-dataset-tools/explorer/gpt-3.html\\nhttps://datasettes.github.io/datasette-dataset-tools/explorer/pop3d.html'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Tennis\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UCF101\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n?dataset a orkgc:Dataset; \\nrdfs:label?dataset_lbl. \\nFILTER (str(?dataset_lbl) = \"MPQA\") \\n?benchmark orkgp:HAS_DATASET ; \\norkgp:HAS_EVALUATION?eval. \\n?paper orkgp:HAS_BENCHMARK \\n?benchmark. \\nOPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n} \\n\\nProvide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset?\\n\\nSELECT DISTINCT?model?model_lbl WHERE { \\n?dataset a orkgc:Dataset; \\nrdfs:label?dataset_lbl. \\nFILTER (str(?dataset_lbl) = \"MPQA\") \\n?benchmark orkgp:HAS_DATASET ; \\norkgp:HAS_EVALUATION?eval. \\n?paper orkgp:'}], [{'generated_text': 'BC5CDR chemical dataset: The lowest free energy of a single water molecule in the gas phase has been estimated to be 88.4 kJ/mol by experimental free energy calculations. The best theoretical result was estimated by Rothlisberger & Knowles to be 94.8 kJ/mol in 2014.\\n\\nThe lowest free energy of a single water molecule in the gas phase has been estimated to be 88.4 kJ/mol by experimental free energy calculations. The best theoretical result was estimated by Rothlisberger & Knowles to be 94.8 kJ/mol in 2014.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE { \\n ?metric a orkgc:Metric; \\n  rdfs:label?metric_lbl. \\n  FILTER (str(?metric_lbl) = \"SUCCESS\") \\n  { \\n    SELECT?model?model_lbl WHERE \\n    { \\n     ?dataset a orkgc:Dataset; \\n      rdfs:label?dataset_lbl. \\n      FILTER (str(?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") \\n     ?benchmark orkgp:HAS_DATASET?dataset; \\n      orkgp:HAS_EVALUATION?eval. \\n     ?eval orkgp:HAS_VALUE?value; \\n      orkgp:HAS_METRIC?metric. \\n     ?cont orkgp:HAS_BENCHMARK?benchmark; \\n      orkgp:HAS_MODEL?model. \\n      rdfs:label?model_lbl. \\n    } \\n    ORDER BY DESC(?value) \\n    LIMIT 1 \\n  } \\n  }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a paperfield:Field; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Paper Field\")?benchmark paperfield:HAS_DATASET?dataset; paperfield:HAS_EVALUATION?eval.?eval paperfield:HAS_VALUE?value. OPTIONAL {?eval paperfield:HAS_METRIC?metric.?metric paperfield:label?metric_lbl. }?cont paperfield:HAS_BENCHMARK?benchmark. OPTIONAL {?cont paperfield:HAS_MODEL?model.?model paperfield:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Double Dunk\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n?model a orkgc:Model;\\nrdfs:label?model_lbl. \\nFILTER (str(?model_lbl) = \"BiDAF + Self Attention + ELMo (ensemble)\")\\n?benchmark orkgp:HAS_DATASET?dataset.\\n?cont orkgp:HAS_BENCHMARK?benchmark.\\n?cont orkgp:HAS_MODEL?model;\\norkgp:HAS_SOURCE_CODE?code. \\n}'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cheetah, run (DMControl500k)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STS Benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Alien\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a birdsnap:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Birdsnap\")?benchmark birdsnap:HAS_DATASET?dataset; birdsnap:HAS_EVALUATION?eval.?eval birdsnap:HAS_VALUE?value. OPTIONAL {?eval birdsnap:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont birdsnap:HAS_BENCHMARK?benchmark. OPTIONAL {?cont birdsnap:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Here is a list of the Atari 2600 metrics that are used to evaluate models on the Atari 2600 Star Gunner dataset:\\n\\nHAS_DATASET - whether the model was trained on an Atari 2600 dataset\\nHAS_EVALUATION - whether the model was trained for evaluation purposes\\nHAS_BENCHMARK - whether the model was benchmarked on the Atari 2600 Benchmark dataset\\nHAS_MODEL - whether the model has a model file\\nHAS_METRIC - whether the model file contains a metric\\nHAS_Paper - whether the model was paper published\\nMODEL_TYPE - whether the model is a neural network model\\nOPTIONAL - add more Atari 2600 metrics here'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Stanford Cars\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric;?metric_lbl rdf:label. FILTER (str(?metric_lbl) = \"Precision\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset;?dataset_lbl rdf:label. FILTER (str(?dataset_lbl) = \"RotoWire (Relation Generation)\")?benchmark orkgp:HAS_DATASET?dataset;?eval orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value.?value orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark;?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The metrics used to evaluate models on the SQuAD1.1 dev benchmark dataset are:\\n-  log_loss\\n-  num_examples\\n-  recall\\n-  f1-score\\n-  top-5 validation loss\\n-  top-5 test loss'}], [{'generated_text': '* Unregularised mLSTM\\n* Longformer Large\\n* Large mLSTM'}], [{'generated_text': 'The following models are being evaluated on the ACL-ARC dataset:\\n- SQuAD 2.0\\n- Tendies\\n- ACEMS\\n- LASER\\n- ATA\\n- BERT'}], [{'generated_text': 'The model that achieved the highest Score on the Atari 2600 Yars Revenge benchmark dataset is the \"Shadow\" model.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-disease\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nresults:\\nmodel     model_lbl\\npca      Partial Correlation Analysis\\nPCA      Partial Linear Correlation\\nPLS-DA   Partial Least Squares - Discriminant Analysis\\nLDA      Linear Discriminant Analysis\\nLDA-K    LDA with Kernelising\\nSVC      Support Vector Classification\\nNeural Network\\nSMO      Sequential Minimal Optimization\\nMLP      Multilayer Perceptron\\nLOGIT    Logistic Regression\\nQuadratic Regression\\nNaive Bayes\\nDecision Trees\\nDecision Stump\\nGenetic Programming'}]]\n","0 0\n","1 0\n","2 0\n","0 5\n","1 5\n","0 10\n","0 12\n","0 15\n","1 15\n","2 15\n","0 17\n","1 17\n","2 17\n","3 17\n","0 18\n","1 18\n","2 18\n","3 18\n","4 18\n","5 18\n","6 18\n","7 18\n","8 18\n","9 18\n","0 24\n","0 25\n","0 26\n","0 27\n","0 32\n","0 42\n","0 45\n","1 45\n","2 45\n","3 45\n","4 45\n","5 45\n","6 45\n","7 45\n","0 46\n","1 46\n","2 46\n","3 46\n","0 47\n","1 47\n","2 47\n","3 47\n","0 48\n","63.63636363636365%  [[{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { \\n    { SELECT?metric?metric_lbl?value WHERE \\n            {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \\n                FILTER (str(?dataset_lbl) = \"Softcite\")?benchmark orkgp:HAS_DATASET?dataset; \\n                orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. \\n                OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \\n               ?cont orkgp:HAS_BENCHMARK?benchmark. \\n                OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n            } \\n            ORDER BY DESC(?value) } \\n    } \\n    GROUP BY?metric?metric_lbl\\n} \\n\\nRegarding the Freebase dataset, I didn\\'t find a benchmark for \"Freebase\" (the dataset is marked'}], [{'generated_text': 'The models being evaluated on the Fashion-MNIST dataset are hand-written digit recognition models. The metrics of evaluation are the classification error rate and the number ofhw classes correctly recognized, denoted by  and, respectively. The values of the metrics over the dataset are denoted by.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"ROUGE-2\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"X-Sum\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark. ORKGDP:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nNote that the following data can be found in the X-Sum dataset:\\n+-----------------------+-----------------------------------------------+\\n| Benchmark             | ROUGE-2                                      |\\n+-----------------------+-----------------------------------------------+\\n| Ranking, random 10%'}], [{'generated_text': 'As of now, there is only 1 paper that includes a benchmark on the RotoWire dataset.\\nThe paper - P31 in the dataset collection P31 - P Databases Relational.\\nThe IDs of the papers that include a benchmark on the RotoWire dataset are:\\nP31 - Databases Relational\\nP32 - A Survey of General Database Architectures'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n ?model a orkgc:Model; \\n  rdfs:label?model_lbl. \\n  FILTER (str(?model_lbl) = \"Fine-Grained Gating\")\\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?cont orkgp:HAS_MODEL?model; \\n  orkgp:HAS_SOURCE_CODE?code. \\n}'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Average Return (NoOp)\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Montezumas\\'s Revenge\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'Following is a list of papers that have utilized the Prior noop model for benchmarking purposes: \\n- Bertrand Bansal, Marcus Hutter, Mika Rauniski, Daniel Weinberger, Bart Van Merriikhe, and Yoshua Bengio. \"Prior-Dueling Networks for Episodic Bayesian Games.\" Advances in Neural Information Processing Systems 30 (NIPS 2014).\\n- Alireza Ghavamzadeh, Bhaven Jackson, Henry Sirl, and Alex Graves. \"Deep Reinforcement Learning via Variant of Bayes Optimization.\" Advances in Neural Information Processing Systems 29 (NIPS 2012).\\n- Hector San Miguel, Joaquin Ortega, and Sergio Ortega. \"MCTS with Surge and Dueling Networks.\" International Conference on Machine Learning.\\n- Dimitri Vanhoucke, Cosmin Lesne, Renaud Laça, Théophaneretta Venter, and Sebastien Thivierge. \"Prior Learning Dynamics for Model-Agnostic Objectives.\" European Conference on Machine Learning.\\n- Jonathan Feigenbaum, Emilio Grandolfi, Rossima Batista, and Daniel Weinberger. \"Prior+'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-Romanian\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Top-1 Error Rate\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Oxford 102 Flowers\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'select distinct?paper?paper_lbl where { \\n?dataset a orkgc:Dataset;\\nrdfs:label?dataset_lbl.\\nFILTER (str(?dataset_lbl) = \"Reuters En-De\")\\n?benchmark orkgp:HAS_DATASET?dataset.\\n?cont orkgp:HAS_BENCHMARK?benchmark.\\n?paper orkgp:P31?cont;\\nrdfs:label?paper_lbl.\\n}'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Word Level\") OR?dataset_lbl = \"Penn Treebank (Character Level\") FILTER (str(?dataset_lbl) = \"Penn Treebank (Word Level\") ||?dataset_lbl = \"Penn Treebank (Character Level\") FILTER (str(?dataset_lbl) = \"Penn Treebank (Word Level\") ||?dataset_lbl = \"Penn Treebank (Character Level\").'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n?model a orkgc:Model; \\nrdfs:label?model_lbl. \\nFILTER (str(?model_lbl) = \"ANODE\") \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark. \\n?cont orkgp:HAS_MODEL?model; \\norkgp:HAS_SOURCE_CODE?code. \\n} \\n\\ninput (English text): Provide a list of papers that have utilized the ANODE model and include the links to their code?\\n output (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; \\nrdfs:label?model_lbl. \\nFILTER (str(?model_lbl) = \"ANODE\") \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark. \\n?cont orkgp:HAS_MODEL?model; \\norkgp:'}], [{'generated_text': 'Here is a list of papers that have utilized the DY-MobileNetV2 ×1.0 model and include the links to their code:\\n\\n- https://github.com/YuGuoShu/Dynamically-Adjusted-MobileNet\\n- https://github.com/CIS-BPO/DGMN-SPnet'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"MPAD-path\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The model that has achieved the best performance when benchmarked on the BoolQ dataset is an orkgp:HasModel sub-expression, called \"Fast Majority Classifier\". The string \"Fast Majority Classifier\" is the name of this model and the corresponding label is \"model_lbl\".'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl \\n  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC German-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?benchmark a orkgc:Benchmark; orkgp:HAS_EVALUATION?eval; orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric;?cont a orkgp:Cont;?cont orkgp:HAS_BENCHMARK?benchmark;?cont orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"MEMEN (ensemble)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"ZFNet (ensemble, 6 convnets)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The best performing model is ADADELTA with 51.1 Bits per dim.'}], [{'generated_text': 'The top benchmark result (metric and value) for the CommonsenseQA dataset is as follows:\\n\\n- VALUE: 1415.0\\n- METRIC: QA\\n- SCORE: 1415.0'}], [{'generated_text': 'no'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reacher, easy (DMControl500k)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\\n    ?model rdfs:label \"PNAS paper 2019\"@en\\n    ?model_lbl \"PNAS paper 2019\"@en\\n    ?benchmark \"Reacher, easy (DMControl500k)\"@en\\n    ?score 5.0'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Quasart-T\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': 'MULTIMODAL_PISA_BASELINE_MODEL_ACCURACY_COUPLED_1_BASE_LINEAR_LSTM_LORASSL_KAGAMA_2019_10_31\\n\\n Gaze,LSTM with Attention\\nGaze,LSTM with Attention\\nGaze,LSTM with Attention\\nGaze,LSTM with Attention\\nGaze,LSTM with Attention\\nGaze,LSTM with Attention\\nGaze,LSTM with Attention\\nGaze,LSTM with Attention\\nGaze,LSTM with Attention\\nGaze,LSTM with Attention\\nGaze,LSTM with Attention\\nGaze,LSTM with Attention'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE { \\n   ?dataset a orkgc:Dataset; \\n    rdfs:label?dataset_lbl. \\n    FILTER (str(?dataset_lbl) = \"Barabasi-Albert\") \\n   ?benchmark orkgp:HAS_DATASET?dataset; \\n    orkgp:HAS_EVALUATION?eval. \\n    OPTIONAL { \\n       ?eval orkgp:HAS_METRIC?metric. \\n       ?metric rdfs:label?metric_lbl. \\n    } \\n}'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Zaxxon\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Spanish\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'WMT2016 English-German'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Sentence Classification\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'The metrics that are used to evaluate models on the CUB-200-2011 benchmark dataset are:\\n- AverageOperationTime\\n- AverageTotalProblemSolvingTime\\n- TotalProblemSolvingTime\\n- PercentageOfBaselineTime\\n- PercentageOfTotalTime\\n- PercentageOfSuccessfulTime'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Tokenlearner\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MedSTS\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nThe following metrics are commonly used when benchmarking models on the MedSTS dataset:\\n\\n* precision\\n* recall\\n* F1 score\\n* area under the curve (AUC)\\n* balanced accuracy (BACC)'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL++\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'Tsuruoka et al. \"UCF101: A Large and Accurate Corpus for Fine-Tuning of Deep Neural Network for Sentence Recognition\" \\nNMN17-1424\\nWS-CR:2015'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n   ?dataset a orkgc:Dataset; \\n    rdfs:label?dataset_lbl. \\n    FILTER (str(?dataset_lbl) = \"SciCite\") \\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark. \\n   ?paper orkgp:P31?cont; \\n    rdfs:label?paper_lbl. \\n} \\n\\nSELECT DISTINCT?paper?paper_lbl WHERE { \\n   ?dataset a orkgc:Dataset; \\n    rdfs:label?dataset_lbl. \\n    FILTER (str(?dataset_lbl) = \"Softcite\") \\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark. \\n   ?paper orkgp:P31?cont; \\n    rdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"ImageNet + iNat on WS-DAN model\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\\n\\nhttps://github.com/ORGANISM-Project/DIA_SLDB.git'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC Russian-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"ROUGE-1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AESLC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ModelNet40\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The metrics used to evaluate models on the Penn Treebank (Word Level) benchmark dataset are:\\n\\n-\\taccuracy\\n-\\tbleu\\n-\\teeg\\n-\\tf1 score\\n-\\tpanwen\\n-\\trouge1\\n-\\trouge2\\n-\\trougecom\\n-\\tsentiment\\n-\\ttokencount'}], [{'generated_text': 'Dendrotoxisc \\nNeural Network \\nK-Means \\nSupport Vector Machine \\nNaive Bayes \\nDecision Trees'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Seaquest\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The metrics of evaluation are SUM_OVER_QUADRATIC_DISTANCE, NUM_HITS, F1, R2, R2squared, MAXIMUM_ACCURACY, Mean_absolute_error, Root_mean_squared_error, Mean_squared_error, QUADRATIC_RMS_ERROR, and SIMPLE_ITERS'}], [{'generated_text': 'results: \\n ({ \"paper\" : \"P31\", \"paper_lbl\" : \"ImageNet\" }, { \"paper\" : \"P31\", \"paper_lbl\" : \"ImageNet V2\" })'}], [{'generated_text': 'The metrics used to evaluate models on the Atari 2600 Seaquest benchmark dataset are the following:\\n1. MeanAverageOfMovesPerSecond\\n2. MeanAverageOfTimesPerSecond\\n3. Entropy\\n4. VariationalELBO\\n5. BestScore\\n6. BestTime\\n7. WbestScore\\n8. WbestTime\\n9. ActionAllowed\\n10. AverageActionLength\\n11. MaxActionLength\\n12. ScoreAllocation'}], [{'generated_text': 'SELECT?paper?paper_lbl \\nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon-2\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}]]\n","0 1\n","1 1\n","2 1\n","0 3\n","1 3\n","2 3\n","0 6\n","1 6\n","2 6\n","0 9\n","1 9\n","2 9\n","0 12\n","0 15\n","1 15\n","0 19\n","1 19\n","2 19\n","3 19\n","0 20\n","1 20\n","2 20\n","0 21\n","1 21\n","2 21\n","3 21\n","4 21\n","0 26\n","0 30\n","0 32\n","0 37\n","0 43\n","1 43\n","2 43\n","3 43\n","4 43\n","5 43\n","6 43\n","7 43\n","8 43\n","0 44\n","1 44\n","0 46\n","0 47\n","0 48\n","72.72727272727273%  [[{'generated_text': 'The code links in papers that use the Qbert Rainbow+SEER model in any benchmark?\\nThe code links in papers that have utilized the Rainbow model and include the links to their code?'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy (High)\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RACE\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nThe top performing model for RACE dataset in terms of Accuracy (Middle) score is: \\nSELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_l'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-Romanian\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nThis query returns the following results:\\n# research paper titles\\n% 2 SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-Romanian\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'The metrics used to evaluate models on the TriviaQA dataset are the following:\\n- Binarized Words in Context (BWC)\\n- Binarized Words Outside of Context (BWO)\\n- Maximum Dice Coefficient\\n- Mean Averageprecision\\n- Mean Reciprocal Rank\\n- Mean Averageprecisionlog\\n- Log loss\\n- Logarithmic Loss\\n- Maximum Mutual Information\\n- Maximum Product of Confusion Matrices'}], [{'generated_text': 'Commonly used evaluation metrics when benchmarking models on the ESC-50 dataset are: \\n-Mean Average Precision (mAP)\\n-Human Verification Performance (HVP)'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD1.1 dev\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nThe SQuAD2.0 model, named decoder, achieved the highest F1 score on the benchmark dataset.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset;?dataset_lbl a owl:NamedIndividual; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TempEval-3\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RotoWire\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': \"The top benchmark score and its metric on the Atari 2600 Seaquest dataset are 48.1182 and 'frames per second'\"}], [{'generated_text': 'The following are the models that have been evaluated on the GENIA - UAS dataset:\\n- KATZ\\n- MINT\\n- FastText\\n- BioPhrase\\n- BERT\\n- DistilBERT\\n- TfidfVectorizer\\n- SGDClassifier'}], [{'generated_text': 'select distinct paper as paper_lbl\\nfrom\\n  dataset a\\n ,rdfs:label dataset_lbl\\n ,benchmark\\n ,cont\\nwhere \\n  str(dataset_lbl) = \"Yelp-2\"\\n  and benchmark has dataset\\n  and cont has benchmark\\n  and paper has benchmark\\n  and paper_lbl = \"Yelp Fine-grained classification\"'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Bank Heist\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\ninput (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Apollo dataset?\\noutput (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Apollo\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\ninput (English text):'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"A2\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ANLI test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"QNLI\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n?dataset a orkgc:Dataset; \\nrdfs:label?dataset_lbl. \\nFILTER (str(?dataset_lbl) = \"Atari 2600 Asterix\") \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark. \\n?paper orkgp:P31?cont; \\nrdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'The code used in the papers that benchmark the MPCM model is the following:\\n\\nhttps://knapsack.ccode.gov.br/software/knapsack.html\\nhttps://github.com/andersonkivi/MPC.Porter\\n\\nThe code used in the papers that benchmark the Ning et al. model is:\\n\\nhttps://github.com/carlosem/knn\\nhttps://github.com/caesarani/over_laps_propagation'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"CAIT-XS-36\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nThe CaiT-M-48-448 model is used in the paper \"Neural Network Based Super Resolution on Monopole Convolutional Neural Network\" published in CVPR2020.\\nThe CaiT-S-48 model is used in the paper \"Deep Transfer Learning for Fast Image Captioning\" published in IJCV 2021.\\nThe CAIT-XS-36 model is used in the paper \"CTRW Based Spectral Representation for Internet of Things Visualization\" published in ICCV 2021.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a seel.cse.lsu.edu:Dataset; rdfs:label \"re17.zip\" FILTER (str( seel:DatasetLabel ) = \"re17\")?benchmark seel.cse.lsu.edu:HAS_DATASET?dataset; seel.cse.lsu.edu:HAS_EVALUATION?eval.?paper seel.cse.lsu.edu:HAS_BENCHMARK?benchmark. OPTIONAL {?paper seel.cse.lsu.edu:HAS_MODEL?model.?model rdfs:label \"re17.zip\". } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Montezuma\\'s Revenge\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet ReaL\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"ResNet-152\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\ninput (English text): Can you provide links to code used in papers that benchmark the BiLSTM-TDN(ResNet-101) model?\\noutput (Sparql query): SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiLSTM-TDN(ResNet-101)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\ninput'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PubMed\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"Walker, walk (DMControl100k)\")?benchmark orkgp:HAS_DATASET?dataset; \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?paper orkgp:P31?cont; \\n  rdfs:label?paper_lbl. \\n}'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"REDDIT-B\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {\\n ?dataset a orkgc:Dataset;\\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"SciCite\")\\n ?benchmark orkgp:HAS_DATASET?dataset;\\n  orkgp:HAS_EVALUATION?eval. \\n ?paper orkgp:HAS_BENCHMARK?benchmark. \\n  OPTIONAL {?paper orkgp:HAS_MODEL?model. \\n   ?model rdfs:label?model_lbl. }\\n}'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SST-2 Binary classification\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'The Hendrycks Test benchmark score with its metric \"Accuracy (%)\" achieved the highest score of 0.968 and is used as the answer for this question.'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"KD-LSTMreg\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\ninput (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model?\\n\\noutput (Sparql query): \\n\\n?code'}], [{'generated_text': 'TDM Tagged Corpus\\nNLP-TDMS (Exp, arXiv only)'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ClueWeb09-B\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IMDb-B\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'The following metrics are used to evaluate models on the IMDb-B dataset:\\n- BPC (Bleu Per Cli): A metric used to evaluate the language model on its ability to capture the correct aspect of the input text.\\n- ROUGE (Ratio of subject word edges over total word edges in reference text): A metric used to evaluate the language model on its ability to extract relevant context from the input.\\n- TER (Term Frequencies): A metric used to evaluate the language model on the quality of its produced parses.\\n- CONLL (Conditional Next Words): A metric used to evaluate the language model on the quality of its production of grammatical and syntactical Next Word expressions.\\n- DOU (Downwardtrainability): A metric used to evaluate the model’s ability to generate data for the model by asking it to generate simple sentences in the target language.'}], [{'generated_text': 'SPARQL query:'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n?model a orkgc:Model; \\nrdfs:label?model_lbl. \\nFILTER (str(?model_lbl) = \"FG\") \\n?benchmark orkgp:HAS_DATASET?dataset. \\n?cont orkgp:HAS_BENCHMARK?benchmark. \\n?cont orkgp:HAS_MODEL?model; \\norkgp:HAS_SOURCE_CODE?code. \\n} \\n\\nThese are links to code used in papers that benchmark the SciKG model:\\nhttps://www.semanticweb.org/scikit-learn/references/0/index.html\\nhttps://www.semanticweb.org/cudatree/references/0/index.html\\nhttps://www.semanticweb.org/fg-fine-grained-gate-model/references/0/index.html'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT29\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The metrics that are used to evaluate models on QNLI are:\\n- BLEU\\n- ROUGE\\n- METEOR'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Relation F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SST-5 Fine-grained classification\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NaturalQuestions (long)\"){ {?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Czech-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'There are six energy sources considered in this analysis: nuclear energy, solar energy, hydroelectric power, wind power, geothermal power, and other sources.\\n\\nThe nuclear energy, solar energy, and wind power are modeled as energy sectors 23 times, 19 times, and 19 times, respectively. \\n\\nThe hydroelectric power is modeled as an energy sector 14 times, the geothermal power is modeled as an energy sector 6 times, and the other sources are modeled as energy sources in the general category of other sources 50 times.'}], [{'generated_text': 'Please find the following paper titles and their corresponding IDs for the list provided above:\\n\\npaper_1, paper_2, paper_3, paper_4, paper_5, paper_6, paper_7, paper_8, paper_9, paper_10, paper_11, paper_12'}], [{'generated_text': 'The following are the titles and IDs of research papers that include a benchmark for the RotoWire dataset:\\n- P31 Content Selection\\n- P31 Content Ordering'}], [{'generated_text': 'precision   : 30\\nrecall      : 0.667\\nf1          : 0.667'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Dmlab-30\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-German\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\\n\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IWSLT2015 English-German\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { \\n  SELECT?model?model_lbl WHERE { \\n   ?dataset a orkgc:Dataset; \\n    rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cartpole, swingup (DMControl500k)\")?benchmark orkgp:HAS_DATASET?dataset; \\n    orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; \\n    orkgp:HAS_METRIC?metric. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark; \\n    orkgp:HAS_MODEL?model. \\n   ?model rdfs:label?model_lbl. \\n  } \\n  ORDER BY DESC(?value) \\n  LIMIT 1 \\n } \\n}'}], [{'generated_text': 'Models:\\ncausestats, pbm25, srathe, pixture, logreg, decisioncurve, randomforecast'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"LSTM\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Grave et al. (2016) - LSTM + continuous cache pointer\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) ='}], [{'generated_text': 'The best performing model in terms of Accuracy metric on the Oxford-IIIT Pets benchmark dataset is:\\n\\nrdfs:label   \"ParametricRegressionModel\"\\n\\nThe top benchmark result (metric and value) over the dataset Oxford-IIIT Pets is:\\n\\nrdfs:label   \"ParametricRegressionModel\"\\n  \"0.945316\"'}]]\n","0 0\n","1 0\n","2 0\n","3 0\n","4 0\n","5 0\n","0 3\n","1 3\n","0 4\n","0 8\n","0 9\n","0 10\n","0 16\n","1 16\n","0 27\n","0 29\n","0 31\n","0 32\n","0 35\n","1 35\n","0 40\n","1 40\n","2 40\n","3 40\n","0 41\n","1 41\n","2 41\n","0 42\n","0 43\n","1 43\n","2 43\n","3 43\n","4 43\n","5 43\n","6 43\n","7 43\n","8 43\n","9 43\n","0 47\n","1 47\n","2 47\n","3 47\n","4 47\n","5 47\n","6 47\n","7 47\n","0 49\n","1 49\n","81.81818181818183%  [[{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"XLNet (Large)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"EfficientNetV2-L\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WOS-46985\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'The highest benchmark result achieved on the Supervised: dataset, including the metric and its value?\\nThe highest benchmark result on the WSC dataset, including the metric and its value?\\nThe highest benchmark result on the Supervised: dataset, including the metric and its value?'}], [{'generated_text': 'The best performing model benchmarking the ANLI test dataset in terms of A3 metric is \"VQA Model with Pointnet + attention + Spatial Attention\". The model achieves an average A3 score of 0.92.'}], [{'generated_text': 'Here are the metrics used to evaluate models over the iNaturalist 2018 dataset:\\n\\n* accuracy\\n* AUC\\n* F1 score\\n* F2 score\\n* macro F1 score\\n* micro F1 score\\n* R2 score\\n* R2 micro score\\n* R2 macro score\\n\\nHere are the metrics of evaluation over the CIFAR-10 dataset:\\n\\n* accuracy\\n* macro F1 score\\n* micro F1 score\\n* R2 score\\n* R2 micro score\\n* R2 macro score'}], [{'generated_text': \"The models that have been benchmarked on the Atari 2600 Atlantis dataset are:\\n- Tobias Welinder's model\\n- Tobias Welinder's updated model\\nThe models that have been benchmarked on the Atari 2600 Yars Revenge dataset are:\\n- Tobias Welinder's model\\n- Tobias Welinder's updated model\\nThe models that have been benchmarked on the Atari 2600 Space Invaders dataset are:\\n- Tobias Welinder's model\"}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cart Pole (OpenAI Gym)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n ?model a orkgc:Model; \\n  rdfs:label?model_lbl. \\n  FILTER (str(?model_lbl) = \"A3C FF hs\") \\n ?benchmark orkgp:HAS_DATASET?dataset. \\n ?cont orkgp:HAS_BENCHMARK?benchmark. \\n ?cont orkgp:HAS_MODEL?model; \\n  orkgp:HAS_SOURCE_CODE?code. \\n}'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiDAF + Self Attention + ELMo\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'In the submitted dataset the metric names are : Has_Dataset, Has_Evaluation, Has_Metric, Has_Label, Has_Organism, Has_Year, Pubmed, BIOSSES, NCBI_Diseasemetric'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"6 layer QRNN\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': \"The WOS-5736 dataset is being evaluated on the following models:\\n\\n1. Tomitaki's dataset regression model.\\n2. Battelle's evaluation dataset regression model.\\n3. MMLearn's web-scale recommendation system.\\n4. CoreML's image classification model.\\n5. Handcrafted dataset regression model.\\n6. MMLearn's text categorization model.\"}], [{'generated_text': 'Please find the answer to your question below. \\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT29\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Robot Navigation\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \\n\\nThe datasets used in the benchmarking of PointGoal Navigation and Robotic Grasping are orkgc:Dataset and orkgp:Dataset respectively.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'The following are the models that have been benchmarked on the IMDb-B dataset:\\n\\n- Adaboost\\n- Adadelta\\n- AdaGrad\\n- Adam\\n- ApeDL\\n- AveragedTimeDistortion\\n- BatchNorm\\n- BiasAdapt\\n- Binder\\n- Cosine\\n- Distributed Memory\\n- EAST\\n- EtE\\n- GIZMO\\n- GradientBoostingRegressor\\n- Highway\\n- Lasso\\n- LBFGS\\n- LightGBM\\n- MixMatch\\n- MonteCarlo\\n- MVNE\\n- Nesterov\\n- OneShot\\n- Perspective\\n- Predicta\\n- Precision\\n- Rprop\\n- SGD\\n- Stochastic Gradient\\n- TAN\\n- TubularNeuralNet'}], [{'generated_text': 'SELECT DISTINCT?code WHERE { \\n   ?model a orkgc:Model; \\n    rdfs:label?model_lbl. \\n    FILTER (str(?model_lbl) = \"PEGASUS\") \\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark. \\n   ?cont orkgp:HAS_MODEL?model; \\n    orkgp:HAS_SOURCE_CODE?code. \\n} \\n\\nSELECT DISTINCT?code WHERE { \\n   ?model a orkgc:Model; \\n    rdfs:label?model_lbl. \\n    FILTER (str(?model_lbl) = \"PEGASUSLARGE\") \\n   ?benchmark orkgp:HAS_DATASET?dataset. \\n   ?cont orkgp:HAS_BENCHMARK?benchmark. \\n   ?cont orkgp:HAS_MODEL?model; \\n    orkgp:HAS_SOURCE_CODE?code. \\n}'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Audio Classification\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }\\n\\nCommonly used benchmark datasets for the Music Modeling research area are:\\n- MusicDSP: Similar to the MusicDSP dataset for acoustic music perception.\\n- RAVEN: A large dataset for music information retrieval.\\n- HollywoodSD: Similar to Hollywood-8 for music genre classification.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cart Pole (OpenAI Gym)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reacher, easy (DMControl500k)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'select distinct?model?model_lbl where { \\n    ?metric a orkgc:Metric; \\n     rdfs:label?metric_lbl. \\n     FILTER (str(?metric_lbl) = \"Top 1 Accuracy\") \\n     { \\n         SELECT?model?model_lbl WHERE { \\n            ?dataset a orkgc:Dataset; \\n             rdfs:label?dataset_lbl. \\n             FILTER (str(?dataset_lbl) = \"ImageNet\") \\n            ?benchmark orkgp:HAS_DATASET?dataset; \\n             orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; \\n             orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; \\n             orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. \\n         } \\n         ORDER BY DESC(?value) \\n         LIMIT 1 \\n     } \\n }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a ae05:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2005\")?benchmark ae05:HAS_DATASET?dataset; ae05:HAS_EVALUATION ae05:HAS_VALUE?value. OPTIONAL { ae05:HAS_METRIC ae05:METRIC }?cont ae05:HAS_BENCHMARK ae05:HAS_MODEL ae05:MODEL?model. ae05:MODEL ae05:LABEL?model_lbl. } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The following metrics are used to evaluate models on the IMDb benchmark dataset:\\n- ASG\\n- BLD\\n- CTR\\n- ENTR\\n- GET \\n- IOB \\n- META \\n- NDCG\\n- POS \\n- PREMT\\n- ROUTE\\n- TRANS \\nThe following metrics are used to evaluate models on the Amazon benchmark dataset:\\n- ASG\\n- BLD\\n- CTR\\n- ENTR\\n- GET \\n- IOB \\n- META \\n- NDCG\\n- POS \\n- PREMT\\n- ROUTE\\n- TRANS'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Enduro\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"HutterPrize\")?benchmark orkgp:HAS_DATASET?dataset; \\n  orkgp:HAS_EVALUATION?eval. \\n  OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \\n}'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Wizard of Wor\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"12-layer Transformer-XL\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"24-layer Transformer-XL\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Up and Down\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl \\n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NLP-TDMS\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nA list of models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset: \\n\\npaper=Zhiqing Xu, 0000-0002-3094-9020; 0000-0003-2888-0691; 0000-0002-4787-7621; 0000-0002-4788-7489\\nmodel=Xu, Zhiqing; Xu, Ren; Luckey, Ben; et al. Recognizing textual data with deep learning. arXiv preprint arXiv:1810.04607 [cs, abstract]. 2018.\\nmodel=G'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy (%)\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \" Jacquard dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The following metrics are commonly used when benchmarking models on ObjectNet (Bounding Box):'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {\\n?dataset a orkgc:Dataset;\\n rdfs:label?dataset_lbl.\\n FILTER (str(?dataset_lbl) = \"enwiki8\")\\n?benchmark orkgp:HAS_DATASET?dataset.\\n?cont orkgp:HAS_BENCHMARK?benchmark.\\n?paper orkgp:P31?cont;\\n rdfs:label?paper_lbl.\\n}'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Top-1 Error Rate\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FGVC Aircraft\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The metrics used to evaluate models on the Atari 2600 Seaquest benchmark dataset are?\\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Seaquest\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\nThe metrics used to evaluate models on the Atari 2600 Pitfall! benchmark dataset are?\\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Pitfall!\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Road Runner\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Boxing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 French-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\nCommon metrics used for benchmarking NER and dependency parsing models on the WMT14 dataset include precision, recall, f-score, and the area under the curve (AUC).'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 French-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BiDAF + Self Attention + ELMo (single model)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nIn the input text, please provide references to code that has been used in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes.'}], [{'generated_text': 'models being evaluated on the ACL-ARC, ESC-50 and AESLC datasets are:\\n\\n- random forests\\n- support vector machines (linear and sigmoid)\\n- k- nearest neighbours\\n- AdaBoost'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"PAR Transformer Large\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"BERTwwm + SQuAD 2\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"CL-Titles-Parser\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The top benchmark result (metric and value) over the dataset Habitat 2020 Object Nav test-std is the value \"98.76\" with a metric of \"Recall\".'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Name This Game\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nAssuming you have already executed the following queries to retrieve all datasets, models, and evaluations:\\n\\nSELECT?dataset?dataset_lbl?benchmark \\nFROM  orkgc:Dataset, orkgp:HAS_EVAL'}], [{'generated_text': 'The highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value, is { \"score\" : 86.680296 }.'}]]\n","0 3\n","0 4\n","1 4\n","2 4\n","3 4\n","4 4\n","5 4\n","6 4\n","7 4\n","8 4\n","9 4\n","0 5\n","0 6\n","0 11\n","1 11\n","2 11\n","0 13\n","1 13\n","0 17\n","1 17\n","0 24\n","0 33\n","1 33\n","0 42\n","0 47\n","1 47\n","2 47\n","3 47\n","0 49\n","1 49\n","90.90909090909092%  [[{'generated_text': 'A preliminary list of models that have been benchmarked on the Atari 2600 Breakout dataset can be found at https://github.com/armnn/ARM-Badged-Datasheets/blob/master/Breakout/models.csv.  Please let me know if you have any additional resources.'}], [{'generated_text': 'The papers that utilize the EfficientNetV2-M model are listed below:\\n1) https://github.com/zhangshaolan/ECN_MobileNet_V2_Paper/tree/master/ECN_MobileNet_V2_Paper\\n2) https://arxiv.org/abs/1905.00695\\n3) https://arxiv.org/abs/1804.06805\\n4) https://github.com/zhangshaolan/EfficientNetV2_Paper/blob/master/EfficientNetV2_Paper.ipynb\\n5) https://github.com/UCB-CSNR/EfficientNet-v2/blob/master/EfficientNet_V2.py\\n6) https://github.com/zhangshaolan/EfficientNetV2_Paper/blob/master/EfficientNetV2_Paper.ipynb'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Ms. Pacman\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'The evaluation metrics commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset are:\\n - mappa_score\\n - precision\\n - recall\\n - f1-score'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE {\\n\\n    {\\n\\n        {\\n\\n            SELECT?metric?metric_lbl?value WHERE {\\n\\n               ?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.\\n\\n                FILTER (str(?dataset_lbl) = \"BC5CDR-disease\")\\n\\n               ?benchmark orkgp:HAS_DATASET?dataset;\\n\\n                orkgp:HAS_EVALUATION?eval.\\n\\n               ?eval orkgp:HAS_VALUE?value.\\n\\n                OPTIONAL {\\n\\n               ?eval orkgp:HAS_METRIC?metric.\\n\\n               ?metric rdfs:label?metric_lbl.\\n\\n                }\\n\\n               ?cont orkgp:HAS_BENCHMARK?benchmark.\\n\\n                OPTIONAL {\\n\\n               ?cont orkgp:HAS_MODEL?model.\\n\\n               ?model rdfs:label?model_lbl.\\n\\n                }\\n\\n            }\\n\\n            ORDER BY DESC(?value)\\n\\n        }\\n\\n    }\\n\\n    {\\n\\n        SELECT?metric?metric'}], [{'generated_text': 'The following metrics are used to evaluate models on the ImageNet 64x64 benchmark dataset:\\n- Top-1 error\\n- Top-5 error\\n- ELBO'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a dbpprop:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DBpedia\")?benchmark dbpprop:HAS_DATASET?dataset.?cont dbpprop:HAS_BENCHMARK?benchmark.?paper dbpprop:P31?cont; rdfs:label?paper_lbl. } \\n\\nMy Sparql query returns these results:\\n\\ntitle: Paper Field dataset\\n  paper_lbl: Paper Field\\n  paper: P31\\n\\ntitle: DBpedia\\n  paper_lbl: DBpedia'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"HRLRE\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }\\n\\ninput (English text): Provide a list of papers that have utilized the HRLRE model and include the links to their code?\\n output (Sparql query):'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"T-ConvS2S\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Best Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Q*Bert\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The top benchmark score and its metric on the Words in Context dataset are:\\n{ \"score\": 72835.6869442, \"metric\": \"n-gram\" }'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Common Sense Reasoning\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }\\n\\nRelation Extraction:\\n\\nSELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"relation extraction\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2004\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}]]\n","0 0\n","0 1\n","0 3\n","1 3\n","2 3\n","0 5\n","0 10\n"]}],"source":["import json\n","import torch\n","from sentence_transformers import SentenceTransformer\n","from sentence_transformers.util import cos_sim\n","from datasets import load_dataset\n","from transformers import pipeline, AutoTokenizer\n","\n","threshold = 0.25\n","\n","model = SentenceTransformer('all-mpnet-base-v2', device='cuda' if torch.cuda.is_available() else \"cpu\")\n","# model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda' if torch.cuda.is_available() else \"cpu\")\n","raw_datasets = load_dataset(\"orkg/SciQA\")\n","print(raw_datasets)\n","embed_data = torch.load('train_embeddings.pt')\n","dolly = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n","tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side=\"left\")\n","\n","\n","def divide_chunks(l_, n_):\n","    for i_ in range(0, len(l_), n_):\n","        yield l_[i_:i_ + n_]\n","\n","\n","def save_json(filename, data):\n","    with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n","        print(json.dumps(data), file=json_file)\n","\n","\n","def get_similar(element, items=None, embeddings=None, num=None):\n","    emb_items = None\n","\n","    if items is None and embeddings is not None:\n","        emb_items = embeddings[\"emb_questions\"]\n","        items = embeddings[\"keys\"]\n","    elif items is not None:\n","        emb_items = model.encode(items)\n","\n","    if len(element) == 0 or emb_items is None:\n","        return []\n","\n","    emb_element = model.encode(element)\n","\n","    result = []\n","    scores = cos_sim(emb_element, emb_items)\n","\n","    if num is None or num < 2:\n","        maximus = torch.max(scores, 1)\n","        m = float(maximus.values[0])\n","        i = int(maximus.indices[0])\n","        if m > threshold:\n","            result = [[round(m, 4), items[i], embeddings[\"questions\"][i], embeddings[\"queries\"][i]]]\n","        return result\n","    else:\n","        scored_texts = []\n","        for i, score in enumerate(scores[0]):\n","            scored_texts.append(\n","                [round(score.item(), 4), items[i], embeddings[\"questions\"][i], embeddings[\"queries\"][i]])\n","        sorted_scored_texts = sorted(scored_texts, key=lambda x: x[0], reverse=True)\n","        return sorted_scored_texts[:num]\n","\n","\n","def clean(st):\n","    st = st.replace(\"\\n\", \" \")\n","    st = st.replace(\"?\", \" ?\")\n","    st = st.replace(\"{\", \" { \")\n","    st = st.replace(\"}\", \" } \")\n","    st = st.replace(\"\\\\'\", \"'\")\n","\n","    while \"  \" in st:\n","        st = st.replace(\"  \", \" \")\n","    return st\n","\n","\n","def get_key(q):\n","    t0 = q.get('template_id')\n","    if t0 is None:\n","        t0 = \"None\"\n","    t = str(q.get(\"number_of_patterns\")) + \"-\" + t0\n","    return t\n","\n","\n","def save_embedding():\n","    train = raw_datasets.get(\"train\")\n","    questions = [q[\"question\"][\"string\"] for q in train]\n","    queries = [clean(q[\"query\"][\"sparql\"]) for q in train]\n","    keys = [get_key(q) for q in train]\n","    embeddings = {}\n","    emb_questions = model.encode(questions)\n","    embeddings[\"questions\"] = questions\n","    embeddings[\"emb_questions\"] = emb_questions\n","    embeddings[\"queries\"] = queries\n","    embeddings[\"keys\"] = keys\n","    torch.save(embeddings, 'train_embeddings.pt')\n","    return embeddings\n","\n","\n","def prepare_queries(n_):\n","    data = raw_datasets.get(\"test\")\n","    queries = []\n","    suggestions = []\n","    for q in data:\n","        t = get_key(q)\n","        question = q[\"question\"][\"string\"]\n","        suggestion = get_similar(question, embeddings=embed_data, num=n_)\n","        suggestions.append([[[x[0], x[1]] for x in suggestion], t])\n","\n","        if suggestion is None or len(suggestion) == 0:\n","            print(\"Error with key\", t)\n","            queries.append(\"translate the following English text '\" + question + \"' to a sparql query\")\n","        else:\n","            final_q = \"\"\n","            for i_, k in enumerate(suggestion):\n","                final_q += \"\\n input (English text): \" + k[2]\n","                final_q += \"\\n output (Sparql query): \" + k[3]\n","\n","            # works better with gpt\n","            # final_q += \"\\n with this example what is the sparql query for:  \" + question\n","\n","            # works better with dolly\n","            final_q += \"\\n input (English text): \" + question\n","            final_q += \"\\n output (Sparql query): \"\n","            queries.append(final_q)\n","    return queries, suggestions\n","\n","\n","def main(shots=2, attempts=10, batch=50):\n","    query_list, suggestions = prepare_queries(shots)\n","    print(len(query_list))\n","\n","    n = batch\n","    q_list = list(divide_chunks(query_list, n))\n","    sparql = [clean(x[\"query\"][\"sparql\"]) for x in raw_datasets.get(\"test\")]\n","\n","    gs = []\n","    lens = []\n","    i = 0\n","\n","    for group in q_list:\n","        print(str(i) + \"%\", end=\"  \")\n","        i += 1 / len(q_list) * 100\n","\n","        res_ = [tokenizer.encode(question) for question in group]\n","        len_ = [len(x) for x in res_]\n","        warning = [x for x in len_ if x > 2048]\n","        if len(warning) > 0:\n","            print(warning)\n","            quit()\n","        lens += len_\n","\n","        res = dolly(group)\n","        print(res)\n","        gst = [x[0][\"generated_text\"] for x in res]\n","\n","        for ii, l in enumerate(gst):\n","            for iii in range(attempts):\n","                if \"SELECT\" not in l:\n","                    print(iii, ii)\n","                    res = dolly(group[ii])\n","                    gst[ii] = res[0][\"generated_text\"]\n","                    l = gst[ii]\n","                else:\n","                    break\n","        gs += gst\n","\n","        result = {\"questions\": query_list, \"sparql\": sparql, \"generated_sparql\": gs, \"prompt_len\": lens,\n","                  \"suggestions\": suggestions}\n","        save_json(\"nlp_dolly_\" + str(shots) + \"_shot_results_tok.json\", result)\n","\n","main()"]}]}