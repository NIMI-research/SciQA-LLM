{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNpHz2za4YzDxyJvkhy7gRR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n","!pip -q install accelerate>=0.12.0\n","!pip install datasets\n","!pip install sentence_transformers"],"metadata":{"id":"zuwLc--q7RtP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd drive/MyDrive/en2sparql"],"metadata":{"id":"YcU-KRrrF9JR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xF_bcVRM61-s"},"outputs":[],"source":["import json\n","import torch\n","from sentence_transformers import SentenceTransformer\n","from sentence_transformers.util import cos_sim\n","from datasets import load_dataset\n","from transformers import pipeline, AutoTokenizer\n","\n","threshold = 0.25\n","\n","model = SentenceTransformer('all-mpnet-base-v2', device='cuda' if torch.cuda.is_available() else \"cpu\")\n","# model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda' if torch.cuda.is_available() else \"cpu\")\n","raw_datasets = load_dataset(\"orkg/SciQA\")\n","print(raw_datasets)\n","embed_data = torch.load('train_embeddings.pt')\n","dolly = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n","tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side=\"left\")\n","\n","\n","def load_json(file__name):\n","    data_file = open(file__name, \"r\", encoding='utf-8')\n","    file_data = json.loads(data_file.read())\n","    data_file.close()\n","    return file_data\n","\n","\n","def divide_chunks(l_, n_):\n","    for i_ in range(0, len(l_), n_):\n","        yield l_[i_:i_ + n_]\n","\n","\n","def save_json(filename, data):\n","    with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n","        print(json.dumps(data), file=json_file)\n","\n","\n","def get_similar(element, items=None, embeddings=None, num=None, reversed=False):\n","    emb_items = None\n","\n","    if items is None and embeddings is not None:\n","        emb_items = embeddings[\"emb_questions\"]\n","        items = embeddings[\"keys\"]\n","    elif items is not None:\n","        emb_items = model.encode(items)\n","\n","    if len(element) == 0 or emb_items is None:\n","        return []\n","\n","    emb_element = model.encode(element)\n","\n","    result = []\n","    scores = cos_sim(emb_element, emb_items)\n","\n","    if num is None or num < 2:\n","        maximus = torch.max(scores, 1)\n","        m = float(maximus.values[0])\n","        i = int(maximus.indices[0])\n","        if m > threshold:\n","            result = [[round(m, 4), items[i], embeddings[\"questions\"][i], embeddings[\"queries\"][i]]]\n","        return result\n","    else:\n","        scored_texts = []\n","        for i, score in enumerate(scores[0]):\n","            scored_texts.append(\n","                [round(score.item(), 4), items[i], embeddings[\"questions\"][i], embeddings[\"queries\"][i]])\n","        sorted_scored_texts = sorted(scored_texts, key=lambda x: x[0], reverse=True)\n","\n","        key = sorted_scored_texts[0][1]\n","        samples = []\n","        for sample in sorted_scored_texts:\n","            if sample[1] == key:\n","                samples.append(sample)\n","\n","        samples = samples[:num]\n","        if reversed:\n","            samples.reverse()\n","        return samples\n","\n","\n","def clean(st):\n","    st = st.replace(\"\\n\", \" \")\n","    st = st.replace(\"?\", \" ?\")\n","    st = st.replace(\"{\", \" { \")\n","    st = st.replace(\"}\", \" } \")\n","    st = st.replace(\"\\\\'\", \"'\")\n","\n","    while \"  \" in st:\n","        st = st.replace(\"  \", \" \")\n","    return st\n","\n","\n","def get_key(q):\n","    t0 = q.get('template_id')\n","    if t0 is None:\n","        t0 = \"None\"\n","    t = str(q.get(\"number_of_patterns\")) + \"-\" + t0\n","    return t\n","\n","\n","def save_embedding():\n","    train = raw_datasets.get(\"train\")\n","    questions = [q[\"question\"][\"string\"] for q in train]\n","    queries = [clean(q[\"query\"][\"sparql\"]) for q in train]\n","    keys = [get_key(q) for q in train]\n","    embeddings = {}\n","    emb_questions = model.encode(questions)\n","    embeddings[\"questions\"] = questions\n","    embeddings[\"emb_questions\"] = emb_questions\n","    embeddings[\"queries\"] = queries\n","    embeddings[\"keys\"] = keys\n","    torch.save(embeddings, 'train_embeddings.pt')\n","    return embeddings\n","\n","\n","def prepare_queries(n_, reversed=False):\n","    data = raw_datasets.get(\"test\")\n","    queries = []\n","    suggestions = []\n","    for q in data:\n","        t = get_key(q)\n","        question = q[\"question\"][\"string\"]\n","        suggestion = get_similar(question, embeddings=embed_data, num=n_, reversed=reversed)\n","        suggestions.append([[[x[0], x[1]] for x in suggestion], t])\n","\n","        if suggestion is None or len(suggestion) == 0:\n","            print(\"Error with key\", t)\n","            queries.append(\"translate the following English text '\" + question + \"' to a sparql query\")\n","        else:\n","            final_q = \"\"\n","            for i_, k in enumerate(suggestion):\n","                final_q += \"\\n input (English text): \" + k[2]\n","                final_q += \"\\n output (Sparql query): \" + k[3]\n","\n","            # works better with gpt\n","            # final_q += \"\\n with this example what is the sparql query for:  \" + question\n","\n","            # works better with dolly\n","            final_q += \"\\n input (English text): \" + question\n","            final_q += \"\\n output (Sparql query): \"\n","            queries.append(final_q)\n","    return queries, suggestions\n","\n","\n","def main(shots=7, attempts=10, batch=50, reversed=False, filename=None):\n","    if filename is None:\n","        query_list, suggestions = prepare_queries(shots, reversed)\n","        print(len(query_list))\n","    else:\n","        data = load_json(filename)\n","        complete_query_list = data[\"questions\"]\n","        suggestions = data[\"suggestions\"]\n","        generated_sparql = data[\"generated_sparql\"]\n","        done = len(generated_sparql) - batch\n","        query_list = complete_query_list[done:]\n","        suggestions = suggestions[done:]\n","\n","    n = batch\n","    q_list = list(divide_chunks(query_list, n))\n","    sparql = [clean(x[\"query\"][\"sparql\"]) for x in raw_datasets.get(\"test\")]\n","\n","    gs = []\n","    lens = []\n","    i = 0\n","\n","    for group in q_list:\n","        print(str(i) + \"%\", end=\"  \")\n","        i += 1 / len(q_list) * 100\n","\n","        res_ = [tokenizer.encode(question) for question in group]\n","        len_ = [len(x) for x in res_]\n","        warning = [x for x in len_ if x > 2048]\n","        if len(warning) > 0:\n","            print(warning)\n","            quit()\n","        lens += len_\n","\n","        res = dolly(group)\n","        print(res)\n","        gst = [x[0][\"generated_text\"] for x in res]\n","\n","        for ii, l in enumerate(gst):\n","            for iii in range(attempts):\n","                if \"SELECT\" not in l:\n","                    print(iii, ii)\n","                    res = dolly(group[ii])\n","                    gst[ii] = res[0][\"generated_text\"]\n","                    l = gst[ii]\n","                else:\n","                    break\n","        gs += gst\n","\n","        result = {\"questions\": query_list, \"sparql\": sparql, \"generated_sparql\": gs, \"prompt_len\": lens,\n","                  \"suggestions\": suggestions}\n","        save_json(\"cont_test_B_nlp_dolly_\" + str(shots) + \"_shot_results_tok.json\", result)\n","\n","main(reversed=True)"]}]}