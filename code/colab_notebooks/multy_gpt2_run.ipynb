{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zuwLc--q7RtP"},"outputs":[],"source":["!pip install transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24117,"status":"ok","timestamp":1698253111465,"user":{"displayName":"Antonello Meloni","userId":"09336063038722863523"},"user_tz":-120},"id":"YcU-KRrrF9JR","outputId":"32debde1-7ba0-4925-fa2c-72d29fc299f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/en2sparql\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd drive/MyDrive/en2sparql"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2yj0l4Yf3SEc"},"outputs":[],"source":["import torch\n","import json\n","from transformers import pipeline, AutoTokenizer\n","\n","gpt2 = pipeline(model=\"gpt2-large\", max_new_tokens=384, device='cuda' if torch.cuda.is_available() else \"cpu\", return_full_text=False)\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xF_bcVRM61-s"},"outputs":[],"source":["\n","def divide_chunks(l_, n_):\n","    for i_ in range(0, len(l_), n_):\n","        yield l_[i_:i_ + n_]\n","\n","\n","def clean(st):\n","    st = st.replace(\"\\n\", \" \")\n","    st = st.replace(\"?\", \" ?\")\n","    st = st.replace(\"{\", \" { \")\n","    st = st.replace(\"}\", \" } \")\n","    st = st.replace(\"\\\\'\", \"'\")\n","\n","    while \"  \" in st:\n","        st = st.replace(\"  \", \" \")\n","    return st\n","\n","\n","def load_json(file__name):\n","    data_file = open(file__name, \"r\", encoding='utf-8')\n","    file_data = json.loads(data_file.read())\n","    data_file.close()\n","    return file_data\n","\n","\n","def save_json(filename,data):\n","    with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n","        print(json.dumps(data), file=json_file)\n","\n","\n","def main(filename):\n","    all_data = load_json(filename)\n","    for key in all_data:\n","        data = all_data[key]\n","        q_list = data[\"questions\"]\n","        suggestions = data[\"templates\"]\n","        template = data[\"template\"]\n","        gs = data[\"generated_sparql\"]\n","        sparql = data[\"sparql\"]\n","        lens = data.get(\"prompt_len\")\n","        if lens is None:\n","            lens = []\n","\n","        print(len(q_list))\n","\n","        for i, question in enumerate(q_list[len(gs):]):\n","            print(i, end=\" \")\n","            res_ = tokenizer.encode(question)\n","            len_ = len(res_)\n","            lens.append(len_)\n","            print(\"len: \",len_)\n","\n","            if len_ > 600:\n","                question = tokenizer.decode(res_[-600:])\n","                len_ = 600\n","\n","            res = gpt2(question)\n","            gs.append(res[0][\"generated_text\"])\n","            result = {\"questions\": q_list, \"sparql\": sparql, \"generated_sparql\": gs, \"prompt_len\": lens, \"templates\": suggestions, \"template\": template}\n","            all_data[key] = result\n","            save_json(filename, all_data)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kn5VcgNY91u5"},"outputs":[],"source":["main(\"test_3_mult_diversity_pt_gpt2.json\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"113UtZclrQ0lymfwF3NUd5OqY8cwYjdZS","timestamp":1692434060531},{"file_id":"11r0-FEMO2UG7b2oZd4ySW0LpOjsly0mm","timestamp":1691515342412}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}