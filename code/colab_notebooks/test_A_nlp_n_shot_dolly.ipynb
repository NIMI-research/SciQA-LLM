{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNQ0lp7n3x2GKSJ29+eT3gp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n","!pip -q install accelerate>=0.12.0\n","!pip install datasets\n","!pip install sentence_transformers"],"metadata":{"id":"zuwLc--q7RtP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692691932364,"user_tz":-120,"elapsed":54203,"user":{"displayName":"Antonello","userId":"01962207529893687178"}},"outputId":"a917f81e-a604-4f98-98dd-ad2e0a1529a5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting datasets\n","  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-2.14.4 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n","Collecting sentence_transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.33.0.dev0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n","Collecting sentencepiece (from sentence_transformers)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n","Building wheels for collected packages: sentence_transformers\n","  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=29181a78ae9acbfbf1d2fa0d406efdd95c7d2a1558b57637bcf6cae647ddddbd\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built sentence_transformers\n","Installing collected packages: sentencepiece, sentence_transformers\n","Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd drive/MyDrive/en2sparql"],"metadata":{"id":"YcU-KRrrF9JR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692691965987,"user_tz":-120,"elapsed":27839,"user":{"displayName":"Antonello","userId":"01962207529893687178"}},"outputId":"c8d8e2db-2706-4f82-a145-f79b7eaa7ce7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/en2sparql\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xF_bcVRM61-s","colab":{"base_uri":"https://localhost:8080/"},"outputId":"612b757a-8720-462d-b56d-83e32b7e20f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'query_type', 'question', 'paraphrased_question', 'query', 'template_id', 'query_shape', 'query_class', 'auto_generated', 'number_of_patterns'],\n","        num_rows: 1795\n","    })\n","    validation: Dataset({\n","        features: ['id', 'query_type', 'question', 'paraphrased_question', 'query', 'template_id', 'query_shape', 'query_class', 'auto_generated', 'number_of_patterns'],\n","        num_rows: 257\n","    })\n","    test: Dataset({\n","        features: ['id', 'query_type', 'question', 'paraphrased_question', 'query', 'template_id', 'query_shape', 'query_class', 'auto_generated', 'number_of_patterns'],\n","        num_rows: 513\n","    })\n","})\n","513\n","0%  [[{'generated_text': 'SELECT DISTINCT?model?model_lbl \\n  WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WSC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Word Level)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UrbanSound8k\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WOS-5736\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': 'The top score on the MAZEA dataset is 97.0 and the top performing model is Baseline : Extractive Oracle with ID : P32'}], [{'generated_text': 'The following research paper titles and IDs have benchmarked models on the TDMSci dataset:\\n\\nP31. Schlichting, Jared; Noren, Elliot; Parkhi, Amir; Reutens, Frederik; Grohmann, Thorsten; Chen, Yan-Chuang; Pasupat, Arindam; Olah, Toby; Chen, Cheng-Hsu; Cheng, Kai-Ting; Hendricks, Troy; Zhang, Zhi-An; Ramakrishnan, Muthu; Yih, Wansong; Cukuroyan, Ali; Gupta, Amit; Rusu, Dragoș; Baker, David; Raden, Deniz; Gupta, Siddharth; Abo, Amin; Raste, Florian; Zhou, Xiang-Long; Cheung, Kin-Wing; Aslan, Musa; Kizilcec, Zeljko; Dunja, Milan; Wu, Lei; Lu, Yu; Zhu, Hongbo; Patel, Sandeep; Gupta, Milind; and Joachims, Kurt; \"A Benchmarking Study on Common DDMs for XML Processing\" (PDF).'}], [{'generated_text': 'The mean capacity of a carbon-based fuel is'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Russian\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': 'Concrete Supervised Learning Model KNN'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Romanian-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Here is the highest score benchmark result over the Finger, spin (DMControl100k) dataset:\\n\\nModel: hs DDQN (tuned) hs\\nMetric:  Score\\nValue:  0.349\\n\\nNote: 0.349 is the highest score, which means the model hs DDQN (tuned) hs achieves the score 0.349 on the Finger, spin (DMControl100k) dataset.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } \\n  GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Micro F1\\n\\n<|endoftext|>Imagine an iceberg: an icy mass roughly the size of Manhattan, embedded in an ocean that covers almost all of the Earth’s surface.\\n\\nThat’s the iceberg that is Greenland and its influence on our climate. When it melts, it leaves behind tons of greenhouse gases in its wake. Climate scientist James Hansen once dubbed it the “ greenhouse gases cannon ball.”\\n\\nEven the increase of about 300 billion tons of ice melts per year is significant. To put it into context, it would melt about five years’ worth of ice in the Great Lakes every year, and enough ice to cover Texas four meters deep.\\n\\nIf we were able to melt all of Greenland’s ice, we’d triple the amount of greenhouse gases in the atmosphere.\\n\\nAs it is, the Greenland ice sheet contains a trillion tons of ice. And since it\\'s a completely frozen ocean lying on top of land, it also stores the equivalent of about 600 billion cubic meters of water.\\n\\nWhile it may be tempting to call the Greenland ice sheet a \"doomed ice monument,\" the surface of the Greenland ice sheet is actually fairly stable: it has held up over the last 150 years, with a'}], [{'generated_text': 'Here is a list of research papers along with their titles and IDs, that have performed benchmarks on the NCBI Disease dataset:\\n\\n* https://peerj-sls.s3.ap-northeast-2.amazonaws.com/BenchmarkDatasets/GenomeDatasets/nr-disease.json\\n* https://peerj-sls.s3.ap-northeast-2.amazonaws.com/BenchmarkDatasets/GenomeDatasets/nr-disease.json\\n* https://peerj-sls.s3.ap-northeast-2.amazonaws.com/BenchmarkDatasets/GenomeDatasets/nr-disease.json\\n* https://peerj-sls.s3.ap-northeast-2.amazonaws.com/BenchmarkDatasets/GenomeDatasets/nr-disease.json\\n* https://peerj-sls.s3.ap-northeast-2.amazonaws.com/BenchmarkDatasets/GenomeDatasets/nr-disease.json\\n* https://peerj'}], [{'generated_text': '\"QA-GNN\"@P32'}], [{'generated_text': 'Parameters,'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet 64x64\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \\n\\nA:\\n\\nBelow are links to datasets and papers related to the Neural Architecture Search (NAS) research area:\\n\\nDatasets\\nImageNet ReaL Dataset: https://www.image-net.org/challenges/LSVRC/2018/ert\\nImageNet V2 Dataset: https://www.image-net.org/challenges/LSVRC/2015/ert\\nNeural Architecture Search Datasets:\\nBenchmarks: https://github.com/neuralasm/benchmarks\\nModel Performance (accuracy): https://github.com/cs-jenkins/cross_dataset_results/blob'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\nDISTINCT?metric?metric_lbl returns metrics values in Bits per byte'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Flair-TDM\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciCite\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': \"The Linguistic Input Features model has achieved the highest F1 score score on the BUCC French-to-English benchmark dataset. The model's F1 score was 0.816.\"}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"SAN (single model)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDMSci\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) ='}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2005\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PNDec\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nSELECT DISTINCT?title WHERE {?paper a orkgc:Paper; orkgp:P31 [ orkgp:P2005 [ rdfs:label \"DBLP\"^^xsd:string ] ]; rdfs:label?title. } ORDER BY?title'}], [{'generated_text': 'Code references for papers that have used the CATTS-XSUM model for benchmarking purposes can be found in the CoNLL04 package documentation at https://github.com/conll/conll-suite/blob/master/software/conll-tutorial-package.md.'}], [{'generated_text': 'The top benchmark result (metric and value) over the dataset IMDb-B is Accuracy Score: 0.94'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot German-to-French\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value;?cont orkgp:HAS_BENCHMARK?benchmark;?model orkgp:HAS_METRIC?metric.?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\nORDER BY DESC(?value)'}], [{'generated_text': 'Here are the papers that benchmarked the SemExp model:\\n\\n- De Santis, G., Grisetti, S., & Baroni, D. (2016). Real-Time Recursive Data Expansion for Semantic Web Search.  In Proceedings of the 20th International Conference on World Wide Web. ACM, 665-678.\\n- De Santis, G., Grisetti, S., & Baroni, D. (2016). A Real-Time Recursive Data Expansion Approach to Semantic Web Search. In Proceedings of the 34th International Conference on Data Engineering. ACM, 593-604.\\n- De Santis, G., Grisetti, S., & Baroni, D. (2016). Scalable Real-Time Recursive Data Expansion for Semantic Web Search. In Proceedings of the 35th International Conference on Data Engineering. ACM, 381-392.\\n\\nAnd the models that were benchmarked on SemEval-2010 Task 8 dataset:\\n\\n- De Santis, G., Grisetti, S., & Baroni, D. (2016). Semantic Web Search with Real-time Recursive Data Expansion. In Proceedings of the 34th International'}], [{'generated_text': \"Commonly used metrics for benchmarking models on the Atari 2600 Up and Down dataset include:\\n* Score\\n* Average Loss\\n* Root Mean Square Error\\n* Cumulative Distribution Function (CDF)\\n* Area Under the Curve (AUC)\\n* Hann O'Keefe (HOF)\\n* Jarvis Index\"}], [{'generated_text': 'Neural cache model (size = 2,000)'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE {\\n  { SELECT?model?model_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) }\\n}\\nGROUP BY?metric?metric_lbl\\n\\nDatasets:\\n/orkgc:Dataset rdf:type=Dataset[rdfs:label, rdf:datatype, orkgp:HAS_DATAS'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Video Pinball\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value.?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }?cont orkgp:HAS_SOURCE_CODE?code. } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The model that achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset is the Inception-v3 model with a score of 99.63.'}], [{'generated_text': 'The most common Knowledge representation method is Rule-Based.'}], [{'generated_text': 'The following research papers have benchmarked the BioSentVec (PubMed + MIMIC-III) model:\\n- Salanti E, Ribeiro RT, Ribeiro F, Mioke K, Vieira RC, Mateescu I, Guzmán LA, Almeida-Filho AN, Novak A, Deng Y, Jing B, Deng S, Huang Y, He X, Wang W, He H, Liu C, Hu Q. \"BioSentVec: Multi-Task Sentence Embedding for Cross-Site Prediction\". ArXiv 2019. [https://arxiv.org/abs/1907.09349].\\n- Salanti E, Ribeiro RT, Ribeiro F, Mioke K, Vieira RC, Mateescu I, Guzmán LA, Almeida-Filho AN, Novak A, Deng Y, Jing B, Deng S, Huang Y, He X, Wang W, He H, Liu C, Hu Q. \"BioSentVec: Multi-Task Sentence Embedding for Cross-Site Prediction\". ArXiv 2019. [https://arxiv.org/abs/1907.09349'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Entity Disambiguation\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \\n\\nThe most common metrics are the F1 score, precision, recall, and CideR. In addition, the metrics commonly used for evaluation in entity disambiguation are entropy and Matthew\\'s correlation coefficient.'}], [{'generated_text': 'title\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods\\n- P2005 - Performance comparisons of neural network language model pretraining methods'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC Chinese-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Axiomatic Method for Machine Learning (2017)\\nInformation-theoretic loss functions for deep learning\\nUsing genetic algorithms to speed up learning of deep neural networks\\nLarge-scale deep learning with SGD: Part II - Performance and optimization\\nLarge-scale deep learning with SGD: Part III - Parameter optimization\\nLarge-scale deep learning with SGD: Part IV - Design decisions\\nLarge-scale deep learning with SGD: Part V - Code and experiments\\nLarge-scale deep learning with SGD: Part VI - Ablation study\\nLarge-scale deep learning with SGD: Part VII - Datasets\\nLarge-scale deep learning with SGD: Part VIII - Decision-theoretic lower bounds\\nLarge-scale deep learning with SGD: Part IX - Bayesian optimization with noisy gradients\\nLarge-scale deep learning with SGD: Part X - Bounds on the generalization gap\\nLarge-scale deep learning with SGD: Part XI - Beyond precision and recall: F1 score\\nLarge-scale deep learning with SGD: Part XII - Transfer learning on simple datasets\\nLarge-scale deep learning with SGD: Part XIII - Scalable training of deep neural networks\\nLarge-scale deep learning with SGD: Part XIV -'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Gibson PointGoal Navigation\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL++\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PIQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'select distinct?model?model_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Crazy Climber\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } order by desc(?value) limit 1'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'select distinct?code \\nwhere {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Funnel Transformer\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'Common metrics when benchmarking on the Reuters De-En dataset include:\\n1. Recall (or Sensitivity) \\n2. Precision\\n3. F1 Score'}], [{'generated_text': 'In the provided SPARQL query, the model that performed best in terms of F1 metric on the PubMed 20k RCT dataset is OverFeat - 7 accurate models.'}], [{'generated_text': 'Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DocRED (Human-annotated)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_'}], [{'generated_text': 'RNNModel outperforms all other models on the FLOOPS metric.'}]]\n","0 4\n","1 4\n","2 4\n","3 4\n","4 4\n","0 5\n","1 5\n","0 6\n","1 6\n","0 8\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["0 10\n","1 10\n","0 13\n","1 13\n","2 13\n","3 13\n","4 13\n","0 14\n","0 15\n","1 15\n","2 15\n","3 15\n","4 15\n","5 15\n","6 15\n","7 15\n","8 15\n","9 15\n","0 16\n","0 21\n","0 25\n","0 26\n","1 26\n","0 28\n","1 28\n","0 29\n","0 30\n","1 30\n","2 30\n","3 30\n","4 30\n","5 30\n","6 30\n","7 30\n","8 30\n","9 30\n","0 33\n","1 33\n","2 33\n","3 33\n","4 33\n","5 33\n","6 33\n","7 33\n","8 33\n","9 33\n","0 34\n","1 34\n","0 35\n","1 35\n","2 35\n","0 37\n","1 37\n","2 37\n","0 39\n","1 39\n","2 39\n","3 39\n","0 43\n","0 45\n","1 45\n","0 46\n","1 46\n","2 46\n","0 47\n","1 47\n","0 49\n","9.090909090909092%  [[{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AESLC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Please find attached a query to list the paper titles and IDs that include a benchmark for the Oxford-IIIT Pets dataset.\\nFor each paper, it will also provide the title of the research paper.\\nPlease note that this list is from the 3rd page of a page-long paper, and this may not contain all the details.\\n  \\nSELECT?paper?title WHERE {?paper a orkgc:Paper; orkgp:P31 [ orkgp:P2005 [ rdfs:label \"DBLP\"^^xsd:string ] ]; rdfs:label?title. } ORDER BY?title'}], [{'generated_text': 'The highest reported Accuracy score on the WOS-46985 dataset is 0.954. The corresponding metric is ROC_AUC.'}], [{'generated_text': 'SELECT DISTINCT?title WHERE {?paper a orkgc:Paper; orkgp:P31 [ orkgp:P2005 [ rdfs:label \"DBLP\"^^xsd:string ] ]; rdfs:label?title. } ORDER BY?title'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SemEval 2013\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'List the title and ID of research papers that contain a benchmark over the SciFACT dataset?\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciFACT\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nCan you provide the highest benchmark result, including the metric and score, for the SciERC dataset?\\nSELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_'}], [{'generated_text': '0.5 kg'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"LAMBADA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'https://dl.dropboxusercontent.com/u/15479050/images/DQN-pixelCNN-Papers.pdf\\nhttps://dl.dropboxusercontent.com/u/15479050/images/DQN-pixelCNN-Code.pdf'}], [{'generated_text': 'The following models have been benchmarked on the Atari 2600 Freeway dataset:\\n- Transformer-based One-Shot NAS (Neural Architecture Search) model\\n- Neural Architectural Search network (DBLP:journals/corr/abs-1903-10147)\\n- Doubly Supervised Domain-adversarial Network (TAC-DSN)\\n- Long Short-term Memory Recurrent Neural Network (RNNLM)\\n- Attention-based Neural Architecture Search (A*Net)\\n- Progjective-CNN-AA'}], [{'generated_text': '<paper_link><paper_link_id>code</paper_link_id></paper_link>'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The top benchmark score and its metric on the Atari 2600 Tennis dataset is 90.0.'}], [{'generated_text': 'The top-performing model for question answering on the DuIE dataset was DIAW-RN (ResNeXten), which achieves an F1 score of 0.76. \\n\\nThe paper that presented this model, \"Reverting Neural Attentiveness from One Image to Another\" by Chen et al., authored by Xinwen He, Zhaoxia Chen, Xiaoou Tang, Ting Lin, Lu Zhu, Ping Lu, Hao Wang, is available here: https://github.com/sparql-query/paper-topics/blob/master/2020/Reverting%20Neural%20Attentiveness.pdf'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cheetah, run (DMControl500k)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reuters RCV1/RCV2 English-to-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nSELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"'}], [{'generated_text': 'Dendritic Polymers, Liposomes, Solid Lipid Nanoparticles, Nanocapsules, Nanogels, Polymeric Micelles, and Nanoshells'}], [{'generated_text': 'FusionNet (single model)'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MNIST\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': '* SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Unpermuted Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential MNIST\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciTLDR\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The metrics that are commonly used to evaluate models on the CommonsenseQA benchmark dataset include:\\n- Accuracy\\n- F1 score\\n- R2 score\\n- AUROC\\n- AUPRC'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"iNaturalist 2019\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?model?model_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts\\' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MultiNLI\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\nSELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MultiNLI\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTION'}], [{'generated_text': 'md5\\nShannon entropy\\n perplexity'}], [{'generated_text': 'The highest benchmark result for the Sequential MNIST dataset is 71.5% unpermuted accuracy, using the AttentionOCR_Inception-resnet-v2_Location model.'}], [{'generated_text': \"Ade Corpus: ADE_Corpus\\nAbstracts' entities and relations annotated corpus: ADE_Abstracts\\nPenn Treebank (Word Level): PTB\\nTest perplexity: TestPerplexity\\nResults from model: \\nPerplexity: TestPerplexity\\nNGram: NGram\\nConvolutional Neural Network: ConvolutionalNet\\nSkip-Gram: Skipgram\\nLinguistically-Aware Sentiment Classification: LASeCap\\nSentiment analysis: SentAnal\"}], [{'generated_text': 'F1 = 0.988'}], [{'generated_text': 'F1'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': '1. VGG: 19.2 FLOPS\\n2. Inception: 26.8 FLOPS\\n3. ResNet: 28.7 FLOPS'}], [{'generated_text': 'RotoWire'}], [{'generated_text': 'The best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric is DEEPER.'}], [{'generated_text': 'M. Y. Choi, S. Seyedi, S. Madden, B. Ghanbari, and N. Pappas. \"Onboard methods for fast and lightweight object detection using instance segmentation and detection restitution.\" arXiv preprint arXiv:1907.03181 (2019).\\nM. Y. Choi, B. Ghanbari, and S. Madden. \"Learning monocular depth with shape constancy for real-time and massively scalable panning motion estimation.\" arXiv preprint arXiv:1911.09002 (2019).'}], [{'generated_text': 'SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DQNMCe+SR\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The COPA benchmark dataset has achieved the highest Score of 34.4261 on the Ball in cup, catch (DMControl500k) dataset.'}], [{'generated_text': 'The top performing model on the NYT-single dataset is FusionNet (single model).'}], [{'generated_text': '[\\n  {\\n    \"repo\": \"P31\",\\n    \"scenario\": \"Cartpole, swingup (DMControl100k)\",\\n    \"simulation_parameters\": {\\n      \"model\": \"Cartpole, swingup (DMControl100k)\",\\n      \"data\": \"https://www.dropbox.com/s/viipcuiua21za89/data_seel.cse.lsu.edu.zip\",\\n      \"type\": \"Technical\"\\n    }\\n  },\\n  {\\n    \"repo\": \"P39010\",\\n    \"scenario\": \"Cartpole, swingup (DMControl100k)\",\\n    \"simulation_parameters\": {\\n      \"model\": \"Cartpole, swingup (DMControl100k)\",\\n      \"data\": \"https://www.dropbox.com/s/viipcuiua21za89/data_seel.cse.lsu.edu.zip\",\\n      \"type\": \"Technical\"\\n    }\\n  },\\n  {\\n    \"repo\": \"P4077\",\\n    \"scenario\": \"\",\\n    \"simulation_parameters\": {'}], [{'generated_text': '-- Provide a list of papers that have utilized the MMV TSM-50x2 model and include the links to their code?\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MMV TSM-50x2\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P35?cont; rdfs:label?paper_lbl. }\\n-- Provide a list of research paper titles and IDs that have benchmarked models on the ESC-50 dataset?\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ESC-50\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL'}], [{'generated_text': 'The custom RNN model Five Base + Five HiRes with the 95th percentile on the CIFAR-10 dataset has achieved the highest Accuracy score on the Yelp-5 benchmark dataset. The model uses the following metrics to evaluate the accuracy of the predictions: top-1, top-5, Mean_Dissimilarity, log-loss.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'Provide a list of papers that have utilized the SARSA-ε model and include the links to their code?\\nselect distinct?code where {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Sarsa-ε\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'Provide a list of research paper titles and IDs that have benchmarked models on the TDMSci dataset?\\nselect distinct?paper?paper_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDMSci\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'The following code references are present in the papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes:\\n\\n* https://github.com/ageitgee/Ageitgee-EMNLP-2020-demo/blob/master/src/main/resources/pretrained_model/ade_corpus_pretrained_model.pbtxt\\n* https://github.com/clab/Adaptive-Window-Decoding-for-LSTMs-with-Dynamic-Evaluation-Jiaogang-Liu/blob/master/demo/src/main/resources/pretrained_model/ade_corpus_pretrained_model.pbtxt'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Assault\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"200k Short Texts for Humor Detection\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DBLP\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } ORDER BY?paper'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Number of params\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"enwik8\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nName (English text): enwiki8'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nThis gives the following output:\\n|         |                 |          |               |\\n|     F1    |     model          |     model_lbl    |\\n|  0.494667|    BERT             |     SciBERT'}]]\n","0 2\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 8\n","1 8\n","2 8\n","3 8\n","0 9\n","0 10\n","1 10\n","2 10\n","3 10\n","4 10\n","5 10\n","6 10\n","7 10\n","8 10\n","9 10\n","0 12\n","1 12\n","2 12\n","3 12\n","4 12\n","5 12\n","6 12\n","0 13\n","0 16\n","1 16\n","2 16\n","3 16\n","4 16\n","5 16\n","6 16\n","7 16\n","8 16\n","9 16\n","0 17\n","0 21\n","1 21\n","2 21\n","3 21\n","4 21\n","5 21\n","0 25\n","1 25\n","2 25\n","3 25\n","0 26\n","0 27\n","0 28\n","0 29\n","1 29\n","2 29\n","3 29\n","4 29\n","5 29\n","6 29\n","7 29\n","8 29\n","9 29\n","0 31\n","1 31\n","2 31\n","3 31\n","4 31\n","5 31\n","6 31\n","7 31\n","8 31\n","9 31\n","0 32\n","1 32\n","2 32\n","3 32\n","4 32\n","5 32\n","6 32\n","0 33\n","0 34\n","1 34\n","2 34\n","3 34\n","0 36\n","1 36\n","2 36\n","3 36\n","4 36\n","5 36\n","0 37\n","0 38\n","1 38\n","0 40\n","1 40\n","2 40\n","3 40\n","4 40\n","5 40\n","0 42\n","1 42\n","2 42\n","0 43\n","0 44\n","18.181818181818183%  [[{'generated_text': 'Natural Questions'}], [{'generated_text': 'The common evaluation metrics used for evaluating models on the SQuAD2.0 dataset are:\\n- Maximum Parsimony (MP)\\n- Classification Error Rate (CER)\\n- F1 score\\n- Exponential Weighted Average Precision (EWAP)'}], [{'generated_text': 'Common evaluation metrics used on the WMT2014 English-German dataset include:\\n\\n- BLEU score\\n- ROUGE-1, ROUGE-2\\n- METEOR'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Reading Comprehension\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \\n\\n{\\n ?paper o{\\n    o{\\n      o{\\n        o{\\n          o{\\n            o{\\n              o{\\n                o{\\n                 ?rf a orkgc:ResearchField;\\n                }?paper_1\\n              }?paper_2\\n            }?paper_3\\n          }?paper_4\\n        }?paper_5\\n      }?paper_6\\n    }?paper_7\\n  }?paper_8\\n}'}], [{'generated_text': 'The Bidirectional Long Short-Term Memory with Fusedchar Convolution model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset'}], [{'generated_text': 'Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WNLI\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WebNLG\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nProvide'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Ohsumed\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'List the code links in papers that use the LeViT-128 model in any benchmark?\\nProvide a list of benchmarked datasets related to the Neural Architecture Search research area?'}], [{'generated_text': 'method  |  variables\\n-------------+--------------------------\\nKOSMIK     |  population, temperature\\nSUN          |  pressure, temperature\\nSRES        |  humidity, pressure\\nAIRS        |  pressure, temperature\\nQBOAM       |  pressure, temperature'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDMSci\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DDI\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PIQA\")'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Text8\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'Sure, I can provide links to the code used in papers that benchmark the Rational DQN Average model. The Rational DQN Average model uses code from the open source QNLI model repository provided by ORKG.\\n\\nThe papers that benchmark the Rational DQN Average model include:\\n\\n- Abujudeh et al. \"Autopilot: End-to-End Learning of Time Series Prediction and Control on Hardware\" (2019). Code used: https://github.com/datarobot/autopilot'}], [{'generated_text': 'The name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset is Zhao et al. (2015) (auto-encoder)'}], [{'generated_text': '1-of-100 Accuracy'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WNLI\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nSELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WLPC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_B'}], [{'generated_text': 'the QA-GNN model'}], [{'generated_text': 'Raman spectroscopy is a light based analytical tool for structural and chemical analysis of matter.'}], [{'generated_text': \"The top benchmark result (metric and value) over the dataset TDMSci is:\\nPARAMS 0.2309511522942395\\n\\nWith this information it's easy to see that the SparkR Benchmark Model is performing the best.\"}], [{'generated_text': 'The top performing model on the CIFAR-10 dataset is CitClus. The corresponding metric is Accuracy (%).'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SemEval-2010 Task 8\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nSELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label'}], [{'generated_text': \"The metrics used to evaluate models on the AG News dataset are the following: \\n  \\nError\\nMetricError\\nRMSE\\nR2\\nCohen's Kappa\\n\\nPlease note that a paper may use different metrics, depending on the models that were used in the benchmark.\"}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NCBI Disease\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Provide a list of papers that have benchmarked models on the NCBI-disease dataset?\\n# Please provide a list of papers that have benchmarked models on the NCBI-disease dataset?\\n# Please provide a list of papers that have benchmarked models on the NCBI-disease dataset?'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GCNN-8\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'OfKGp:HAS_DATASET CHEMDNER.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CUB-200-2011\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nSELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SEEL\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTION'}], [{'generated_text': '# each line represents a paper\\n# the code column holds the ORKG ID of the GitHub repository containing the code\\n# the benchmark column holds the name of the benchmark dataset the code was evaluated on\\n# the dataset column holds the name of the datasets the code was evaluated on \\n# the evaluation column holds the name of the evaluation problem the code was trained on\\n# the evaluation method column holds the name of the evaluation method that was used by the code\\n# the code contributors column holds the names of the contributors who developed the code \\n# the model column holds the name of the model trained on the dataset\\n\\n# the dataset column holds the name of the datasets the code was evaluated on \\n# the benchmark column holds the name of the benchmark dataset the code was evaluated on \\n# the evaluation column holds the name of the evaluation problem the code was trained on\\n# the evaluation method column holds the name of the evaluation method that was used by the code\\n# the code contributors column holds the names of the contributors who developed the code \\n# the model column holds the name of the model trained on the dataset\\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_l'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DocRED (Human-annotated)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\nORDER BY DESC(?value)'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Solaris\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Here are the paperIDs and paper titles for the papers that have performed benchmarks on the MultiNLI dataset:\\n\\n* Ponto, S., Lample, C.N., & LuaRNA, B. (2018). Formal Evaluations of Sequence-to-Sequence Grammars. In International Conference on Empirical Methods in Natural Language Processing.\\n\\n* Ponto, S., Lample, C.N., & LuaRNA, B. (2019). Query Generation and Grammar Performance Analysis. In International Conference on Language Resources and Evaluation.\\n\\n* Quirk, M. (2020). BERT: Pre-trained Language Understanding for Sports Queries. CoRR, abs/2004.09554.\\n\\n* Mitchell, H. (2020). Universal Question Answering with Pre-trained Language Models. CoRR, abs/2004.11580.\\n\\n* Nikolov, D., Mehdad, M., Narayanan, K., et al. (2020). Language Models for Sentence-Level Question Answering. CoRR, abs/2002.07173.'}], [{'generated_text': 'I could not find any paper that has benchmarked the MMV model for NLP tasks. However, you can find the source code of the model at https://github.com/openmind/MMV-BERT and https://github.com/openmind/MMV and the model configuration files at https://github.com/openmind/mmv/blob/master/mmv/models/bert/config.md. In addition, the model is also listed in https://github.com/openmind/mmv/blob/master/mmv/models/ - you can browse through that list to find similar models.'}], [{'generated_text': 'The title and ID of the research papers that include a benchmark for the Atari 2600 Centipede dataset are:\\n\\n* P31: Thomas Dietterich, Eric Price, William Pugh, \"How to Benchmark Machine Learning Algorithms on Atari Games,\" https://arxiv.org/abs/1604.00776\\n* P32: Thomas Dietterich, Eric Price, William Pugh, \"What Every Computer Scientist Should Know About Training Artificial Neural Networks,\" https://www.cs.cmu.edu/Groups/ ICML/documents/neuralnets.pdf'}], [{'generated_text': 'The GR-ConvNet model is the best performing model on the Sequential MNIST benchmark dataset. The GR-ConvNet model achieves a Percentage error score of 1.59%. The code of the model can be found here: https://github.com/XLTechnology/GR-ConvNet'}], [{'generated_text': 'The research paper about geopolitics that conducted the research on the largest number of participants was conducted in the United States of America.'}], [{'generated_text': 'Based on your comments, here is the list of research paper titles and IDs that have benchmarked models on the WLPC dataset:\\n\\n- \"SemEval-2020- Subtitle Dataset: A Large-Scale Web-Based Open Data Set for Subtitle Analysis\", Vincent et al. \\n- \"SemEval-2020- Task 2: Negative Sampling, Subtitle Analysis in the Wild\", Vincent et al. \\n- \"SemEval-2020- Task 1: Document-level Sentiment Analysis with Negative Sampling\", \\n- \"MCS-20: Multi-Label Classifier Selection for Semantic Understanding\", Meng et al. \\n- \"Effective Representation Learning of Document-level Sentiment in Semantic Sentence Composition\", Lei et al. \\n- \"SemEval-2020- Task 1: Document-level Sentiment Analysis with Negative Sampling\", Kudo et al. \\n- \"SemEval-2020- Task 1: Document-level Sentiment Analysis with Negative Sampling\", Paulus et al. \\n- \"Non-local Neural Language Model for Document-level Semantic Sentiment Analysis\", Liu et al. \\n- \"Noisy Low-Resource Semantic Sentiment Analysis'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TempEval-3\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl \\n  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': 'The model that performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset was the SPARQL query optimization model presented in the paper \"Improving Query Answering with Optimized Constraints via Model Ensemble Learning\" by Qiang Yang, Wenjun Zhang, Jingchun Hu, and Xiao-Jie Bi. The best F1 score was 0.718.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\nList the code links in papers that use the cross-sentence model in any benchmark?\\n\\nSELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"cross-sentence\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nList the code links in papers that use the cross-sentence model in any benchmark?\\n\\nrov submission, Margaritis'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp-14\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n   ?benchmark orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n   ?benchmark orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\nOptimized implementation for deep'}], [{'generated_text': 'Model               model_lbl\\n-----------         --------------\\nResNet50            RTE\\nX-Net                RTE\\nVGG-MNAS             RTE\\nDART-MS               RTE\\nAWS-QNN              RTE\\nSNAPSHOT-LS          RTE\\nMS-Capsule           RTE\\nX-101                RTE\\nPathNet-PFS          RTE\\nNAS-MetaFlow        RTE\\nGA-X-Net            RTE\\nBASelines-Random    Baseline : Random\\nPNAS-net             RTE'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WOS-5736\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'List the code links in papers that use the entity and relations table model model in any benchmark\\n\\njdbc:firebirds://localhost: AnalogModels;model code - http://seel.cse.lsu.edu/data/refsq17.zip\\nList the metrics that are used to evaluate models on the Pubmed benchmark dataset\\n\\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Pubmed\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'Parameters\\n\\nोkgc:Metric\\n\\nFortunately, Neural Architecture Search (NAS) is a work in progress. The comparison of different NAS architectures and the evaluation of the models is a long-term research problem. So, the most suitable metrics for a given NAS model are hard to identify, and these metrics should be able to measure the model performance in terms of efficiency and flexibility.\\n\\nIn the following paragraphs, we consider two types of metrics: metrics that can be evaluated for any model, regardless of the underlying approach, and metrics that are particularly suitable for NAS. We provide some example papers where the proposed metrics are evaluated.\\n\\nThe comparison of different NAS architectures and the evaluation of the models is a long-term research problem. So, the most suitable metrics for a given NAS model are hard to identify, and these metrics should be able to measure the model performance in terms of efficiency and flexibility.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1 (Short)\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Natural Questions\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The highest benchmark result, including the metric and score, for the WMT2014 French-English dataset is 85.5.'}], [{'generated_text': 'The best performing model in terms of Test perplexity score on the WikiText-2 benchmark dataset is the transformer architecture used in the paper \"Self-Attention Positional encoding of Synonyms and antonymy in Neural Language Models\" by Moses Orbach, Amir Hessel, Hava op de hoef, Oriol Vinyals, Quang N. Tran.\\nThe paper\\'s code link is:\\nhttps://github.com/openai/transformer-base-knowledge- distillation.\\nThe paper\\'s scores can be found in the following RERL triple:\\nPerplexity = 0.769\\nModel: Moses Orbach, Amir Hessel, Hava Op de Hoef, Oriol Vinyals, Quang Nguyen Tran.\\nSource code: https://github.com/openai/transformer-base-knowledge- distillation.'}]]\n","0 0\n","1 0\n","2 0\n","0 1\n","1 1\n","2 1\n","3 1\n","4 1\n","5 1\n","0 2\n","0 4\n","1 4\n","2 4\n","3 4\n","4 4\n","5 4\n","6 4\n","7 4\n","0 7\n","0 8\n","1 8\n","2 8\n","3 8\n","4 8\n","5 8\n","6 8\n","7 8\n","8 8\n","9 8\n","0 11\n","0 12\n","1 12\n","2 12\n","3 12\n","4 12\n","5 12\n","6 12\n","7 12\n","0 13\n","1 13\n","2 13\n","3 13\n","4 13\n","5 13\n","6 13\n","7 13\n","8 13\n","9 13\n","0 15\n","1 15\n","2 15\n","3 15\n","4 15\n","0 16\n","1 16\n","2 16\n","3 16\n","4 16\n","5 16\n","6 16\n","7 16\n","8 16\n","9 16\n","0 17\n","0 18\n","0 20\n","1 20\n","2 20\n","0 22\n","0 24\n","1 24\n","2 24\n","3 24\n","4 24\n","5 24\n","6 24\n","7 24\n","8 24\n","9 24\n","0 31\n","0 32\n","0 33\n","0 34\n","1 34\n","2 34\n","3 34\n","4 34\n","5 34\n","6 34\n","7 34\n","8 34\n","9 34\n","0 35\n","1 35\n","2 35\n","3 35\n","4 35\n","5 35\n","0 36\n","0 39\n","1 39\n","0 43\n","1 43\n","2 43\n","3 43\n","4 43\n","0 46\n","1 46\n","2 46\n","3 46\n","4 46\n","5 46\n","0 48\n","1 48\n","2 48\n","0 49\n","1 49\n","2 49\n","3 49\n","27.272727272727273%  [[{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SemEval-2018 Task 7 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\nThe code link references for papers that use the All-attention network - 18 layers model for benchmarking purposes are:\\n\\n@inproceedings{Sagonas2017,\\n  author = {Sagonas, Odysseas and Zemel, Aram and Graves, Alex and Parag, Ashish and Yang, Xiangyu and Polosukhin, Illia and Gidaris, Nikos and Laine, Kai and Uszkoreit, Jakob and Sutskever, Ilya and Goodfellow, Ian and Kavukcuoglu, Numadhar and Databar, John and Jouppi, Sam and Krizan'}], [{'generated_text': 'The paper uses Cosine similarity to evaluate the disambiguation performance.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Pearson Correlation\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STS Benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Freeway\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\n+--------+---------+\\n| model_lbl | score    |\\n+--------+---------+\\n| OverFeat - 7 accurate models |\\n+--------+---------+'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DCASE\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'These are the evaluation metrics that are commonly used when benchmarking models on the Yelp Fine-grained classification dataset:\\n -  Average precision (AP)\\n -  F1-score\\n -  Area under the curve (AUC)\\n -  Precision at 1 (P1)\\n -  Recall at 1 (R1)\\n -  Normalized discounted cumulative gain (NDCG)\\n -  Rank-precision (PR)\\n -  ROUGE-L'}], [{'generated_text': 'The Neural cache model (size = 2,000)'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ADE Corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?benchmark orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nOPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': \"The Performer model performed best on the MedSTS benchmark dataset. The Performer model's Pearson Correlation value was 0.978.\"}], [{'generated_text': 'The paper titled \"WSC: A Large-Scale Semantic Web-Scale Dataset for Text Classification\" and its ID is: \\nP31\\nThe paper title and ID contains the word \"WSC\" so I will assume that the paper includes a benchmark for the WSC dataset.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\n--Sparql execution plan:\\n--Filter (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")\\n--Project\\n--  - Filter (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")\\n--  - Project\\n--    - Filter (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")\\n--    - Project\\n--      - Filter (str(?dataset_lbl'}], [{'generated_text': 'The code used in these papers that benchmark the DrQA model are:\\nhttps://github.com/createmeta/model-eval/blob/master/examples/dokka-clinical-task-classification.py\\nhttps://github.com/createmeta/model-eval/blob/master/examples/dokka-coqa-tasks.py\\nhttps://github.com/createmeta/model-eval/blob/master/examples/dokka-mrpcrd-tasks.py\\nhttps://github.com/createmeta/model-eval/blob/master/examples/dokka-mrpcrd.py\\nhttps://github.com/createmeta/model-eval/blob/master/examples/dokka-mrpcrd.ini\\nhttps://github.com/createmeta/model-eval/blob/master/examples/dokka-mrpcrd-config.ini\\nhttps://github.com/createmeta/model-eval/blob/master/examples/dokka-mrpcrd-connection.ini\\nhttps://github.com/createmeta/model-eval/blob/master/examples/dok'}], [{'generated_text': 'The model FS-LSTM-4 has achieved the highest Top 5 Accuracy score on the SST-5 Fine-grained classification benchmark dataset.'}], [{'generated_text': 'The most commonly used metrics to evaluate a model on the ARC-PDN dataset are:\\n- Parameters: It measures the number of parameters (number of fully connected layers) of a model. The lower the number, the better the model.\\n- FLOPs: It measures the floating-point operations per second of a model. Higher FLOPs means higher performance.\\n- Support: It measures the fraction of the training data the model saw during training. Lower support indicates better generalization.\\n- R-Squared: It measures the association between the model predictions and actual targets. Higher r-squared indicates better performance.\\n- AUC: It measures the Area Under the ROC (receiver operating characteristics) Curve.\\n- Top1, Top5, Average: It measures the best, top 5, and average scores of the validation set.\\nOther metrics commonly used for benchmarking models on the ARC-PDN dataset include:\\n- Training Loss: It measures the amount of loss (Euclidean distance) the model incurred on the training set.\\n- Test Loss: It measures the amount of loss the model incurred on the test set.\\n- Test Accuracy: It measures the accuracy of the model on the test set.\\nThe'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl \\n            (MAX(?value) AS?score) \\nWHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Quora Question Pairs\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Common evaluation metrics for models on the Atari 2600 Berzerk dataset include:\\n - Score: average score of the ball against each level \\n- Accuracy: proportion of the shots that hit the target\\n- Mean Average precision: Mean average precision of the predicted results against the real results \\n- Mean Average Recall: Mean average recall of the predicted results against the real results\\n- MAP: mean average precision against the ground truth \\nMore details are available at https://machinelearning.au.santander.es/atari/dataset/ atari2600berzerk/'}], [{'generated_text': 'Results for subquery:\\n     paper    paper_lbl\\n------------------\\n    @inproceedings \"jordan\"\\n    @article{hinton2006alexa,\\n        author = \"Hinton, Geoffrey\",\\n        doi = \"10.1162/08997660560620004\",\\n        isbn = \"97808997660565\",\\n        language = \"en\",\\n        number = \"60\",\\n        pages = \"57--67\",\\n        publisher = \"Addison-Wesley\",\\n        series = \"Computational Intelligence: Models, Algorithms, and Application\",\\n        title = \"AlexNet: A Large-Scale Hierarchical Neural Network for Image Recognition\",\\n        url = \"http://papers.nips.cc/paper/4824-alexnet-a-large-scale-hierarchical-neural-network-for-image-recognition.pdf\",\\n    }\\n    @inproceedings \"simonyan2014very\"\\n    @inproceedings{grosse2014net,\\n        author = \"Simonyan, Karen\",\\n        doi = \"10.1162/ repertoires_a_roberta'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE { \\n ?dataset a orkgc:Dataset; \\n  rdfs:label?dataset_lbl. \\n  FILTER (str(?dataset_lbl) = \"BUCC German-to-English\") \\n ?benchmark orkgp:HAS_DATASET?dataset; \\n  orkgp:HAS_EVALUATION?eval. \\n  OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \\n}'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CIFAR-10\") { SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Stanford Cars\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ShARe/CLEF eHealth corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'Sandwich panel material'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\nSELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:'}], [{'generated_text': 'The top metric that is often benchmarked is Score. \\nThe best performing model on the Atari 2600 Fishing Derby dataset has been identified as RUDDER with a score of 0.9.'}], [{'generated_text': '* ZFNet (ensemble, 6 convnets)'}], [{'generated_text': 'SAN-MTC-BZ: <https://arxiv.org/abs/2005.11336> (Neural Architecture Search (NARX) model)\\nTRADES: <https://arxiv.org/abs/2004.08740> (Trained linear interpolation network (TLIN) model)'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ScienceCite\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'The highest benchmark result achieved on the Atari 2600 Space Invaders dataset, including the metric and its value, is 711.\\nThe Neural cache model (size = 2,000)'}], [{'generated_text': 'Links to code used in papers that benchmark the NASCell model: https://github.com/opencv/opencv/tree/master/modules/contrib/pascal_py, https://github.com/fawziyya/neural-architecture-search'}], [{'generated_text': 'Provide a list of research paper titles and IDs that have benchmarked models on the TDMSci dataset?\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TDMSci\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT \\n  DISTINCT?metric?metric_lbl (MAX(?value) AS?score) \\nWHERE { \\n  { \\n    SELECT?metric?metric_lbl?value \\n    WHERE { \\n     ?dataset a orkgc:Dataset; \\n          rdfs:label?dataset_lbl. \\n          FILTER (str(?dataset_lbl) = \"BUCC French-to-English\") \\n     ?benchmark orkgp:HAS_DATASET?dataset; \\n          orkgp:HAS_EVALUATION?eval. \\n          OPTIONAL {?eval orkgp:HAS_METRIC?metric. \\n                     ?metric rdfs:label?metric_lbl. \\n          } \\n    } \\n    ORDER BY DESC(?value) \\n  } \\n  GROUP BY?metric?metric_lbl\\n}'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD2.0 dev\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Top Result: BiLSTM + char CNN + CRF  F1 entity level 57.0'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Story Cloze Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Juan Carlos Calzada'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DuIE\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'he objective of the ASTRID reactor is to perform multichannel experiments under H2/air, with low power consumption and sufficient temperature control, allowing for high throughput and ease of use. The ASTRID reactor uses Helium as the working fluid.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Berzerk\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Skiing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The most commonly used benchmark datasets for the Text Summarization research field are BMUTT and CNN.'}], [{'generated_text': 'The top benchmark result (metric and value) over the dataset Atari 2600 Tutankham is:\\n- Score: 18.525\\n- SemGraphQA: 0.75\\nThe best performing model benchmarking the Atari 2600 Atlantis dataset in terms of Score metric is:\\n- SemGraphQA: 0.75'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Tutankham\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The PROTEINS dataset model used in this challenge, based on the Reuters transactional 15 news corpus, is used in the paper: https://arxiv.org/pdf/1710.06515.pdf'}], [{'generated_text': 'The model that has the best F1 score on the CoNLL 2012 dataset is the BiLSTM + char CNN + CRF model. The corresponding reference is The QuASAR Models (P16), which can be found on the link below:\\nhttp://www.cs.cornell.edu/teams/acdis/Papers/QuASAR.pdf'}], [{'generated_text': 'The models that have been evaluated on the CommitmentBank benchmark dataset are the following:\\n+--+-------------+---------------+\\n| metric     | metric_lbl    |\\n+--+-------------+---------------+\\n| F1          | F1            |\\n+--+-------------+---------------+\\n| F1_L2       | F1@L2          |\\n+--+-------------+---------------+\\n| D0          | D0            |\\n+--+-------------+---------------+\\n| MDcd       | MDcd          |\\n+--+-------------+---------------+'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2004\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value.?eval orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': 'The top benchmark score and its metric on the Nottingham dataset is 0.96 with a model called  \"Baseline : Extractive Oracle\".'}], [{'generated_text': 'The highest benchmark result achieved on the STEM-ECR v1.0 dataset is 3.2531407 in terms of Score metric.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Matched\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MultiNLI\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The following models are being evaluated on the GAD dataset:\\n- GRU (Bai et al., 2018)\\n- LightGram\\n- XLNet\\n- BERT'}]]\n","0 1\n","1 1\n","0 5\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","0 8\n","1 8\n","2 8\n","3 8\n","0 9\n","0 11\n","0 12\n","0 13\n","1 13\n","2 13\n","3 13\n","0 15\n","1 15\n","2 15\n","3 15\n","4 15\n","5 15\n","6 15\n","7 15\n","8 15\n","9 15\n","0 16\n","1 16\n","0 20\n","1 20\n","2 20\n","3 20\n","4 20\n","5 20\n","6 20\n","7 20\n","8 20\n","9 20\n","0 22\n","0 23\n","1 23\n","0 24\n","1 24\n","0 26\n","0 27\n","0 31\n","1 31\n","0 33\n","0 35\n","1 35\n","2 35\n","3 35\n","4 35\n","5 35\n","6 35\n","7 35\n","8 35\n","9 35\n","0 38\n","1 38\n","0 39\n","1 39\n","2 39\n","0 41\n","0 42\n","1 42\n","0 43\n","1 43\n","2 43\n","0 46\n","1 46\n","2 46\n","3 46\n","4 46\n","5 46\n","6 46\n","7 46\n","0 47\n","1 47\n","2 47\n","3 47\n","4 47\n","0 49\n","1 49\n","2 49\n","36.36363636363637%  [[{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PubMed 20k RCT\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\\n\\nMost commonly used medical named entity recognition benchmark datasets are:\\n\\n* MedCALM\\n* Reuters RCV1\\n* Reuters RCV2\\n* RAVE\\n* UMD\\n* UMD-2\\n* UMD-2+\\n* UMD-23\\n* UMD-23+\\n* UMD-93\\n* UMD-93+\\n* UMD-MIA\\n* UMD-MIA+\\n\\nThe best performing model on the PubMed 20k RCT dataset is MedTLS (Multi-'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Habitat 2020 Point Nav test-std\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The models that have been benchmarked on the AAPD dataset are:\\n-    ACL-ARC\\n-    Transductive Dual-View  \\n-    Transductive Spectral\\n-    Deep Dual-View\\n-    Deep Spectral\\n-    A3C'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AxCell\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\nAnd here are a few example outputs:\\npaper    paper_lbl   metric    score\\n---------------------------------------------------------------------------------------------------------------------------------\\nYang \\xa0*\\taxcel-baseline\\tBeta\\t0.61248\\nShrivastava *\\taxcell-baselines\\tBeta\\t0.854922\\nAcharya, S. (2019). Model Zoo: A Survey of Architectural Components for Driving State-of-the-art Machine Learning. Singapore: University of Adelaide. https://pdfs.semanticscholar.org/3a4b/394580b37a74e6fa3b71aa9d986a04ebe4726.pdf'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ArcFace\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PubMedQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nSELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PubMedQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION'}], [{'generated_text': 'The most common benchmarks are:\\n - CIFAR-10 Image Classification\\n - ImageNet V2\\n - ImageNet ReaL'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Gibson PointGoal Navigation\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The highest scoring model on the Cheetah, run (DMControl500k) benchmark dataset is'}], [{'generated_text': 'The top performing model in terms of Accuracy (%) score when benchmarked on the Oxford-IIIT Pets dataset is the Prioritized Deep Q-Network.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Supervised:\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?model_lbl. } \\n\\n?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Supervised:\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P32?cont; rdfs:label?model_lbl. }'}], [{'generated_text': 'The model that performed best in terms of Test perplexity metric on the WikiText-103 dataset is COMET.'}], [{'generated_text': 'The highest BLEU score score achieved on the WMT2016 Russian-English benchmark dataset is 66.35. This score has been achieved by the Lord laboratory using the Iterative BackTranslation (IBT) model.'}], [{'generated_text': 'Ethanin'}], [{'generated_text': 'Neural Cache model (size = 2,000)'}], [{'generated_text': 'select distinct?code where \\n{\\n ?model a orkgc:Model\\n  orkgp:HAS_EVALUATION?eval\\n ?eval orkgp:HAS_EVALUATION?eval_lbl\\n ?eval_lbl rdfs:label?eval_lbl\\n ?benchmark orkgp:HAS_DATASET?dataset\\n ?dataset orkgp:Dataset; rdfs:label?dataset_lbl\\n ?cont orkgp:HAS_MODEL?model\\n  orkgp:HAS_SOURCE_CODE?code\\n}'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"fb15k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nThis query returns the following results:\\nThe highest benchmark result, including the metric and score, for the FB15k dataset is Parameters=3.2310e-04, Parameters score\\n\\n scoop\\n\\n circon inversionnalbe\\n\\n circon inversionnalbe\\n\\n circon inversionnalbe\\n\\n circon inversionnalbe\\n\\n circon inversionnalbe\\n\\n circon inversionnalbe\\n\\n circon inversionnalbe\\n\\n circon inversion'}], [{'generated_text': 'EMERGENCY'}], [{'generated_text': 'The metrics that are used to evaluate models on the CoQA benchmark dataset are:\\n\\nOverall: which assesses models in terms of their capacity to identify and grade textual, question and answer pairs from a large, unstructured collection of web pages.\\n\\nThe top performing model on the CoQA dataset is FusionNet (single model). It achieves an overall score of 41.89.\\n\\nFusionNet (single model) is a multi-stage end-to-end model for question answering on the CoQA dataset. The architecture is composed of the following components:\\n\\nA large Wikipedia knowledge base that contains high-quality factoids and article summaries for answering Common-Sense questions.\\nAn RNN language model for generating Question Representations (QRs).\\nA representation layer that maps the QR sequence to a fixed dimensional space using an attention mechanism to integrate information from the RNN and select important Qiwords.\\nAn LSTM question encoder that generates the final answer representation based on the question, the previous answer representations and a lexicon of pre-defined anwser vectors.\\n\\nThe model achieves its top performance on the SearchQA dataset because:\\n\\nIt is capable of extracting high quality and diverse factual summaries, through the use of'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"OntoNotes\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The benchmarks that are included in the papers that use the NAS Cell model are:\\n    * Atari 2600 Atlantis\\n    * Atari 2600 Defender'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DBLP\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Switch Transformer\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The following models have been evaluated on the CommonsenseQA dataset:\\n- Conversational Response Selection\\n- OpenBookQA'}], [{'generated_text': 'Common evaluation metrics used when benchmarking models on the Amazon-2 dataset include: \\n- Top-1 Error (1-the percentage of the test instances for which the model predicted the correct class label)\\n- Top-5 Error (5-the percentage of the test instances for which the model predicted the incorrect class label 5 times in a row)\\n- Cumulative Error (CE) (CE = (1-Top-1 Error) + (1-Top-5 Error) +... + (1-100%))'}], [{'generated_text': 'Below is a list of code links that have been used for the A3C-CTS model in the above mentioned benchmarks:\\n\\n* https://github.com/point-at-infinity/a3c\\n* https://github.com/point-at-infinity/a3c-cts'}], [{'generated_text': 'The top performing model on the Hutter Prize dataset had 38 parameters, that is an average of 7 parameters per model, and the top metric was MSE which measures the Mean Squared Error metric for regression models'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL 2003 (English)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Artificial Intelligence\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?problem orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'There are no code references in papers that have benchmarked the BiLSTM-Attention + ELMo model. However, the best result for the metric Macro F1 on the NLP-TDMS (Exp, arXiv only) benchmark dataset was 0.716.'}], [{'generated_text': 'The highest Pearson Correlation score on the STS Benchmark benchmark dataset is achieved by the CAIT-XXS-36 model.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nSELECT DISTINCT?title WHERE {?paper a orkgc:Paper; orkgp:P31 [ orkgp:P2005 [ rdfs:label \"DBLP\"^^xsd:string ] ]; rdfs:label?title. } ORDER BY?title'}], [{'generated_text': 'end-to-end relation extraction model'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MRPC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the CoNLL++ dataset?\\n\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL++\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'The Sparql query above retrieved the following results:\\n\\n?model - rdfs:label \"Mario Kart\"\\n?model_lbl - \"rdfs:label\"\\n?benchmark - orkgp:HAS_DATASET \"Atari 2600 Asteroids\"\\n?benchmark_lbl - \"Atari 2600 Asteroids\"\\n?metric - orkgp:HAS_METRIC \"Distance\"\\n?metric_lbl - \"Distance\"\\n?cont - orkgp:HAS_MODEL \"Mario Kart\"\\n?cont_lbl - \"Mario Kart\"\\n?paper - orkgp:P31 \"Atari 2600 Name This Game\"\\n?paper_lbl - \"Atari 2600 Name This Game\"\\n?eval - orkgp:HAS_EVALUATION \"R2+1D-18 on Audioset\"\\n?eval_lbl - \"R2+1D-18 on Audioset\"\\n?value - orkgp:HAS_VALUE \"2.636063\"\\n?model - rdfs:label \"Atari 2600 Asteroids\"\\n?model_lbl -'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 German-English\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \\n\\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IWSLT2014 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl'}], [{'generated_text': 'The QA-GNN model'}], [{'generated_text': 'The metrics of evaluation for the OntoNotes dataset are:\\n- F1\\n- R2'}], [{'generated_text': 'The following metrics have been used to benchmark the Atari 2600 Defender dataset:\\n\\n* Score\\n* Mins\\n* Maxs\\n* Mean (Average)'}], [{'generated_text': 'The highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value is BLEU 20.2.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WOS-11967\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'metrics: Number of params, Size of model, Top-1 accuracy, Top-5 accuracy, Number of unique parameters, Number of model parameters, Number of weights, Number of connections, Total params, Runtime, Wall-time'}], [{'generated_text': 'The commonly used evaluation metrics on the TempEval-3 dataset are temporal awareness and mean average precision (mAP).'}], [{'generated_text': 'SELECT?country\\n\\nkered to get the countries that were part of the five most common research fields'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-chemical\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HMDB51 (finetuned)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}]]\n","0 2\n","0 6\n","1 6\n","0 8\n","0 9\n","0 11\n","1 11\n","0 12\n","1 12\n","2 12\n","3 12\n","4 12\n","5 12\n","6 12\n","0 13\n","1 13\n","2 13\n","3 13\n","4 13\n","5 13\n","6 13\n","0 14\n","1 14\n","2 14\n","3 14\n","4 14\n","5 14\n","6 14\n","7 14\n","8 14\n","9 14\n","0 15\n","0 17\n","1 17\n","2 17\n","3 17\n","4 17\n","5 17\n","6 17\n","7 17\n","8 17\n","9 17\n","0 18\n","1 18\n","2 18\n","0 20\n","0 23\n","0 24\n","1 24\n","2 24\n","3 24\n","4 24\n","5 24\n","0 25\n","0 26\n","1 26\n","2 26\n","3 26\n","4 26\n","5 26\n","6 26\n","7 26\n","0 32\n","0 33\n","1 33\n","2 33\n","3 33\n","4 33\n","5 33\n","6 33\n","7 33\n","8 33\n","9 33\n","0 35\n","0 38\n","0 40\n","1 40\n","2 40\n","3 40\n","4 40\n","5 40\n","6 40\n","7 40\n","8 40\n","9 40\n","0 41\n","0 42\n","1 42\n","0 43\n","0 45\n","1 45\n","0 46\n","1 46\n","45.45454545454546%  [[{'generated_text': 'You have provided a list of models that have been evaluated on the AudioSet benchmark dataset:\\n\\nAudioset R2+1D-18 on AVID (Modified R2+1D-18 on Audioset)\\n\\tKMI-ADAD R2+1D-18\\n\\tAvnet Deep Spectrum\\n\\tScadaLab ViSi-Net\\n\\tBeat-Craft ViSi-Net\\n\\tDeepSpeech 2\\n\\tWARP\\n\\tMemisek DeepL\\n\\tDeepL\\n\\tMIND\\n\\tRefineNet\\n\\tSwitchNet\\n\\tMedDis\\n\\tFAUST\\n\\tSECON\\n\\tRBM'}], [{'generated_text': 'The model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset is the FS-LSTM-4 model.'}], [{'generated_text': 'The model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset was:\\nOverFeat - 7 accurate models\\nThe F1 metric is an accuracy metric computed using the area under the receiver operating characteristic curve (AUROC).'}], [{'generated_text': \"commonly used evaluation metrics for the Atari 2600 Skiing dataset are:\\n- absolute accuracy\\n- average accuracy\\n- kurtosis\\n- precision@k\\n- root mean square error (RMSE)\\n- the number of learning iterations (from the paper)\\n- specificity\\n- sensitivity\\n- symmetry\\n- variance\\n- Zero-shot accuracy\\n\\nNote that these metrics can be computed from the model's output or the target values of the benchmark.\\n\\nThe paper presenting the best model on the Atari 2600 Skiing dataset achieved an accuracy of 0.8.\"}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?problem a orkgc:Problem; rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Atari Games\")?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }'}], [{'generated_text': 'I am not aware of any papers that have benchmarked the COMET - Direct model. However, there are many papers that have benchmarked the COMET - DynaGen model. You can find a list of papers that have benchmarked the COMET - DynaGen model on the following page. https://www.kendryte.com/white-paper-benchmarking-sparc-hyper-parameter-tuning-tools-with-comet.'}], [{'generated_text': 'SELECT?model?model_lbl \\n    WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1 score\") {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC Russian-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n    {?model rdfs:label \"state of the art\"@en. ;\\n        ?model_lbl \"state of the art\"@en. } \\n\\nP31\\tstate of the art\\nP32\\tThe best performing model is'}], [{'generated_text': 'The neural cache model (size = 2,000) achieves the highest Score score on the Atari 2600 Battle Zone benchmark dataset with a value of 11,964.'}], [{'generated_text': 'The Random Forest model achieved the highest F1 score on the Penn Treebank benchmark dataset.'}], [{'generated_text': 'The best performing model is iteratively backtranslated with the SMT + iterative backtranslation (unsupervised) model from English to German and vice versa. The maximum BLEU score for this model is 68.34.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Frostbite\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The model that has achieved the highest Matched score on the MultiNLI benchmark dataset is the ATON model. The corresponding RNNBLST model, a multi-layer neural language model, achieves a Matched score of 54.388.'}], [{'generated_text': 'The best models evaluated on the ImageNet ReaL dataset are:\\n* SNAC\\n* GoogLeNet\\n* DenseNet-121\\n* DenseNet-161\\n* SqueezeNet\\n* MobileNet\\n* WideResNet-28-10\\n* Inception-v4\\n* Inception-v3\\n* Inception-i3\\n* ResNet-50\\n* Xception\\n* VGG-19'}], [{'generated_text': 'Enerex'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kinetics-600\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The metrics that are used to evaluate models on the Yelp-5 dataset are the following:\\n\\n-  precision:  the fraction of relevant reviews that are correctly identified as such\\n-  recall:  the fraction of relevant reviews that are correctly identified as such\\n-  F-measure: (precision x recall)/ (precision + recall)\\n-  AUPRC:  the area under the precision-recall curve.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Barabasi-Albert\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'The top benchmark score and its metric on the Natural Questions (short) dataset is 82.3\\nThe best performing model is orkgp:Model_With_LSTM_Attention_Networks with ID: P31\\nThe metric that are used to evaluate models on the Natural Questions (short) benchmark dataset is orkgc:Metric'}], [{'generated_text': 'Selecting the best model with the lowest error on the CIFAR-10 dataset is typically done via three metrics, namely classification error, regression error, and mean absolute error (MAE). The three most common evaluation metrics for a classifier on the CIFAR-10 dataset are classification error (the number of incorrectly classified examples)/(the total number of examples), mean absolute error (the average absolute error), and the regression coefficient of the linear least squares model applied to the training set.'}], [{'generated_text': 'The top score on the PubMedQA dataset was achieved by a model that uses multi-kernel principal component analysis for named entity recognition in PubMed abstracts with a 0.822 Accuracy score.'}], [{'generated_text': 'The datasets that have been used for benchmarking in the citation classification research problem are the Annotated development corpus dataset, the Scholarly entity usage detection (SUE) dataset, and the ScienceCite dataset.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"H-NLI\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'Here are the metrics that are used to evaluate models on the Pubmed dataset:\\n- Brier score\\n- K-Nearest Neighbours\\n- Log-loss\\n- Mean Absolute Deviation\\n- Mean Squared Error'}], [{'generated_text': 'P32    NCBI_BERT(base) (P)'}], [{'generated_text': 'The AWD-LSTM-MoS + dynamic eval model'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DDI\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nThat should do it, let me know if you have any questions.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"iNaturalist 2018\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The best performing model for the Atari 2600 Ice Hockey benchmark is AVID (Modified R2+1D-18 on Audioset)'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl (MAX(?value) AS?score) WHERE { { SELECT?paper?paper_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DBLP\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?score'}], [{'generated_text': 'The top-performing model on the Walker, walk (DMControl500k) dataset was a Performer model. \\nThe best performing metric for the Performer model on the Walker, walk (DMControl500k) dataset was Score.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Semantic Web\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\\n\\n?model?model_lbl\\n\\nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts\\' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkg'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WikiText-2\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n    WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WikiText-2\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODE'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PWC Leaderboards (restricted)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'The code links are available in the public repository: https://github.com/ducolina/DUcoliNAno\\nThe code models are available here: https://orcid.org/0000-0002-6654-7954'}], [{'generated_text': 'CIFAR-10 Image Classification\\nSST-5 Fine-grained classification'}], [{'generated_text': 'RE Macro F1'}], [{'generated_text': \"PARAMS\\n\\n<|endoftext|>Q:\\n\\nIntelliJ IDEA only highlights /** when I have a tag above it\\n\\nI'm sure I've seen this before, but I can't remember where. The symptom I'm seeing is that when I have a tag above a tag that is underlined in blue or underlined in red, the blue or red underline remains but the text beneath is now grayed out, but not highlighted. It is possible to highlight the entire block using the keyboard, but this feels very cumbersome.\\nIt doesn't happen everywhere, but most often in my codebase. I'm using IntelliJ IDEA 2016.2.4\\nIt's a problem because I frequently want to change the tag but don't want to have to go hunt down the underlined portion of the comment in order to make sure I haven't accidentally changed something.\\n\\nA:\\n\\nThis is one of the known issues with IntelliJ IDEA's highlighting. Please star the ticket here: https://youtrack.jetbrains.com/issue/IDEA-167931.\\nThere's also a feature request to improve the behavior, you can vote for it here: https://youtrack.jetbrains.com/issue\"}], [{'generated_text': 'The most permuted performing model on the CIFAR-10 dataset is a word BiLSTM + char CNN + CRF model.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STL-10, 1000 Labels\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciCite\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\n--SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciCite\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\n--SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset'}], [{'generated_text': 'The papers that have benchmarked RNN models on the Atari 2600 Skiing dataset are:\\n\\nAcer, P., Bacskó, G., Barto, P., Barany, Z., Leibo, M., Nenchev, D. and Ortiz, D. (2017), RankNet:\\xa0A Neural Architecture for Ranking and Indirect Relationships in Web Documents.  \\nChen, X. and Lv, J. (2016), Learned Semantic Corephrase Boundaries for Document Classification and Regression. \\nYao, C. and Bratman, M. (2018), Reading Atari Games on the Atari 2600:\\xa0A Critically Recognized Dataset.'}], [{'generated_text': 'The metrics that are used to evaluate models on the VTAB-1k benchmark dataset include:\\n- Parameters\\n- Entropy\\n- ReductionInVariance\\n- MeanSquaredError'}], [{'generated_text': \"The name of the top performing model in terms of Score when benchmarked on the Cart Pole (OpenAI Gym) dataset is 'SAN (single model)'.\"}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACL-ARC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n            WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': 'The most commonly used benchmark datasets for Medical Named Entity Recognition research field are ADE Dataset and ShARe/CLEF eHealth corpus. The best reported results are on the ADE Dataset.'}], [{'generated_text': 'SELECT DISTINCT?name WHERE { [ orkgp:P34 [ rdfs:label?name ] ]. FILTER(STRLEN(?name) > 0) } ORDER BY?name\\n\\nyzed'}], [{'generated_text': 'The best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric is the Neural cache model (size = 2,000)'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Dwarakan eta\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"SemEval 2007\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Classic\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The model used in the paper \"Comparing Energy System Models: A Model-Based Analysis\" by N. Dave, S. Shah, P. Lobo, D. Kabir, and A. Patrinos, ORKG (2020), and technical details (such as the specific model, platform, etc.) are available on https://doi.org/10.4225/41/47a5d3d74985. This is a sparql query to get the paper and associated details.'}]]\n","0 0\n","1 0\n","0 1\n","1 1\n","2 1\n","3 1\n","0 2\n","1 2\n","0 3\n","1 3\n","2 3\n","3 3\n","4 3\n","5 3\n","6 3\n","7 3\n","8 3\n","9 3\n","0 5\n","1 5\n","2 5\n","3 5\n","4 5\n","5 5\n","0 7\n","1 7\n","2 7\n","0 8\n","1 8\n","2 8\n","0 9\n","0 11\n","0 12\n","0 13\n","1 13\n","2 13\n","3 13\n","4 13\n","5 13\n","6 13\n","7 13\n","0 15\n","1 15\n","2 15\n","3 15\n","4 15\n","5 15\n","6 15\n","0 17\n","0 18\n","1 18\n","0 19\n","1 19\n","0 20\n","1 20\n","2 20\n","3 20\n","4 20\n","5 20\n","6 20\n","7 20\n","8 20\n","9 20\n","0 22\n","1 22\n","2 22\n","0 23\n","0 24\n","1 24\n","2 24\n","3 24\n","0 27\n","1 27\n","2 27\n","3 27\n","4 27\n","0 29\n","0 33\n","0 34\n","1 34\n","2 34\n","3 34\n","0 35\n","0 36\n","1 36\n","2 36\n","3 36\n","4 36\n","5 36\n","6 36\n","7 36\n","8 36\n","9 36\n","0 37\n","0 40\n","0 41\n","1 41\n","2 41\n","0 42\n","0 44\n","1 44\n","0 46\n","1 46\n","2 46\n","3 46\n","4 46\n","5 46\n","6 46\n","7 46\n","8 46\n","9 46\n","0 49\n","54.545454545454554%  [[{'generated_text': 'Rwanda'}], [{'generated_text': 'The model achieving the highest F1 score on the EBM-NLP benchmark dataset is BioSentVec (PubMed + MIMIC-III)'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Top-1 Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HMDB51\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\\nPREFIX wkp: <http://wapg.nal.usda.gov/ns/ontology/#>\\nPREFIX wkpp: <http://www.w3.org/ns/Publicpwg/AmidarData#>\\nPREFIX dat: <http://www.amindar.org/ontologies/amend2.0#>\\nPREFIX dataset: <http://www.amindar.org/ontologies/amend2.0#>\\nPREFIX problem: <http://www.amindar.org/ontologies/amend2.0#>\\nPREFIX benchmark: <http://www.amindar.org/ontologies/amend2.0#>\\nPREFIX eval: <http://www.amindar.org/ontologies/amend2.0#>\\nPREFIX model: <http://www.amindar.org/ontologies/amend2.0#>\\nPREFIX cont: <http://www.amindar.org/ontologies/amend2.0#>'}], [{'generated_text': 'The following metrics are used to evaluate models on the Rotowire (Content Selection) benchmark dataset:\\n\\n- Precision\\n- Recall\\n- F1 score'}], [{'generated_text': 'Sally Yang, Andrew Giusto, Jingwen Shao, Xiangyu Chen, Quanshi Hu, Kaiming Jin, Ali Ghodsi, and Xi Wang. \"The Weighted Tsetlin Machine: Accelerating the search for highly accurate NLP models with small GPU memory.\" arXiv preprint arXiv:1906.02483 (2019). https://arxiv.org/abs/1906.02483\\nAndrew Giusto, Jingwen Shao, Quanshi Hu, Kaiming Jin, Sally Yang, Ali Ghodsi, and Xi Wang. \"The Weighted Tsetlin Machine: Accelerating the search for highly accurate NLP models with small GPU memory.\" arXiv preprint arXiv:1906.02483 (2019). https://arxiv.org/abs/1906.02483\\nAndrew Giusto, Jingwen Shao, Quanshi Hu, Kaiming Jin, Sally Yang, Ali Ghodsi, and Xi Wang. \"The Weighted Tsetlin Machine: Accelerating the search for highly accurate NLP models with small GPU memory.\" arXiv preprint arXiv:1906.02483 (2019). https://arxiv.org/abs/19'}], [{'generated_text': 'The best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric is the Baseline : Extractive Oracle model with 0.994787 score.'}], [{'generated_text': 'The following evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset:\\n-Hit Rate\\n-Mean Average Precision\\n-Normalized Discounted Cumulative Gain\\n-R2\\n-Root Mean Square Error'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ScienceCite\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } }'}], [{'generated_text': 'The best model was the BERT-FS model based on the sequence error metric.'}], [{'generated_text': 'The top benchmark result (metric and value) over the dataset ARC-PDN is for \"Accuracy\":\\n{ \"metric\" : \"Accuracy\", \"score\" : 4.673519193452423 }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciREX\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nSELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciREX\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:'}], [{'generated_text': 'The metrics that are used to evaluate models on the TSE-NER dataset are:\\n- NER Macro F1\\n- F1\\n- Precision\\n- Recall\\n- F1-score'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Concept Mention Extraction\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nSELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Concept Mention Extraction\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ORKG-TDM\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'A3C LSTM hs paper:\\nhttps://openreview.net/record viewer/Re58sx9bTg?from=MAIN\\nRNN paper:\\nhttps://openreview.net/publication=history?id=rnn-model-rnn-lstm-han-in-nenkova-smiljana-siparov\\nAWD-LSTM paper:\\nhttps://openreview.net/publication=history?id=adv-lstm\\nNORML-LSTM paper:\\nhttps://openreview.net/thread/lztJQdZB/'}], [{'generated_text': 'The best model on the SVHN dataset is the SINGLE MODEL with a Percentage error of 4.94%'}], [{'generated_text': '{\\n  \"paper\" : {\\n    \"has_dataset\" : [ {\\n      \"has_benchmark\" : [ {\\n        \"label\" : \"HMDB51 (finetuned)\",\\n        \"has_evaluation\" : [ {\\n          \"has_value\" : true,\\n          \"has_metric\" : true,\\n          \"has_label\" : \"Top-1 Accuracy\"\\n        } ],\\n        \"has_model\" : true,\\n        \"has_code\" : true\\n      } ]\\n    } ],\\n    \"label\" : \"Neural Architecture Search\",\\n    \"has_code\" : true\\n  }\\n}'}], [{'generated_text': 'select distinct problem,dataset_lbl fromProblem where exists (\\n  select benchmark fromHasBenchmark where hasbenchmark =?dataset and benchmark@in (\\n    select * fromHasModel\\n  )\\n)\\n\\nthe datasets benchmarked under the SPARQL query optimization research problem are \\n* orkgc:Dataset\\n* rdfs:label \"Winograd Schema Challenge\"\\n* orkgp:HAS_DATASET orkgc:Dataset\\n* orkgp:HAS_EVALUATION eval\\n* orkgp:HAS_VALUE \"refsq17.zip\"\\n* orkgp:HAS_METRIC \"seel.cse.lsu.edu/data/refsq17.zip\"\\n* orkgp:HAS_MODEL orkgc:Model\\n* orkgp:HAS_MODEL \"seel.cse.lsu.edu/data/refsq17-model.zip\"\\n* orkgp:HAS_MODEL orkgc:Dataset\\n* orkgp:HAS_MODEL \"seel.cse.lsu.edu/data/refsq17-model.zip\"'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'the \"Temporal Convolutional Network\" model is one of the winning models of the \"First-place Team\" at the \"First-place [Worldwide] AI Challenge 2019\" organized by the \"Sixth Central Engine Chinese Committee\" in Wuzhen, China. \\nThe winning model achieved a top-1 accuracy of 99.91% and a top-5 accuracy of 75.73% on the \"Mini-ImageNet\" test set. \\nBenchmarking the \"Temporal Convolutional Network\" model on the \"Mini-ImageNet\" test set, we find that its memory overhead is 40.54%, training time is 9.22 GPU days, and search time is 0.67 GPU days. \\nTo better understand the specific contributions of each component, we break down the network into 3 components. The first is the time dimension component that applies 3 × 3 convolution operations to the time axis to extract spatial features of input images. The second is the channel dimension component that applies 3 × 3 convolution operations to the channel axis to extract high-level semantic features from the images. The final component is the fully connected (FC) layer, which is responsible for high-level semantic feature extraction, final classification, and'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'The code link for the WDec model used in the WNLI benchmark can be found here: https://github.com/UCB/wdec.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AAPD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'OverFeat - 7 accurate models'}], [{'generated_text': 'The best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric is the BiLSTM + char CNN + CRF model.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Walker, walk (DMControl500k)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'commonly used evaluation metrics for the the SciGEN dataset: F1, R2, ACC\\nexample code that was used in the reference paper that includes a benchmark for the SciGEN dataset:\\nhttps://github.com/andernordw/ner_eval/blob/master/example_benchmark.R\\nexample code for another research paper that benchmarked the SciGEN model:\\nhttps://github.com/lginn/docfilt/blob/master/docfilt.Rmd'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"pop3d.hemp.coreos.com\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'OverFeat - 7 accurate models'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UCF101 (finetuned)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MPQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. } \\n\\nDISTINCT?benchmark?benchmark_lbl (MAX(?value) AS?score) WHERE { { SELECT?benchmark?benchmark_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MPQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_'}], [{'generated_text': 'The highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value is F1 entity level score: 0.82723'}], [{'generated_text': 'The model that achieved the highest SUCCESS score on the Habitat 2020 Point Nav test-std benchmark dataset is AttentionOCR_Inception-resnet-v2_Location.'}], [{'generated_text': 'The highest benchmark result, including the metric and score, for the Paper Field dataset is F1=0.98.'}], [{'generated_text': 'The model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset was a pre-trained BERT model from Papers competitive against previous state-of-the-art, auto-encoders such as the well-known Deep Belief Networks (DBNs) and Convolutional Autoencoders.'}], [{'generated_text': 'The highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value, is 439.84.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Macro F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BiDAF + Self Attention + ELMo (ensemble)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'The best scored model on the Cheetah, run (DMControl500k) dataset was the Prioritized DQN model using the DQN method with the default hyperparameter values. The model achieved an average result of 13.4 with a 95th percentile of 27.7. The score metric used was Euclidean distance. The best metrics provided by the model were mean average precision, accuracy, and mean square error. The metrics provided by the best performing model can be found in the following list:\\n\\nmetric                    |  metric_label  | score\\n-----------------------------+----------------+---------\\nEuclidean distance         |  metric       | 13.4\\nmean average precision     |  metric       | 5.86\\naccuracy                    |  metric       | 89.9%\\nmean square error           |  metric       | 1.53'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STS Benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'The top score achieved on the Atari 2600 Alien dataset is 47.92. The model that achieved this highest score was the Extended Sparse Generative Model with VAE architecture.'}], [{'generated_text': 'The highest benchmark result achieved on the Birdsnap dataset, including the metric and its value, is 100.0%'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?model?model_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Chopper Command\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Stanford Cars\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'entity and relations table\\n\\nPrior work shows the entity and relations table model provides state of the art performance for relation classification using 12 entity types. It performs better than the current state of the art models based on several metrics including Precision, Recall and F1-Score'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?model?model_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD1.1 dev\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Large mLSTM\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\ninput (English text): List the metrics that are used to evaluate models on the MRPC benchmark dataset?\\n\\noutput (Sparql query): \\n\\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MRPC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\ninput (English text): Can you list benchmarked'}], [{'generated_text': 'The highest F1 score on the AAPD benchmark dataset is achieved by the cross-sentence ALB model.'}], [{'generated_text': 'Neural cache model (size = 2,000)'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NCBI-disease\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}]]\n","0 0\n","1 0\n","2 0\n","3 0\n","4 0\n","5 0\n","6 0\n","7 0\n","8 0\n","9 0\n","0 1\n","0 3\n","1 3\n","0 4\n","1 4\n","0 5\n","1 5\n","2 5\n","3 5\n","4 5\n","0 6\n","1 6\n","2 6\n","3 6\n","4 6\n","5 6\n","6 6\n","7 6\n","8 6\n","9 6\n","0 7\n","0 9\n","1 9\n","0 10\n","0 12\n","0 15\n","1 15\n","2 15\n","0 16\n","1 16\n","2 16\n","0 17\n","1 17\n","2 17\n","3 17\n","0 18\n","0 20\n","0 22\n","1 22\n","2 22\n","3 22\n","4 22\n","5 22\n","6 22\n","7 22\n","8 22\n","0 24\n","1 24\n","0 25\n","0 27\n","1 27\n","2 27\n","3 27\n","4 27\n","5 27\n","6 27\n","7 27\n","8 27\n","9 27\n","0 29\n","0 32\n","1 32\n","2 32\n","3 32\n","4 32\n","0 33\n","1 33\n","0 34\n","1 34\n","2 34\n","0 35\n","1 35\n","2 35\n","3 35\n","0 36\n","1 36\n","0 38\n","1 38\n","2 38\n","3 38\n","4 38\n","5 38\n","6 38\n","7 38\n","8 38\n","9 38\n","0 40\n","1 40\n","2 40\n","3 40\n","4 40\n","5 40\n","6 40\n","7 40\n","8 40\n","9 40\n","0 41\n","1 41\n","2 41\n","3 41\n","4 41\n","5 41\n","0 44\n","1 44\n","0 47\n","0 48\n","1 48\n","63.63636363636365%  [[{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Softcite\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nSELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciCite\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont or'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Fashion-MNIST\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\nSELECT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Fashion-MNIST\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"X-Sum\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\nORDER BY DESC(?value) \\n\\nLIMIT 1'}], [{'generated_text': 'title\\tPapers benchmarked on the RotoWire dataset\\nid\\t\\t*:P30, :P31, :P32\\npaper\\t:P30, :P31, :P32\\npaper_lbl\\tRotoWire (Relation Generation)\\ndataset\\tRotoWire\\ndataset_lbl\\tRotoWire (Relation Generation)'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Fine-Grained Gating\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value.?benchmark orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': \"The best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric is Prior+Duel noop\"}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Prior noop\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL04\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Science'}], [{'generated_text': 'The best performing model on the WMT2016 English-Romanian dataset is \\n\\tthe Z Sense model with beam width 5 and attentionAtt for both the encoder and decoder layers.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Oxford-IIIT Pets\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\nORDER BY DESC(?value) LIMIT 1'}], [{'generated_text': 'Provide a list of research paper titles and IDs that have benchmarked models on the Reuters En-De dataset?\\n\\nList the models that have been benchmarked on the NYT dataset?\\n\\nWhat are the models that have been benchmarked on the NYT dataset?\\n\\nGive me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters-21578 dataset?'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ANLI test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n  SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ANLI test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n  SELECT?metric?metric_lbl WHERE {?dataset a'}], [{'generated_text': 'List the metrics that are used to evaluate models on the WebNLG benchmark dataset?\\n\\nNLG Performance Metrics\\n\\nTransformer NLG\\n\\nMean Length of Transcript (MLT)\\n\\nPearson correlation coefficient (PCC)\\n\\nLog Loss\\n\\nTime-Weighted Error Rate (TWEER)\\n\\nRoot Mean Squared Error (RMSE)\\n\\nRoot Mean Squared Log Error (RMSLED)\\n\\nZero-Shot Recognition on WebNLG\\n\\nBilingual Unified Model (BUM)\\n\\nOpenSubJECT (Microsoft)\\n\\nPNLI\\n\\nWexford Path Minimization for NLG\\n\\nOpenSubJECT (Microsoft)\\n\\nRankSVM\\n\\nK-Balls\\n\\nLayers Performance Metric\\n\\nImageNet Classification accuracy\\n\\nPerplexity\\n\\nCoverage\\n\\nAccuracy on validation set\\n\\nAccuracy on test set\\n\\nAverage Throughput\\n\\nMicro-averaged per-layer cost'}], [{'generated_text': 'The highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value, is 38.23408441919484 with a F1 score of 0.9048987641771365'}], [{'generated_text': 'select distinct?code where {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"MPAD-path\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'TriviaQA,BiT-L (ResNet)'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC German-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The code that is being evaluated on the BBCSport dataset is the DDQN (tuned) hs model. The DDQN (tuned) hs model achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset.'}], [{'generated_text': 'output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MultiNLI\") {?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } } \\n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MultiRC benchmark dataset?\\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER'}], [{'generated_text': 'The best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric is the AlexNet model.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CommonsenseQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'No'}], [{'generated_text': 'The highest benchmark result achieved on the CoQA dataset, including the metric and its value is 3918.5.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet V2\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The highest score on the Reacher, easy (DMControl500k) benchmark dataset has been achieved by the Point-Gov nav system model (ID: P32). The model has achieved 90.76 points.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Quasart-T\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The models that have been evaluated on the Multimodal PISA dataset are the Perceiver model, https://github.com/eriklindernoren/keras-pets/blob/master/pets/perceiver/PERceiver.py,, https://github.com/lerain/WACV2017/blob/master/Pets/Perceiver.py, Text-GAN, https://github.com/alharbi/Text-GAN.py,  and Attention-based Models, https://github.com/xiaoxinjing/attention/blob/master/model.py, https://github.com/jfeng/AttentionModel.py.'}], [{'generated_text': 'metrics\\t\\trdfs:label\\nEntropy Difference\\t\\tEntropy Difference\\nSELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Barabasi-Albert\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value)'}], [{'generated_text': \"Score\\n\\n<|endoftext|>How to make a budget: smart money-management tips\\n\\nBudgets, in their essence, are meant to help you control your finances, and that is the main purpose for which most of us actually prepare them. However, we could still live quite well even with no budget at all. Here are some smart money-management tips that can help you do just that:\\n\\nBudget first\\n\\nThe first and most obvious point to remember when you are budgeting is that you should always start by making a list of all your expenses. How much do you spend on rent/utilities? What about food, clothes and other essentials? Once you have this information you can calculate your total monthly expenses. If you are in college or a person working on your own, you can first look at available financial aid and see if that helps you narrow down your options. In any case, it's always good to start off with realistic numbers and see if you can make ends meet on your own.\\n\\nCreate rules\\n\\nOnce you have a rough idea of your total expenses, you can try to create rules or benchmarks based on which you can judge how much money should be spent on each category. In terms of rent, for example, you can\"}], [{'generated_text': 'The best performing model is the Iterative Backtranslation (Supervision) model of SMT + Iterative Backtranslation (Unsupervised) benchmarking  with the accuracy metric.'}], [{'generated_text': 'The paper using the BiLSTM + char CNN + CRF model performed the best on the WMT2016 English-German benchmark dataset with a BLEU score of 33.'}], [{'generated_text': 'There are currently 3 datasets in the ACE 2005 benchmarking suite that have been evaluated using BERT based binary classification models:\\n- ACE2005\\n- Corpus\\n- MultiNLI\\nListed in order of their evaluated metric:\\n- ACE2005: 0.755 (Sentence Encoder)\\n- Corpus: 0.768\\n- MultiNLI: 0.758'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RotoWire (Content Ordering)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Papers utilizing TokenFuser model include:\\n\\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n* \\n*'}], [{'generated_text': 'The models being evaluated on the MedSTS benchmark dataset are:\\n- BioSentVec (PubMed + MIMIC-III)\\n- xGAP'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL++\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\\n\\nSELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL 2003 (English)\")?'}], [{'generated_text': 'CIFAR10 Benchmark Using Wide & Deep Learning, Ciaramella, et al. Proceedings of the 25th International Conference on Neural Information Processing Systems (NIPS 2018)\\n     paper = C13 | urn:nlp:csemr:synth-unfiltered:189537 | C13'}], [{'generated_text': 'Sparql results:\\n Resource \"orkgp\" does not exist.\\n Resource \"orkgp:HAS_BENCHMARK\" does not exist.\\n Resource \"orkgp:HAS_MODEL\" does not exist.\\n Resource \"orkgp:HAS_DATASET\" does not exist.\\n Resource \"orkgp:HAS_EVALUATION\" does not exist.\\n Resource \"orkgp:HAS_VALUE\" does not exist.\\n Resource \"orkgp:P31\" does not exist.\\n Resource \"orkgp:P32\" does not exist.\\n Resource \"orkgc\" does not exist.\\n Resource \"rdfs\" does not exist.\\n Resource \"rdfs:label\" does not exist.\\n Resource \"rdfs:label:SciCite\" does not exist.\\n Resource \"orkgp:HAS_DATASET\" does not exist.\\n Resource \"orkgp:P30\" does not exist.\\n Resource \"orkgp:P31\" does not exist.\\n Resource \"orkgp:P32\" does not exist.\\n Resource \"orkgp:HAS_MODEL\" does not exist.\\n Resource \"'}], [{'generated_text': 'link to code used in papers that benchmark the ImageNet + iNat on WS-DAN model: https://github.com/huggingface/pytorch-fairseq/blob/master/examples/icml2020_WN18.ipynb'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC Russian-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACL-ARC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The top score and metric is:\\nModelNet40: top-1 accuracy 77.45% \\nImageNet 64x64: top-5 accuracy 25.85% \\nfusion-nets-cifar10: top-5 accuracy 23.34%'}], [{'generated_text': 'The metrics used to evaluate the SPTree model on the Penn Treebank benchmark dataset include:\\n- Accuracy\\n- F-score\\n- MCC\\n- R2\\n- ROC Curve\\n- Root Mean Squared Error\\n- Spearman Rank Correlation\\n- STRC\\n- TPR (true positive rate)\\n- FPR (false positive rate)\\n- F-measure\\n- Harmony score\\n- Marginalized counterfactual accuracy\\n- MAP@t\\n- Minimum description length\\n- OcGA\\n- Posterior predictive likelihood\\n- Positive predictive value\\n- Negative predictive value\\n- ROC F1 score'}], [{'generated_text': '-- name = Sarsa-φ-EB\\n-- model = Sarsa-φ-EB\\n-- evaluation_metric = Accuracy\\n-- paper = Hopfield+Method for Stochastic Memorizing Ant System (2018)\\n-- paper_url = https://doi.org/10.1145/3228805.3228893\\n-- code_url = https://github.com/hopfieldinstitute/Hopfield+Method_for_Stochastic_Memorizing_Ant_System_Diederichs_Hopfield_and_Schmemann_Hopfield_2017\\n-- dataset = Reuters En-De\\n-- evaluation_metric = Accuracy\\n-- paper_title = Hopfield+Method for Stochastic Memorizing Ant System (2018)\\n-- paper_id = P32\\n-- code_title =\\n-- code_id = P32'}], [{'generated_text': 'The best performing model for the Atari 2600 Seaquest dataset is the One-Shot NAS (Neural Architecture Search) model. The model outperforms the current state-of-the-art by 15.65% in terms of score metric.'}], [{'generated_text': 'scores: \\n- 622.2789187836986 \\n- 598.3163365372214 \\n- 627.2747394379604 \\n- 619.6794391180556 \\n- 873.7770665106145 \\n- 567.6557809535338 \\n- 646.1365874919155 \\n- 732.9452445565307 \\n- 721.3675647632798 \\n- 722.3969142705677 \\n- 647.4120302875887 \\n- 674.3218602338704 \\n- 716.2974231632963 \\n- 717.3605477223369 \\n- 767.9738682380039 \\n- 702.4959659362887 \\n- 697.3705329996287 \\n- 769.9307903544796 \\n- 693.0855344035891 \\n- 693.1883035204562 \\n- 650.9275334194205 \\n- 702.350389437'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score)\\nWHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ObjectNet\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The following metrics were used in the evaluation of models on the Atari 2600 Asterix benchmark dataset:\\n\\n- Score\\n- Time\\n- Number of frames'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}]]\n","0 3\n","1 3\n","2 3\n","3 3\n","0 5\n","0 7\n","1 7\n","0 9\n","0 12\n","0 13\n","0 14\n","0 15\n","1 15\n","2 15\n","0 17\n","0 19\n","0 21\n","1 21\n","0 22\n","1 22\n","2 22\n","3 22\n","4 22\n","5 22\n","6 22\n","7 22\n","8 22\n","9 22\n","0 24\n","0 26\n","1 26\n","2 26\n","0 28\n","0 29\n","1 29\n","2 29\n","0 30\n","1 30\n","2 30\n","3 30\n","4 30\n","0 31\n","1 31\n","0 34\n","1 34\n","0 35\n","0 37\n","0 38\n","0 39\n","1 39\n","0 42\n","0 43\n","1 43\n","2 43\n","3 43\n","4 43\n","5 43\n","6 43\n","0 44\n","1 44\n","2 44\n","3 44\n","0 45\n","1 45\n","2 45\n","3 45\n","4 45\n","5 45\n","6 45\n","7 45\n","8 45\n","9 45\n","0 46\n","1 46\n","2 46\n","0 48\n","1 48\n","2 48\n","3 48\n","4 48\n","72.72727272727273%  [[{'generated_text': 'Achim Miskin, Georg Ostermann, Michele Soares, Ravi Kumar, Juan Carlos Nieblas, Francesco Pasqua, John Salway, João Morais, Kunal Talwar, Jakob Uszkoreit,  Anima Banerjee, Stefan Thimwe, Peter Pinkal, Michael Hickey, Daryl J. Farah. \"Benchmarking and Ensemble Performance on the Qbert Rainbow+SEER Model\" (2019)'}], [{'generated_text': 'The name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset is Prioritized DQN.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-Romanian\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-Romanian\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont'}], [{'generated_text': 'metrics used to evaluate models on MUTAG dataset include:\\n\\n - Accuracy\\n - F1 score\\n - Macro-average precision\\n - Macro-average recall\\n - F1 score within label\\n - Macro-average precision within label\\n - Macro-average recall within label'}], [{'generated_text': 'Some of the most common evaluation metrics used on the ESC-50 dataset include:\\n\\n  - Classification accuracy: The percent of correct predictions for each class.  Formula: 100 * (1 - ∑(false positive / total positive + false negative / total negative))\\n\\n  - F-measure: (2 * precision * recall) / (precision + recall)\\n\\n  - Mean Average Precision (MAP): Average of precision across each class (e.g., \"cars\") and then the average across all classes.  Formula: (1 / N) * sum (prec * recall)\\n\\n  - Recall: Number of correct predictions for each class (e.g., \"cars\") divided by the total number of predictions for all classes.  Formula: 100 * (1 - ∑(false positive / total positive + false negative / total negative))\\n\\n  - Root Mean Squared Error (RMSE): (measure - predicted value) / standard deviation\\n\\n  - Scores on the problems used to create the dataset.  A list of problems and their scores is available in the following URL.\\n\\n\\nThe following paper benchmarked architectures on the ESC-50 dataset:\\n\\n  - Awan, Irshad, and I'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD1.1 dev\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\n/* No models on SQuAD2.0 dev dataset */\\n/* Only the FusionNet (single model) model has been benchmarked on SQuAD2.0 dev */\\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD2.0 dev\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Temporal awareness\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TempEval-3\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'word BiLSTM + char CNN + CRF'}], [{'generated_text': 'The top benchmark score and its metric on the Atari 2600 Seaquest dataset are:\\n{ \"value\" : 987.8, \"score\" : 987.8 }'}], [{'generated_text': '+-------------------------------------------+--------------------+\\n| Model                                      | Paper               |\\n+-------------------------------------------+--------------------+\\n| GR-ConvNet                                 | [LAMP, Section 4.1] |\\n+-------------------------------------------+--------------------+\\n| DARTS                                      | [LAMP, Section 4.2] |\\n+-------------------------------------------+--------------------+\\n| DARTS-BYTE                                 | [LAMP, Section 4.2] |\\n+-------------------------------------------+--------------------+\\n| DARTS-SIMD                                 | [LAMP, Section 4.2] |\\n+-------------------------------------------+--------------------+\\n| DARTS-SIMD + Data Parallelism (SPMD)       | [LAMP, Section 4.2] |\\n+-------------------------------------------+--------------------+\\n| DARTS-BYTE + Data Parallelism (SPMD)       | [LAMP, Section 4.2] |\\n+-------------------------------------------+--------------------+\\n| AG-DARTS-BYTE                              | [LAMP, Section 4.3] |\\n+-------------------------------------------+--------------------+\\n| AG-DARTS-SIMD                              | [LAMP, Section 4.3] |\\n+-------------------------------------------+--------------------+\\n| AG-DART'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp-2\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n  SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp Binary classification\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \\n  SELECT DISTINCT?metric?metric_l'}], [{'generated_text': 'The model that performed best in terms of Accuracy metric on the Amazon benchmark dataset was the multi-layer perceptron model.\\nThe model that performed best in terms of the metrics evaluated on the Amazon benchmark dataset was the three-layer Adaptive Weight Deep Neural Network.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Bank Heist\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'The top performing model in terms of Accuracy score when benchmarked on the ARC (Easy) dataset is the A2C+CoEX model from the code link in the paper \"A2C+CoEX: Autoregressive DNN model for sequence-level ranking\", also known as P31 in the paper\\'s bibliographic record.'}], [{'generated_text': '* QA-GNN\\n* DARTNet\\n* Miracl'}], [{'generated_text': 'SELECT DISTINCT?dataset?dataset_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Asterix\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MFEC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n input (English text): Please provide the benchmark results for the MFEC model on the MFEco dataset.\\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL04\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Quasart-T\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Montezuma\\'s Revenge\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet ReaL\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value.?cont orkgp:HAS_BENCHMARK?benchmark.?model orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkg'}], [{'generated_text': 'The highest benchmark result for the Pubmed dataset is 83.29 (0.69 accuracy, run on 2019-10-11)'}], [{'generated_text': 'select distinct?paper?paper_lbl \\nwhere \\n{?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Walker, walk (DMControl100k)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\nORDER BY DESC(?value) LIMIT 1\\n\\ngroup by?metric?metric_lbl (max(?value) as?score)\\n\\nA Sparql query that returns a list of research paper titles and IDs that have benchmarked models on the Walker, walk (DMControl500k) dataset?\\n\\nselect distinct?paper?paper_lbl \\nwhere \\n{?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FIL'}], [{'generated_text': 'The highest benchmark result achieved on the REDDIT-B dataset, including the metric and its value is \"Accuracy\" with value 0.872058'}], [{'generated_text': 'Ours: cross-sentence ALB'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SST-2 Binary classification\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The Hendrycks Test dataset, which is a benchmark for the Leaderboard extraction problem, achieved the highest accuracy score of 99.30% when evaluated on the DDQN (tuned) hs model. The other models that were benchmarked against the Hendrycks Test dataset included the BERT model, the ELMO model, and the MARGOT model. The Hendrycks Test dataset is available at:\\n<https://archive.ics.uci.edu/ml/datasets/Hendrycks+Test>.'}], [{'generated_text': 'select distinct?model?model_lbl where {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"PARAMS\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"KD-LSTMreg\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NLP-TDMS\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. } \\n\\nThe best performing model benchmarking the NLP-TDMS (Exp, arXiv only) dataset in terms of Micro F1 metric is DocTAET-TDM.'}], [{'generated_text': 'Provide a list of papers that have utilized the Ours: cross-sentence ALB model and include the links to their code?'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DocRED (Human-annotated)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \\n\\nSELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Error\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DBpedia\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:'}], [{'generated_text': 'The papers that have utilized the CvT-13-NAS model are:\\n\\nWeng et al., 2015. \\t\"Evaluation of Techniques for Fine-Grained Part-of-Speech Tagging\"\\nLu et al., 2013.\\t\"Modeling Affective Word in Text With Linguistically-Attentive Semantic Models\"\\nChristopoulos et al., 2013.\\t\"Contextualizing Large Language Model Perplexity with Term-wise Silhouette\"'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"FLOPS\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FG-FG\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nSELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"FLOPS\") { SELECT?model?'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT29\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_BENCHMARK?benchmark.?cont; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\n<paper xmlns=\"http://www.crossref.org/crossref.xml\" id=\"p31\">\\n\\n  <title>High Performance Vision Localization using a Reduced Model with a Global View</title>\\n\\n  <journal>BMVC</journal>\\n\\n  <year>2016</year>\\n\\n  <volume>23</volume>\\n\\n  <pages>hvc-ny29-F1-model-reduce-globalview-p31.xml</pages>\\n\\n  <abstract><p>We present a method to leverage prior information when performing localization in a reduced vision system. The approach first trains a'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"QNLI\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Entity F1\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'Most commonly used evaluation metrics for the SST-5 Fine-grained classification dataset are:\\n- Average classification error (Cosine Similarity)\\n- Cross-entropy loss\\n- Logarithmic loss\\n- Mean Squared Error\\n- Spearman rank correlation coefficient'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Natural Questions (long)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Czech-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'AREAS           AREA\\t     AREAS_LABELS\\nSAJORTECH      7.72%\\t     ELECTRICAL AND MECHANICAL ENGINEERING\\nFINANCE       4.61%\\t     FINANCE AND INDUSTRY\\nENVIRONMENT   4.55%\\t     ENVIRONMENTAL POLICY\\nFINANCE       4.54%\\t     ENERGY MANAGEMENT\\nFINANCE       4.42%\\t     ENERGY INDUSTRY\\nFINANCE       4.42%\\t     ENERGY POLICY\\nENVIRONMENT   4.36%\\t     ENVIRONMENTAL POLICY\\nFINANCE       4.31%\\t     HEALTH, MEDICINE AND SPORTS\\nENVIRONMENT   4.25%\\t     ENVIRONMENTAL POLICY\\nFINANCE       4.24%\\t     ENERGY POLICY\\nFINANCE       4.14%\\t     MANAGEMENT\\nENVIRONMENT   4.14%\\t     ENVIRONMENTAL POLICY\\nFINANCE       4.07%\\t     RESEARCH\\nFINANCE       3.97%\\t     ENER'}], [{'generated_text': 'The model used in this research paper is called AVID+CMA (Modified R2+1D-18 on Kinetics) and it achieves Top-1 Accuracy of 92.2%.'}], [{'generated_text': 'Title                                                                                                                                                    Paper_lbl\\n  ---------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------------------------------------\\n  Neural Content Planning + conditional copy                                                                                                              Neural Content Planning + conditional copy\\n  Neural Planning and Scheduling for Multi-Class Content-Based Generation                                                                                Neural Planning and Scheduling for Multi-Class Content-Based Generation\\n  Neural Machine Translation with Explicit Metric Forcing                                                                                                 Neural Machine Translation with Explicit Metric Forcing\\n  Modeling Document Attributes with Neural Machine Translation                                                                                            Modeling Document Attributes with Neural Machine Translation\\n  Benchmarking Neural Machine Translation Models Using the RQ-arkency-2015 Dataset                                                                      Benchmarking Neural Machine Translation Models Using the RQ-arkency-2015 Dataset\\n  Neural Machine Translation with Explicit Metric Forcing for Information Retrieval                                                                      Neural Machine Translation with Explicit Metric Forcing for Information Retrieval\\n  Neural Machine Translation: New Architectures, Datasets, and Technical Details                                                                         Neural Machine Translation: New Architectures, Datasets, and Technical Details\\n  Neural Machine Translation with Explicit Met'}], [{'generated_text': 'precision   .947\\nrecall      .941\\nf1-score    .943'}], [{'generated_text': 'In this example query, we select the highest result for each metric of the metric query, grouped by metric_lbl. We provide both the metric and metric_lbl fields because we may be interested in the metric of a benchmark, but also to show the corresponding benchmark metric_lbl value if it is available.\\n\\nThe following are the results of the query:\\n\\nPlease note that all of the results contain two or more benchmark metric values, since metrics are benchmarked multiple times in different papers.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'The best performing model is the CAIT-XXS-36 model, which achieved an overall score of 0.782 on the Cartpole, swingup (DMControl100k) dataset, scoring 62.0 for the Score metric.'}], [{'generated_text': 'The most commonly used benchmark datasets for the Medical Named Entity Recognition research field are the ShARe/CLEF eHealth corpus dataset, the Natural Language Inference Benchmark (NLB) dataset and the Medical Information Real-World (MIREX) corpus. The Top-1-per-gpu-thread-per-evalfile on the ShARe/CLEF eHealth corpus is 82.11 with a F1 score of 0.931, on the NLB dataset is 81.83 with a F1 score of 0.923 and on the MIREX corpus is 80.17 with a F1 score of 0.905. The largest improvement in terms of F1 score is achieved by @DBLP:conf/semweb/ChenL17 who use a siamese network.'}], [{'generated_text': 'Yes, please find the following links:\\n\\n* Bai, X., Ratwani, S., Wang, J., Conrad, B. (2018). Deep dilated gated LSTM architectures for open language understanding. arXiv preprint arXiv:1801.02776.\\n* Bai, X., Zhang, X., Song, H., Wang, J., Zhang, L., Conrad, B. (2019). Dilated gated long short-term memory for open language understanding. arXiv preprint arXiv:1902.08909.\\n* Li, Z., Huang, S., Zhao, Q., Liang, B. (2020). A Better Recurrent Neural Network: Replay-Attend-Combine with Robustness to Data Augmentation. arXiv preprint arXiv:2005.06423.\\n* Li, Z., Zhao, Q., Huang, S., Liang, B. (2020). Synchronous Gated Recurrent Unit: A Novel Recurrent Neural Network with Synchronized Gate for Better Language Understanding. arXiv preprint arXiv:2007.00131.\\n* Wang, J., Conrad, B., Bai, X. (2020). : Deep Reinforcement Learning with LSTM for Open-'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Oxford-IIIT Pets\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}]]\n","0 0\n","0 1\n","0 3\n","0 4\n","1 4\n","2 4\n","3 4\n","4 4\n","5 4\n","0 7\n","0 8\n","0 9\n","0 11\n","1 11\n","2 11\n","3 11\n","4 11\n","5 11\n","6 11\n","7 11\n","8 11\n","9 11\n","0 13\n","1 13\n","2 13\n","3 13\n","4 13\n","5 13\n","6 13\n","7 13\n","8 13\n","9 13\n","0 14\n","0 22\n","0 23\n","0 24\n","1 24\n","2 24\n","0 25\n","1 25\n","2 25\n","3 25\n","4 25\n","5 25\n","6 25\n","7 25\n","8 25\n","9 25\n","0 27\n","0 30\n","0 32\n","0 37\n","1 37\n","2 37\n","3 37\n","4 37\n","5 37\n","6 37\n","7 37\n","8 37\n","0 40\n","1 40\n","2 40\n","3 40\n","4 40\n","5 40\n","6 40\n","0 41\n","0 42\n","0 43\n","1 43\n","0 44\n","0 46\n","1 46\n","2 46\n","3 46\n","4 46\n","0 47\n","1 47\n","0 48\n","81.81818181818183%  [[{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"XLNet\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet V2\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': 'The best performing model benchmarking the WOS-46985 dataset in terms of Accuracy metric is the Wide & Deep model with Adam.'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Supervised:\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'Indicate the model that performed best in terms of A1 metric on the ANLI test benchmark dataset?'}], [{'generated_text': 'The top performing model is called DS2 with a Top-1 Accuracy score of 0.962 when benchmarked on the iNaturalist 2019 dataset.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Space Invaders\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\nSELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Best Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Space Invaders\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'In order of most to least best score:\\n\\nAutoencoders for Replicated Stan Filters (DMControl100k)\\nBenchmarking Autoencoder Architectures for Replicated Stan Filters (DMControl100k)\\nNAS: Benchmarking Autoencoder Architectures for Neural Architecture Search (T-distributed Stochastic Neighborhood Networks)\\nNeural Architecture Search (T-distributed Stochastic Neighborhood Networks)\\nSelf-Supervised Learning of Object Categories (Diabetic Retinopathy)\\nNeural Architecture Search (Quartic Root Solver)\\nNeural Architecture Search (Quartic Root Solver)'}], [{'generated_text': 'A3C FF hs'}], [{'generated_text': 'Provide a list of research paper titles and IDs that have benchmarked models on the Atari-57 dataset?\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari-57\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\\n\\nCan you provide a list of models that have been tested on the DocRED (Human-annotated) benchmark dataset?\\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DocRED (Human-annotated)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTION'}], [{'generated_text': 'The metrics used to evaluate models on the NCBI Disease benchmark dataset are:\\n1.  Hit-to-ingredient (HtoI): the proportion of correctly identified food and drug IDs out of all IDs in a dataset.\\n2.  Mean average precision (mAP): the mean of the average precision (AP) of each entity for each annotated record.\\n3.  Confusion table (CT): confusion matrix with true and predicted classes.\\n4.  F1-score: F1 score computed from the formula (2 * TP / (TP + FN)) where TP = true positive and FN = false negative.'}], [{'generated_text': 'From the provided list of papers, the following papers have code references that use the 6-layer QRNN model: \\n\\n<a href=\"https://www.cs.toronto.edu/~kalantari/papers/knw17.pdf\" rel=\"nofollow\">Kalantari et al.</a>, \"A Performance Comparison of Transduction Variational Autoencoders\", Proc. 27th ACL, 2017\\n\\n<a href=\"https://arxiv.org/abs/1610.08039\" rel=\"nofollow\">Havaei et al.</a>, \"Neural Machine Translation with Experienced Sentence Redaction\", Proc. ACL, 2016\\n\\n<a href=\"http://www.cstr.edu/~xiaoqing/resnet/shallow-supervision.html\" rel=\"nofollow\">Xiaoqing et al.</a>, \"A Shallow Supervision Method for Improving Mobilenetv2\", ICCV, 2017\\n\\n<a href=\"https://dl.acm.org/doi/10.1145/3130150.3130164\" rel=\"nofollow\">Lu et al.</a>, \"Benchmarking Convolutional Neural'}], [{'generated_text': 'The WOS-5736 dataset is used to benchmark the WDEC model. The best performing models on the WOS-11967 dataset have been evaluated against on WDEC model.'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT29\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nThe returned list should contain the following 2 samples:\\n\\n- \"New York Times Dataset: 29\"\\n- \"Language Models Benchmarked on the New York Times Dataset: 29\"'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Score\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Robotank\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"Accuracy\") { SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IMDb-M\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nList the metrics that are used to evaluate models on the IMDb benchmark dataset?\\n\\nSELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER ('}], [{'generated_text': 'A list of research paper titles and IDs that have benchmarked models on the Flowers-102 dataset:'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AudioSet\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cart Pole (OpenAI Gym)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl \\nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reacher, easy (DMControl100k)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\\n\\nThe benchmark dataset used in the evaluation is Reacher, easy (DMControl100k) \\nThe evaluation metrics are commonly used when benchmarking models on the Reacher, easy (DMControl100k) dataset are:\\nMetric    Metric Label\\nGPUTime   Computational Time\\nValidation lag time\\nValidation final lag time\\nFinal lag time\\nValidation overprediction\\nValidation overprediction\\nValidation double prediction\\nValidation double prediction\\nValidation classification error\\nValidation classification error\\nValidation out-of-distribution classification error\\nValidation out-of-distribution classification error\\nValidation out-of-distribution misclassification error\\nValidation out-of-distribution misclassification error'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACL-ARC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl (MAX(?value) AS?score) WHERE { { SELECT?model?model_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DBpedia\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Enduro\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Hutter Prize\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n\\n<!--- Optionnal variables --->\\n<var>\\n   ?dataset a orkgc:Dataset;\\n    rdfs:label?dataset_lbl.\\n</var>\\n\\n<!--- Dataset properties --->\\n<property>\\n   ?dataset a orkgc:Dataset;\\n    rdfs:label?dataset_lbl.\\n</property>\\n\\n<!--- Benchmark variables --->\\n<var>\\n   ?benchmark orkgp:HAS_DATASET?dataset;\\n    orkgp:HAS_EVALUATION?eval.\\n</var>\\n\\n<!---'}], [{'generated_text': 'The R2+1D-18 on Audioset model achieved the highest Score on the Atari 2600 Wizard of Wor dataset.\\nThe benchmark scores are commonly associated with the following metrics:\\navg./max../mean. over a fixed number of frames.\\nReliability (hit ratio)\\nAverage path length\\nNetwork efficiency'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Transformer-XL (12 layers)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }'}], [{'generated_text': 'The best performing model on the Atari 2600 Up and Down dataset is the Transformer-based One-Shot NAS (Neural Architecture Search) model.'}], [{'generated_text': 'doctaet-tdm'}], [{'generated_text': 'None'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl (MAX(?value) AS?score) WHERE { { SELECT?metric?metric_lbl?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ObjectNet\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }?cont orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?cont orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } ORDER BY DESC(?value) } } GROUP BY?metric?metric_lbl'}], [{'generated_text': 'The most recent benchmark performed on the enwik8 dataset, for models optimized for document classification, is the following:\\n\\nBenchmark on the enwik8 dataset for different ML techniques (Methodology Github Repository)'}], [{'generated_text': 'OverFeat - 7 accurate models'}], [{'generated_text': 'The metrics that were used to benchmark the models on the Atari 2600 Battle Zone dataset are:\\n- Score\\n- Timer\\n- Number of Shots Per Frame'}], [{'generated_text': 'select distinct?paper?paper_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Road Runner\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \\n    ORDER BY DESC(?value)'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Boxing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \\n\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Boxing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?cont orkgp:HAS_MODEL?model; orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P32?problem. }'}], [{'generated_text': 'SELECT DISTINCT?metric?metric_lbl WHERE {?metric a orkgc:Metric; rdfs:label?metric_lbl. FILTER (str(?metric_lbl) = \"BLEU\") { SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 French-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?metric_lbl. } ORDER BY DESC(?value) LIMIT 1 } } \\n\\nThe metric \"BLEU\" is commonly used when benchmarking models on the WMT2014 French-English dataset.\\n\\nThe highest benchmark result achieved on the WMT2014 English-French dataset, including the metric and its value, is 33.'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC French-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \\n\\nA model is evaluated on a dataset if it has been used in the benchmark paper. To find out models that have been evaluated on a dataset, use the following Sparql query:\\n\\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC French-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_'}], [{'generated_text': 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BiDAF + Self Attention + ELMo (single model)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_VALUE?value; orkgp:HAS_METRIC?metric.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }'}], [{'generated_text': 'The models being evaluated on the ACL-ARC dataset are the:\\n  * A2C+CoEX: https://arxiv.org/abs/1810.10375\\n  * BERT: https://arxiv.org/abs/1810.10375\\n  * SQUELC: https://arxiv.org/abs/1810.10375'}], [{'generated_text': 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'}], [{'generated_text': 'SELECT DISTINCT?code WHERE {?model a orkgc:Model; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"PAR Transformer Large\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?cont orkgp:HAS_MODEL?model; orkgp:HAS_SOURCE_CODE?code. }'}], [{'generated_text': 'The BERTwwm + SQuAD 2 model is one of the best performing BERT + SQuAD model.\\nThe BERT+SQuAD model is benchmarked on the SQuAD1.1 dev and SQuAD2.0 dev datasets.\\nThe BERT model is benchmarked on the SQuAD1.1 and SQuAD2.0 datasets.\\nThe metrics used to evaluate the models include the increase in Micro/Macro F1 score, EM score, log-likelihood.\\nThe highest EM score achieved by a model is 44.34. The BERT model achieves an EM score of 41.21 on the SQuAD1.1 dev dataset.\\nThe BERT model achieves an EM score of 48.05 on the SQuAD2.0 dev dataset.'}], [{'generated_text': 'input (English text): Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?\\noutput (Sparql query): SELECT DISTINCT?paper WHERE {?paper a orkgc:Paper; orkgp:P30 [ orkgp:P2005 [ rdfs:label \"CL-Titles-Parser\"^^xsd:string ] ]; rdfs:label?paper. } \\n input (English text): List the datasets that are related to the Constituency Parsing research area?\\n output (Sparql query): SELECT DISTINCT?dataset WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Constituency Parsing\")?benchmark orkgp:HAS_DATASET?dataset. } \\n input (English text): Which papers use the dataset DBLP?\\n output (Sparql query): SELECT DISTINCT?title WHERE {?paper a orkgc:Paper; orkgp:P30 [ orkgp:P2005 [ rdfs:label \"DBLP\"^^xsd'}], [{'generated_text': 'The top benchmark result (metric and value) over the dataset Habitat 2020 Object Nav test-std is DISTANCE_TO_GOAL with score: 18.9 mm.\\nThe model that has achieved the highest DISTANCE_TO_GOAL score on the Habitat 2020 Object Nav test-std benchmark dataset is AttentionOCR_Inception-resnet-v2_Location with model ID: P31.'}], [{'generated_text': 'The top performing model in terms of Score is: Transformer-based One-Shot NAS (Neural Architecture Search) model'}], [{'generated_text': 'The highest score achieved on the Atari 2600 Amidar benchmark dataset is 291749.9 (on the Neural cache model (size = 2,000) model).'}]]\n","0 2\n","0 4\n","1 4\n","2 4\n"]}],"source":["import json\n","import torch\n","from sentence_transformers import SentenceTransformer\n","from sentence_transformers.util import cos_sim\n","from datasets import load_dataset\n","from transformers import pipeline, AutoTokenizer\n","\n","threshold = 0.25\n","\n","model = SentenceTransformer('all-mpnet-base-v2', device='cuda' if torch.cuda.is_available() else \"cpu\")\n","# model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda' if torch.cuda.is_available() else \"cpu\")\n","raw_datasets = load_dataset(\"orkg/SciQA\")\n","print(raw_datasets)\n","embed_data = torch.load('train_embeddings.pt')\n","dolly = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n","tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side=\"left\")\n","\n","\n","def divide_chunks(l_, n_):\n","    for i_ in range(0, len(l_), n_):\n","        yield l_[i_:i_ + n_]\n","\n","\n","def save_json(filename, data):\n","    with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n","        print(json.dumps(data), file=json_file)\n","\n","\n","def get_similar(element, items=None, embeddings=None, num=None, reversed=False):\n","    emb_items = None\n","\n","    if items is None and embeddings is not None:\n","        emb_items = embeddings[\"emb_questions\"]\n","        items = embeddings[\"keys\"]\n","    elif items is not None:\n","        emb_items = model.encode(items)\n","\n","    if len(element) == 0 or emb_items is None:\n","        return []\n","\n","    emb_element = model.encode(element)\n","\n","    result = []\n","    scores = cos_sim(emb_element, emb_items)\n","\n","    if num is None or num < 2:\n","        maximus = torch.max(scores, 1)\n","        m = float(maximus.values[0])\n","        i = int(maximus.indices[0])\n","        if m > threshold:\n","            result = [[round(m, 4), items[i], embeddings[\"questions\"][i], embeddings[\"queries\"][i]]]\n","        return result\n","    else:\n","        scored_texts = []\n","        for i, score in enumerate(scores[0]):\n","            scored_texts.append(\n","                [round(score.item(), 4), items[i], embeddings[\"questions\"][i], embeddings[\"queries\"][i]])\n","        sorted_scored_texts = sorted(scored_texts, key=lambda x: x[0], reverse=True)\n","\n","        keys = []\n","        samples = []\n","        for sample in sorted_scored_texts:\n","            if sample[1] not in keys:\n","                samples.append(sample)\n","                keys.append(sample[1])\n","\n","        samples = samples[:num]\n","        if reversed:\n","            samples.reversed()\n","        return samples\n","\n","\n","def clean(st):\n","    st = st.replace(\"\\n\", \" \")\n","    st = st.replace(\"?\", \" ?\")\n","    st = st.replace(\"{\", \" { \")\n","    st = st.replace(\"}\", \" } \")\n","    st = st.replace(\"\\\\'\", \"'\")\n","\n","    while \"  \" in st:\n","        st = st.replace(\"  \", \" \")\n","    return st\n","\n","\n","def get_key(q):\n","    t0 = q.get('template_id')\n","    if t0 is None:\n","        t0 = \"None\"\n","    t = str(q.get(\"number_of_patterns\")) + \"-\" + t0\n","    return t\n","\n","\n","def save_embedding():\n","    train = raw_datasets.get(\"train\")\n","    questions = [q[\"question\"][\"string\"] for q in train]\n","    queries = [clean(q[\"query\"][\"sparql\"]) for q in train]\n","    keys = [get_key(q) for q in train]\n","    embeddings = {}\n","    emb_questions = model.encode(questions)\n","    embeddings[\"questions\"] = questions\n","    embeddings[\"emb_questions\"] = emb_questions\n","    embeddings[\"queries\"] = queries\n","    embeddings[\"keys\"] = keys\n","    torch.save(embeddings, 'train_embeddings.pt')\n","    return embeddings\n","\n","\n","def prepare_queries(n_, reversed=False):\n","    data = raw_datasets.get(\"test\")\n","    queries = []\n","    suggestions = []\n","    for q in data:\n","        t = get_key(q)\n","        question = q[\"question\"][\"string\"]\n","        suggestion = get_similar(question, embeddings=embed_data, num=n_, reversed=reversed)\n","        suggestions.append([[[x[0], x[1]] for x in suggestion], t])\n","\n","        if suggestion is None or len(suggestion) == 0:\n","            print(\"Error with key\", t)\n","            queries.append(\"translate the following English text '\" + question + \"' to a sparql query\")\n","        else:\n","            final_q = \"\"\n","            for i_, k in enumerate(suggestion):\n","                final_q += \"\\n input (English text): \" + k[2]\n","                final_q += \"\\n output (Sparql query): \" + k[3]\n","\n","            # works better with gpt\n","            # final_q += \"\\n with this example what is the sparql query for:  \" + question\n","\n","            # works better with dolly\n","            final_q += \"\\n input (English text): \" + question\n","            final_q += \"\\n output (Sparql query): \"\n","            queries.append(final_q)\n","    return queries, suggestions\n","\n","\n","def main(shots=7, attempts=10, batch=50, reversed=False):\n","    query_list, suggestions = prepare_queries(shots, reversed)\n","    print(len(query_list))\n","\n","    n = batch\n","    q_list = list(divide_chunks(query_list, n))\n","    sparql = [clean(x[\"query\"][\"sparql\"]) for x in raw_datasets.get(\"test\")]\n","\n","    gs = []\n","    lens = []\n","    i = 0\n","\n","    for group in q_list:\n","        print(str(i) + \"%\", end=\"  \")\n","        i += 1 / len(q_list) * 100\n","\n","        res_ = [tokenizer.encode(question) for question in group]\n","        len_ = [len(x) for x in res_]\n","        warning = [x for x in len_ if x > 2048]\n","        if len(warning) > 0:\n","            print(warning)\n","            quit()\n","        lens += len_\n","\n","        res = dolly(group)\n","        print(res)\n","        gst = [x[0][\"generated_text\"] for x in res]\n","\n","        for ii, l in enumerate(gst):\n","            for iii in range(attempts):\n","                if \"SELECT\" not in l:\n","                    print(iii, ii)\n","                    res = dolly(group[ii])\n","                    gst[ii] = res[0][\"generated_text\"]\n","                    l = gst[ii]\n","                else:\n","                    break\n","        gs += gst\n","\n","        result = {\"questions\": query_list, \"sparql\": sparql, \"generated_sparql\": gs, \"prompt_len\": lens,\n","                  \"suggestions\": suggestions}\n","        save_json(\"test_A_nlp_dolly_\" + str(shots) + \"_shot_results_tok.json\", result)\n","\n","main()"]}]}